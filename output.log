Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
upstairs
daghar
return single class data and labels, class is upstairs
data shape is (12639, 3, 1, 30)
label shape is (12639,)
790

lets see

saw?
Epochs between ckechpoint: 2



Saving checkpoint 1 in logs/test/upstairs_5000_D_30_2024_10_17_19_36_31/Model



[Epoch 0/7] [Batch 0/790] [D loss: 1.404360] [G loss: 0.880141] [ema: 0.000000] 
[Epoch 0/7] [Batch 100/790] [D loss: 0.429760] [G loss: 0.192999] [ema: 0.933033] 
[Epoch 0/7] [Batch 200/790] [D loss: 0.385523] [G loss: 0.191838] [ema: 0.965936] 
[Epoch 0/7] [Batch 300/790] [D loss: 0.253311] [G loss: 0.280742] [ema: 0.977160] 
[Epoch 0/7] [Batch 400/790] [D loss: 0.287309] [G loss: 0.197574] [ema: 0.982821] 
[Epoch 0/7] [Batch 500/790] [D loss: 0.339648] [G loss: 0.176338] [ema: 0.986233] 
[Epoch 0/7] [Batch 600/790] [D loss: 0.393667] [G loss: 0.188111] [ema: 0.988514] 
[Epoch 0/7] [Batch 700/790] [D loss: 0.403283] [G loss: 0.201703] [ema: 0.990147] 
[Epoch 1/7] [Batch 0/790] [D loss: 0.393225] [G loss: 0.224660] [ema: 0.991264] 
[Epoch 1/7] [Batch 100/790] [D loss: 0.293984] [G loss: 0.194483] [ema: 0.992242] 
[Epoch 1/7] [Batch 200/790] [D loss: 0.315461] [G loss: 0.204270] [ema: 0.993023] 
[Epoch 1/7] [Batch 300/790] [D loss: 0.334558] [G loss: 0.169169] [ema: 0.993661] 
[Epoch 1/7] [Batch 400/790] [D loss: 0.313933] [G loss: 0.200667] [ema: 0.994192] 
[Epoch 1/7] [Batch 500/790] [D loss: 0.342908] [G loss: 0.209552] [ema: 0.994641] 
[Epoch 1/7] [Batch 600/790] [D loss: 0.369348] [G loss: 0.205260] [ema: 0.995026] 
[Epoch 1/7] [Batch 700/790] [D loss: 0.443110] [G loss: 0.213462] [ema: 0.995359] 



Saving checkpoint 2 in logs/test/upstairs_5000_D_30_2024_10_17_19_36_31/Model



[Epoch 2/7] [Batch 0/790] [D loss: 0.457237] [G loss: 0.164433] [ema: 0.995623] 
[Epoch 2/7] [Batch 100/790] [D loss: 0.425746] [G loss: 0.170392] [ema: 0.995883] 
[Epoch 2/7] [Batch 200/790] [D loss: 0.398575] [G loss: 0.185527] [ema: 0.996113] 
[Epoch 2/7] [Batch 300/790] [D loss: 0.408997] [G loss: 0.207026] [ema: 0.996320] 
[Epoch 2/7] [Batch 400/790] [D loss: 0.432371] [G loss: 0.204711] [ema: 0.996505] 
[Epoch 2/7] [Batch 500/790] [D loss: 0.379223] [G loss: 0.216544] [ema: 0.996673] 
[Epoch 2/7] [Batch 600/790] [D loss: 0.393551] [G loss: 0.191672] [ema: 0.996825] 
[Epoch 2/7] [Batch 700/790] [D loss: 0.454269] [G loss: 0.170579] [ema: 0.996964] 
[Epoch 3/7] [Batch 0/790] [D loss: 0.475469] [G loss: 0.139802] [ema: 0.997080] 
[Epoch 3/7] [Batch 100/790] [D loss: 0.511867] [G loss: 0.169761] [ema: 0.997198] 
[Epoch 3/7] [Batch 200/790] [D loss: 0.465778] [G loss: 0.118351] [ema: 0.997307] 
[Epoch 3/7] [Batch 300/790] [D loss: 0.450700] [G loss: 0.160909] [ema: 0.997407] 
[Epoch 3/7] [Batch 400/790] [D loss: 0.393126] [G loss: 0.133523] [ema: 0.997501] 
[Epoch 3/7] [Batch 500/790] [D loss: 0.422390] [G loss: 0.163269] [ema: 0.997588] 
[Epoch 3/7] [Batch 600/790] [D loss: 0.475166] [G loss: 0.181398] [ema: 0.997669] 
[Epoch 3/7] [Batch 700/790] [D loss: 0.568490] [G loss: 0.168817] [ema: 0.997745] 



Saving checkpoint 3 in logs/test/upstairs_5000_D_30_2024_10_17_19_36_31/Model



[Epoch 4/7] [Batch 0/790] [D loss: 0.465715] [G loss: 0.146857] [ema: 0.997809] 
[Epoch 4/7] [Batch 100/790] [D loss: 0.440915] [G loss: 0.186202] [ema: 0.997876] 
[Epoch 4/7] [Batch 200/790] [D loss: 0.425224] [G loss: 0.150696] [ema: 0.997939] 
[Epoch 4/7] [Batch 300/790] [D loss: 0.391508] [G loss: 0.168218] [ema: 0.997999] 
[Epoch 4/7] [Batch 400/790] [D loss: 0.453644] [G loss: 0.161543] [ema: 0.998055] 
[Epoch 4/7] [Batch 500/790] [D loss: 0.449959] [G loss: 0.154586] [ema: 0.998108] 
[Epoch 4/7] [Batch 600/790] [D loss: 0.399973] [G loss: 0.161483] [ema: 0.998158] 
[Epoch 4/7] [Batch 700/790] [D loss: 0.440183] [G loss: 0.163465] [ema: 0.998206] 
[Epoch 5/7] [Batch 0/790] [D loss: 0.412590] [G loss: 0.176315] [ema: 0.998247] 
[Epoch 5/7] [Batch 100/790] [D loss: 0.408961] [G loss: 0.131719] [ema: 0.998290] 
[Epoch 5/7] [Batch 200/790] [D loss: 0.371930] [G loss: 0.177118] [ema: 0.998331] 
[Epoch 5/7] [Batch 300/790] [D loss: 0.404797] [G loss: 0.201955] [ema: 0.998370] 
[Epoch 5/7] [Batch 400/790] [D loss: 0.420604] [G loss: 0.169228] [ema: 0.998408] 
[Epoch 5/7] [Batch 500/790] [D loss: 0.371100] [G loss: 0.190143] [ema: 0.998444] 
[Epoch 5/7] [Batch 600/790] [D loss: 0.418149] [G loss: 0.198487] [ema: 0.998478] 
[Epoch 5/7] [Batch 700/790] [D loss: 0.443412] [G loss: 0.193297] [ema: 0.998510] 



Saving checkpoint 4 in logs/test/upstairs_5000_D_30_2024_10_17_19_36_31/Model



[Epoch 6/7] [Batch 0/790] [D loss: 0.359884] [G loss: 0.156467] [ema: 0.998539] 
[Epoch 6/7] [Batch 100/790] [D loss: 0.401898] [G loss: 0.175449] [ema: 0.998569] 
[Epoch 6/7] [Batch 200/790] [D loss: 0.389884] [G loss: 0.200477] [ema: 0.998598] 
[Epoch 6/7] [Batch 300/790] [D loss: 0.407841] [G loss: 0.191894] [ema: 0.998626] 
[Epoch 6/7] [Batch 400/790] [D loss: 0.479616] [G loss: 0.168561] [ema: 0.998652] 
[Epoch 6/7] [Batch 500/790] [D loss: 0.398581] [G loss: 0.158000] [ema: 0.998678] 
[Epoch 6/7] [Batch 600/790] [D loss: 0.329235] [G loss: 0.205297] [ema: 0.998703] 
[Epoch 6/7] [Batch 700/790] [D loss: 0.446574] [G loss: 0.164070] [ema: 0.998727] 
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
run
daghar
return single class data and labels, class is run
data shape is (15989, 3, 1, 30)
label shape is (15989,)
1000

lets see

saw?
Epochs between checkpoint: 2



Saving checkpoint 1 in logs/test/run_5000_D_30_2024_10_17_19_40_07/Model



[Epoch 0/5] [Batch 0/1000] [D loss: 1.149738] [G loss: 0.892987] [ema: 0.000000] 
[Epoch 0/5] [Batch 100/1000] [D loss: 0.441196] [G loss: 0.223204] [ema: 0.933033] 
[Epoch 0/5] [Batch 200/1000] [D loss: 0.327163] [G loss: 0.235886] [ema: 0.965936] 
[Epoch 0/5] [Batch 300/1000] [D loss: 0.261498] [G loss: 0.271330] [ema: 0.977160] 
[Epoch 0/5] [Batch 400/1000] [D loss: 0.263306] [G loss: 0.234186] [ema: 0.982821] 
[Epoch 0/5] [Batch 500/1000] [D loss: 0.352491] [G loss: 0.227959] [ema: 0.986233] 
[Epoch 0/5] [Batch 600/1000] [D loss: 0.330496] [G loss: 0.248136] [ema: 0.988514] 
[Epoch 0/5] [Batch 700/1000] [D loss: 0.413375] [G loss: 0.211258] [ema: 0.990147] 
[Epoch 0/5] [Batch 800/1000] [D loss: 0.390291] [G loss: 0.189832] [ema: 0.991373] 
[Epoch 0/5] [Batch 900/1000] [D loss: 0.413827] [G loss: 0.175051] [ema: 0.992328] 
[Epoch 1/5] [Batch 0/1000] [D loss: 0.471682] [G loss: 0.150136] [ema: 0.993092] 
[Epoch 1/5] [Batch 100/1000] [D loss: 0.376782] [G loss: 0.217537] [ema: 0.993718] 
[Epoch 1/5] [Batch 200/1000] [D loss: 0.339552] [G loss: 0.208208] [ema: 0.994240] 
[Epoch 1/5] [Batch 300/1000] [D loss: 0.320828] [G loss: 0.204237] [ema: 0.994682] 
[Epoch 1/5] [Batch 400/1000] [D loss: 0.309346] [G loss: 0.245387] [ema: 0.995061] 
[Epoch 1/5] [Batch 500/1000] [D loss: 0.255361] [G loss: 0.245587] [ema: 0.995390] 
[Epoch 1/5] [Batch 600/1000] [D loss: 0.297740] [G loss: 0.231993] [ema: 0.995677] 
[Epoch 1/5] [Batch 700/1000] [D loss: 0.275728] [G loss: 0.203301] [ema: 0.995931] 
[Epoch 1/5] [Batch 800/1000] [D loss: 0.312048] [G loss: 0.245592] [ema: 0.996157] 
[Epoch 1/5] [Batch 900/1000] [D loss: 0.340574] [G loss: 0.222421] [ema: 0.996359] 



Saving checkpoint 2 in logs/test/run_5000_D_30_2024_10_17_19_40_07/Model



[Epoch 2/5] [Batch 0/1000] [D loss: 0.343845] [G loss: 0.218661] [ema: 0.996540] 
[Epoch 2/5] [Batch 100/1000] [D loss: 0.418974] [G loss: 0.157173] [ema: 0.996705] 
[Epoch 2/5] [Batch 200/1000] [D loss: 0.371039] [G loss: 0.161937] [ema: 0.996854] 
[Epoch 2/5] [Batch 300/1000] [D loss: 0.429690] [G loss: 0.150924] [ema: 0.996991] 
[Epoch 2/5] [Batch 400/1000] [D loss: 0.467082] [G loss: 0.186220] [ema: 0.997116] 
[Epoch 2/5] [Batch 500/1000] [D loss: 0.496881] [G loss: 0.153345] [ema: 0.997231] 
[Epoch 2/5] [Batch 600/1000] [D loss: 0.456853] [G loss: 0.172267] [ema: 0.997338] 
[Epoch 2/5] [Batch 700/1000] [D loss: 0.440579] [G loss: 0.161728] [ema: 0.997436] 
[Epoch 2/5] [Batch 800/1000] [D loss: 0.384086] [G loss: 0.176643] [ema: 0.997528] 
[Epoch 2/5] [Batch 900/1000] [D loss: 0.396798] [G loss: 0.171387] [ema: 0.997613] 
[Epoch 3/5] [Batch 0/1000] [D loss: 0.399710] [G loss: 0.210498] [ema: 0.997692] 
[Epoch 3/5] [Batch 100/1000] [D loss: 0.346181] [G loss: 0.147188] [ema: 0.997767] 
[Epoch 3/5] [Batch 200/1000] [D loss: 0.350289] [G loss: 0.209721] [ema: 0.997836] 
[Epoch 3/5] [Batch 300/1000] [D loss: 0.348511] [G loss: 0.209307] [ema: 0.997902] 
[Epoch 3/5] [Batch 400/1000] [D loss: 0.431392] [G loss: 0.174128] [ema: 0.997963] 
[Epoch 3/5] [Batch 500/1000] [D loss: 0.441951] [G loss: 0.213334] [ema: 0.998022] 
[Epoch 3/5] [Batch 600/1000] [D loss: 0.451850] [G loss: 0.202430] [ema: 0.998076] 
[Epoch 3/5] [Batch 700/1000] [D loss: 0.409903] [G loss: 0.177268] [ema: 0.998128] 
[Epoch 3/5] [Batch 800/1000] [D loss: 0.398839] [G loss: 0.192768] [ema: 0.998178] 
[Epoch 3/5] [Batch 900/1000] [D loss: 0.397115] [G loss: 0.182046] [ema: 0.998224] 



Saving checkpoint 3 in logs/test/run_5000_D_30_2024_10_17_19_40_07/Model



[Epoch 4/5] [Batch 0/1000] [D loss: 0.379048] [G loss: 0.201615] [ema: 0.998269] 
[Epoch 4/5] [Batch 100/1000] [D loss: 0.365844] [G loss: 0.180549] [ema: 0.998311] 
[Epoch 4/5] [Batch 200/1000] [D loss: 0.352021] [G loss: 0.185548] [ema: 0.998351] 
[Epoch 4/5] [Batch 300/1000] [D loss: 0.437747] [G loss: 0.189116] [ema: 0.998389] 
[Epoch 4/5] [Batch 400/1000] [D loss: 0.451606] [G loss: 0.162684] [ema: 0.998426] 
[Epoch 4/5] [Batch 500/1000] [D loss: 0.491488] [G loss: 0.176462] [ema: 0.998461] 
[Epoch 4/5] [Batch 600/1000] [D loss: 0.456611] [G loss: 0.140760] [ema: 0.998494] 
[Epoch 4/5] [Batch 700/1000] [D loss: 0.412438] [G loss: 0.183743] [ema: 0.998526] 
[Epoch 4/5] [Batch 800/1000] [D loss: 0.438018] [G loss: 0.190022] [ema: 0.998557] 
[Epoch 4/5] [Batch 900/1000] [D loss: 0.316248] [G loss: 0.239713] [ema: 0.998586] 
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
walk
daghar
return single class data and labels, class is walk
data shape is (16940, 3, 1, 30)
label shape is (16940,)
1059

lets see

saw?
Epochs between checkpoint: 2



Saving checkpoint 1 in logs/test/walk_5000_D_30_2024_10_17_19_43_27/Model



[Epoch 0/5] [Batch 0/1059] [D loss: 1.163414] [G loss: 0.924447] [ema: 0.000000] 
[Epoch 0/5] [Batch 100/1059] [D loss: 0.406219] [G loss: 0.193059] [ema: 0.933033] 
[Epoch 0/5] [Batch 200/1059] [D loss: 0.361810] [G loss: 0.195976] [ema: 0.965936] 
[Epoch 0/5] [Batch 300/1059] [D loss: 0.313682] [G loss: 0.275845] [ema: 0.977160] 
[Epoch 0/5] [Batch 400/1059] [D loss: 0.300626] [G loss: 0.203168] [ema: 0.982821] 
[Epoch 0/5] [Batch 500/1059] [D loss: 0.323996] [G loss: 0.198172] [ema: 0.986233] 
[Epoch 0/5] [Batch 600/1059] [D loss: 0.375252] [G loss: 0.210085] [ema: 0.988514] 
[Epoch 0/5] [Batch 700/1059] [D loss: 0.403987] [G loss: 0.226621] [ema: 0.990147] 
[Epoch 0/5] [Batch 800/1059] [D loss: 0.410477] [G loss: 0.191821] [ema: 0.991373] 
[Epoch 0/5] [Batch 900/1059] [D loss: 0.308991] [G loss: 0.224334] [ema: 0.992328] 
[Epoch 0/5] [Batch 1000/1059] [D loss: 0.367914] [G loss: 0.196666] [ema: 0.993092] 
[Epoch 1/5] [Batch 0/1059] [D loss: 0.349272] [G loss: 0.185020] [ema: 0.993476] 
[Epoch 1/5] [Batch 100/1059] [D loss: 0.303006] [G loss: 0.215137] [ema: 0.994037] 
[Epoch 1/5] [Batch 200/1059] [D loss: 0.326795] [G loss: 0.199698] [ema: 0.994510] 
[Epoch 1/5] [Batch 300/1059] [D loss: 0.345252] [G loss: 0.202358] [ema: 0.994913] 
[Epoch 1/5] [Batch 400/1059] [D loss: 0.413067] [G loss: 0.219007] [ema: 0.995260] 
[Epoch 1/5] [Batch 500/1059] [D loss: 0.448059] [G loss: 0.184354] [ema: 0.995564] 
[Epoch 1/5] [Batch 600/1059] [D loss: 0.505683] [G loss: 0.132438] [ema: 0.995831] 
[Epoch 1/5] [Batch 700/1059] [D loss: 0.408532] [G loss: 0.163683] [ema: 0.996067] 
[Epoch 1/5] [Batch 800/1059] [D loss: 0.389270] [G loss: 0.204659] [ema: 0.996278] 
[Epoch 1/5] [Batch 900/1059] [D loss: 0.419405] [G loss: 0.196513] [ema: 0.996468] 
[Epoch 1/5] [Batch 1000/1059] [D loss: 0.372393] [G loss: 0.224658] [ema: 0.996639] 



Saving checkpoint 2 in logs/test/walk_5000_D_30_2024_10_17_19_43_27/Model



[Epoch 2/5] [Batch 0/1059] [D loss: 0.347843] [G loss: 0.257525] [ema: 0.996733] 
[Epoch 2/5] [Batch 100/1059] [D loss: 0.384568] [G loss: 0.198239] [ema: 0.996880] 
[Epoch 2/5] [Batch 200/1059] [D loss: 0.401679] [G loss: 0.208250] [ema: 0.997014] 
[Epoch 2/5] [Batch 300/1059] [D loss: 0.436991] [G loss: 0.140661] [ema: 0.997137] 
[Epoch 2/5] [Batch 400/1059] [D loss: 0.504313] [G loss: 0.127364] [ema: 0.997251] 
[Epoch 2/5] [Batch 500/1059] [D loss: 0.546493] [G loss: 0.117632] [ema: 0.997356] 
[Epoch 2/5] [Batch 600/1059] [D loss: 0.549681] [G loss: 0.107653] [ema: 0.997453] 
[Epoch 2/5] [Batch 700/1059] [D loss: 0.511600] [G loss: 0.140719] [ema: 0.997543] 
[Epoch 2/5] [Batch 800/1059] [D loss: 0.468270] [G loss: 0.141824] [ema: 0.997627] 
[Epoch 2/5] [Batch 900/1059] [D loss: 0.449051] [G loss: 0.189092] [ema: 0.997706] 
[Epoch 2/5] [Batch 1000/1059] [D loss: 0.482741] [G loss: 0.126463] [ema: 0.997779] 
[Epoch 3/5] [Batch 0/1059] [D loss: 0.496778] [G loss: 0.158111] [ema: 0.997821] 
[Epoch 3/5] [Batch 100/1059] [D loss: 0.454144] [G loss: 0.179364] [ema: 0.997887] 
[Epoch 3/5] [Batch 200/1059] [D loss: 0.459705] [G loss: 0.158360] [ema: 0.997950] 
[Epoch 3/5] [Batch 300/1059] [D loss: 0.440966] [G loss: 0.178849] [ema: 0.998008] 
[Epoch 3/5] [Batch 400/1059] [D loss: 0.512158] [G loss: 0.145484] [ema: 0.998064] 
[Epoch 3/5] [Batch 500/1059] [D loss: 0.454558] [G loss: 0.133131] [ema: 0.998117] 
[Epoch 3/5] [Batch 600/1059] [D loss: 0.497490] [G loss: 0.114184] [ema: 0.998167] 
[Epoch 3/5] [Batch 700/1059] [D loss: 0.469305] [G loss: 0.164267] [ema: 0.998214] 
[Epoch 3/5] [Batch 800/1059] [D loss: 0.476209] [G loss: 0.142220] [ema: 0.998259] 
[Epoch 3/5] [Batch 900/1059] [D loss: 0.471515] [G loss: 0.161446] [ema: 0.998301] 
[Epoch 3/5] [Batch 1000/1059] [D loss: 0.420791] [G loss: 0.116849] [ema: 0.998342] 



Saving checkpoint 3 in logs/test/walk_5000_D_30_2024_10_17_19_43_27/Model



[Epoch 4/5] [Batch 0/1059] [D loss: 0.406891] [G loss: 0.157480] [ema: 0.998365] 
[Epoch 4/5] [Batch 100/1059] [D loss: 0.441660] [G loss: 0.152110] [ema: 0.998403] 
[Epoch 4/5] [Batch 200/1059] [D loss: 0.476643] [G loss: 0.129764] [ema: 0.998439] 
[Epoch 4/5] [Batch 300/1059] [D loss: 0.428072] [G loss: 0.200967] [ema: 0.998473] 
[Epoch 4/5] [Batch 400/1059] [D loss: 0.416212] [G loss: 0.130338] [ema: 0.998506] 
[Epoch 4/5] [Batch 500/1059] [D loss: 0.464339] [G loss: 0.147274] [ema: 0.998537] 
[Epoch 4/5] [Batch 600/1059] [D loss: 0.506646] [G loss: 0.180487] [ema: 0.998568] 
[Epoch 4/5] [Batch 700/1059] [D loss: 0.478131] [G loss: 0.140158] [ema: 0.998597] 
[Epoch 4/5] [Batch 800/1059] [D loss: 0.468561] [G loss: 0.144872] [ema: 0.998625] 
[Epoch 4/5] [Batch 900/1059] [D loss: 0.455934] [G loss: 0.166208] [ema: 0.998651] 
[Epoch 4/5] [Batch 1000/1059] [D loss: 0.429181] [G loss: 0.159388] [ema: 0.998677] 
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
downstairs
daghar
return single class data and labels, class is downstairs
data shape is (12639, 3, 1, 30)
label shape is (12639,)
790

lets see

saw?
Epochs between checkpoint: 2



Saving checkpoint 1 in logs/test/downstairs_5000_D_30_2024_10_17_19_46_58/Model



[Epoch 0/7] [Batch 0/790] [D loss: 1.213166] [G loss: 0.883360] [ema: 0.000000] 
[Epoch 0/7] [Batch 100/790] [D loss: 0.469353] [G loss: 0.198142] [ema: 0.933033] 
[Epoch 0/7] [Batch 200/790] [D loss: 0.397805] [G loss: 0.188986] [ema: 0.965936] 
[Epoch 0/7] [Batch 300/790] [D loss: 0.272691] [G loss: 0.271859] [ema: 0.977160] 
[Epoch 0/7] [Batch 400/790] [D loss: 0.287708] [G loss: 0.198227] [ema: 0.982821] 
[Epoch 0/7] [Batch 500/790] [D loss: 0.346768] [G loss: 0.171502] [ema: 0.986233] 
[Epoch 0/7] [Batch 600/790] [D loss: 0.424298] [G loss: 0.169059] [ema: 0.988514] 
[Epoch 0/7] [Batch 700/790] [D loss: 0.439032] [G loss: 0.179751] [ema: 0.990147] 
[Epoch 1/7] [Batch 0/790] [D loss: 0.460675] [G loss: 0.193481] [ema: 0.991264] 
[Epoch 1/7] [Batch 100/790] [D loss: 0.350185] [G loss: 0.168092] [ema: 0.992242] 
[Epoch 1/7] [Batch 200/790] [D loss: 0.334206] [G loss: 0.195954] [ema: 0.993023] 
[Epoch 1/7] [Batch 300/790] [D loss: 0.356226] [G loss: 0.141512] [ema: 0.993661] 
[Epoch 1/7] [Batch 400/790] [D loss: 0.390108] [G loss: 0.207171] [ema: 0.994192] 
[Epoch 1/7] [Batch 500/790] [D loss: 0.372965] [G loss: 0.197591] [ema: 0.994641] 
[Epoch 1/7] [Batch 600/790] [D loss: 0.459160] [G loss: 0.170054] [ema: 0.995026] 
[Epoch 1/7] [Batch 700/790] [D loss: 0.492950] [G loss: 0.143688] [ema: 0.995359] 



Saving checkpoint 2 in logs/test/downstairs_5000_D_30_2024_10_17_19_46_58/Model



[Epoch 2/7] [Batch 0/790] [D loss: 0.565164] [G loss: 0.142363] [ema: 0.995623] 
[Epoch 2/7] [Batch 100/790] [D loss: 0.448459] [G loss: 0.151568] [ema: 0.995883] 
[Epoch 2/7] [Batch 200/790] [D loss: 0.502031] [G loss: 0.169203] [ema: 0.996113] 
[Epoch 2/7] [Batch 300/790] [D loss: 0.528507] [G loss: 0.166953] [ema: 0.996320] 
[Epoch 2/7] [Batch 400/790] [D loss: 0.492805] [G loss: 0.181440] [ema: 0.996505] 
[Epoch 2/7] [Batch 500/790] [D loss: 0.473384] [G loss: 0.162559] [ema: 0.996673] 
[Epoch 2/7] [Batch 600/790] [D loss: 0.377097] [G loss: 0.184461] [ema: 0.996825] 
[Epoch 2/7] [Batch 700/790] [D loss: 0.423039] [G loss: 0.204439] [ema: 0.996964] 
[Epoch 3/7] [Batch 0/790] [D loss: 0.465681] [G loss: 0.182214] [ema: 0.997080] 
[Epoch 3/7] [Batch 100/790] [D loss: 0.393599] [G loss: 0.142584] [ema: 0.997198] 
[Epoch 3/7] [Batch 200/790] [D loss: 0.347775] [G loss: 0.160038] [ema: 0.997307] 
[Epoch 3/7] [Batch 300/790] [D loss: 0.398827] [G loss: 0.208795] [ema: 0.997407] 
[Epoch 3/7] [Batch 400/790] [D loss: 0.411164] [G loss: 0.192632] [ema: 0.997501] 
[Epoch 3/7] [Batch 500/790] [D loss: 0.499294] [G loss: 0.120965] [ema: 0.997588] 
[Epoch 3/7] [Batch 600/790] [D loss: 0.512675] [G loss: 0.097070] [ema: 0.997669] 
[Epoch 3/7] [Batch 700/790] [D loss: 0.533163] [G loss: 0.149730] [ema: 0.997745] 



Saving checkpoint 3 in logs/test/downstairs_5000_D_30_2024_10_17_19_46_58/Model



[Epoch 4/7] [Batch 0/790] [D loss: 0.466693] [G loss: 0.171283] [ema: 0.997809] 
[Epoch 4/7] [Batch 100/790] [D loss: 0.414903] [G loss: 0.209106] [ema: 0.997876] 
[Epoch 4/7] [Batch 200/790] [D loss: 0.434215] [G loss: 0.170666] [ema: 0.997939] 
[Epoch 4/7] [Batch 300/790] [D loss: 0.462116] [G loss: 0.135140] [ema: 0.997999] 
[Epoch 4/7] [Batch 400/790] [D loss: 0.452724] [G loss: 0.144477] [ema: 0.998055] 
[Epoch 4/7] [Batch 500/790] [D loss: 0.479473] [G loss: 0.137199] [ema: 0.998108] 
[Epoch 4/7] [Batch 600/790] [D loss: 0.490418] [G loss: 0.189134] [ema: 0.998158] 
[Epoch 4/7] [Batch 700/790] [D loss: 0.483321] [G loss: 0.120548] [ema: 0.998206] 
[Epoch 5/7] [Batch 0/790] [D loss: 0.535920] [G loss: 0.138306] [ema: 0.998247] 
[Epoch 5/7] [Batch 100/790] [D loss: 0.521814] [G loss: 0.118720] [ema: 0.998290] 
[Epoch 5/7] [Batch 200/790] [D loss: 0.552234] [G loss: 0.109416] [ema: 0.998331] 
[Epoch 5/7] [Batch 300/790] [D loss: 0.545756] [G loss: 0.115902] [ema: 0.998370] 
[Epoch 5/7] [Batch 400/790] [D loss: 0.522613] [G loss: 0.112597] [ema: 0.998408] 
[Epoch 5/7] [Batch 500/790] [D loss: 0.541119] [G loss: 0.119951] [ema: 0.998444] 
[Epoch 5/7] [Batch 600/790] [D loss: 0.533223] [G loss: 0.117081] [ema: 0.998478] 
[Epoch 5/7] [Batch 700/790] [D loss: 0.565617] [G loss: 0.125878] [ema: 0.998510] 



Saving checkpoint 4 in logs/test/downstairs_5000_D_30_2024_10_17_19_46_58/Model



[Epoch 6/7] [Batch 0/790] [D loss: 0.533479] [G loss: 0.140779] [ema: 0.998539] 
[Epoch 6/7] [Batch 100/790] [D loss: 0.525479] [G loss: 0.112942] [ema: 0.998569] 
[Epoch 6/7] [Batch 200/790] [D loss: 0.494765] [G loss: 0.115720] [ema: 0.998598] 
[Epoch 6/7] [Batch 300/790] [D loss: 0.546587] [G loss: 0.129397] [ema: 0.998626] 
[Epoch 6/7] [Batch 400/790] [D loss: 0.566103] [G loss: 0.117415] [ema: 0.998652] 
[Epoch 6/7] [Batch 500/790] [D loss: 0.581386] [G loss: 0.128300] [ema: 0.998678] 
[Epoch 6/7] [Batch 600/790] [D loss: 0.522493] [G loss: 0.138763] [ema: 0.998703] 
[Epoch 6/7] [Batch 700/790] [D loss: 0.522262] [G loss: 0.131137] [ema: 0.998727] 
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
sit
daghar
return single class data and labels, class is sit
data shape is (16940, 3, 1, 30)
label shape is (16940,)
1059

lets see

saw?
Epochs between checkpoint: 2



Saving checkpoint 1 in logs/test/sit_5000_D_30_2024_10_17_19_50_35/Model



[Epoch 0/5] [Batch 0/1059] [D loss: 1.265055] [G loss: 0.891576] [ema: 0.000000] 
[Epoch 0/5] [Batch 100/1059] [D loss: 0.542528] [G loss: 0.116618] [ema: 0.933033] 
[Epoch 0/5] [Batch 200/1059] [D loss: 0.614056] [G loss: 0.107754] [ema: 0.965936] 
[Epoch 0/5] [Batch 300/1059] [D loss: 0.601729] [G loss: 0.107603] [ema: 0.977160] 
[Epoch 0/5] [Batch 400/1059] [D loss: 0.516155] [G loss: 0.123856] [ema: 0.982821] 
[Epoch 0/5] [Batch 500/1059] [D loss: 0.488970] [G loss: 0.141799] [ema: 0.986233] 
[Epoch 0/5] [Batch 600/1059] [D loss: 0.554308] [G loss: 0.125471] [ema: 0.988514] 
[Epoch 0/5] [Batch 700/1059] [D loss: 0.604672] [G loss: 0.122231] [ema: 0.990147] 
[Epoch 0/5] [Batch 800/1059] [D loss: 0.535109] [G loss: 0.129489] [ema: 0.991373] 
[Epoch 0/5] [Batch 900/1059] [D loss: 0.534040] [G loss: 0.112319] [ema: 0.992328] 
[Epoch 0/5] [Batch 1000/1059] [D loss: 0.600270] [G loss: 0.104279] [ema: 0.993092] 
[Epoch 1/5] [Batch 0/1059] [D loss: 0.511733] [G loss: 0.138517] [ema: 0.993476] 
[Epoch 1/5] [Batch 100/1059] [D loss: 0.555507] [G loss: 0.116853] [ema: 0.994037] 
[Epoch 1/5] [Batch 200/1059] [D loss: 0.550909] [G loss: 0.106601] [ema: 0.994510] 
[Epoch 1/5] [Batch 300/1059] [D loss: 0.593014] [G loss: 0.105364] [ema: 0.994913] 
[Epoch 1/5] [Batch 400/1059] [D loss: 0.528100] [G loss: 0.109051] [ema: 0.995260] 
[Epoch 1/5] [Batch 500/1059] [D loss: 0.534445] [G loss: 0.112508] [ema: 0.995564] 
[Epoch 1/5] [Batch 600/1059] [D loss: 0.515613] [G loss: 0.125405] [ema: 0.995831] 
[Epoch 1/5] [Batch 700/1059] [D loss: 0.516105] [G loss: 0.121370] [ema: 0.996067] 
[Epoch 1/5] [Batch 800/1059] [D loss: 0.539221] [G loss: 0.120239] [ema: 0.996278] 
[Epoch 1/5] [Batch 900/1059] [D loss: 0.539733] [G loss: 0.119336] [ema: 0.996468] 
[Epoch 1/5] [Batch 1000/1059] [D loss: 0.522451] [G loss: 0.115979] [ema: 0.996639] 



Saving checkpoint 2 in logs/test/sit_5000_D_30_2024_10_17_19_50_35/Model



[Epoch 2/5] [Batch 0/1059] [D loss: 0.578626] [G loss: 0.113448] [ema: 0.996733] 
[Epoch 2/5] [Batch 100/1059] [D loss: 0.529956] [G loss: 0.114290] [ema: 0.996880] 
[Epoch 2/5] [Batch 200/1059] [D loss: 0.547838] [G loss: 0.121194] [ema: 0.997014] 
[Epoch 2/5] [Batch 300/1059] [D loss: 0.511593] [G loss: 0.113050] [ema: 0.997137] 
[Epoch 2/5] [Batch 400/1059] [D loss: 0.565394] [G loss: 0.119272] [ema: 0.997251] 
[Epoch 2/5] [Batch 500/1059] [D loss: 0.580602] [G loss: 0.120159] [ema: 0.997356] 
[Epoch 2/5] [Batch 600/1059] [D loss: 0.571144] [G loss: 0.108398] [ema: 0.997453] 
[Epoch 2/5] [Batch 700/1059] [D loss: 0.570513] [G loss: 0.123349] [ema: 0.997543] 
[Epoch 2/5] [Batch 800/1059] [D loss: 0.534385] [G loss: 0.109409] [ema: 0.997627] 
[Epoch 2/5] [Batch 900/1059] [D loss: 0.533554] [G loss: 0.112802] [ema: 0.997706] 
[Epoch 2/5] [Batch 1000/1059] [D loss: 0.572574] [G loss: 0.106826] [ema: 0.997779] 
[Epoch 3/5] [Batch 0/1059] [D loss: 0.559910] [G loss: 0.114087] [ema: 0.997821] 
[Epoch 3/5] [Batch 100/1059] [D loss: 0.554722] [G loss: 0.114077] [ema: 0.997887] 
[Epoch 3/5] [Batch 200/1059] [D loss: 0.530875] [G loss: 0.111882] [ema: 0.997950] 
[Epoch 3/5] [Batch 300/1059] [D loss: 0.561743] [G loss: 0.114556] [ema: 0.998008] 
[Epoch 3/5] [Batch 400/1059] [D loss: 0.560937] [G loss: 0.103601] [ema: 0.998064] 
[Epoch 3/5] [Batch 500/1059] [D loss: 0.556858] [G loss: 0.102200] [ema: 0.998117] 
[Epoch 3/5] [Batch 600/1059] [D loss: 0.565686] [G loss: 0.099733] [ema: 0.998167] 
[Epoch 3/5] [Batch 700/1059] [D loss: 0.549782] [G loss: 0.117791] [ema: 0.998214] 
[Epoch 3/5] [Batch 800/1059] [D loss: 0.567663] [G loss: 0.118833] [ema: 0.998259] 
[Epoch 3/5] [Batch 900/1059] [D loss: 0.549845] [G loss: 0.115762] [ema: 0.998301] 
[Epoch 3/5] [Batch 1000/1059] [D loss: 0.557251] [G loss: 0.121473] [ema: 0.998342] 



Saving checkpoint 3 in logs/test/sit_5000_D_30_2024_10_17_19_50_35/Model



[Epoch 4/5] [Batch 0/1059] [D loss: 0.543452] [G loss: 0.114809] [ema: 0.998365] 
[Epoch 4/5] [Batch 100/1059] [D loss: 0.568619] [G loss: 0.127936] [ema: 0.998403] 
[Epoch 4/5] [Batch 200/1059] [D loss: 0.566932] [G loss: 0.134604] [ema: 0.998439] 
[Epoch 4/5] [Batch 300/1059] [D loss: 0.514156] [G loss: 0.113613] [ema: 0.998473] 
[Epoch 4/5] [Batch 400/1059] [D loss: 0.597790] [G loss: 0.126927] [ema: 0.998506] 
[Epoch 4/5] [Batch 500/1059] [D loss: 0.560134] [G loss: 0.116429] [ema: 0.998537] 
[Epoch 4/5] [Batch 600/1059] [D loss: 0.559378] [G loss: 0.119483] [ema: 0.998568] 
[Epoch 4/5] [Batch 700/1059] [D loss: 0.540135] [G loss: 0.125857] [ema: 0.998597] 
[Epoch 4/5] [Batch 800/1059] [D loss: 0.532556] [G loss: 0.129011] [ema: 0.998625] 
[Epoch 4/5] [Batch 900/1059] [D loss: 0.508564] [G loss: 0.136914] [ema: 0.998651] 
[Epoch 4/5] [Batch 1000/1059] [D loss: 0.554937] [G loss: 0.109785] [ema: 0.998677] 
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
stand
daghar
return single class data and labels, class is stand
data shape is (16940, 3, 1, 30)
label shape is (16940,)
1059

lets see

saw?
Epochs between checkpoint: 2



Saving checkpoint 1 in logs/test/stand_5000_D_30_2024_10_17_19_54_03/Model



[Epoch 0/5] [Batch 0/1059] [D loss: 1.246002] [G loss: 0.893515] [ema: 0.000000] 
[Epoch 0/5] [Batch 100/1059] [D loss: 0.533717] [G loss: 0.119290] [ema: 0.933033] 
[Epoch 0/5] [Batch 200/1059] [D loss: 0.626516] [G loss: 0.107501] [ema: 0.965936] 
[Epoch 0/5] [Batch 300/1059] [D loss: 0.584517] [G loss: 0.108741] [ema: 0.977160] 
[Epoch 0/5] [Batch 400/1059] [D loss: 0.516785] [G loss: 0.126777] [ema: 0.982821] 
[Epoch 0/5] [Batch 500/1059] [D loss: 0.495834] [G loss: 0.140375] [ema: 0.986233] 
[Epoch 0/5] [Batch 600/1059] [D loss: 0.565821] [G loss: 0.122598] [ema: 0.988514] 
[Epoch 0/5] [Batch 700/1059] [D loss: 0.600904] [G loss: 0.123017] [ema: 0.990147] 
[Epoch 0/5] [Batch 800/1059] [D loss: 0.529738] [G loss: 0.129223] [ema: 0.991373] 
[Epoch 0/5] [Batch 900/1059] [D loss: 0.573973] [G loss: 0.111516] [ema: 0.992328] 
[Epoch 0/5] [Batch 1000/1059] [D loss: 0.609570] [G loss: 0.103903] [ema: 0.993092] 
[Epoch 1/5] [Batch 0/1059] [D loss: 0.514967] [G loss: 0.134302] [ema: 0.993476] 
[Epoch 1/5] [Batch 100/1059] [D loss: 0.566092] [G loss: 0.115522] [ema: 0.994037] 
[Epoch 1/5] [Batch 200/1059] [D loss: 0.563896] [G loss: 0.101831] [ema: 0.994510] 
[Epoch 1/5] [Batch 300/1059] [D loss: 0.589270] [G loss: 0.107302] [ema: 0.994913] 
[Epoch 1/5] [Batch 400/1059] [D loss: 0.523987] [G loss: 0.112740] [ema: 0.995260] 
[Epoch 1/5] [Batch 500/1059] [D loss: 0.534170] [G loss: 0.113232] [ema: 0.995564] 
[Epoch 1/5] [Batch 600/1059] [D loss: 0.526487] [G loss: 0.123394] [ema: 0.995831] 
[Epoch 1/5] [Batch 700/1059] [D loss: 0.517567] [G loss: 0.117108] [ema: 0.996067] 
[Epoch 1/5] [Batch 800/1059] [D loss: 0.541225] [G loss: 0.118164] [ema: 0.996278] 
[Epoch 1/5] [Batch 900/1059] [D loss: 0.537372] [G loss: 0.118797] [ema: 0.996468] 
[Epoch 1/5] [Batch 1000/1059] [D loss: 0.526323] [G loss: 0.113107] [ema: 0.996639] 



Saving checkpoint 2 in logs/test/stand_5000_D_30_2024_10_17_19_54_03/Model



[Epoch 2/5] [Batch 0/1059] [D loss: 0.577141] [G loss: 0.116070] [ema: 0.996733] 
[Epoch 2/5] [Batch 100/1059] [D loss: 0.539810] [G loss: 0.112936] [ema: 0.996880] 
[Epoch 2/5] [Batch 200/1059] [D loss: 0.552441] [G loss: 0.122553] [ema: 0.997014] 
[Epoch 2/5] [Batch 300/1059] [D loss: 0.527804] [G loss: 0.108049] [ema: 0.997137] 
[Epoch 2/5] [Batch 400/1059] [D loss: 0.558537] [G loss: 0.121171] [ema: 0.997251] 
[Epoch 2/5] [Batch 500/1059] [D loss: 0.577988] [G loss: 0.119487] [ema: 0.997356] 
[Epoch 2/5] [Batch 600/1059] [D loss: 0.570024] [G loss: 0.110221] [ema: 0.997453] 
[Epoch 2/5] [Batch 700/1059] [D loss: 0.565432] [G loss: 0.125874] [ema: 0.997543] 
[Epoch 2/5] [Batch 800/1059] [D loss: 0.540172] [G loss: 0.107689] [ema: 0.997627] 
[Epoch 2/5] [Batch 900/1059] [D loss: 0.553344] [G loss: 0.112603] [ema: 0.997706] 
[Epoch 2/5] [Batch 1000/1059] [D loss: 0.576880] [G loss: 0.107472] [ema: 0.997779] 
[Epoch 3/5] [Batch 0/1059] [D loss: 0.564430] [G loss: 0.116111] [ema: 0.997821] 
[Epoch 3/5] [Batch 100/1059] [D loss: 0.558052] [G loss: 0.112600] [ema: 0.997887] 
[Epoch 3/5] [Batch 200/1059] [D loss: 0.546596] [G loss: 0.114521] [ema: 0.997950] 
[Epoch 3/5] [Batch 300/1059] [D loss: 0.560534] [G loss: 0.111702] [ema: 0.998008] 
[Epoch 3/5] [Batch 400/1059] [D loss: 0.563595] [G loss: 0.111523] [ema: 0.998064] 
[Epoch 3/5] [Batch 500/1059] [D loss: 0.554122] [G loss: 0.110537] [ema: 0.998117] 
[Epoch 3/5] [Batch 600/1059] [D loss: 0.554803] [G loss: 0.100373] [ema: 0.998167] 
[Epoch 3/5] [Batch 700/1059] [D loss: 0.576128] [G loss: 0.115339] [ema: 0.998214] 
[Epoch 3/5] [Batch 800/1059] [D loss: 0.565280] [G loss: 0.114176] [ema: 0.998259] 
[Epoch 3/5] [Batch 900/1059] [D loss: 0.558360] [G loss: 0.113475] [ema: 0.998301] 
[Epoch 3/5] [Batch 1000/1059] [D loss: 0.568563] [G loss: 0.113966] [ema: 0.998342] 



Saving checkpoint 3 in logs/test/stand_5000_D_30_2024_10_17_19_54_03/Model



[Epoch 4/5] [Batch 0/1059] [D loss: 0.538334] [G loss: 0.119125] [ema: 0.998365] 
[Epoch 4/5] [Batch 100/1059] [D loss: 0.558884] [G loss: 0.118467] [ema: 0.998403] 
[Epoch 4/5] [Batch 200/1059] [D loss: 0.549286] [G loss: 0.117850] [ema: 0.998439] 
[Epoch 4/5] [Batch 300/1059] [D loss: 0.552595] [G loss: 0.113567] [ema: 0.998473] 
[Epoch 4/5] [Batch 400/1059] [D loss: 0.555420] [G loss: 0.112989] [ema: 0.998506] 
[Epoch 4/5] [Batch 500/1059] [D loss: 0.559674] [G loss: 0.106902] [ema: 0.998537] 
[Epoch 4/5] [Batch 600/1059] [D loss: 0.553885] [G loss: 0.112022] [ema: 0.998568] 
[Epoch 4/5] [Batch 700/1059] [D loss: 0.559447] [G loss: 0.112957] [ema: 0.998597] 
[Epoch 4/5] [Batch 800/1059] [D loss: 0.550758] [G loss: 0.112498] [ema: 0.998625] 
[Epoch 4/5] [Batch 900/1059] [D loss: 0.556163] [G loss: 0.110770] [ema: 0.998651] 
[Epoch 4/5] [Batch 1000/1059] [D loss: 0.559286] [G loss: 0.114581] [ema: 0.998677] 
