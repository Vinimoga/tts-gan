
 Starting training
Total of classes being trained: 6

['upstairs.csv', 'run.csv', 'walk.csv', 'downstairs.csv', 'sit.csv', 'stand.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
upstairs training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
upstairs
daghar
return single class data and labels, class is upstairs
data shape is (12854, 3, 1, 30)
label shape is (12854,)
804
Epochs between checkpoint: 16



Saving checkpoint 1 in logs/daghar_50000_30_100/upstairs_50000_D_30_2024_10_18_01_07_13/Model



[Epoch 0/63] [Batch 0/804] [D loss: 1.326752] [G loss: 0.841100] [ema: 0.000000] 
[Epoch 0/63] [Batch 100/804] [D loss: 0.485614] [G loss: 0.173624] [ema: 0.933033] 
[Epoch 0/63] [Batch 200/804] [D loss: 0.383379] [G loss: 0.180746] [ema: 0.965936] 
[Epoch 0/63] [Batch 300/804] [D loss: 0.323692] [G loss: 0.281011] [ema: 0.977160] 
[Epoch 0/63] [Batch 400/804] [D loss: 0.269532] [G loss: 0.203176] [ema: 0.982821] 
[Epoch 0/63] [Batch 500/804] [D loss: 0.327642] [G loss: 0.192863] [ema: 0.986233] 
[Epoch 0/63] [Batch 600/804] [D loss: 0.371993] [G loss: 0.206953] [ema: 0.988514] 
[Epoch 0/63] [Batch 700/804] [D loss: 0.452778] [G loss: 0.187565] [ema: 0.990147] 
[Epoch 0/63] [Batch 800/804] [D loss: 0.442456] [G loss: 0.143559] [ema: 0.991373] 
[Epoch 1/63] [Batch 0/804] [D loss: 0.381557] [G loss: 0.191250] [ema: 0.991416] 
[Epoch 1/63] [Batch 100/804] [D loss: 0.389229] [G loss: 0.180257] [ema: 0.992362] 
[Epoch 1/63] [Batch 200/804] [D loss: 0.366267] [G loss: 0.248105] [ema: 0.993120] 
[Epoch 1/63] [Batch 300/804] [D loss: 0.299405] [G loss: 0.231240] [ema: 0.993741] 
[Epoch 1/63] [Batch 400/804] [D loss: 0.338077] [G loss: 0.215116] [ema: 0.994260] 
[Epoch 1/63] [Batch 500/804] [D loss: 0.394921] [G loss: 0.205045] [ema: 0.994699] 
[Epoch 1/63] [Batch 600/804] [D loss: 0.405809] [G loss: 0.208284] [ema: 0.995075] 
[Epoch 1/63] [Batch 700/804] [D loss: 0.480693] [G loss: 0.157821] [ema: 0.995402] 
[Epoch 1/63] [Batch 800/804] [D loss: 0.465945] [G loss: 0.167968] [ema: 0.995688] 
[Epoch 2/63] [Batch 0/804] [D loss: 0.456987] [G loss: 0.174077] [ema: 0.995699] 
[Epoch 2/63] [Batch 100/804] [D loss: 0.453022] [G loss: 0.187475] [ema: 0.995950] 
[Epoch 2/63] [Batch 200/804] [D loss: 0.430507] [G loss: 0.179596] [ema: 0.996174] 
[Epoch 2/63] [Batch 300/804] [D loss: 0.438011] [G loss: 0.213833] [ema: 0.996374] 
[Epoch 2/63] [Batch 400/804] [D loss: 0.492470] [G loss: 0.187445] [ema: 0.996554] 
[Epoch 2/63] [Batch 500/804] [D loss: 0.414177] [G loss: 0.163421] [ema: 0.996717] 
[Epoch 2/63] [Batch 600/804] [D loss: 0.415007] [G loss: 0.145969] [ema: 0.996866] 
[Epoch 2/63] [Batch 700/804] [D loss: 0.454652] [G loss: 0.164856] [ema: 0.997001] 
[Epoch 2/63] [Batch 800/804] [D loss: 0.352667] [G loss: 0.230988] [ema: 0.997126] 
[Epoch 3/63] [Batch 0/804] [D loss: 0.406099] [G loss: 0.210120] [ema: 0.997130] 
[Epoch 3/63] [Batch 100/804] [D loss: 0.386674] [G loss: 0.202281] [ema: 0.997244] 
[Epoch 3/63] [Batch 200/804] [D loss: 0.395508] [G loss: 0.210255] [ema: 0.997350] 
[Epoch 3/63] [Batch 300/804] [D loss: 0.410082] [G loss: 0.204825] [ema: 0.997447] 
[Epoch 3/63] [Batch 400/804] [D loss: 0.355475] [G loss: 0.210358] [ema: 0.997538] 
[Epoch 3/63] [Batch 500/804] [D loss: 0.419255] [G loss: 0.175226] [ema: 0.997623] 
[Epoch 3/63] [Batch 600/804] [D loss: 0.399014] [G loss: 0.216040] [ema: 0.997701] 
[Epoch 3/63] [Batch 700/804] [D loss: 0.470978] [G loss: 0.167233] [ema: 0.997775] 
[Epoch 3/63] [Batch 800/804] [D loss: 0.491313] [G loss: 0.169978] [ema: 0.997844] 
[Epoch 4/63] [Batch 0/804] [D loss: 0.428078] [G loss: 0.185702] [ema: 0.997847] 
[Epoch 4/63] [Batch 100/804] [D loss: 0.397387] [G loss: 0.174268] [ema: 0.997912] 
[Epoch 4/63] [Batch 200/804] [D loss: 0.399756] [G loss: 0.226215] [ema: 0.997973] 
[Epoch 4/63] [Batch 300/804] [D loss: 0.426870] [G loss: 0.184647] [ema: 0.998031] 
[Epoch 4/63] [Batch 400/804] [D loss: 0.383885] [G loss: 0.143258] [ema: 0.998085] 
[Epoch 4/63] [Batch 500/804] [D loss: 0.362945] [G loss: 0.259481] [ema: 0.998136] 
[Epoch 4/63] [Batch 600/804] [D loss: 0.436787] [G loss: 0.188494] [ema: 0.998185] 
[Epoch 4/63] [Batch 700/804] [D loss: 0.420532] [G loss: 0.199041] [ema: 0.998232] 
[Epoch 4/63] [Batch 800/804] [D loss: 0.441958] [G loss: 0.194586] [ema: 0.998276] 
[Epoch 5/63] [Batch 0/804] [D loss: 0.454154] [G loss: 0.193164] [ema: 0.998277] 
[Epoch 5/63] [Batch 100/804] [D loss: 0.406214] [G loss: 0.180360] [ema: 0.998319] 
[Epoch 5/63] [Batch 200/804] [D loss: 0.476553] [G loss: 0.177722] [ema: 0.998359] 
[Epoch 5/63] [Batch 300/804] [D loss: 0.467790] [G loss: 0.128932] [ema: 0.998397] 
[Epoch 5/63] [Batch 400/804] [D loss: 0.456768] [G loss: 0.129705] [ema: 0.998433] 
[Epoch 5/63] [Batch 500/804] [D loss: 0.520749] [G loss: 0.139331] [ema: 0.998468] 
[Epoch 5/63] [Batch 600/804] [D loss: 0.578901] [G loss: 0.109780] [ema: 0.998501] 
[Epoch 5/63] [Batch 700/804] [D loss: 0.479897] [G loss: 0.116430] [ema: 0.998533] 
[Epoch 5/63] [Batch 800/804] [D loss: 0.532381] [G loss: 0.146719] [ema: 0.998563] 
[Epoch 6/63] [Batch 0/804] [D loss: 0.527765] [G loss: 0.152939] [ema: 0.998564] 
[Epoch 6/63] [Batch 100/804] [D loss: 0.522375] [G loss: 0.134204] [ema: 0.998593] 
[Epoch 6/63] [Batch 200/804] [D loss: 0.513818] [G loss: 0.162015] [ema: 0.998621] 
[Epoch 6/63] [Batch 300/804] [D loss: 0.478311] [G loss: 0.160653] [ema: 0.998648] 
[Epoch 6/63] [Batch 400/804] [D loss: 0.465027] [G loss: 0.132317] [ema: 0.998674] 
[Epoch 6/63] [Batch 500/804] [D loss: 0.463682] [G loss: 0.156391] [ema: 0.998699] 
[Epoch 6/63] [Batch 600/804] [D loss: 0.562631] [G loss: 0.127030] [ema: 0.998723] 
[Epoch 6/63] [Batch 700/804] [D loss: 0.482307] [G loss: 0.151765] [ema: 0.998746] 
[Epoch 6/63] [Batch 800/804] [D loss: 0.527779] [G loss: 0.134343] [ema: 0.998768] 
[Epoch 7/63] [Batch 0/804] [D loss: 0.516278] [G loss: 0.122336] [ema: 0.998769] 
[Epoch 7/63] [Batch 100/804] [D loss: 0.497677] [G loss: 0.146467] [ema: 0.998791] 
[Epoch 7/63] [Batch 200/804] [D loss: 0.488593] [G loss: 0.134081] [ema: 0.998811] 
[Epoch 7/63] [Batch 300/804] [D loss: 0.452575] [G loss: 0.142665] [ema: 0.998831] 
[Epoch 7/63] [Batch 400/804] [D loss: 0.481274] [G loss: 0.174759] [ema: 0.998851] 
[Epoch 7/63] [Batch 500/804] [D loss: 0.503135] [G loss: 0.136222] [ema: 0.998870] 
[Epoch 7/63] [Batch 600/804] [D loss: 0.530552] [G loss: 0.130721] [ema: 0.998888] 
[Epoch 7/63] [Batch 700/804] [D loss: 0.475653] [G loss: 0.119962] [ema: 0.998905] 
[Epoch 7/63] [Batch 800/804] [D loss: 0.506018] [G loss: 0.170648] [ema: 0.998922] 
[Epoch 8/63] [Batch 0/804] [D loss: 0.456160] [G loss: 0.161335] [ema: 0.998923] 
[Epoch 8/63] [Batch 100/804] [D loss: 0.516269] [G loss: 0.139801] [ema: 0.998939] 
[Epoch 8/63] [Batch 200/804] [D loss: 0.489563] [G loss: 0.142539] [ema: 0.998955] 
[Epoch 8/63] [Batch 300/804] [D loss: 0.510278] [G loss: 0.175785] [ema: 0.998971] 
[Epoch 8/63] [Batch 400/804] [D loss: 0.447944] [G loss: 0.147060] [ema: 0.998986] 
[Epoch 8/63] [Batch 500/804] [D loss: 0.400285] [G loss: 0.146699] [ema: 0.999001] 
[Epoch 8/63] [Batch 600/804] [D loss: 0.547394] [G loss: 0.111540] [ema: 0.999015] 
[Epoch 8/63] [Batch 700/804] [D loss: 0.536903] [G loss: 0.143289] [ema: 0.999029] 
[Epoch 8/63] [Batch 800/804] [D loss: 0.474069] [G loss: 0.134992] [ema: 0.999042] 
[Epoch 9/63] [Batch 0/804] [D loss: 0.529220] [G loss: 0.152649] [ema: 0.999043] 
[Epoch 9/63] [Batch 100/804] [D loss: 0.455160] [G loss: 0.165797] [ema: 0.999056] 
[Epoch 9/63] [Batch 200/804] [D loss: 0.471350] [G loss: 0.177329] [ema: 0.999068] 
[Epoch 9/63] [Batch 300/804] [D loss: 0.437520] [G loss: 0.152945] [ema: 0.999081] 
[Epoch 9/63] [Batch 400/804] [D loss: 0.536343] [G loss: 0.136781] [ema: 0.999093] 
[Epoch 9/63] [Batch 500/804] [D loss: 0.523542] [G loss: 0.166504] [ema: 0.999104] 
[Epoch 9/63] [Batch 600/804] [D loss: 0.466195] [G loss: 0.152891] [ema: 0.999116] 
[Epoch 9/63] [Batch 700/804] [D loss: 0.493160] [G loss: 0.123610] [ema: 0.999127] 
[Epoch 9/63] [Batch 800/804] [D loss: 0.454932] [G loss: 0.175773] [ema: 0.999138] 
[Epoch 10/63] [Batch 0/804] [D loss: 0.430520] [G loss: 0.137517] [ema: 0.999138] 
[Epoch 10/63] [Batch 100/804] [D loss: 0.473094] [G loss: 0.145015] [ema: 0.999149] 
[Epoch 10/63] [Batch 200/804] [D loss: 0.497706] [G loss: 0.144894] [ema: 0.999159] 
[Epoch 10/63] [Batch 300/804] [D loss: 0.500489] [G loss: 0.177686] [ema: 0.999169] 
[Epoch 10/63] [Batch 400/804] [D loss: 0.457661] [G loss: 0.151124] [ema: 0.999179] 
[Epoch 10/63] [Batch 500/804] [D loss: 0.505285] [G loss: 0.152861] [ema: 0.999189] 
[Epoch 10/63] [Batch 600/804] [D loss: 0.475290] [G loss: 0.171584] [ema: 0.999198] 
[Epoch 10/63] [Batch 700/804] [D loss: 0.507388] [G loss: 0.139347] [ema: 0.999207] 
[Epoch 10/63] [Batch 800/804] [D loss: 0.399572] [G loss: 0.182224] [ema: 0.999216] 
[Epoch 11/63] [Batch 0/804] [D loss: 0.422395] [G loss: 0.164059] [ema: 0.999217] 
[Epoch 11/63] [Batch 100/804] [D loss: 0.437035] [G loss: 0.171695] [ema: 0.999225] 
[Epoch 11/63] [Batch 200/804] [D loss: 0.489866] [G loss: 0.167011] [ema: 0.999234] 
[Epoch 11/63] [Batch 300/804] [D loss: 0.469031] [G loss: 0.200069] [ema: 0.999242] 
[Epoch 11/63] [Batch 400/804] [D loss: 0.452473] [G loss: 0.156337] [ema: 0.999250] 
[Epoch 11/63] [Batch 500/804] [D loss: 0.430207] [G loss: 0.129648] [ema: 0.999258] 
[Epoch 11/63] [Batch 600/804] [D loss: 0.435905] [G loss: 0.179322] [ema: 0.999266] 
[Epoch 11/63] [Batch 700/804] [D loss: 0.426002] [G loss: 0.167225] [ema: 0.999274] 
[Epoch 11/63] [Batch 800/804] [D loss: 0.432861] [G loss: 0.149139] [ema: 0.999282] 
[Epoch 12/63] [Batch 0/804] [D loss: 0.465544] [G loss: 0.176758] [ema: 0.999282] 
[Epoch 12/63] [Batch 100/804] [D loss: 0.431448] [G loss: 0.168506] [ema: 0.999289] 
[Epoch 12/63] [Batch 200/804] [D loss: 0.473317] [G loss: 0.174014] [ema: 0.999296] 
[Epoch 12/63] [Batch 300/804] [D loss: 0.483020] [G loss: 0.166291] [ema: 0.999303] 
[Epoch 12/63] [Batch 400/804] [D loss: 0.432255] [G loss: 0.168227] [ema: 0.999310] 
[Epoch 12/63] [Batch 500/804] [D loss: 0.488756] [G loss: 0.142034] [ema: 0.999317] 
[Epoch 12/63] [Batch 600/804] [D loss: 0.479731] [G loss: 0.175211] [ema: 0.999324] 
[Epoch 12/63] [Batch 700/804] [D loss: 0.459281] [G loss: 0.145801] [ema: 0.999330] 
[Epoch 12/63] [Batch 800/804] [D loss: 0.421932] [G loss: 0.151278] [ema: 0.999337] 
[Epoch 13/63] [Batch 0/804] [D loss: 0.485884] [G loss: 0.169062] [ema: 0.999337] 
[Epoch 13/63] [Batch 100/804] [D loss: 0.452567] [G loss: 0.160821] [ema: 0.999343] 
[Epoch 13/63] [Batch 200/804] [D loss: 0.445179] [G loss: 0.167517] [ema: 0.999349] 
[Epoch 13/63] [Batch 300/804] [D loss: 0.521979] [G loss: 0.139152] [ema: 0.999356] 
[Epoch 13/63] [Batch 400/804] [D loss: 0.448410] [G loss: 0.156776] [ema: 0.999361] 
[Epoch 13/63] [Batch 500/804] [D loss: 0.499204] [G loss: 0.155037] [ema: 0.999367] 
[Epoch 13/63] [Batch 600/804] [D loss: 0.476031] [G loss: 0.163710] [ema: 0.999373] 
[Epoch 13/63] [Batch 700/804] [D loss: 0.427793] [G loss: 0.196383] [ema: 0.999379] 
[Epoch 13/63] [Batch 800/804] [D loss: 0.426030] [G loss: 0.187856] [ema: 0.999384] 
[Epoch 14/63] [Batch 0/804] [D loss: 0.462630] [G loss: 0.149996] [ema: 0.999384] 
[Epoch 14/63] [Batch 100/804] [D loss: 0.411122] [G loss: 0.152677] [ema: 0.999390] 
[Epoch 14/63] [Batch 200/804] [D loss: 0.441044] [G loss: 0.183845] [ema: 0.999395] 
[Epoch 14/63] [Batch 300/804] [D loss: 0.438146] [G loss: 0.146951] [ema: 0.999400] 
[Epoch 14/63] [Batch 400/804] [D loss: 0.455482] [G loss: 0.154387] [ema: 0.999406] 
[Epoch 14/63] [Batch 500/804] [D loss: 0.480317] [G loss: 0.161438] [ema: 0.999411] 
[Epoch 14/63] [Batch 600/804] [D loss: 0.468295] [G loss: 0.143101] [ema: 0.999416] 
[Epoch 14/63] [Batch 700/804] [D loss: 0.499929] [G loss: 0.152989] [ema: 0.999420] 
[Epoch 14/63] [Batch 800/804] [D loss: 0.445820] [G loss: 0.174015] [ema: 0.999425] 
[Epoch 15/63] [Batch 0/804] [D loss: 0.447130] [G loss: 0.154633] [ema: 0.999425] 
[Epoch 15/63] [Batch 100/804] [D loss: 0.427633] [G loss: 0.158104] [ema: 0.999430] 
[Epoch 15/63] [Batch 200/804] [D loss: 0.454580] [G loss: 0.157104] [ema: 0.999435] 
[Epoch 15/63] [Batch 300/804] [D loss: 0.481752] [G loss: 0.166747] [ema: 0.999439] 
[Epoch 15/63] [Batch 400/804] [D loss: 0.419506] [G loss: 0.154739] [ema: 0.999444] 
[Epoch 15/63] [Batch 500/804] [D loss: 0.498133] [G loss: 0.187256] [ema: 0.999448] 
[Epoch 15/63] [Batch 600/804] [D loss: 0.463095] [G loss: 0.173632] [ema: 0.999453] 
[Epoch 15/63] [Batch 700/804] [D loss: 0.499964] [G loss: 0.169789] [ema: 0.999457] 
[Epoch 15/63] [Batch 800/804] [D loss: 0.463869] [G loss: 0.194888] [ema: 0.999461] 



Saving checkpoint 2 in logs/daghar_50000_30_100/upstairs_50000_D_30_2024_10_18_01_07_13/Model



[Epoch 16/63] [Batch 0/804] [D loss: 0.491506] [G loss: 0.200315] [ema: 0.999461] 
[Epoch 16/63] [Batch 100/804] [D loss: 0.409449] [G loss: 0.142525] [ema: 0.999465] 
[Epoch 16/63] [Batch 200/804] [D loss: 0.406632] [G loss: 0.183997] [ema: 0.999470] 
[Epoch 16/63] [Batch 300/804] [D loss: 0.506153] [G loss: 0.171743] [ema: 0.999474] 
[Epoch 16/63] [Batch 400/804] [D loss: 0.488853] [G loss: 0.145420] [ema: 0.999478] 
[Epoch 16/63] [Batch 500/804] [D loss: 0.413898] [G loss: 0.149883] [ema: 0.999481] 
[Epoch 16/63] [Batch 600/804] [D loss: 0.467639] [G loss: 0.134586] [ema: 0.999485] 
[Epoch 16/63] [Batch 700/804] [D loss: 0.452850] [G loss: 0.126102] [ema: 0.999489] 
[Epoch 16/63] [Batch 800/804] [D loss: 0.426815] [G loss: 0.189375] [ema: 0.999493] 
[Epoch 17/63] [Batch 0/804] [D loss: 0.433605] [G loss: 0.153338] [ema: 0.999493] 
[Epoch 17/63] [Batch 100/804] [D loss: 0.387600] [G loss: 0.165422] [ema: 0.999497] 
[Epoch 17/63] [Batch 200/804] [D loss: 0.403935] [G loss: 0.156682] [ema: 0.999500] 
[Epoch 17/63] [Batch 300/804] [D loss: 0.453390] [G loss: 0.153237] [ema: 0.999504] 
[Epoch 17/63] [Batch 400/804] [D loss: 0.403719] [G loss: 0.172378] [ema: 0.999507] 
[Epoch 17/63] [Batch 500/804] [D loss: 0.456432] [G loss: 0.215418] [ema: 0.999511] 
[Epoch 17/63] [Batch 600/804] [D loss: 0.380098] [G loss: 0.186783] [ema: 0.999514] 
[Epoch 17/63] [Batch 700/804] [D loss: 0.443397] [G loss: 0.150441] [ema: 0.999518] 
[Epoch 17/63] [Batch 800/804] [D loss: 0.509417] [G loss: 0.144710] [ema: 0.999521] 
[Epoch 18/63] [Batch 0/804] [D loss: 0.493913] [G loss: 0.146093] [ema: 0.999521] 
[Epoch 18/63] [Batch 100/804] [D loss: 0.468637] [G loss: 0.171460] [ema: 0.999524] 
[Epoch 18/63] [Batch 200/804] [D loss: 0.472956] [G loss: 0.153293] [ema: 0.999528] 
[Epoch 18/63] [Batch 300/804] [D loss: 0.444199] [G loss: 0.194508] [ema: 0.999531] 
[Epoch 18/63] [Batch 400/804] [D loss: 0.417937] [G loss: 0.177934] [ema: 0.999534] 
[Epoch 18/63] [Batch 500/804] [D loss: 0.392791] [G loss: 0.164867] [ema: 0.999537] 
[Epoch 18/63] [Batch 600/804] [D loss: 0.428541] [G loss: 0.185768] [ema: 0.999540] 
[Epoch 18/63] [Batch 700/804] [D loss: 0.456325] [G loss: 0.158940] [ema: 0.999543] 
[Epoch 18/63] [Batch 800/804] [D loss: 0.396211] [G loss: 0.171420] [ema: 0.999546] 
[Epoch 19/63] [Batch 0/804] [D loss: 0.362197] [G loss: 0.158793] [ema: 0.999546] 
[Epoch 19/63] [Batch 100/804] [D loss: 0.426647] [G loss: 0.150949] [ema: 0.999549] 
[Epoch 19/63] [Batch 200/804] [D loss: 0.475806] [G loss: 0.176817] [ema: 0.999552] 
[Epoch 19/63] [Batch 300/804] [D loss: 0.421742] [G loss: 0.183900] [ema: 0.999555] 
[Epoch 19/63] [Batch 400/804] [D loss: 0.382095] [G loss: 0.187730] [ema: 0.999558] 
[Epoch 19/63] [Batch 500/804] [D loss: 0.440773] [G loss: 0.157332] [ema: 0.999561] 
[Epoch 19/63] [Batch 600/804] [D loss: 0.447320] [G loss: 0.175517] [ema: 0.999563] 
[Epoch 19/63] [Batch 700/804] [D loss: 0.372650] [G loss: 0.161040] [ema: 0.999566] 
[Epoch 19/63] [Batch 800/804] [D loss: 0.412680] [G loss: 0.160859] [ema: 0.999569] 
[Epoch 20/63] [Batch 0/804] [D loss: 0.371831] [G loss: 0.192511] [ema: 0.999569] 
[Epoch 20/63] [Batch 100/804] [D loss: 0.415779] [G loss: 0.162532] [ema: 0.999572] 
[Epoch 20/63] [Batch 200/804] [D loss: 0.449554] [G loss: 0.157362] [ema: 0.999574] 
[Epoch 20/63] [Batch 300/804] [D loss: 0.408187] [G loss: 0.159440] [ema: 0.999577] 
[Epoch 20/63] [Batch 400/804] [D loss: 0.389061] [G loss: 0.190905] [ema: 0.999579] 
[Epoch 20/63] [Batch 500/804] [D loss: 0.435484] [G loss: 0.172927] [ema: 0.999582] 
[Epoch 20/63] [Batch 600/804] [D loss: 0.389043] [G loss: 0.161268] [ema: 0.999585] 
[Epoch 20/63] [Batch 700/804] [D loss: 0.388446] [G loss: 0.184196] [ema: 0.999587] 
[Epoch 20/63] [Batch 800/804] [D loss: 0.445508] [G loss: 0.143328] [ema: 0.999589] 
[Epoch 21/63] [Batch 0/804] [D loss: 0.441003] [G loss: 0.164473] [ema: 0.999590] 
[Epoch 21/63] [Batch 100/804] [D loss: 0.512314] [G loss: 0.151265] [ema: 0.999592] 
[Epoch 21/63] [Batch 200/804] [D loss: 0.439251] [G loss: 0.176293] [ema: 0.999594] 
[Epoch 21/63] [Batch 300/804] [D loss: 0.405687] [G loss: 0.184455] [ema: 0.999597] 
[Epoch 21/63] [Batch 400/804] [D loss: 0.416704] [G loss: 0.165319] [ema: 0.999599] 
[Epoch 21/63] [Batch 500/804] [D loss: 0.443775] [G loss: 0.160534] [ema: 0.999601] 
[Epoch 21/63] [Batch 600/804] [D loss: 0.496458] [G loss: 0.167174] [ema: 0.999604] 
[Epoch 21/63] [Batch 700/804] [D loss: 0.459730] [G loss: 0.150444] [ema: 0.999606] 
[Epoch 21/63] [Batch 800/804] [D loss: 0.428289] [G loss: 0.190141] [ema: 0.999608] 
[Epoch 22/63] [Batch 0/804] [D loss: 0.457914] [G loss: 0.194897] [ema: 0.999608] 
[Epoch 22/63] [Batch 100/804] [D loss: 0.376155] [G loss: 0.192354] [ema: 0.999610] 
[Epoch 22/63] [Batch 200/804] [D loss: 0.374356] [G loss: 0.175643] [ema: 0.999613] 
[Epoch 22/63] [Batch 300/804] [D loss: 0.467189] [G loss: 0.185549] [ema: 0.999615] 
[Epoch 22/63] [Batch 400/804] [D loss: 0.409217] [G loss: 0.181927] [ema: 0.999617] 
[Epoch 22/63] [Batch 500/804] [D loss: 0.470739] [G loss: 0.169960] [ema: 0.999619] 
[Epoch 22/63] [Batch 600/804] [D loss: 0.432169] [G loss: 0.159247] [ema: 0.999621] 
[Epoch 22/63] [Batch 700/804] [D loss: 0.417451] [G loss: 0.184142] [ema: 0.999623] 
[Epoch 22/63] [Batch 800/804] [D loss: 0.421078] [G loss: 0.144494] [ema: 0.999625] 
[Epoch 23/63] [Batch 0/804] [D loss: 0.424819] [G loss: 0.183853] [ema: 0.999625] 
[Epoch 23/63] [Batch 100/804] [D loss: 0.458408] [G loss: 0.177743] [ema: 0.999627] 
[Epoch 23/63] [Batch 200/804] [D loss: 0.485172] [G loss: 0.157701] [ema: 0.999629] 
[Epoch 23/63] [Batch 300/804] [D loss: 0.390447] [G loss: 0.170512] [ema: 0.999631] 
[Epoch 23/63] [Batch 400/804] [D loss: 0.442160] [G loss: 0.184569] [ema: 0.999633] 
[Epoch 23/63] [Batch 500/804] [D loss: 0.375589] [G loss: 0.165335] [ema: 0.999635] 
[Epoch 23/63] [Batch 600/804] [D loss: 0.401512] [G loss: 0.186893] [ema: 0.999637] 
[Epoch 23/63] [Batch 700/804] [D loss: 0.401513] [G loss: 0.172565] [ema: 0.999639] 
[Epoch 23/63] [Batch 800/804] [D loss: 0.406121] [G loss: 0.201478] [ema: 0.999641] 
[Epoch 24/63] [Batch 0/804] [D loss: 0.365766] [G loss: 0.181118] [ema: 0.999641] 
[Epoch 24/63] [Batch 100/804] [D loss: 0.388479] [G loss: 0.177589] [ema: 0.999643] 
[Epoch 24/63] [Batch 200/804] [D loss: 0.406174] [G loss: 0.161679] [ema: 0.999645] 
[Epoch 24/63] [Batch 300/804] [D loss: 0.433178] [G loss: 0.196347] [ema: 0.999646] 
[Epoch 24/63] [Batch 400/804] [D loss: 0.519103] [G loss: 0.172836] [ema: 0.999648] 
[Epoch 24/63] [Batch 500/804] [D loss: 0.427361] [G loss: 0.148058] [ema: 0.999650] 
[Epoch 24/63] [Batch 600/804] [D loss: 0.436908] [G loss: 0.210676] [ema: 0.999652] 
[Epoch 24/63] [Batch 700/804] [D loss: 0.410055] [G loss: 0.199505] [ema: 0.999653] 
[Epoch 24/63] [Batch 800/804] [D loss: 0.509768] [G loss: 0.155791] [ema: 0.999655] 
[Epoch 25/63] [Batch 0/804] [D loss: 0.474749] [G loss: 0.150705] [ema: 0.999655] 
[Epoch 25/63] [Batch 100/804] [D loss: 0.455918] [G loss: 0.192565] [ema: 0.999657] 
[Epoch 25/63] [Batch 200/804] [D loss: 0.452341] [G loss: 0.155184] [ema: 0.999659] 
[Epoch 25/63] [Batch 300/804] [D loss: 0.449653] [G loss: 0.178666] [ema: 0.999660] 
[Epoch 25/63] [Batch 400/804] [D loss: 0.429776] [G loss: 0.184675] [ema: 0.999662] 
[Epoch 25/63] [Batch 500/804] [D loss: 0.395907] [G loss: 0.158730] [ema: 0.999664] 
[Epoch 25/63] [Batch 600/804] [D loss: 0.409927] [G loss: 0.172660] [ema: 0.999665] 
[Epoch 25/63] [Batch 700/804] [D loss: 0.438677] [G loss: 0.185418] [ema: 0.999667] 
[Epoch 25/63] [Batch 800/804] [D loss: 0.468269] [G loss: 0.173323] [ema: 0.999668] 
[Epoch 26/63] [Batch 0/804] [D loss: 0.380660] [G loss: 0.188397] [ema: 0.999668] 
[Epoch 26/63] [Batch 100/804] [D loss: 0.454017] [G loss: 0.167899] [ema: 0.999670] 
[Epoch 26/63] [Batch 200/804] [D loss: 0.423673] [G loss: 0.153313] [ema: 0.999672] 
[Epoch 26/63] [Batch 300/804] [D loss: 0.448232] [G loss: 0.172015] [ema: 0.999673] 
[Epoch 26/63] [Batch 400/804] [D loss: 0.405664] [G loss: 0.144124] [ema: 0.999675] 
[Epoch 26/63] [Batch 500/804] [D loss: 0.414724] [G loss: 0.146522] [ema: 0.999676] 
[Epoch 26/63] [Batch 600/804] [D loss: 0.467468] [G loss: 0.154126] [ema: 0.999678] 
[Epoch 26/63] [Batch 700/804] [D loss: 0.454203] [G loss: 0.176366] [ema: 0.999679] 
[Epoch 26/63] [Batch 800/804] [D loss: 0.442681] [G loss: 0.156031] [ema: 0.999681] 
[Epoch 27/63] [Batch 0/804] [D loss: 0.420318] [G loss: 0.179504] [ema: 0.999681] 
[Epoch 27/63] [Batch 100/804] [D loss: 0.417828] [G loss: 0.194306] [ema: 0.999682] 
[Epoch 27/63] [Batch 200/804] [D loss: 0.364786] [G loss: 0.198901] [ema: 0.999684] 
[Epoch 27/63] [Batch 300/804] [D loss: 0.416554] [G loss: 0.170114] [ema: 0.999685] 
[Epoch 27/63] [Batch 400/804] [D loss: 0.409403] [G loss: 0.151855] [ema: 0.999687] 
[Epoch 27/63] [Batch 500/804] [D loss: 0.371561] [G loss: 0.179459] [ema: 0.999688] 
[Epoch 27/63] [Batch 600/804] [D loss: 0.389200] [G loss: 0.151426] [ema: 0.999689] 
[Epoch 27/63] [Batch 700/804] [D loss: 0.429113] [G loss: 0.178680] [ema: 0.999691] 
[Epoch 27/63] [Batch 800/804] [D loss: 0.416710] [G loss: 0.203812] [ema: 0.999692] 
[Epoch 28/63] [Batch 0/804] [D loss: 0.478822] [G loss: 0.165838] [ema: 0.999692] 
[Epoch 28/63] [Batch 100/804] [D loss: 0.422582] [G loss: 0.176158] [ema: 0.999694] 
[Epoch 28/63] [Batch 200/804] [D loss: 0.381504] [G loss: 0.127753] [ema: 0.999695] 
[Epoch 28/63] [Batch 300/804] [D loss: 0.406273] [G loss: 0.153203] [ema: 0.999696] 
[Epoch 28/63] [Batch 400/804] [D loss: 0.347218] [G loss: 0.189566] [ema: 0.999698] 
[Epoch 28/63] [Batch 500/804] [D loss: 0.410691] [G loss: 0.148624] [ema: 0.999699] 
[Epoch 28/63] [Batch 600/804] [D loss: 0.437713] [G loss: 0.149857] [ema: 0.999700] 
[Epoch 28/63] [Batch 700/804] [D loss: 0.397072] [G loss: 0.155096] [ema: 0.999701] 
[Epoch 28/63] [Batch 800/804] [D loss: 0.437694] [G loss: 0.190512] [ema: 0.999703] 
[Epoch 29/63] [Batch 0/804] [D loss: 0.405997] [G loss: 0.154090] [ema: 0.999703] 
[Epoch 29/63] [Batch 100/804] [D loss: 0.383970] [G loss: 0.160937] [ema: 0.999704] 
[Epoch 29/63] [Batch 200/804] [D loss: 0.379078] [G loss: 0.187605] [ema: 0.999705] 
[Epoch 29/63] [Batch 300/804] [D loss: 0.393778] [G loss: 0.181777] [ema: 0.999707] 
[Epoch 29/63] [Batch 400/804] [D loss: 0.439721] [G loss: 0.186079] [ema: 0.999708] 
[Epoch 29/63] [Batch 500/804] [D loss: 0.452120] [G loss: 0.179210] [ema: 0.999709] 
[Epoch 29/63] [Batch 600/804] [D loss: 0.447870] [G loss: 0.175782] [ema: 0.999710] 
[Epoch 29/63] [Batch 700/804] [D loss: 0.418595] [G loss: 0.187275] [ema: 0.999711] 
[Epoch 29/63] [Batch 800/804] [D loss: 0.421775] [G loss: 0.183980] [ema: 0.999713] 
[Epoch 30/63] [Batch 0/804] [D loss: 0.459581] [G loss: 0.178116] [ema: 0.999713] 
[Epoch 30/63] [Batch 100/804] [D loss: 0.390916] [G loss: 0.186140] [ema: 0.999714] 
[Epoch 30/63] [Batch 200/804] [D loss: 0.408622] [G loss: 0.173409] [ema: 0.999715] 
[Epoch 30/63] [Batch 300/804] [D loss: 0.349852] [G loss: 0.149813] [ema: 0.999716] 
[Epoch 30/63] [Batch 400/804] [D loss: 0.382819] [G loss: 0.159851] [ema: 0.999717] 
[Epoch 30/63] [Batch 500/804] [D loss: 0.362719] [G loss: 0.162003] [ema: 0.999719] 
[Epoch 30/63] [Batch 600/804] [D loss: 0.504904] [G loss: 0.170545] [ema: 0.999720] 
[Epoch 30/63] [Batch 700/804] [D loss: 0.461805] [G loss: 0.178252] [ema: 0.999721] 
[Epoch 30/63] [Batch 800/804] [D loss: 0.411602] [G loss: 0.161449] [ema: 0.999722] 
[Epoch 31/63] [Batch 0/804] [D loss: 0.411256] [G loss: 0.180368] [ema: 0.999722] 
[Epoch 31/63] [Batch 100/804] [D loss: 0.368127] [G loss: 0.165632] [ema: 0.999723] 
[Epoch 31/63] [Batch 200/804] [D loss: 0.375420] [G loss: 0.181303] [ema: 0.999724] 
[Epoch 31/63] [Batch 300/804] [D loss: 0.434447] [G loss: 0.189594] [ema: 0.999725] 
[Epoch 31/63] [Batch 400/804] [D loss: 0.505095] [G loss: 0.169847] [ema: 0.999726] 
[Epoch 31/63] [Batch 500/804] [D loss: 0.363691] [G loss: 0.177138] [ema: 0.999727] 
[Epoch 31/63] [Batch 600/804] [D loss: 0.346462] [G loss: 0.194052] [ema: 0.999728] 
[Epoch 31/63] [Batch 700/804] [D loss: 0.384628] [G loss: 0.172166] [ema: 0.999730] 
[Epoch 31/63] [Batch 800/804] [D loss: 0.397544] [G loss: 0.193795] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_50000_30_100/upstairs_50000_D_30_2024_10_18_01_07_13/Model



[Epoch 32/63] [Batch 0/804] [D loss: 0.444640] [G loss: 0.196943] [ema: 0.999731] 
[Epoch 32/63] [Batch 100/804] [D loss: 0.372819] [G loss: 0.186676] [ema: 0.999732] 
[Epoch 32/63] [Batch 200/804] [D loss: 0.392877] [G loss: 0.178373] [ema: 0.999733] 
[Epoch 32/63] [Batch 300/804] [D loss: 0.359435] [G loss: 0.206890] [ema: 0.999734] 
[Epoch 32/63] [Batch 400/804] [D loss: 0.378740] [G loss: 0.199036] [ema: 0.999735] 
[Epoch 32/63] [Batch 500/804] [D loss: 0.397368] [G loss: 0.168835] [ema: 0.999736] 
[Epoch 32/63] [Batch 600/804] [D loss: 0.495769] [G loss: 0.151098] [ema: 0.999737] 
[Epoch 32/63] [Batch 700/804] [D loss: 0.394904] [G loss: 0.167453] [ema: 0.999738] 
[Epoch 32/63] [Batch 800/804] [D loss: 0.347867] [G loss: 0.192472] [ema: 0.999739] 
[Epoch 33/63] [Batch 0/804] [D loss: 0.384433] [G loss: 0.174202] [ema: 0.999739] 
[Epoch 33/63] [Batch 100/804] [D loss: 0.412739] [G loss: 0.148207] [ema: 0.999740] 
[Epoch 33/63] [Batch 200/804] [D loss: 0.388939] [G loss: 0.170448] [ema: 0.999741] 
[Epoch 33/63] [Batch 300/804] [D loss: 0.396133] [G loss: 0.163205] [ema: 0.999742] 
[Epoch 33/63] [Batch 400/804] [D loss: 0.366989] [G loss: 0.202212] [ema: 0.999743] 
[Epoch 33/63] [Batch 500/804] [D loss: 0.349456] [G loss: 0.176098] [ema: 0.999744] 
[Epoch 33/63] [Batch 600/804] [D loss: 0.403281] [G loss: 0.172856] [ema: 0.999745] 
[Epoch 33/63] [Batch 700/804] [D loss: 0.366695] [G loss: 0.155247] [ema: 0.999745] 
[Epoch 33/63] [Batch 800/804] [D loss: 0.436655] [G loss: 0.157448] [ema: 0.999746] 
[Epoch 34/63] [Batch 0/804] [D loss: 0.412510] [G loss: 0.179718] [ema: 0.999746] 
[Epoch 34/63] [Batch 100/804] [D loss: 0.407323] [G loss: 0.160341] [ema: 0.999747] 
[Epoch 34/63] [Batch 200/804] [D loss: 0.375391] [G loss: 0.213258] [ema: 0.999748] 
[Epoch 34/63] [Batch 300/804] [D loss: 0.429881] [G loss: 0.200378] [ema: 0.999749] 
[Epoch 34/63] [Batch 400/804] [D loss: 0.414466] [G loss: 0.157308] [ema: 0.999750] 
[Epoch 34/63] [Batch 500/804] [D loss: 0.384549] [G loss: 0.183268] [ema: 0.999751] 
[Epoch 34/63] [Batch 600/804] [D loss: 0.435333] [G loss: 0.217780] [ema: 0.999752] 
[Epoch 34/63] [Batch 700/804] [D loss: 0.392069] [G loss: 0.162607] [ema: 0.999753] 
[Epoch 34/63] [Batch 800/804] [D loss: 0.401305] [G loss: 0.178414] [ema: 0.999754] 
[Epoch 35/63] [Batch 0/804] [D loss: 0.421105] [G loss: 0.169021] [ema: 0.999754] 
[Epoch 35/63] [Batch 100/804] [D loss: 0.396109] [G loss: 0.164110] [ema: 0.999755] 
[Epoch 35/63] [Batch 200/804] [D loss: 0.323845] [G loss: 0.178511] [ema: 0.999755] 
[Epoch 35/63] [Batch 300/804] [D loss: 0.389092] [G loss: 0.212670] [ema: 0.999756] 
[Epoch 35/63] [Batch 400/804] [D loss: 0.417675] [G loss: 0.174231] [ema: 0.999757] 
[Epoch 35/63] [Batch 500/804] [D loss: 0.406960] [G loss: 0.218319] [ema: 0.999758] 
[Epoch 35/63] [Batch 600/804] [D loss: 0.462825] [G loss: 0.186998] [ema: 0.999759] 
[Epoch 35/63] [Batch 700/804] [D loss: 0.424355] [G loss: 0.177362] [ema: 0.999760] 
[Epoch 35/63] [Batch 800/804] [D loss: 0.399261] [G loss: 0.171939] [ema: 0.999761] 
[Epoch 36/63] [Batch 0/804] [D loss: 0.398417] [G loss: 0.158379] [ema: 0.999761] 
[Epoch 36/63] [Batch 100/804] [D loss: 0.413559] [G loss: 0.171602] [ema: 0.999761] 
[Epoch 36/63] [Batch 200/804] [D loss: 0.440365] [G loss: 0.162242] [ema: 0.999762] 
[Epoch 36/63] [Batch 300/804] [D loss: 0.374603] [G loss: 0.170511] [ema: 0.999763] 
[Epoch 36/63] [Batch 400/804] [D loss: 0.432766] [G loss: 0.149355] [ema: 0.999764] 
[Epoch 36/63] [Batch 500/804] [D loss: 0.422286] [G loss: 0.158859] [ema: 0.999765] 
[Epoch 36/63] [Batch 600/804] [D loss: 0.387939] [G loss: 0.184600] [ema: 0.999765] 
[Epoch 36/63] [Batch 700/804] [D loss: 0.353117] [G loss: 0.197968] [ema: 0.999766] 
[Epoch 36/63] [Batch 800/804] [D loss: 0.329967] [G loss: 0.224465] [ema: 0.999767] 
[Epoch 37/63] [Batch 0/804] [D loss: 0.410329] [G loss: 0.158417] [ema: 0.999767] 
[Epoch 37/63] [Batch 100/804] [D loss: 0.366619] [G loss: 0.195433] [ema: 0.999768] 
[Epoch 37/63] [Batch 200/804] [D loss: 0.468621] [G loss: 0.167762] [ema: 0.999769] 
[Epoch 37/63] [Batch 300/804] [D loss: 0.423590] [G loss: 0.192937] [ema: 0.999769] 
[Epoch 37/63] [Batch 400/804] [D loss: 0.431298] [G loss: 0.206348] [ema: 0.999770] 
[Epoch 37/63] [Batch 500/804] [D loss: 0.415691] [G loss: 0.160978] [ema: 0.999771] 
[Epoch 37/63] [Batch 600/804] [D loss: 0.364608] [G loss: 0.180326] [ema: 0.999772] 
[Epoch 37/63] [Batch 700/804] [D loss: 0.362055] [G loss: 0.222104] [ema: 0.999772] 
[Epoch 37/63] [Batch 800/804] [D loss: 0.417456] [G loss: 0.139524] [ema: 0.999773] 
[Epoch 38/63] [Batch 0/804] [D loss: 0.460243] [G loss: 0.163499] [ema: 0.999773] 
[Epoch 38/63] [Batch 100/804] [D loss: 0.380337] [G loss: 0.195095] [ema: 0.999774] 
[Epoch 38/63] [Batch 200/804] [D loss: 0.465165] [G loss: 0.174081] [ema: 0.999775] 
[Epoch 38/63] [Batch 300/804] [D loss: 0.406723] [G loss: 0.209214] [ema: 0.999775] 
[Epoch 38/63] [Batch 400/804] [D loss: 0.358025] [G loss: 0.175308] [ema: 0.999776] 
[Epoch 38/63] [Batch 500/804] [D loss: 0.400062] [G loss: 0.171024] [ema: 0.999777] 
[Epoch 38/63] [Batch 600/804] [D loss: 0.381118] [G loss: 0.220474] [ema: 0.999778] 
[Epoch 38/63] [Batch 700/804] [D loss: 0.436517] [G loss: 0.153369] [ema: 0.999778] 
[Epoch 38/63] [Batch 800/804] [D loss: 0.346998] [G loss: 0.168930] [ema: 0.999779] 
[Epoch 39/63] [Batch 0/804] [D loss: 0.399447] [G loss: 0.185360] [ema: 0.999779] 
[Epoch 39/63] [Batch 100/804] [D loss: 0.404213] [G loss: 0.171632] [ema: 0.999780] 
[Epoch 39/63] [Batch 200/804] [D loss: 0.373015] [G loss: 0.173998] [ema: 0.999780] 
[Epoch 39/63] [Batch 300/804] [D loss: 0.392332] [G loss: 0.173919] [ema: 0.999781] 
[Epoch 39/63] [Batch 400/804] [D loss: 0.434800] [G loss: 0.196513] [ema: 0.999782] 
[Epoch 39/63] [Batch 500/804] [D loss: 0.376533] [G loss: 0.188526] [ema: 0.999782] 
[Epoch 39/63] [Batch 600/804] [D loss: 0.449583] [G loss: 0.176130] [ema: 0.999783] 
[Epoch 39/63] [Batch 700/804] [D loss: 0.507172] [G loss: 0.180380] [ema: 0.999784] 
[Epoch 39/63] [Batch 800/804] [D loss: 0.389796] [G loss: 0.187192] [ema: 0.999784] 
[Epoch 40/63] [Batch 0/804] [D loss: 0.372472] [G loss: 0.203408] [ema: 0.999784] 
[Epoch 40/63] [Batch 100/804] [D loss: 0.454113] [G loss: 0.204255] [ema: 0.999785] 
[Epoch 40/63] [Batch 200/804] [D loss: 0.416386] [G loss: 0.194472] [ema: 0.999786] 
[Epoch 40/63] [Batch 300/804] [D loss: 0.372849] [G loss: 0.202378] [ema: 0.999786] 
[Epoch 40/63] [Batch 400/804] [D loss: 0.425612] [G loss: 0.157049] [ema: 0.999787] 
[Epoch 40/63] [Batch 500/804] [D loss: 0.390455] [G loss: 0.179300] [ema: 0.999788] 
[Epoch 40/63] [Batch 600/804] [D loss: 0.446764] [G loss: 0.185300] [ema: 0.999788] 
[Epoch 40/63] [Batch 700/804] [D loss: 0.410364] [G loss: 0.174180] [ema: 0.999789] 
[Epoch 40/63] [Batch 800/804] [D loss: 0.350038] [G loss: 0.201137] [ema: 0.999790] 
[Epoch 41/63] [Batch 0/804] [D loss: 0.423484] [G loss: 0.219122] [ema: 0.999790] 
[Epoch 41/63] [Batch 100/804] [D loss: 0.357089] [G loss: 0.183479] [ema: 0.999790] 
[Epoch 41/63] [Batch 200/804] [D loss: 0.418170] [G loss: 0.184640] [ema: 0.999791] 
[Epoch 41/63] [Batch 300/804] [D loss: 0.382688] [G loss: 0.172104] [ema: 0.999792] 
[Epoch 41/63] [Batch 400/804] [D loss: 0.408257] [G loss: 0.159297] [ema: 0.999792] 
[Epoch 41/63] [Batch 500/804] [D loss: 0.372548] [G loss: 0.214938] [ema: 0.999793] 
[Epoch 41/63] [Batch 600/804] [D loss: 0.402751] [G loss: 0.177952] [ema: 0.999794] 
[Epoch 41/63] [Batch 700/804] [D loss: 0.408203] [G loss: 0.202766] [ema: 0.999794] 
[Epoch 41/63] [Batch 800/804] [D loss: 0.415669] [G loss: 0.187073] [ema: 0.999795] 
[Epoch 42/63] [Batch 0/804] [D loss: 0.330716] [G loss: 0.184689] [ema: 0.999795] 
[Epoch 42/63] [Batch 100/804] [D loss: 0.365476] [G loss: 0.190172] [ema: 0.999795] 
[Epoch 42/63] [Batch 200/804] [D loss: 0.422866] [G loss: 0.144910] [ema: 0.999796] 
[Epoch 42/63] [Batch 300/804] [D loss: 0.390314] [G loss: 0.198050] [ema: 0.999797] 
[Epoch 42/63] [Batch 400/804] [D loss: 0.446735] [G loss: 0.157551] [ema: 0.999797] 
[Epoch 42/63] [Batch 500/804] [D loss: 0.415753] [G loss: 0.145786] [ema: 0.999798] 
[Epoch 42/63] [Batch 600/804] [D loss: 0.463321] [G loss: 0.154109] [ema: 0.999798] 
[Epoch 42/63] [Batch 700/804] [D loss: 0.362888] [G loss: 0.178556] [ema: 0.999799] 
[Epoch 42/63] [Batch 800/804] [D loss: 0.347101] [G loss: 0.188149] [ema: 0.999800] 
[Epoch 43/63] [Batch 0/804] [D loss: 0.446372] [G loss: 0.162457] [ema: 0.999800] 
[Epoch 43/63] [Batch 100/804] [D loss: 0.408081] [G loss: 0.195530] [ema: 0.999800] 
[Epoch 43/63] [Batch 200/804] [D loss: 0.361323] [G loss: 0.213393] [ema: 0.999801] 
[Epoch 43/63] [Batch 300/804] [D loss: 0.413948] [G loss: 0.168372] [ema: 0.999801] 
[Epoch 43/63] [Batch 400/804] [D loss: 0.406689] [G loss: 0.136619] [ema: 0.999802] 
[Epoch 43/63] [Batch 500/804] [D loss: 0.399770] [G loss: 0.196058] [ema: 0.999802] 
[Epoch 43/63] [Batch 600/804] [D loss: 0.428950] [G loss: 0.187587] [ema: 0.999803] 
[Epoch 43/63] [Batch 700/804] [D loss: 0.376345] [G loss: 0.182224] [ema: 0.999804] 
[Epoch 43/63] [Batch 800/804] [D loss: 0.389822] [G loss: 0.159244] [ema: 0.999804] 
[Epoch 44/63] [Batch 0/804] [D loss: 0.413793] [G loss: 0.157466] [ema: 0.999804] 
[Epoch 44/63] [Batch 100/804] [D loss: 0.399987] [G loss: 0.169648] [ema: 0.999805] 
[Epoch 44/63] [Batch 200/804] [D loss: 0.385546] [G loss: 0.172090] [ema: 0.999805] 
[Epoch 44/63] [Batch 300/804] [D loss: 0.482604] [G loss: 0.173062] [ema: 0.999806] 
[Epoch 44/63] [Batch 400/804] [D loss: 0.373332] [G loss: 0.202763] [ema: 0.999806] 
[Epoch 44/63] [Batch 500/804] [D loss: 0.464424] [G loss: 0.209915] [ema: 0.999807] 
[Epoch 44/63] [Batch 600/804] [D loss: 0.419318] [G loss: 0.193231] [ema: 0.999807] 
[Epoch 44/63] [Batch 700/804] [D loss: 0.452090] [G loss: 0.188352] [ema: 0.999808] 
[Epoch 44/63] [Batch 800/804] [D loss: 0.349162] [G loss: 0.131474] [ema: 0.999808] 
[Epoch 45/63] [Batch 0/804] [D loss: 0.434894] [G loss: 0.163774] [ema: 0.999808] 
[Epoch 45/63] [Batch 100/804] [D loss: 0.404324] [G loss: 0.176392] [ema: 0.999809] 
[Epoch 45/63] [Batch 200/804] [D loss: 0.376708] [G loss: 0.179955] [ema: 0.999809] 
[Epoch 45/63] [Batch 300/804] [D loss: 0.424209] [G loss: 0.177919] [ema: 0.999810] 
[Epoch 45/63] [Batch 400/804] [D loss: 0.423793] [G loss: 0.172669] [ema: 0.999811] 
[Epoch 45/63] [Batch 500/804] [D loss: 0.387973] [G loss: 0.184320] [ema: 0.999811] 
[Epoch 45/63] [Batch 600/804] [D loss: 0.424913] [G loss: 0.179627] [ema: 0.999812] 
[Epoch 45/63] [Batch 700/804] [D loss: 0.439188] [G loss: 0.183942] [ema: 0.999812] 
[Epoch 45/63] [Batch 800/804] [D loss: 0.406338] [G loss: 0.181160] [ema: 0.999813] 
[Epoch 46/63] [Batch 0/804] [D loss: 0.371083] [G loss: 0.182686] [ema: 0.999813] 
[Epoch 46/63] [Batch 100/804] [D loss: 0.436850] [G loss: 0.172680] [ema: 0.999813] 
[Epoch 46/63] [Batch 200/804] [D loss: 0.384473] [G loss: 0.221578] [ema: 0.999814] 
[Epoch 46/63] [Batch 300/804] [D loss: 0.356464] [G loss: 0.204441] [ema: 0.999814] 
[Epoch 46/63] [Batch 400/804] [D loss: 0.397196] [G loss: 0.228933] [ema: 0.999815] 
[Epoch 46/63] [Batch 500/804] [D loss: 0.411320] [G loss: 0.168117] [ema: 0.999815] 
[Epoch 46/63] [Batch 600/804] [D loss: 0.409891] [G loss: 0.195086] [ema: 0.999816] 
[Epoch 46/63] [Batch 700/804] [D loss: 0.426422] [G loss: 0.161675] [ema: 0.999816] 
[Epoch 46/63] [Batch 800/804] [D loss: 0.374090] [G loss: 0.197185] [ema: 0.999817] 
[Epoch 47/63] [Batch 0/804] [D loss: 0.407512] [G loss: 0.204982] [ema: 0.999817] 
[Epoch 47/63] [Batch 100/804] [D loss: 0.368912] [G loss: 0.166410] [ema: 0.999817] 
[Epoch 47/63] [Batch 200/804] [D loss: 0.403443] [G loss: 0.178953] [ema: 0.999818] 
[Epoch 47/63] [Batch 300/804] [D loss: 0.371096] [G loss: 0.202809] [ema: 0.999818] 
[Epoch 47/63] [Batch 400/804] [D loss: 0.368702] [G loss: 0.204066] [ema: 0.999819] 
[Epoch 47/63] [Batch 500/804] [D loss: 0.365991] [G loss: 0.180047] [ema: 0.999819] 
[Epoch 47/63] [Batch 600/804] [D loss: 0.409243] [G loss: 0.193132] [ema: 0.999819] 
[Epoch 47/63] [Batch 700/804] [D loss: 0.411372] [G loss: 0.187379] [ema: 0.999820] 
[Epoch 47/63] [Batch 800/804] [D loss: 0.451403] [G loss: 0.158480] [ema: 0.999820] 



Saving checkpoint 4 in logs/daghar_50000_30_100/upstairs_50000_D_30_2024_10_18_01_07_13/Model



[Epoch 48/63] [Batch 0/804] [D loss: 0.440184] [G loss: 0.196343] [ema: 0.999820] 
[Epoch 48/63] [Batch 100/804] [D loss: 0.424038] [G loss: 0.159866] [ema: 0.999821] 
[Epoch 48/63] [Batch 200/804] [D loss: 0.408541] [G loss: 0.184458] [ema: 0.999821] 
[Epoch 48/63] [Batch 300/804] [D loss: 0.374429] [G loss: 0.174369] [ema: 0.999822] 
[Epoch 48/63] [Batch 400/804] [D loss: 0.391263] [G loss: 0.190214] [ema: 0.999822] 
[Epoch 48/63] [Batch 500/804] [D loss: 0.384552] [G loss: 0.199179] [ema: 0.999823] 
[Epoch 48/63] [Batch 600/804] [D loss: 0.414059] [G loss: 0.149641] [ema: 0.999823] 
[Epoch 48/63] [Batch 700/804] [D loss: 0.403107] [G loss: 0.183634] [ema: 0.999824] 
[Epoch 48/63] [Batch 800/804] [D loss: 0.471924] [G loss: 0.163383] [ema: 0.999824] 
[Epoch 49/63] [Batch 0/804] [D loss: 0.475121] [G loss: 0.200359] [ema: 0.999824] 
[Epoch 49/63] [Batch 100/804] [D loss: 0.419674] [G loss: 0.198172] [ema: 0.999825] 
[Epoch 49/63] [Batch 200/804] [D loss: 0.379478] [G loss: 0.188829] [ema: 0.999825] 
[Epoch 49/63] [Batch 300/804] [D loss: 0.362760] [G loss: 0.179814] [ema: 0.999825] 
[Epoch 49/63] [Batch 400/804] [D loss: 0.396826] [G loss: 0.176354] [ema: 0.999826] 
[Epoch 49/63] [Batch 500/804] [D loss: 0.466820] [G loss: 0.187489] [ema: 0.999826] 
[Epoch 49/63] [Batch 600/804] [D loss: 0.422944] [G loss: 0.180861] [ema: 0.999827] 
[Epoch 49/63] [Batch 700/804] [D loss: 0.420909] [G loss: 0.197271] [ema: 0.999827] 
[Epoch 49/63] [Batch 800/804] [D loss: 0.478595] [G loss: 0.190608] [ema: 0.999828] 
[Epoch 50/63] [Batch 0/804] [D loss: 0.465847] [G loss: 0.195563] [ema: 0.999828] 
[Epoch 50/63] [Batch 100/804] [D loss: 0.333601] [G loss: 0.173589] [ema: 0.999828] 
[Epoch 50/63] [Batch 200/804] [D loss: 0.487128] [G loss: 0.195764] [ema: 0.999828] 
[Epoch 50/63] [Batch 300/804] [D loss: 0.350091] [G loss: 0.184613] [ema: 0.999829] 
[Epoch 50/63] [Batch 400/804] [D loss: 0.385333] [G loss: 0.198517] [ema: 0.999829] 
[Epoch 50/63] [Batch 500/804] [D loss: 0.398602] [G loss: 0.163404] [ema: 0.999830] 
[Epoch 50/63] [Batch 600/804] [D loss: 0.451356] [G loss: 0.197194] [ema: 0.999830] 
[Epoch 50/63] [Batch 700/804] [D loss: 0.387819] [G loss: 0.178887] [ema: 0.999831] 
[Epoch 50/63] [Batch 800/804] [D loss: 0.411069] [G loss: 0.169523] [ema: 0.999831] 
[Epoch 51/63] [Batch 0/804] [D loss: 0.357636] [G loss: 0.194903] [ema: 0.999831] 
[Epoch 51/63] [Batch 100/804] [D loss: 0.356055] [G loss: 0.201666] [ema: 0.999831] 
[Epoch 51/63] [Batch 200/804] [D loss: 0.488028] [G loss: 0.207988] [ema: 0.999832] 
[Epoch 51/63] [Batch 300/804] [D loss: 0.441542] [G loss: 0.183932] [ema: 0.999832] 
[Epoch 51/63] [Batch 400/804] [D loss: 0.407842] [G loss: 0.162832] [ema: 0.999833] 
[Epoch 51/63] [Batch 500/804] [D loss: 0.391125] [G loss: 0.177673] [ema: 0.999833] 
[Epoch 51/63] [Batch 600/804] [D loss: 0.397737] [G loss: 0.182049] [ema: 0.999833] 
[Epoch 51/63] [Batch 700/804] [D loss: 0.392588] [G loss: 0.187996] [ema: 0.999834] 
[Epoch 51/63] [Batch 800/804] [D loss: 0.436135] [G loss: 0.171334] [ema: 0.999834] 
[Epoch 52/63] [Batch 0/804] [D loss: 0.491433] [G loss: 0.179371] [ema: 0.999834] 
[Epoch 52/63] [Batch 100/804] [D loss: 0.364363] [G loss: 0.195203] [ema: 0.999835] 
[Epoch 52/63] [Batch 200/804] [D loss: 0.385038] [G loss: 0.187291] [ema: 0.999835] 
[Epoch 52/63] [Batch 300/804] [D loss: 0.429482] [G loss: 0.185073] [ema: 0.999835] 
[Epoch 52/63] [Batch 400/804] [D loss: 0.331739] [G loss: 0.146273] [ema: 0.999836] 
[Epoch 52/63] [Batch 500/804] [D loss: 0.438790] [G loss: 0.200390] [ema: 0.999836] 
[Epoch 52/63] [Batch 600/804] [D loss: 0.369064] [G loss: 0.165582] [ema: 0.999837] 
[Epoch 52/63] [Batch 700/804] [D loss: 0.485246] [G loss: 0.187654] [ema: 0.999837] 
[Epoch 52/63] [Batch 800/804] [D loss: 0.473196] [G loss: 0.181865] [ema: 0.999837] 
[Epoch 53/63] [Batch 0/804] [D loss: 0.352006] [G loss: 0.188633] [ema: 0.999837] 
[Epoch 53/63] [Batch 100/804] [D loss: 0.429963] [G loss: 0.175739] [ema: 0.999838] 
[Epoch 53/63] [Batch 200/804] [D loss: 0.345612] [G loss: 0.198019] [ema: 0.999838] 
[Epoch 53/63] [Batch 300/804] [D loss: 0.454419] [G loss: 0.196463] [ema: 0.999838] 
[Epoch 53/63] [Batch 400/804] [D loss: 0.481691] [G loss: 0.149621] [ema: 0.999839] 
[Epoch 53/63] [Batch 500/804] [D loss: 0.515800] [G loss: 0.153718] [ema: 0.999839] 
[Epoch 53/63] [Batch 600/804] [D loss: 0.419084] [G loss: 0.180341] [ema: 0.999840] 
[Epoch 53/63] [Batch 700/804] [D loss: 0.389867] [G loss: 0.171237] [ema: 0.999840] 
[Epoch 53/63] [Batch 800/804] [D loss: 0.388054] [G loss: 0.177807] [ema: 0.999840] 
[Epoch 54/63] [Batch 0/804] [D loss: 0.409406] [G loss: 0.185056] [ema: 0.999840] 
[Epoch 54/63] [Batch 100/804] [D loss: 0.342355] [G loss: 0.199612] [ema: 0.999841] 
[Epoch 54/63] [Batch 200/804] [D loss: 0.369995] [G loss: 0.184730] [ema: 0.999841] 
[Epoch 54/63] [Batch 300/804] [D loss: 0.451721] [G loss: 0.191288] [ema: 0.999841] 
[Epoch 54/63] [Batch 400/804] [D loss: 0.419518] [G loss: 0.182021] [ema: 0.999842] 
[Epoch 54/63] [Batch 500/804] [D loss: 0.387519] [G loss: 0.171178] [ema: 0.999842] 
[Epoch 54/63] [Batch 600/804] [D loss: 0.412703] [G loss: 0.194561] [ema: 0.999843] 
[Epoch 54/63] [Batch 700/804] [D loss: 0.366631] [G loss: 0.178678] [ema: 0.999843] 
[Epoch 54/63] [Batch 800/804] [D loss: 0.375772] [G loss: 0.169931] [ema: 0.999843] 
[Epoch 55/63] [Batch 0/804] [D loss: 0.374041] [G loss: 0.192541] [ema: 0.999843] 
[Epoch 55/63] [Batch 100/804] [D loss: 0.435379] [G loss: 0.204875] [ema: 0.999844] 
[Epoch 55/63] [Batch 200/804] [D loss: 0.394988] [G loss: 0.181596] [ema: 0.999844] 
[Epoch 55/63] [Batch 300/804] [D loss: 0.471251] [G loss: 0.184367] [ema: 0.999844] 
[Epoch 55/63] [Batch 400/804] [D loss: 0.458746] [G loss: 0.203896] [ema: 0.999845] 
[Epoch 55/63] [Batch 500/804] [D loss: 0.429391] [G loss: 0.173245] [ema: 0.999845] 
[Epoch 55/63] [Batch 600/804] [D loss: 0.384059] [G loss: 0.196614] [ema: 0.999845] 
[Epoch 55/63] [Batch 700/804] [D loss: 0.352853] [G loss: 0.179470] [ema: 0.999846] 
[Epoch 55/63] [Batch 800/804] [D loss: 0.367393] [G loss: 0.148784] [ema: 0.999846] 
[Epoch 56/63] [Batch 0/804] [D loss: 0.411894] [G loss: 0.163204] [ema: 0.999846] 
[Epoch 56/63] [Batch 100/804] [D loss: 0.454235] [G loss: 0.201946] [ema: 0.999846] 
[Epoch 56/63] [Batch 200/804] [D loss: 0.428574] [G loss: 0.169212] [ema: 0.999847] 
[Epoch 56/63] [Batch 300/804] [D loss: 0.389729] [G loss: 0.214451] [ema: 0.999847] 
[Epoch 56/63] [Batch 400/804] [D loss: 0.384838] [G loss: 0.180142] [ema: 0.999847] 
[Epoch 56/63] [Batch 500/804] [D loss: 0.369381] [G loss: 0.186338] [ema: 0.999848] 
[Epoch 56/63] [Batch 600/804] [D loss: 0.452658] [G loss: 0.164634] [ema: 0.999848] 
[Epoch 56/63] [Batch 700/804] [D loss: 0.389304] [G loss: 0.182589] [ema: 0.999848] 
[Epoch 56/63] [Batch 800/804] [D loss: 0.421839] [G loss: 0.193884] [ema: 0.999849] 
[Epoch 57/63] [Batch 0/804] [D loss: 0.406945] [G loss: 0.157793] [ema: 0.999849] 
[Epoch 57/63] [Batch 100/804] [D loss: 0.401624] [G loss: 0.182528] [ema: 0.999849] 
[Epoch 57/63] [Batch 200/804] [D loss: 0.384187] [G loss: 0.195640] [ema: 0.999849] 
[Epoch 57/63] [Batch 300/804] [D loss: 0.364209] [G loss: 0.205275] [ema: 0.999850] 
[Epoch 57/63] [Batch 400/804] [D loss: 0.432913] [G loss: 0.195869] [ema: 0.999850] 
[Epoch 57/63] [Batch 500/804] [D loss: 0.363779] [G loss: 0.181332] [ema: 0.999850] 
[Epoch 57/63] [Batch 600/804] [D loss: 0.401164] [G loss: 0.236222] [ema: 0.999851] 
[Epoch 57/63] [Batch 700/804] [D loss: 0.436930] [G loss: 0.166488] [ema: 0.999851] 
[Epoch 57/63] [Batch 800/804] [D loss: 0.407341] [G loss: 0.183326] [ema: 0.999851] 
[Epoch 58/63] [Batch 0/804] [D loss: 0.391720] [G loss: 0.175794] [ema: 0.999851] 
[Epoch 58/63] [Batch 100/804] [D loss: 0.451242] [G loss: 0.200700] [ema: 0.999852] 
[Epoch 58/63] [Batch 200/804] [D loss: 0.471567] [G loss: 0.153310] [ema: 0.999852] 
[Epoch 58/63] [Batch 300/804] [D loss: 0.435075] [G loss: 0.168048] [ema: 0.999852] 
[Epoch 58/63] [Batch 400/804] [D loss: 0.409815] [G loss: 0.196520] [ema: 0.999853] 
[Epoch 58/63] [Batch 500/804] [D loss: 0.371191] [G loss: 0.220788] [ema: 0.999853] 
[Epoch 58/63] [Batch 600/804] [D loss: 0.382502] [G loss: 0.183398] [ema: 0.999853] 
[Epoch 58/63] [Batch 700/804] [D loss: 0.364772] [G loss: 0.200427] [ema: 0.999854] 
[Epoch 58/63] [Batch 800/804] [D loss: 0.387720] [G loss: 0.167690] [ema: 0.999854] 
[Epoch 59/63] [Batch 0/804] [D loss: 0.379902] [G loss: 0.202254] [ema: 0.999854] 
[Epoch 59/63] [Batch 100/804] [D loss: 0.413421] [G loss: 0.152331] [ema: 0.999854] 
[Epoch 59/63] [Batch 200/804] [D loss: 0.395222] [G loss: 0.142782] [ema: 0.999855] 
[Epoch 59/63] [Batch 300/804] [D loss: 0.432222] [G loss: 0.173733] [ema: 0.999855] 
[Epoch 59/63] [Batch 400/804] [D loss: 0.359845] [G loss: 0.203656] [ema: 0.999855] 
[Epoch 59/63] [Batch 500/804] [D loss: 0.399745] [G loss: 0.188135] [ema: 0.999855] 
[Epoch 59/63] [Batch 600/804] [D loss: 0.423823] [G loss: 0.181988] [ema: 0.999856] 
[Epoch 59/63] [Batch 700/804] [D loss: 0.425894] [G loss: 0.177738] [ema: 0.999856] 
[Epoch 59/63] [Batch 800/804] [D loss: 0.414252] [G loss: 0.153366] [ema: 0.999856] 
[Epoch 60/63] [Batch 0/804] [D loss: 0.435495] [G loss: 0.152065] [ema: 0.999856] 
[Epoch 60/63] [Batch 100/804] [D loss: 0.359059] [G loss: 0.193572] [ema: 0.999857] 
[Epoch 60/63] [Batch 200/804] [D loss: 0.467961] [G loss: 0.165973] [ema: 0.999857] 
[Epoch 60/63] [Batch 300/804] [D loss: 0.341479] [G loss: 0.194564] [ema: 0.999857] 
[Epoch 60/63] [Batch 400/804] [D loss: 0.388360] [G loss: 0.176469] [ema: 0.999858] 
[Epoch 60/63] [Batch 500/804] [D loss: 0.390526] [G loss: 0.157226] [ema: 0.999858] 
[Epoch 60/63] [Batch 600/804] [D loss: 0.364165] [G loss: 0.171339] [ema: 0.999858] 
[Epoch 60/63] [Batch 700/804] [D loss: 0.397364] [G loss: 0.160422] [ema: 0.999858] 
[Epoch 60/63] [Batch 800/804] [D loss: 0.480479] [G loss: 0.191209] [ema: 0.999859] 
[Epoch 61/63] [Batch 0/804] [D loss: 0.359750] [G loss: 0.182097] [ema: 0.999859] 
[Epoch 61/63] [Batch 100/804] [D loss: 0.404736] [G loss: 0.178037] [ema: 0.999859] 
[Epoch 61/63] [Batch 200/804] [D loss: 0.448173] [G loss: 0.166217] [ema: 0.999859] 
[Epoch 61/63] [Batch 300/804] [D loss: 0.434914] [G loss: 0.197658] [ema: 0.999860] 
[Epoch 61/63] [Batch 400/804] [D loss: 0.475095] [G loss: 0.181084] [ema: 0.999860] 
[Epoch 61/63] [Batch 500/804] [D loss: 0.468464] [G loss: 0.220275] [ema: 0.999860] 
[Epoch 61/63] [Batch 600/804] [D loss: 0.361903] [G loss: 0.181349] [ema: 0.999860] 
[Epoch 61/63] [Batch 700/804] [D loss: 0.389793] [G loss: 0.180171] [ema: 0.999861] 
[Epoch 61/63] [Batch 800/804] [D loss: 0.420471] [G loss: 0.155421] [ema: 0.999861] 
[Epoch 62/63] [Batch 0/804] [D loss: 0.459134] [G loss: 0.149310] [ema: 0.999861] 
[Epoch 62/63] [Batch 100/804] [D loss: 0.420681] [G loss: 0.176239] [ema: 0.999861] 
[Epoch 62/63] [Batch 200/804] [D loss: 0.423603] [G loss: 0.204429] [ema: 0.999862] 
[Epoch 62/63] [Batch 300/804] [D loss: 0.455687] [G loss: 0.175960] [ema: 0.999862] 
[Epoch 62/63] [Batch 400/804] [D loss: 0.446506] [G loss: 0.204242] [ema: 0.999862] 
[Epoch 62/63] [Batch 500/804] [D loss: 0.365462] [G loss: 0.177163] [ema: 0.999862] 
[Epoch 62/63] [Batch 600/804] [D loss: 0.411801] [G loss: 0.149595] [ema: 0.999863] 
[Epoch 62/63] [Batch 700/804] [D loss: 0.338290] [G loss: 0.203734] [ema: 0.999863] 
[Epoch 62/63] [Batch 800/804] [D loss: 0.394044] [G loss: 0.169809] [ema: 0.999863] 

----------------------------------------------------------------------------------------------------

 Starting individual training
run training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
run
daghar
return single class data and labels, class is run
data shape is (16260, 3, 1, 30)
label shape is (16260,)
1017
Epochs between checkpoint: 13



Saving checkpoint 1 in logs/daghar_50000_30_100/run_50000_D_30_2024_10_18_01_39_30/Model



[Epoch 0/50] [Batch 0/1017] [D loss: 1.272794] [G loss: 1.010274] [ema: 0.000000] 
[Epoch 0/50] [Batch 100/1017] [D loss: 0.410043] [G loss: 0.219422] [ema: 0.933033] 
[Epoch 0/50] [Batch 200/1017] [D loss: 0.365411] [G loss: 0.199840] [ema: 0.965936] 
[Epoch 0/50] [Batch 300/1017] [D loss: 0.287741] [G loss: 0.251247] [ema: 0.977160] 
[Epoch 0/50] [Batch 400/1017] [D loss: 0.257820] [G loss: 0.231142] [ema: 0.982821] 
[Epoch 0/50] [Batch 500/1017] [D loss: 0.336992] [G loss: 0.211910] [ema: 0.986233] 
[Epoch 0/50] [Batch 600/1017] [D loss: 0.346558] [G loss: 0.228376] [ema: 0.988514] 
[Epoch 0/50] [Batch 700/1017] [D loss: 0.331683] [G loss: 0.224714] [ema: 0.990147] 
[Epoch 0/50] [Batch 800/1017] [D loss: 0.390219] [G loss: 0.211806] [ema: 0.991373] 
[Epoch 0/50] [Batch 900/1017] [D loss: 0.317506] [G loss: 0.212581] [ema: 0.992328] 
[Epoch 0/50] [Batch 1000/1017] [D loss: 0.384947] [G loss: 0.157056] [ema: 0.993092] 
[Epoch 1/50] [Batch 0/1017] [D loss: 0.333974] [G loss: 0.231515] [ema: 0.993208] 
[Epoch 1/50] [Batch 100/1017] [D loss: 0.379739] [G loss: 0.218705] [ema: 0.993814] 
[Epoch 1/50] [Batch 200/1017] [D loss: 0.368522] [G loss: 0.187076] [ema: 0.994321] 
[Epoch 1/50] [Batch 300/1017] [D loss: 0.369026] [G loss: 0.212078] [ema: 0.994751] 
[Epoch 1/50] [Batch 400/1017] [D loss: 0.351196] [G loss: 0.201568] [ema: 0.995120] 
[Epoch 1/50] [Batch 500/1017] [D loss: 0.291263] [G loss: 0.250200] [ema: 0.995441] 
[Epoch 1/50] [Batch 600/1017] [D loss: 0.276918] [G loss: 0.193876] [ema: 0.995723] 
[Epoch 1/50] [Batch 700/1017] [D loss: 0.368779] [G loss: 0.236690] [ema: 0.995971] 
[Epoch 1/50] [Batch 800/1017] [D loss: 0.357652] [G loss: 0.237918] [ema: 0.996192] 
[Epoch 1/50] [Batch 900/1017] [D loss: 0.346925] [G loss: 0.191275] [ema: 0.996391] 
[Epoch 1/50] [Batch 1000/1017] [D loss: 0.424205] [G loss: 0.215948] [ema: 0.996569] 
[Epoch 2/50] [Batch 0/1017] [D loss: 0.441565] [G loss: 0.181230] [ema: 0.996598] 
[Epoch 2/50] [Batch 100/1017] [D loss: 0.425284] [G loss: 0.159580] [ema: 0.996757] 
[Epoch 2/50] [Batch 200/1017] [D loss: 0.386654] [G loss: 0.182855] [ema: 0.996902] 
[Epoch 2/50] [Batch 300/1017] [D loss: 0.355978] [G loss: 0.179483] [ema: 0.997035] 
[Epoch 2/50] [Batch 400/1017] [D loss: 0.368077] [G loss: 0.148103] [ema: 0.997156] 
[Epoch 2/50] [Batch 500/1017] [D loss: 0.424323] [G loss: 0.170683] [ema: 0.997268] 
[Epoch 2/50] [Batch 600/1017] [D loss: 0.375109] [G loss: 0.191053] [ema: 0.997372] 
[Epoch 2/50] [Batch 700/1017] [D loss: 0.386975] [G loss: 0.182863] [ema: 0.997468] 
[Epoch 2/50] [Batch 800/1017] [D loss: 0.346779] [G loss: 0.204255] [ema: 0.997557] 
[Epoch 2/50] [Batch 900/1017] [D loss: 0.362004] [G loss: 0.192867] [ema: 0.997640] 
[Epoch 2/50] [Batch 1000/1017] [D loss: 0.353749] [G loss: 0.201052] [ema: 0.997718] 
[Epoch 3/50] [Batch 0/1017] [D loss: 0.394530] [G loss: 0.184754] [ema: 0.997731] 
[Epoch 3/50] [Batch 100/1017] [D loss: 0.399740] [G loss: 0.197034] [ema: 0.997803] 
[Epoch 3/50] [Batch 200/1017] [D loss: 0.405910] [G loss: 0.178396] [ema: 0.997870] 
[Epoch 3/50] [Batch 300/1017] [D loss: 0.455951] [G loss: 0.224822] [ema: 0.997934] 
[Epoch 3/50] [Batch 400/1017] [D loss: 0.453655] [G loss: 0.179817] [ema: 0.997993] 
[Epoch 3/50] [Batch 500/1017] [D loss: 0.363667] [G loss: 0.170454] [ema: 0.998050] 
[Epoch 3/50] [Batch 600/1017] [D loss: 0.392027] [G loss: 0.210180] [ema: 0.998103] 
[Epoch 3/50] [Batch 700/1017] [D loss: 0.501518] [G loss: 0.180797] [ema: 0.998154] 
[Epoch 3/50] [Batch 800/1017] [D loss: 0.431854] [G loss: 0.172817] [ema: 0.998202] 
[Epoch 3/50] [Batch 900/1017] [D loss: 0.458664] [G loss: 0.205214] [ema: 0.998247] 
[Epoch 3/50] [Batch 1000/1017] [D loss: 0.421707] [G loss: 0.149024] [ema: 0.998290] 
[Epoch 4/50] [Batch 0/1017] [D loss: 0.426704] [G loss: 0.222154] [ema: 0.998298] 
[Epoch 4/50] [Batch 100/1017] [D loss: 0.391109] [G loss: 0.185728] [ema: 0.998338] 
[Epoch 4/50] [Batch 200/1017] [D loss: 0.464455] [G loss: 0.177181] [ema: 0.998377] 
[Epoch 4/50] [Batch 300/1017] [D loss: 0.490482] [G loss: 0.177136] [ema: 0.998414] 
[Epoch 4/50] [Batch 400/1017] [D loss: 0.465548] [G loss: 0.191796] [ema: 0.998450] 
[Epoch 4/50] [Batch 500/1017] [D loss: 0.351576] [G loss: 0.214703] [ema: 0.998484] 
[Epoch 4/50] [Batch 600/1017] [D loss: 0.374297] [G loss: 0.193732] [ema: 0.998516] 
[Epoch 4/50] [Batch 700/1017] [D loss: 0.314928] [G loss: 0.189797] [ema: 0.998547] 
[Epoch 4/50] [Batch 800/1017] [D loss: 0.380962] [G loss: 0.166065] [ema: 0.998577] 
[Epoch 4/50] [Batch 900/1017] [D loss: 0.457957] [G loss: 0.130769] [ema: 0.998606] 
[Epoch 4/50] [Batch 1000/1017] [D loss: 0.431896] [G loss: 0.135162] [ema: 0.998633] 
[Epoch 5/50] [Batch 0/1017] [D loss: 0.503981] [G loss: 0.163073] [ema: 0.998638] 
[Epoch 5/50] [Batch 100/1017] [D loss: 0.477244] [G loss: 0.192225] [ema: 0.998664] 
[Epoch 5/50] [Batch 200/1017] [D loss: 0.359407] [G loss: 0.192970] [ema: 0.998689] 
[Epoch 5/50] [Batch 300/1017] [D loss: 0.418204] [G loss: 0.165422] [ema: 0.998714] 
[Epoch 5/50] [Batch 400/1017] [D loss: 0.373637] [G loss: 0.219611] [ema: 0.998737] 
[Epoch 5/50] [Batch 500/1017] [D loss: 0.448567] [G loss: 0.189129] [ema: 0.998760] 
[Epoch 5/50] [Batch 600/1017] [D loss: 0.419268] [G loss: 0.194931] [ema: 0.998781] 
[Epoch 5/50] [Batch 700/1017] [D loss: 0.385703] [G loss: 0.153803] [ema: 0.998803] 
[Epoch 5/50] [Batch 800/1017] [D loss: 0.313188] [G loss: 0.222538] [ema: 0.998823] 
[Epoch 5/50] [Batch 900/1017] [D loss: 0.341518] [G loss: 0.185828] [ema: 0.998843] 
[Epoch 5/50] [Batch 1000/1017] [D loss: 0.405762] [G loss: 0.154475] [ema: 0.998862] 
[Epoch 6/50] [Batch 0/1017] [D loss: 0.482855] [G loss: 0.171225] [ema: 0.998865] 
[Epoch 6/50] [Batch 100/1017] [D loss: 0.350588] [G loss: 0.206440] [ema: 0.998883] 
[Epoch 6/50] [Batch 200/1017] [D loss: 0.329132] [G loss: 0.196427] [ema: 0.998901] 
[Epoch 6/50] [Batch 300/1017] [D loss: 0.355954] [G loss: 0.178648] [ema: 0.998918] 
[Epoch 6/50] [Batch 400/1017] [D loss: 0.454473] [G loss: 0.152821] [ema: 0.998935] 
[Epoch 6/50] [Batch 500/1017] [D loss: 0.392600] [G loss: 0.199844] [ema: 0.998951] 
[Epoch 6/50] [Batch 600/1017] [D loss: 0.345635] [G loss: 0.167206] [ema: 0.998966] 
[Epoch 6/50] [Batch 700/1017] [D loss: 0.460447] [G loss: 0.164602] [ema: 0.998981] 
[Epoch 6/50] [Batch 800/1017] [D loss: 0.398478] [G loss: 0.212771] [ema: 0.998996] 
[Epoch 6/50] [Batch 900/1017] [D loss: 0.376712] [G loss: 0.212292] [ema: 0.999011] 
[Epoch 6/50] [Batch 1000/1017] [D loss: 0.361976] [G loss: 0.216522] [ema: 0.999024] 
[Epoch 7/50] [Batch 0/1017] [D loss: 0.315464] [G loss: 0.221282] [ema: 0.999027] 
[Epoch 7/50] [Batch 100/1017] [D loss: 0.309179] [G loss: 0.197292] [ema: 0.999040] 
[Epoch 7/50] [Batch 200/1017] [D loss: 0.457010] [G loss: 0.153572] [ema: 0.999053] 
[Epoch 7/50] [Batch 300/1017] [D loss: 0.412227] [G loss: 0.123506] [ema: 0.999066] 
[Epoch 7/50] [Batch 400/1017] [D loss: 0.413850] [G loss: 0.194041] [ema: 0.999079] 
[Epoch 7/50] [Batch 500/1017] [D loss: 0.336865] [G loss: 0.203863] [ema: 0.999091] 
[Epoch 7/50] [Batch 600/1017] [D loss: 0.370829] [G loss: 0.194906] [ema: 0.999102] 
[Epoch 7/50] [Batch 700/1017] [D loss: 0.463654] [G loss: 0.198716] [ema: 0.999114] 
[Epoch 7/50] [Batch 800/1017] [D loss: 0.341664] [G loss: 0.175081] [ema: 0.999125] 
[Epoch 7/50] [Batch 900/1017] [D loss: 0.398460] [G loss: 0.210913] [ema: 0.999136] 
[Epoch 7/50] [Batch 1000/1017] [D loss: 0.346160] [G loss: 0.224347] [ema: 0.999147] 
[Epoch 8/50] [Batch 0/1017] [D loss: 0.353945] [G loss: 0.193574] [ema: 0.999148] 
[Epoch 8/50] [Batch 100/1017] [D loss: 0.409291] [G loss: 0.163601] [ema: 0.999159] 
[Epoch 8/50] [Batch 200/1017] [D loss: 0.436541] [G loss: 0.198287] [ema: 0.999169] 
[Epoch 8/50] [Batch 300/1017] [D loss: 0.349693] [G loss: 0.204921] [ema: 0.999179] 
[Epoch 8/50] [Batch 400/1017] [D loss: 0.354522] [G loss: 0.235744] [ema: 0.999188] 
[Epoch 8/50] [Batch 500/1017] [D loss: 0.392355] [G loss: 0.192902] [ema: 0.999198] 
[Epoch 8/50] [Batch 600/1017] [D loss: 0.408130] [G loss: 0.188340] [ema: 0.999207] 
[Epoch 8/50] [Batch 700/1017] [D loss: 0.383651] [G loss: 0.200002] [ema: 0.999216] 
[Epoch 8/50] [Batch 800/1017] [D loss: 0.288728] [G loss: 0.250820] [ema: 0.999225] 
[Epoch 8/50] [Batch 900/1017] [D loss: 0.389260] [G loss: 0.184373] [ema: 0.999233] 
[Epoch 8/50] [Batch 1000/1017] [D loss: 0.469917] [G loss: 0.180099] [ema: 0.999242] 
[Epoch 9/50] [Batch 0/1017] [D loss: 0.441301] [G loss: 0.156208] [ema: 0.999243] 
[Epoch 9/50] [Batch 100/1017] [D loss: 0.387541] [G loss: 0.229731] [ema: 0.999251] 
[Epoch 9/50] [Batch 200/1017] [D loss: 0.326975] [G loss: 0.187580] [ema: 0.999259] 
[Epoch 9/50] [Batch 300/1017] [D loss: 0.448963] [G loss: 0.166010] [ema: 0.999267] 
[Epoch 9/50] [Batch 400/1017] [D loss: 0.447624] [G loss: 0.173306] [ema: 0.999275] 
[Epoch 9/50] [Batch 500/1017] [D loss: 0.376653] [G loss: 0.201382] [ema: 0.999282] 
[Epoch 9/50] [Batch 600/1017] [D loss: 0.349028] [G loss: 0.208466] [ema: 0.999290] 
[Epoch 9/50] [Batch 700/1017] [D loss: 0.343861] [G loss: 0.213276] [ema: 0.999297] 
[Epoch 9/50] [Batch 800/1017] [D loss: 0.403863] [G loss: 0.145432] [ema: 0.999304] 
[Epoch 9/50] [Batch 900/1017] [D loss: 0.418818] [G loss: 0.175588] [ema: 0.999311] 
[Epoch 9/50] [Batch 1000/1017] [D loss: 0.380883] [G loss: 0.188878] [ema: 0.999318] 
[Epoch 10/50] [Batch 0/1017] [D loss: 0.406455] [G loss: 0.225055] [ema: 0.999319] 
[Epoch 10/50] [Batch 100/1017] [D loss: 0.314184] [G loss: 0.235154] [ema: 0.999325] 
[Epoch 10/50] [Batch 200/1017] [D loss: 0.362444] [G loss: 0.218699] [ema: 0.999332] 
[Epoch 10/50] [Batch 300/1017] [D loss: 0.390104] [G loss: 0.182877] [ema: 0.999338] 
[Epoch 10/50] [Batch 400/1017] [D loss: 0.381377] [G loss: 0.191350] [ema: 0.999344] 
[Epoch 10/50] [Batch 500/1017] [D loss: 0.377867] [G loss: 0.185455] [ema: 0.999351] 
[Epoch 10/50] [Batch 600/1017] [D loss: 0.335445] [G loss: 0.204960] [ema: 0.999357] 
[Epoch 10/50] [Batch 700/1017] [D loss: 0.328106] [G loss: 0.181555] [ema: 0.999363] 
[Epoch 10/50] [Batch 800/1017] [D loss: 0.389601] [G loss: 0.210574] [ema: 0.999368] 
[Epoch 10/50] [Batch 900/1017] [D loss: 0.414251] [G loss: 0.181704] [ema: 0.999374] 
[Epoch 10/50] [Batch 1000/1017] [D loss: 0.438090] [G loss: 0.155590] [ema: 0.999380] 
[Epoch 11/50] [Batch 0/1017] [D loss: 0.396560] [G loss: 0.173513] [ema: 0.999381] 
[Epoch 11/50] [Batch 100/1017] [D loss: 0.434690] [G loss: 0.218896] [ema: 0.999386] 
[Epoch 11/50] [Batch 200/1017] [D loss: 0.303992] [G loss: 0.212660] [ema: 0.999391] 
[Epoch 11/50] [Batch 300/1017] [D loss: 0.329545] [G loss: 0.230055] [ema: 0.999397] 
[Epoch 11/50] [Batch 400/1017] [D loss: 0.458475] [G loss: 0.210120] [ema: 0.999402] 
[Epoch 11/50] [Batch 500/1017] [D loss: 0.447837] [G loss: 0.200524] [ema: 0.999407] 
[Epoch 11/50] [Batch 600/1017] [D loss: 0.323848] [G loss: 0.218797] [ema: 0.999412] 
[Epoch 11/50] [Batch 700/1017] [D loss: 0.318522] [G loss: 0.197424] [ema: 0.999417] 
[Epoch 11/50] [Batch 800/1017] [D loss: 0.369691] [G loss: 0.227941] [ema: 0.999422] 
[Epoch 11/50] [Batch 900/1017] [D loss: 0.309936] [G loss: 0.212516] [ema: 0.999427] 
[Epoch 11/50] [Batch 1000/1017] [D loss: 0.420536] [G loss: 0.167388] [ema: 0.999431] 
[Epoch 12/50] [Batch 0/1017] [D loss: 0.415303] [G loss: 0.179096] [ema: 0.999432] 
[Epoch 12/50] [Batch 100/1017] [D loss: 0.408262] [G loss: 0.177426] [ema: 0.999437] 
[Epoch 12/50] [Batch 200/1017] [D loss: 0.312632] [G loss: 0.240102] [ema: 0.999441] 
[Epoch 12/50] [Batch 300/1017] [D loss: 0.294222] [G loss: 0.238647] [ema: 0.999446] 
[Epoch 12/50] [Batch 400/1017] [D loss: 0.349200] [G loss: 0.232464] [ema: 0.999450] 
[Epoch 12/50] [Batch 500/1017] [D loss: 0.302836] [G loss: 0.190246] [ema: 0.999455] 
[Epoch 12/50] [Batch 600/1017] [D loss: 0.439732] [G loss: 0.180477] [ema: 0.999459] 
[Epoch 12/50] [Batch 700/1017] [D loss: 0.429922] [G loss: 0.182954] [ema: 0.999463] 
[Epoch 12/50] [Batch 800/1017] [D loss: 0.338110] [G loss: 0.187186] [ema: 0.999467] 
[Epoch 12/50] [Batch 900/1017] [D loss: 0.283477] [G loss: 0.232573] [ema: 0.999471] 
[Epoch 12/50] [Batch 1000/1017] [D loss: 0.293763] [G loss: 0.217227] [ema: 0.999475] 



Saving checkpoint 2 in logs/daghar_50000_30_100/run_50000_D_30_2024_10_18_01_39_30/Model



[Epoch 13/50] [Batch 0/1017] [D loss: 0.311155] [G loss: 0.173225] [ema: 0.999476] 
[Epoch 13/50] [Batch 100/1017] [D loss: 0.304549] [G loss: 0.227830] [ema: 0.999480] 
[Epoch 13/50] [Batch 200/1017] [D loss: 0.288191] [G loss: 0.219784] [ema: 0.999484] 
[Epoch 13/50] [Batch 300/1017] [D loss: 0.306278] [G loss: 0.215748] [ema: 0.999487] 
[Epoch 13/50] [Batch 400/1017] [D loss: 0.410845] [G loss: 0.193698] [ema: 0.999491] 
[Epoch 13/50] [Batch 500/1017] [D loss: 0.382165] [G loss: 0.202861] [ema: 0.999495] 
[Epoch 13/50] [Batch 600/1017] [D loss: 0.361606] [G loss: 0.202407] [ema: 0.999499] 
[Epoch 13/50] [Batch 700/1017] [D loss: 0.343697] [G loss: 0.193675] [ema: 0.999502] 
[Epoch 13/50] [Batch 800/1017] [D loss: 0.289973] [G loss: 0.221492] [ema: 0.999506] 
[Epoch 13/50] [Batch 900/1017] [D loss: 0.313548] [G loss: 0.198458] [ema: 0.999509] 
[Epoch 13/50] [Batch 1000/1017] [D loss: 0.313646] [G loss: 0.204316] [ema: 0.999513] 
[Epoch 14/50] [Batch 0/1017] [D loss: 0.294611] [G loss: 0.212759] [ema: 0.999513] 
[Epoch 14/50] [Batch 100/1017] [D loss: 0.335189] [G loss: 0.216319] [ema: 0.999517] 
[Epoch 14/50] [Batch 200/1017] [D loss: 0.261528] [G loss: 0.235002] [ema: 0.999520] 
[Epoch 14/50] [Batch 300/1017] [D loss: 0.295816] [G loss: 0.209590] [ema: 0.999523] 
[Epoch 14/50] [Batch 400/1017] [D loss: 0.311483] [G loss: 0.216528] [ema: 0.999527] 
[Epoch 14/50] [Batch 500/1017] [D loss: 0.344129] [G loss: 0.186971] [ema: 0.999530] 
[Epoch 14/50] [Batch 600/1017] [D loss: 0.396285] [G loss: 0.175274] [ema: 0.999533] 
[Epoch 14/50] [Batch 700/1017] [D loss: 0.351731] [G loss: 0.231832] [ema: 0.999536] 
[Epoch 14/50] [Batch 800/1017] [D loss: 0.286708] [G loss: 0.216262] [ema: 0.999539] 
[Epoch 14/50] [Batch 900/1017] [D loss: 0.366560] [G loss: 0.226085] [ema: 0.999542] 
[Epoch 14/50] [Batch 1000/1017] [D loss: 0.275476] [G loss: 0.228020] [ema: 0.999545] 
[Epoch 15/50] [Batch 0/1017] [D loss: 0.284322] [G loss: 0.236187] [ema: 0.999546] 
[Epoch 15/50] [Batch 100/1017] [D loss: 0.311798] [G loss: 0.236435] [ema: 0.999549] 
[Epoch 15/50] [Batch 200/1017] [D loss: 0.330372] [G loss: 0.201126] [ema: 0.999552] 
[Epoch 15/50] [Batch 300/1017] [D loss: 0.294870] [G loss: 0.223671] [ema: 0.999554] 
[Epoch 15/50] [Batch 400/1017] [D loss: 0.356645] [G loss: 0.190031] [ema: 0.999557] 
[Epoch 15/50] [Batch 500/1017] [D loss: 0.357575] [G loss: 0.218766] [ema: 0.999560] 
[Epoch 15/50] [Batch 600/1017] [D loss: 0.343575] [G loss: 0.219237] [ema: 0.999563] 
[Epoch 15/50] [Batch 700/1017] [D loss: 0.383591] [G loss: 0.174517] [ema: 0.999566] 
[Epoch 15/50] [Batch 800/1017] [D loss: 0.415424] [G loss: 0.192708] [ema: 0.999568] 
[Epoch 15/50] [Batch 900/1017] [D loss: 0.454711] [G loss: 0.171143] [ema: 0.999571] 
[Epoch 15/50] [Batch 1000/1017] [D loss: 0.294526] [G loss: 0.223590] [ema: 0.999574] 
[Epoch 16/50] [Batch 0/1017] [D loss: 0.389496] [G loss: 0.198688] [ema: 0.999574] 
[Epoch 16/50] [Batch 100/1017] [D loss: 0.309414] [G loss: 0.205367] [ema: 0.999577] 
[Epoch 16/50] [Batch 200/1017] [D loss: 0.440951] [G loss: 0.196874] [ema: 0.999579] 
[Epoch 16/50] [Batch 300/1017] [D loss: 0.454785] [G loss: 0.168854] [ema: 0.999582] 
[Epoch 16/50] [Batch 400/1017] [D loss: 0.364131] [G loss: 0.178482] [ema: 0.999584] 
[Epoch 16/50] [Batch 500/1017] [D loss: 0.372880] [G loss: 0.212165] [ema: 0.999587] 
[Epoch 16/50] [Batch 600/1017] [D loss: 0.390592] [G loss: 0.213299] [ema: 0.999589] 
[Epoch 16/50] [Batch 700/1017] [D loss: 0.466541] [G loss: 0.180109] [ema: 0.999592] 
[Epoch 16/50] [Batch 800/1017] [D loss: 0.419075] [G loss: 0.188294] [ema: 0.999594] 
[Epoch 16/50] [Batch 900/1017] [D loss: 0.385571] [G loss: 0.163114] [ema: 0.999596] 
[Epoch 16/50] [Batch 1000/1017] [D loss: 0.393141] [G loss: 0.168027] [ema: 0.999599] 
[Epoch 17/50] [Batch 0/1017] [D loss: 0.303057] [G loss: 0.201807] [ema: 0.999599] 
[Epoch 17/50] [Batch 100/1017] [D loss: 0.426316] [G loss: 0.243202] [ema: 0.999601] 
[Epoch 17/50] [Batch 200/1017] [D loss: 0.425103] [G loss: 0.181449] [ema: 0.999604] 
[Epoch 17/50] [Batch 300/1017] [D loss: 0.370574] [G loss: 0.171206] [ema: 0.999606] 
[Epoch 17/50] [Batch 400/1017] [D loss: 0.333978] [G loss: 0.206975] [ema: 0.999608] 
[Epoch 17/50] [Batch 500/1017] [D loss: 0.277365] [G loss: 0.218753] [ema: 0.999610] 
[Epoch 17/50] [Batch 600/1017] [D loss: 0.270084] [G loss: 0.220486] [ema: 0.999613] 
[Epoch 17/50] [Batch 700/1017] [D loss: 0.274830] [G loss: 0.236671] [ema: 0.999615] 
[Epoch 17/50] [Batch 800/1017] [D loss: 0.269474] [G loss: 0.252632] [ema: 0.999617] 
[Epoch 17/50] [Batch 900/1017] [D loss: 0.315069] [G loss: 0.228846] [ema: 0.999619] 
[Epoch 17/50] [Batch 1000/1017] [D loss: 0.449750] [G loss: 0.168308] [ema: 0.999621] 
[Epoch 18/50] [Batch 0/1017] [D loss: 0.553667] [G loss: 0.155516] [ema: 0.999621] 
[Epoch 18/50] [Batch 100/1017] [D loss: 0.467564] [G loss: 0.149547] [ema: 0.999623] 
[Epoch 18/50] [Batch 200/1017] [D loss: 0.431599] [G loss: 0.172461] [ema: 0.999626] 
[Epoch 18/50] [Batch 300/1017] [D loss: 0.383916] [G loss: 0.227103] [ema: 0.999628] 
[Epoch 18/50] [Batch 400/1017] [D loss: 0.304251] [G loss: 0.233527] [ema: 0.999630] 
[Epoch 18/50] [Batch 500/1017] [D loss: 0.335672] [G loss: 0.232247] [ema: 0.999631] 
[Epoch 18/50] [Batch 600/1017] [D loss: 0.284080] [G loss: 0.221732] [ema: 0.999633] 
[Epoch 18/50] [Batch 700/1017] [D loss: 0.376742] [G loss: 0.166139] [ema: 0.999635] 
[Epoch 18/50] [Batch 800/1017] [D loss: 0.480921] [G loss: 0.150193] [ema: 0.999637] 
[Epoch 18/50] [Batch 900/1017] [D loss: 0.471957] [G loss: 0.152024] [ema: 0.999639] 
[Epoch 18/50] [Batch 1000/1017] [D loss: 0.396515] [G loss: 0.187468] [ema: 0.999641] 
[Epoch 19/50] [Batch 0/1017] [D loss: 0.394913] [G loss: 0.172648] [ema: 0.999641] 
[Epoch 19/50] [Batch 100/1017] [D loss: 0.395694] [G loss: 0.221254] [ema: 0.999643] 
[Epoch 19/50] [Batch 200/1017] [D loss: 0.326123] [G loss: 0.218699] [ema: 0.999645] 
[Epoch 19/50] [Batch 300/1017] [D loss: 0.358023] [G loss: 0.217249] [ema: 0.999647] 
[Epoch 19/50] [Batch 400/1017] [D loss: 0.377577] [G loss: 0.223768] [ema: 0.999649] 
[Epoch 19/50] [Batch 500/1017] [D loss: 0.380978] [G loss: 0.192991] [ema: 0.999650] 
[Epoch 19/50] [Batch 600/1017] [D loss: 0.323262] [G loss: 0.199241] [ema: 0.999652] 
[Epoch 19/50] [Batch 700/1017] [D loss: 0.389206] [G loss: 0.230521] [ema: 0.999654] 
[Epoch 19/50] [Batch 800/1017] [D loss: 0.369704] [G loss: 0.223828] [ema: 0.999656] 
[Epoch 19/50] [Batch 900/1017] [D loss: 0.398681] [G loss: 0.199043] [ema: 0.999657] 
[Epoch 19/50] [Batch 1000/1017] [D loss: 0.376621] [G loss: 0.190345] [ema: 0.999659] 
[Epoch 20/50] [Batch 0/1017] [D loss: 0.411397] [G loss: 0.169377] [ema: 0.999659] 
[Epoch 20/50] [Batch 100/1017] [D loss: 0.436319] [G loss: 0.172036] [ema: 0.999661] 
[Epoch 20/50] [Batch 200/1017] [D loss: 0.457693] [G loss: 0.166987] [ema: 0.999663] 
[Epoch 20/50] [Batch 300/1017] [D loss: 0.414538] [G loss: 0.168414] [ema: 0.999664] 
[Epoch 20/50] [Batch 400/1017] [D loss: 0.363108] [G loss: 0.194756] [ema: 0.999666] 
[Epoch 20/50] [Batch 500/1017] [D loss: 0.361485] [G loss: 0.187760] [ema: 0.999667] 
[Epoch 20/50] [Batch 600/1017] [D loss: 0.324261] [G loss: 0.206876] [ema: 0.999669] 
[Epoch 20/50] [Batch 700/1017] [D loss: 0.407703] [G loss: 0.190221] [ema: 0.999671] 
[Epoch 20/50] [Batch 800/1017] [D loss: 0.386286] [G loss: 0.167772] [ema: 0.999672] 
[Epoch 20/50] [Batch 900/1017] [D loss: 0.372074] [G loss: 0.159282] [ema: 0.999674] 
[Epoch 20/50] [Batch 1000/1017] [D loss: 0.434687] [G loss: 0.196497] [ema: 0.999675] 
[Epoch 21/50] [Batch 0/1017] [D loss: 0.370032] [G loss: 0.175046] [ema: 0.999675] 
[Epoch 21/50] [Batch 100/1017] [D loss: 0.389852] [G loss: 0.181103] [ema: 0.999677] 
[Epoch 21/50] [Batch 200/1017] [D loss: 0.359826] [G loss: 0.212264] [ema: 0.999679] 
[Epoch 21/50] [Batch 300/1017] [D loss: 0.323034] [G loss: 0.231430] [ema: 0.999680] 
[Epoch 21/50] [Batch 400/1017] [D loss: 0.399867] [G loss: 0.163526] [ema: 0.999681] 
[Epoch 21/50] [Batch 500/1017] [D loss: 0.519096] [G loss: 0.152778] [ema: 0.999683] 
[Epoch 21/50] [Batch 600/1017] [D loss: 0.400243] [G loss: 0.148495] [ema: 0.999684] 
[Epoch 21/50] [Batch 700/1017] [D loss: 0.427857] [G loss: 0.197850] [ema: 0.999686] 
[Epoch 21/50] [Batch 800/1017] [D loss: 0.390196] [G loss: 0.231755] [ema: 0.999687] 
[Epoch 21/50] [Batch 900/1017] [D loss: 0.353445] [G loss: 0.210142] [ema: 0.999689] 
[Epoch 21/50] [Batch 1000/1017] [D loss: 0.357047] [G loss: 0.189260] [ema: 0.999690] 
[Epoch 22/50] [Batch 0/1017] [D loss: 0.332719] [G loss: 0.172727] [ema: 0.999690] 
[Epoch 22/50] [Batch 100/1017] [D loss: 0.385115] [G loss: 0.197372] [ema: 0.999692] 
[Epoch 22/50] [Batch 200/1017] [D loss: 0.491016] [G loss: 0.170590] [ema: 0.999693] 
[Epoch 22/50] [Batch 300/1017] [D loss: 0.468450] [G loss: 0.162395] [ema: 0.999694] 
[Epoch 22/50] [Batch 400/1017] [D loss: 0.388820] [G loss: 0.169609] [ema: 0.999696] 
[Epoch 22/50] [Batch 500/1017] [D loss: 0.397738] [G loss: 0.210380] [ema: 0.999697] 
[Epoch 22/50] [Batch 600/1017] [D loss: 0.320497] [G loss: 0.211703] [ema: 0.999698] 
[Epoch 22/50] [Batch 700/1017] [D loss: 0.387615] [G loss: 0.210712] [ema: 0.999700] 
[Epoch 22/50] [Batch 800/1017] [D loss: 0.379906] [G loss: 0.223653] [ema: 0.999701] 
[Epoch 22/50] [Batch 900/1017] [D loss: 0.418643] [G loss: 0.194053] [ema: 0.999702] 
[Epoch 22/50] [Batch 1000/1017] [D loss: 0.491359] [G loss: 0.179348] [ema: 0.999703] 
[Epoch 23/50] [Batch 0/1017] [D loss: 0.516623] [G loss: 0.157353] [ema: 0.999704] 
[Epoch 23/50] [Batch 100/1017] [D loss: 0.455972] [G loss: 0.155065] [ema: 0.999705] 
[Epoch 23/50] [Batch 200/1017] [D loss: 0.332695] [G loss: 0.181479] [ema: 0.999706] 
[Epoch 23/50] [Batch 300/1017] [D loss: 0.321910] [G loss: 0.226311] [ema: 0.999707] 
[Epoch 23/50] [Batch 400/1017] [D loss: 0.370130] [G loss: 0.199097] [ema: 0.999709] 
[Epoch 23/50] [Batch 500/1017] [D loss: 0.402959] [G loss: 0.180909] [ema: 0.999710] 
[Epoch 23/50] [Batch 600/1017] [D loss: 0.394113] [G loss: 0.180839] [ema: 0.999711] 
[Epoch 23/50] [Batch 700/1017] [D loss: 0.437737] [G loss: 0.159655] [ema: 0.999712] 
[Epoch 23/50] [Batch 800/1017] [D loss: 0.380587] [G loss: 0.184691] [ema: 0.999714] 
[Epoch 23/50] [Batch 900/1017] [D loss: 0.367148] [G loss: 0.178228] [ema: 0.999715] 
[Epoch 23/50] [Batch 1000/1017] [D loss: 0.353948] [G loss: 0.177469] [ema: 0.999716] 
[Epoch 24/50] [Batch 0/1017] [D loss: 0.397953] [G loss: 0.180608] [ema: 0.999716] 
[Epoch 24/50] [Batch 100/1017] [D loss: 0.420982] [G loss: 0.162290] [ema: 0.999717] 
[Epoch 24/50] [Batch 200/1017] [D loss: 0.384009] [G loss: 0.190082] [ema: 0.999718] 
[Epoch 24/50] [Batch 300/1017] [D loss: 0.394010] [G loss: 0.195164] [ema: 0.999720] 
[Epoch 24/50] [Batch 400/1017] [D loss: 0.401537] [G loss: 0.181263] [ema: 0.999721] 
[Epoch 24/50] [Batch 500/1017] [D loss: 0.345281] [G loss: 0.168738] [ema: 0.999722] 
[Epoch 24/50] [Batch 600/1017] [D loss: 0.387884] [G loss: 0.190260] [ema: 0.999723] 
[Epoch 24/50] [Batch 700/1017] [D loss: 0.335484] [G loss: 0.192944] [ema: 0.999724] 
[Epoch 24/50] [Batch 800/1017] [D loss: 0.397689] [G loss: 0.166972] [ema: 0.999725] 
[Epoch 24/50] [Batch 900/1017] [D loss: 0.475478] [G loss: 0.143972] [ema: 0.999726] 
[Epoch 24/50] [Batch 1000/1017] [D loss: 0.440600] [G loss: 0.171897] [ema: 0.999727] 
[Epoch 25/50] [Batch 0/1017] [D loss: 0.413298] [G loss: 0.184532] [ema: 0.999727] 
[Epoch 25/50] [Batch 100/1017] [D loss: 0.341885] [G loss: 0.166216] [ema: 0.999728] 
[Epoch 25/50] [Batch 200/1017] [D loss: 0.352563] [G loss: 0.209838] [ema: 0.999730] 
[Epoch 25/50] [Batch 300/1017] [D loss: 0.412289] [G loss: 0.170607] [ema: 0.999731] 
[Epoch 25/50] [Batch 400/1017] [D loss: 0.409391] [G loss: 0.178752] [ema: 0.999732] 
[Epoch 25/50] [Batch 500/1017] [D loss: 0.394318] [G loss: 0.175325] [ema: 0.999733] 
[Epoch 25/50] [Batch 600/1017] [D loss: 0.423691] [G loss: 0.188633] [ema: 0.999734] 
[Epoch 25/50] [Batch 700/1017] [D loss: 0.410576] [G loss: 0.178184] [ema: 0.999735] 
[Epoch 25/50] [Batch 800/1017] [D loss: 0.379483] [G loss: 0.189216] [ema: 0.999736] 
[Epoch 25/50] [Batch 900/1017] [D loss: 0.434542] [G loss: 0.159768] [ema: 0.999737] 
[Epoch 25/50] [Batch 1000/1017] [D loss: 0.370477] [G loss: 0.166304] [ema: 0.999738] 



Saving checkpoint 3 in logs/daghar_50000_30_100/run_50000_D_30_2024_10_18_01_39_30/Model



[Epoch 26/50] [Batch 0/1017] [D loss: 0.433893] [G loss: 0.168257] [ema: 0.999738] 
[Epoch 26/50] [Batch 100/1017] [D loss: 0.381670] [G loss: 0.190318] [ema: 0.999739] 
[Epoch 26/50] [Batch 200/1017] [D loss: 0.393908] [G loss: 0.194069] [ema: 0.999740] 
[Epoch 26/50] [Batch 300/1017] [D loss: 0.321187] [G loss: 0.202536] [ema: 0.999741] 
[Epoch 26/50] [Batch 400/1017] [D loss: 0.416050] [G loss: 0.168589] [ema: 0.999742] 
[Epoch 26/50] [Batch 500/1017] [D loss: 0.396513] [G loss: 0.178907] [ema: 0.999743] 
[Epoch 26/50] [Batch 600/1017] [D loss: 0.395063] [G loss: 0.165081] [ema: 0.999744] 
[Epoch 26/50] [Batch 700/1017] [D loss: 0.399002] [G loss: 0.213376] [ema: 0.999745] 
[Epoch 26/50] [Batch 800/1017] [D loss: 0.428533] [G loss: 0.180186] [ema: 0.999746] 
[Epoch 26/50] [Batch 900/1017] [D loss: 0.452231] [G loss: 0.181602] [ema: 0.999747] 
[Epoch 26/50] [Batch 1000/1017] [D loss: 0.416330] [G loss: 0.165942] [ema: 0.999747] 
[Epoch 27/50] [Batch 0/1017] [D loss: 0.399617] [G loss: 0.209285] [ema: 0.999748] 
[Epoch 27/50] [Batch 100/1017] [D loss: 0.396551] [G loss: 0.196816] [ema: 0.999749] 
[Epoch 27/50] [Batch 200/1017] [D loss: 0.441938] [G loss: 0.178164] [ema: 0.999749] 
[Epoch 27/50] [Batch 300/1017] [D loss: 0.347098] [G loss: 0.159901] [ema: 0.999750] 
[Epoch 27/50] [Batch 400/1017] [D loss: 0.398157] [G loss: 0.191967] [ema: 0.999751] 
[Epoch 27/50] [Batch 500/1017] [D loss: 0.349759] [G loss: 0.212765] [ema: 0.999752] 
[Epoch 27/50] [Batch 600/1017] [D loss: 0.373166] [G loss: 0.181855] [ema: 0.999753] 
[Epoch 27/50] [Batch 700/1017] [D loss: 0.364413] [G loss: 0.217761] [ema: 0.999754] 
[Epoch 27/50] [Batch 800/1017] [D loss: 0.456881] [G loss: 0.184128] [ema: 0.999755] 
[Epoch 27/50] [Batch 900/1017] [D loss: 0.381233] [G loss: 0.181296] [ema: 0.999756] 
[Epoch 27/50] [Batch 1000/1017] [D loss: 0.452327] [G loss: 0.193074] [ema: 0.999756] 
[Epoch 28/50] [Batch 0/1017] [D loss: 0.333310] [G loss: 0.190982] [ema: 0.999757] 
[Epoch 28/50] [Batch 100/1017] [D loss: 0.380741] [G loss: 0.207435] [ema: 0.999757] 
[Epoch 28/50] [Batch 200/1017] [D loss: 0.405682] [G loss: 0.187447] [ema: 0.999758] 
[Epoch 28/50] [Batch 300/1017] [D loss: 0.453936] [G loss: 0.186583] [ema: 0.999759] 
[Epoch 28/50] [Batch 400/1017] [D loss: 0.443819] [G loss: 0.126385] [ema: 0.999760] 
[Epoch 28/50] [Batch 500/1017] [D loss: 0.478621] [G loss: 0.187693] [ema: 0.999761] 
[Epoch 28/50] [Batch 600/1017] [D loss: 0.388310] [G loss: 0.179550] [ema: 0.999762] 
[Epoch 28/50] [Batch 700/1017] [D loss: 0.409316] [G loss: 0.182804] [ema: 0.999762] 
[Epoch 28/50] [Batch 800/1017] [D loss: 0.452926] [G loss: 0.190435] [ema: 0.999763] 
[Epoch 28/50] [Batch 900/1017] [D loss: 0.354170] [G loss: 0.160291] [ema: 0.999764] 
[Epoch 28/50] [Batch 1000/1017] [D loss: 0.409072] [G loss: 0.177345] [ema: 0.999765] 
[Epoch 29/50] [Batch 0/1017] [D loss: 0.454619] [G loss: 0.188372] [ema: 0.999765] 
[Epoch 29/50] [Batch 100/1017] [D loss: 0.440746] [G loss: 0.191046] [ema: 0.999766] 
[Epoch 29/50] [Batch 200/1017] [D loss: 0.386607] [G loss: 0.169245] [ema: 0.999767] 
[Epoch 29/50] [Batch 300/1017] [D loss: 0.331391] [G loss: 0.176073] [ema: 0.999767] 
[Epoch 29/50] [Batch 400/1017] [D loss: 0.340184] [G loss: 0.219811] [ema: 0.999768] 
[Epoch 29/50] [Batch 500/1017] [D loss: 0.376799] [G loss: 0.156714] [ema: 0.999769] 
[Epoch 29/50] [Batch 600/1017] [D loss: 0.447501] [G loss: 0.171864] [ema: 0.999770] 
[Epoch 29/50] [Batch 700/1017] [D loss: 0.423825] [G loss: 0.185142] [ema: 0.999770] 
[Epoch 29/50] [Batch 800/1017] [D loss: 0.366525] [G loss: 0.197261] [ema: 0.999771] 
[Epoch 29/50] [Batch 900/1017] [D loss: 0.419293] [G loss: 0.170833] [ema: 0.999772] 
[Epoch 29/50] [Batch 1000/1017] [D loss: 0.390694] [G loss: 0.133128] [ema: 0.999773] 
[Epoch 30/50] [Batch 0/1017] [D loss: 0.399429] [G loss: 0.181766] [ema: 0.999773] 
[Epoch 30/50] [Batch 100/1017] [D loss: 0.374081] [G loss: 0.191785] [ema: 0.999774] 
[Epoch 30/50] [Batch 200/1017] [D loss: 0.404328] [G loss: 0.180267] [ema: 0.999774] 
[Epoch 30/50] [Batch 300/1017] [D loss: 0.379305] [G loss: 0.190147] [ema: 0.999775] 
[Epoch 30/50] [Batch 400/1017] [D loss: 0.368172] [G loss: 0.190054] [ema: 0.999776] 
[Epoch 30/50] [Batch 500/1017] [D loss: 0.425185] [G loss: 0.195846] [ema: 0.999777] 
[Epoch 30/50] [Batch 600/1017] [D loss: 0.378366] [G loss: 0.204521] [ema: 0.999777] 
[Epoch 30/50] [Batch 700/1017] [D loss: 0.407640] [G loss: 0.187372] [ema: 0.999778] 
[Epoch 30/50] [Batch 800/1017] [D loss: 0.450993] [G loss: 0.149426] [ema: 0.999779] 
[Epoch 30/50] [Batch 900/1017] [D loss: 0.450947] [G loss: 0.176780] [ema: 0.999779] 
[Epoch 30/50] [Batch 1000/1017] [D loss: 0.409494] [G loss: 0.186652] [ema: 0.999780] 
[Epoch 31/50] [Batch 0/1017] [D loss: 0.405110] [G loss: 0.184896] [ema: 0.999780] 
[Epoch 31/50] [Batch 100/1017] [D loss: 0.375966] [G loss: 0.221257] [ema: 0.999781] 
[Epoch 31/50] [Batch 200/1017] [D loss: 0.376536] [G loss: 0.162622] [ema: 0.999782] 
[Epoch 31/50] [Batch 300/1017] [D loss: 0.387655] [G loss: 0.164293] [ema: 0.999782] 
[Epoch 31/50] [Batch 400/1017] [D loss: 0.401065] [G loss: 0.173477] [ema: 0.999783] 
[Epoch 31/50] [Batch 500/1017] [D loss: 0.385182] [G loss: 0.187044] [ema: 0.999784] 
[Epoch 31/50] [Batch 600/1017] [D loss: 0.417413] [G loss: 0.205475] [ema: 0.999784] 
[Epoch 31/50] [Batch 700/1017] [D loss: 0.355889] [G loss: 0.152797] [ema: 0.999785] 
[Epoch 31/50] [Batch 800/1017] [D loss: 0.431228] [G loss: 0.177757] [ema: 0.999786] 
[Epoch 31/50] [Batch 900/1017] [D loss: 0.422826] [G loss: 0.196789] [ema: 0.999786] 
[Epoch 31/50] [Batch 1000/1017] [D loss: 0.344355] [G loss: 0.187341] [ema: 0.999787] 
[Epoch 32/50] [Batch 0/1017] [D loss: 0.400504] [G loss: 0.203641] [ema: 0.999787] 
[Epoch 32/50] [Batch 100/1017] [D loss: 0.390009] [G loss: 0.192741] [ema: 0.999788] 
[Epoch 32/50] [Batch 200/1017] [D loss: 0.350250] [G loss: 0.204119] [ema: 0.999788] 
[Epoch 32/50] [Batch 300/1017] [D loss: 0.363488] [G loss: 0.196082] [ema: 0.999789] 
[Epoch 32/50] [Batch 400/1017] [D loss: 0.409236] [G loss: 0.177634] [ema: 0.999790] 
[Epoch 32/50] [Batch 500/1017] [D loss: 0.409820] [G loss: 0.190095] [ema: 0.999790] 
[Epoch 32/50] [Batch 600/1017] [D loss: 0.438079] [G loss: 0.170148] [ema: 0.999791] 
[Epoch 32/50] [Batch 700/1017] [D loss: 0.463346] [G loss: 0.179172] [ema: 0.999792] 
[Epoch 32/50] [Batch 800/1017] [D loss: 0.422418] [G loss: 0.174946] [ema: 0.999792] 
[Epoch 32/50] [Batch 900/1017] [D loss: 0.436852] [G loss: 0.197125] [ema: 0.999793] 
[Epoch 32/50] [Batch 1000/1017] [D loss: 0.440865] [G loss: 0.169761] [ema: 0.999793] 
[Epoch 33/50] [Batch 0/1017] [D loss: 0.403046] [G loss: 0.194951] [ema: 0.999793] 
[Epoch 33/50] [Batch 100/1017] [D loss: 0.373919] [G loss: 0.204057] [ema: 0.999794] 
[Epoch 33/50] [Batch 200/1017] [D loss: 0.439878] [G loss: 0.161635] [ema: 0.999795] 
[Epoch 33/50] [Batch 300/1017] [D loss: 0.381792] [G loss: 0.179824] [ema: 0.999795] 
[Epoch 33/50] [Batch 400/1017] [D loss: 0.419256] [G loss: 0.160574] [ema: 0.999796] 
[Epoch 33/50] [Batch 500/1017] [D loss: 0.420005] [G loss: 0.152821] [ema: 0.999797] 
[Epoch 33/50] [Batch 600/1017] [D loss: 0.403966] [G loss: 0.176702] [ema: 0.999797] 
[Epoch 33/50] [Batch 700/1017] [D loss: 0.391996] [G loss: 0.186063] [ema: 0.999798] 
[Epoch 33/50] [Batch 800/1017] [D loss: 0.359616] [G loss: 0.201840] [ema: 0.999798] 
[Epoch 33/50] [Batch 900/1017] [D loss: 0.351212] [G loss: 0.200607] [ema: 0.999799] 
[Epoch 33/50] [Batch 1000/1017] [D loss: 0.359947] [G loss: 0.198107] [ema: 0.999799] 
[Epoch 34/50] [Batch 0/1017] [D loss: 0.416130] [G loss: 0.196904] [ema: 0.999800] 
[Epoch 34/50] [Batch 100/1017] [D loss: 0.352304] [G loss: 0.177285] [ema: 0.999800] 
[Epoch 34/50] [Batch 200/1017] [D loss: 0.398696] [G loss: 0.174386] [ema: 0.999801] 
[Epoch 34/50] [Batch 300/1017] [D loss: 0.448521] [G loss: 0.173540] [ema: 0.999801] 
[Epoch 34/50] [Batch 400/1017] [D loss: 0.409119] [G loss: 0.160915] [ema: 0.999802] 
[Epoch 34/50] [Batch 500/1017] [D loss: 0.444548] [G loss: 0.167697] [ema: 0.999802] 
[Epoch 34/50] [Batch 600/1017] [D loss: 0.413528] [G loss: 0.170158] [ema: 0.999803] 
[Epoch 34/50] [Batch 700/1017] [D loss: 0.496424] [G loss: 0.183244] [ema: 0.999804] 
[Epoch 34/50] [Batch 800/1017] [D loss: 0.337718] [G loss: 0.203363] [ema: 0.999804] 
[Epoch 34/50] [Batch 900/1017] [D loss: 0.434154] [G loss: 0.172287] [ema: 0.999805] 
[Epoch 34/50] [Batch 1000/1017] [D loss: 0.375902] [G loss: 0.214933] [ema: 0.999805] 
[Epoch 35/50] [Batch 0/1017] [D loss: 0.420173] [G loss: 0.210437] [ema: 0.999805] 
[Epoch 35/50] [Batch 100/1017] [D loss: 0.442597] [G loss: 0.219219] [ema: 0.999806] 
[Epoch 35/50] [Batch 200/1017] [D loss: 0.437638] [G loss: 0.184680] [ema: 0.999806] 
[Epoch 35/50] [Batch 300/1017] [D loss: 0.379150] [G loss: 0.175305] [ema: 0.999807] 
[Epoch 35/50] [Batch 400/1017] [D loss: 0.376517] [G loss: 0.204269] [ema: 0.999807] 
[Epoch 35/50] [Batch 500/1017] [D loss: 0.414233] [G loss: 0.191778] [ema: 0.999808] 
[Epoch 35/50] [Batch 600/1017] [D loss: 0.437387] [G loss: 0.167605] [ema: 0.999809] 
[Epoch 35/50] [Batch 700/1017] [D loss: 0.405260] [G loss: 0.204514] [ema: 0.999809] 
[Epoch 35/50] [Batch 800/1017] [D loss: 0.369429] [G loss: 0.180658] [ema: 0.999810] 
[Epoch 35/50] [Batch 900/1017] [D loss: 0.361560] [G loss: 0.174841] [ema: 0.999810] 
[Epoch 35/50] [Batch 1000/1017] [D loss: 0.439304] [G loss: 0.186293] [ema: 0.999811] 
[Epoch 36/50] [Batch 0/1017] [D loss: 0.419624] [G loss: 0.200287] [ema: 0.999811] 
[Epoch 36/50] [Batch 100/1017] [D loss: 0.442715] [G loss: 0.183499] [ema: 0.999811] 
[Epoch 36/50] [Batch 200/1017] [D loss: 0.396115] [G loss: 0.180178] [ema: 0.999812] 
[Epoch 36/50] [Batch 300/1017] [D loss: 0.356208] [G loss: 0.191312] [ema: 0.999812] 
[Epoch 36/50] [Batch 400/1017] [D loss: 0.429886] [G loss: 0.197445] [ema: 0.999813] 
[Epoch 36/50] [Batch 500/1017] [D loss: 0.357532] [G loss: 0.177569] [ema: 0.999813] 
[Epoch 36/50] [Batch 600/1017] [D loss: 0.374709] [G loss: 0.219102] [ema: 0.999814] 
[Epoch 36/50] [Batch 700/1017] [D loss: 0.309504] [G loss: 0.185563] [ema: 0.999814] 
[Epoch 36/50] [Batch 800/1017] [D loss: 0.337428] [G loss: 0.176853] [ema: 0.999815] 
[Epoch 36/50] [Batch 900/1017] [D loss: 0.382225] [G loss: 0.188579] [ema: 0.999815] 
[Epoch 36/50] [Batch 1000/1017] [D loss: 0.370565] [G loss: 0.202037] [ema: 0.999816] 
[Epoch 37/50] [Batch 0/1017] [D loss: 0.455059] [G loss: 0.184818] [ema: 0.999816] 
[Epoch 37/50] [Batch 100/1017] [D loss: 0.386080] [G loss: 0.151454] [ema: 0.999816] 
[Epoch 37/50] [Batch 200/1017] [D loss: 0.353916] [G loss: 0.178802] [ema: 0.999817] 
[Epoch 37/50] [Batch 300/1017] [D loss: 0.349250] [G loss: 0.161970] [ema: 0.999817] 
[Epoch 37/50] [Batch 400/1017] [D loss: 0.388683] [G loss: 0.197045] [ema: 0.999818] 
[Epoch 37/50] [Batch 500/1017] [D loss: 0.377617] [G loss: 0.177704] [ema: 0.999818] 
[Epoch 37/50] [Batch 600/1017] [D loss: 0.428657] [G loss: 0.182124] [ema: 0.999819] 
[Epoch 37/50] [Batch 700/1017] [D loss: 0.464505] [G loss: 0.176069] [ema: 0.999819] 
[Epoch 37/50] [Batch 800/1017] [D loss: 0.454025] [G loss: 0.193657] [ema: 0.999820] 
[Epoch 37/50] [Batch 900/1017] [D loss: 0.356425] [G loss: 0.187140] [ema: 0.999820] 
[Epoch 37/50] [Batch 1000/1017] [D loss: 0.393865] [G loss: 0.177315] [ema: 0.999821] 
[Epoch 38/50] [Batch 0/1017] [D loss: 0.366789] [G loss: 0.196011] [ema: 0.999821] 
[Epoch 38/50] [Batch 100/1017] [D loss: 0.376625] [G loss: 0.175256] [ema: 0.999821] 
[Epoch 38/50] [Batch 200/1017] [D loss: 0.440863] [G loss: 0.187652] [ema: 0.999822] 
[Epoch 38/50] [Batch 300/1017] [D loss: 0.417149] [G loss: 0.187860] [ema: 0.999822] 
[Epoch 38/50] [Batch 400/1017] [D loss: 0.357831] [G loss: 0.180728] [ema: 0.999822] 
[Epoch 38/50] [Batch 500/1017] [D loss: 0.439101] [G loss: 0.192236] [ema: 0.999823] 
[Epoch 38/50] [Batch 600/1017] [D loss: 0.392804] [G loss: 0.190333] [ema: 0.999823] 
[Epoch 38/50] [Batch 700/1017] [D loss: 0.386217] [G loss: 0.167864] [ema: 0.999824] 
[Epoch 38/50] [Batch 800/1017] [D loss: 0.367422] [G loss: 0.184994] [ema: 0.999824] 
[Epoch 38/50] [Batch 900/1017] [D loss: 0.384656] [G loss: 0.190441] [ema: 0.999825] 
[Epoch 38/50] [Batch 1000/1017] [D loss: 0.403197] [G loss: 0.197556] [ema: 0.999825] 



Saving checkpoint 4 in logs/daghar_50000_30_100/run_50000_D_30_2024_10_18_01_39_30/Model



[Epoch 39/50] [Batch 0/1017] [D loss: 0.408538] [G loss: 0.209673] [ema: 0.999825] 
[Epoch 39/50] [Batch 100/1017] [D loss: 0.370340] [G loss: 0.168349] [ema: 0.999826] 
[Epoch 39/50] [Batch 200/1017] [D loss: 0.356630] [G loss: 0.198511] [ema: 0.999826] 
[Epoch 39/50] [Batch 300/1017] [D loss: 0.345505] [G loss: 0.207733] [ema: 0.999827] 
[Epoch 39/50] [Batch 400/1017] [D loss: 0.399224] [G loss: 0.195239] [ema: 0.999827] 
[Epoch 39/50] [Batch 500/1017] [D loss: 0.441263] [G loss: 0.186337] [ema: 0.999827] 
[Epoch 39/50] [Batch 600/1017] [D loss: 0.403094] [G loss: 0.180140] [ema: 0.999828] 
[Epoch 39/50] [Batch 700/1017] [D loss: 0.441982] [G loss: 0.172774] [ema: 0.999828] 
[Epoch 39/50] [Batch 800/1017] [D loss: 0.357654] [G loss: 0.213382] [ema: 0.999829] 
[Epoch 39/50] [Batch 900/1017] [D loss: 0.415339] [G loss: 0.173947] [ema: 0.999829] 
[Epoch 39/50] [Batch 1000/1017] [D loss: 0.410541] [G loss: 0.179078] [ema: 0.999830] 
[Epoch 40/50] [Batch 0/1017] [D loss: 0.393768] [G loss: 0.208449] [ema: 0.999830] 
[Epoch 40/50] [Batch 100/1017] [D loss: 0.408650] [G loss: 0.194637] [ema: 0.999830] 
[Epoch 40/50] [Batch 200/1017] [D loss: 0.386668] [G loss: 0.176030] [ema: 0.999830] 
[Epoch 40/50] [Batch 300/1017] [D loss: 0.366156] [G loss: 0.187483] [ema: 0.999831] 
[Epoch 40/50] [Batch 400/1017] [D loss: 0.422910] [G loss: 0.176460] [ema: 0.999831] 
[Epoch 40/50] [Batch 500/1017] [D loss: 0.398390] [G loss: 0.182660] [ema: 0.999832] 
[Epoch 40/50] [Batch 600/1017] [D loss: 0.399831] [G loss: 0.174740] [ema: 0.999832] 
[Epoch 40/50] [Batch 700/1017] [D loss: 0.390245] [G loss: 0.184470] [ema: 0.999833] 
[Epoch 40/50] [Batch 800/1017] [D loss: 0.388828] [G loss: 0.135969] [ema: 0.999833] 
[Epoch 40/50] [Batch 900/1017] [D loss: 0.393057] [G loss: 0.177377] [ema: 0.999833] 
[Epoch 40/50] [Batch 1000/1017] [D loss: 0.334069] [G loss: 0.164073] [ema: 0.999834] 
[Epoch 41/50] [Batch 0/1017] [D loss: 0.366245] [G loss: 0.191014] [ema: 0.999834] 
[Epoch 41/50] [Batch 100/1017] [D loss: 0.441755] [G loss: 0.179003] [ema: 0.999834] 
[Epoch 41/50] [Batch 200/1017] [D loss: 0.395255] [G loss: 0.192344] [ema: 0.999835] 
[Epoch 41/50] [Batch 300/1017] [D loss: 0.371696] [G loss: 0.192909] [ema: 0.999835] 
[Epoch 41/50] [Batch 400/1017] [D loss: 0.404210] [G loss: 0.208969] [ema: 0.999835] 
[Epoch 41/50] [Batch 500/1017] [D loss: 0.375997] [G loss: 0.173814] [ema: 0.999836] 
[Epoch 41/50] [Batch 600/1017] [D loss: 0.387097] [G loss: 0.189846] [ema: 0.999836] 
[Epoch 41/50] [Batch 700/1017] [D loss: 0.384402] [G loss: 0.171223] [ema: 0.999837] 
[Epoch 41/50] [Batch 800/1017] [D loss: 0.403324] [G loss: 0.199371] [ema: 0.999837] 
[Epoch 41/50] [Batch 900/1017] [D loss: 0.387517] [G loss: 0.191556] [ema: 0.999837] 
[Epoch 41/50] [Batch 1000/1017] [D loss: 0.334599] [G loss: 0.206046] [ema: 0.999838] 
[Epoch 42/50] [Batch 0/1017] [D loss: 0.342747] [G loss: 0.180510] [ema: 0.999838] 
[Epoch 42/50] [Batch 100/1017] [D loss: 0.422988] [G loss: 0.183411] [ema: 0.999838] 
[Epoch 42/50] [Batch 200/1017] [D loss: 0.433595] [G loss: 0.183422] [ema: 0.999838] 
[Epoch 42/50] [Batch 300/1017] [D loss: 0.416983] [G loss: 0.179867] [ema: 0.999839] 
[Epoch 42/50] [Batch 400/1017] [D loss: 0.417145] [G loss: 0.194806] [ema: 0.999839] 
[Epoch 42/50] [Batch 500/1017] [D loss: 0.355820] [G loss: 0.193184] [ema: 0.999840] 
[Epoch 42/50] [Batch 600/1017] [D loss: 0.409522] [G loss: 0.199610] [ema: 0.999840] 
[Epoch 42/50] [Batch 700/1017] [D loss: 0.362666] [G loss: 0.183246] [ema: 0.999840] 
[Epoch 42/50] [Batch 800/1017] [D loss: 0.417541] [G loss: 0.178630] [ema: 0.999841] 
[Epoch 42/50] [Batch 900/1017] [D loss: 0.389242] [G loss: 0.178368] [ema: 0.999841] 
[Epoch 42/50] [Batch 1000/1017] [D loss: 0.358586] [G loss: 0.184479] [ema: 0.999841] 
[Epoch 43/50] [Batch 0/1017] [D loss: 0.400888] [G loss: 0.177427] [ema: 0.999842] 
[Epoch 43/50] [Batch 100/1017] [D loss: 0.366793] [G loss: 0.142636] [ema: 0.999842] 
[Epoch 43/50] [Batch 200/1017] [D loss: 0.375740] [G loss: 0.211271] [ema: 0.999842] 
[Epoch 43/50] [Batch 300/1017] [D loss: 0.318419] [G loss: 0.162880] [ema: 0.999843] 
[Epoch 43/50] [Batch 400/1017] [D loss: 0.369087] [G loss: 0.183172] [ema: 0.999843] 
[Epoch 43/50] [Batch 500/1017] [D loss: 0.403055] [G loss: 0.211837] [ema: 0.999843] 
[Epoch 43/50] [Batch 600/1017] [D loss: 0.454366] [G loss: 0.200378] [ema: 0.999844] 
[Epoch 43/50] [Batch 700/1017] [D loss: 0.402672] [G loss: 0.186175] [ema: 0.999844] 
[Epoch 43/50] [Batch 800/1017] [D loss: 0.388024] [G loss: 0.187094] [ema: 0.999844] 
[Epoch 43/50] [Batch 900/1017] [D loss: 0.398284] [G loss: 0.147551] [ema: 0.999845] 
[Epoch 43/50] [Batch 1000/1017] [D loss: 0.316402] [G loss: 0.176312] [ema: 0.999845] 
[Epoch 44/50] [Batch 0/1017] [D loss: 0.397344] [G loss: 0.192800] [ema: 0.999845] 
[Epoch 44/50] [Batch 100/1017] [D loss: 0.358467] [G loss: 0.182231] [ema: 0.999845] 
[Epoch 44/50] [Batch 200/1017] [D loss: 0.365159] [G loss: 0.187532] [ema: 0.999846] 
[Epoch 44/50] [Batch 300/1017] [D loss: 0.394914] [G loss: 0.176771] [ema: 0.999846] 
[Epoch 44/50] [Batch 400/1017] [D loss: 0.363301] [G loss: 0.188001] [ema: 0.999846] 
[Epoch 44/50] [Batch 500/1017] [D loss: 0.422949] [G loss: 0.184646] [ema: 0.999847] 
[Epoch 44/50] [Batch 600/1017] [D loss: 0.473291] [G loss: 0.194401] [ema: 0.999847] 
[Epoch 44/50] [Batch 700/1017] [D loss: 0.381556] [G loss: 0.200743] [ema: 0.999847] 
[Epoch 44/50] [Batch 800/1017] [D loss: 0.360160] [G loss: 0.196586] [ema: 0.999848] 
[Epoch 44/50] [Batch 900/1017] [D loss: 0.356943] [G loss: 0.204205] [ema: 0.999848] 
[Epoch 44/50] [Batch 1000/1017] [D loss: 0.368966] [G loss: 0.184152] [ema: 0.999848] 
[Epoch 45/50] [Batch 0/1017] [D loss: 0.391999] [G loss: 0.192075] [ema: 0.999849] 
[Epoch 45/50] [Batch 100/1017] [D loss: 0.368924] [G loss: 0.193615] [ema: 0.999849] 
[Epoch 45/50] [Batch 200/1017] [D loss: 0.423905] [G loss: 0.188321] [ema: 0.999849] 
[Epoch 45/50] [Batch 300/1017] [D loss: 0.366214] [G loss: 0.177964] [ema: 0.999850] 
[Epoch 45/50] [Batch 400/1017] [D loss: 0.364271] [G loss: 0.208044] [ema: 0.999850] 
[Epoch 45/50] [Batch 500/1017] [D loss: 0.385589] [G loss: 0.172583] [ema: 0.999850] 
[Epoch 45/50] [Batch 600/1017] [D loss: 0.409955] [G loss: 0.179254] [ema: 0.999851] 
[Epoch 45/50] [Batch 700/1017] [D loss: 0.379538] [G loss: 0.181889] [ema: 0.999851] 
[Epoch 45/50] [Batch 800/1017] [D loss: 0.422852] [G loss: 0.193482] [ema: 0.999851] 
[Epoch 45/50] [Batch 900/1017] [D loss: 0.483686] [G loss: 0.175649] [ema: 0.999851] 
[Epoch 45/50] [Batch 1000/1017] [D loss: 0.470172] [G loss: 0.191097] [ema: 0.999852] 
[Epoch 46/50] [Batch 0/1017] [D loss: 0.393390] [G loss: 0.171717] [ema: 0.999852] 
[Epoch 46/50] [Batch 100/1017] [D loss: 0.420574] [G loss: 0.191231] [ema: 0.999852] 
[Epoch 46/50] [Batch 200/1017] [D loss: 0.479915] [G loss: 0.164102] [ema: 0.999852] 
[Epoch 46/50] [Batch 300/1017] [D loss: 0.446468] [G loss: 0.186743] [ema: 0.999853] 
[Epoch 46/50] [Batch 400/1017] [D loss: 0.393283] [G loss: 0.150017] [ema: 0.999853] 
[Epoch 46/50] [Batch 500/1017] [D loss: 0.422420] [G loss: 0.207753] [ema: 0.999853] 
[Epoch 46/50] [Batch 600/1017] [D loss: 0.460140] [G loss: 0.190508] [ema: 0.999854] 
[Epoch 46/50] [Batch 700/1017] [D loss: 0.476026] [G loss: 0.132466] [ema: 0.999854] 
[Epoch 46/50] [Batch 800/1017] [D loss: 0.419016] [G loss: 0.172738] [ema: 0.999854] 
[Epoch 46/50] [Batch 900/1017] [D loss: 0.358127] [G loss: 0.152242] [ema: 0.999855] 
[Epoch 46/50] [Batch 1000/1017] [D loss: 0.411363] [G loss: 0.165360] [ema: 0.999855] 
[Epoch 47/50] [Batch 0/1017] [D loss: 0.433196] [G loss: 0.161066] [ema: 0.999855] 
[Epoch 47/50] [Batch 100/1017] [D loss: 0.405277] [G loss: 0.208845] [ema: 0.999855] 
[Epoch 47/50] [Batch 200/1017] [D loss: 0.392609] [G loss: 0.185610] [ema: 0.999856] 
[Epoch 47/50] [Batch 300/1017] [D loss: 0.477335] [G loss: 0.180139] [ema: 0.999856] 
[Epoch 47/50] [Batch 400/1017] [D loss: 0.389742] [G loss: 0.163838] [ema: 0.999856] 
[Epoch 47/50] [Batch 500/1017] [D loss: 0.352567] [G loss: 0.206386] [ema: 0.999856] 
[Epoch 47/50] [Batch 600/1017] [D loss: 0.387516] [G loss: 0.187930] [ema: 0.999857] 
[Epoch 47/50] [Batch 700/1017] [D loss: 0.386355] [G loss: 0.181026] [ema: 0.999857] 
[Epoch 47/50] [Batch 800/1017] [D loss: 0.413224] [G loss: 0.201207] [ema: 0.999857] 
[Epoch 47/50] [Batch 900/1017] [D loss: 0.379309] [G loss: 0.170854] [ema: 0.999858] 
[Epoch 47/50] [Batch 1000/1017] [D loss: 0.480084] [G loss: 0.195016] [ema: 0.999858] 
[Epoch 48/50] [Batch 0/1017] [D loss: 0.463410] [G loss: 0.170476] [ema: 0.999858] 
[Epoch 48/50] [Batch 100/1017] [D loss: 0.410350] [G loss: 0.190354] [ema: 0.999858] 
[Epoch 48/50] [Batch 200/1017] [D loss: 0.438610] [G loss: 0.143033] [ema: 0.999859] 
[Epoch 48/50] [Batch 300/1017] [D loss: 0.431966] [G loss: 0.188830] [ema: 0.999859] 
[Epoch 48/50] [Batch 400/1017] [D loss: 0.527585] [G loss: 0.180800] [ema: 0.999859] 
[Epoch 48/50] [Batch 500/1017] [D loss: 0.364459] [G loss: 0.173111] [ema: 0.999859] 
[Epoch 48/50] [Batch 600/1017] [D loss: 0.481583] [G loss: 0.162642] [ema: 0.999860] 
[Epoch 48/50] [Batch 700/1017] [D loss: 0.376204] [G loss: 0.200086] [ema: 0.999860] 
[Epoch 48/50] [Batch 800/1017] [D loss: 0.356604] [G loss: 0.142457] [ema: 0.999860] 
[Epoch 48/50] [Batch 900/1017] [D loss: 0.326118] [G loss: 0.163397] [ema: 0.999861] 
[Epoch 48/50] [Batch 1000/1017] [D loss: 0.436296] [G loss: 0.187304] [ema: 0.999861] 
[Epoch 49/50] [Batch 0/1017] [D loss: 0.444007] [G loss: 0.144942] [ema: 0.999861] 
[Epoch 49/50] [Batch 100/1017] [D loss: 0.410361] [G loss: 0.169485] [ema: 0.999861] 
[Epoch 49/50] [Batch 200/1017] [D loss: 0.515843] [G loss: 0.166021] [ema: 0.999861] 
[Epoch 49/50] [Batch 300/1017] [D loss: 0.412315] [G loss: 0.199224] [ema: 0.999862] 
[Epoch 49/50] [Batch 400/1017] [D loss: 0.385026] [G loss: 0.170938] [ema: 0.999862] 
[Epoch 49/50] [Batch 500/1017] [D loss: 0.440669] [G loss: 0.186613] [ema: 0.999862] 
[Epoch 49/50] [Batch 600/1017] [D loss: 0.394647] [G loss: 0.186594] [ema: 0.999863] 
[Epoch 49/50] [Batch 700/1017] [D loss: 0.405022] [G loss: 0.170739] [ema: 0.999863] 
[Epoch 49/50] [Batch 800/1017] [D loss: 0.497526] [G loss: 0.176961] [ema: 0.999863] 
[Epoch 49/50] [Batch 900/1017] [D loss: 0.439490] [G loss: 0.185927] [ema: 0.999863] 
[Epoch 49/50] [Batch 1000/1017] [D loss: 0.415439] [G loss: 0.165775] [ema: 0.999864] 

----------------------------------------------------------------------------------------------------

 Starting individual training
walk training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
walk
daghar
return single class data and labels, class is walk
data shape is (17228, 3, 1, 30)
label shape is (17228,)
1077
Epochs between checkpoint: 12



Saving checkpoint 1 in logs/daghar_50000_30_100/walk_50000_D_30_2024_10_18_02_11_49/Model



[Epoch 0/47] [Batch 0/1077] [D loss: 1.525884] [G loss: 0.886889] [ema: 0.000000] 
[Epoch 0/47] [Batch 100/1077] [D loss: 0.439425] [G loss: 0.185693] [ema: 0.933033] 
[Epoch 0/47] [Batch 200/1077] [D loss: 0.384710] [G loss: 0.185554] [ema: 0.965936] 
[Epoch 0/47] [Batch 300/1077] [D loss: 0.269674] [G loss: 0.261388] [ema: 0.977160] 
[Epoch 0/47] [Batch 400/1077] [D loss: 0.318694] [G loss: 0.197209] [ema: 0.982821] 
[Epoch 0/47] [Batch 500/1077] [D loss: 0.369791] [G loss: 0.187071] [ema: 0.986233] 
[Epoch 0/47] [Batch 600/1077] [D loss: 0.361009] [G loss: 0.217973] [ema: 0.988514] 
[Epoch 0/47] [Batch 700/1077] [D loss: 0.421674] [G loss: 0.177273] [ema: 0.990147] 
[Epoch 0/47] [Batch 800/1077] [D loss: 0.425600] [G loss: 0.149554] [ema: 0.991373] 
[Epoch 0/47] [Batch 900/1077] [D loss: 0.395885] [G loss: 0.182486] [ema: 0.992328] 
[Epoch 0/47] [Batch 1000/1077] [D loss: 0.360016] [G loss: 0.183338] [ema: 0.993092] 
[Epoch 1/47] [Batch 0/1077] [D loss: 0.383911] [G loss: 0.215778] [ema: 0.993585] 
[Epoch 1/47] [Batch 100/1077] [D loss: 0.363364] [G loss: 0.194469] [ema: 0.994128] 
[Epoch 1/47] [Batch 200/1077] [D loss: 0.330479] [G loss: 0.200696] [ema: 0.994587] 
[Epoch 1/47] [Batch 300/1077] [D loss: 0.344519] [G loss: 0.199207] [ema: 0.994979] 
[Epoch 1/47] [Batch 400/1077] [D loss: 0.347591] [G loss: 0.194119] [ema: 0.995318] 
[Epoch 1/47] [Batch 500/1077] [D loss: 0.446001] [G loss: 0.144117] [ema: 0.995614] 
[Epoch 1/47] [Batch 600/1077] [D loss: 0.497093] [G loss: 0.161411] [ema: 0.995875] 
[Epoch 1/47] [Batch 700/1077] [D loss: 0.547252] [G loss: 0.130940] [ema: 0.996107] 
[Epoch 1/47] [Batch 800/1077] [D loss: 0.483044] [G loss: 0.133122] [ema: 0.996314] 
[Epoch 1/47] [Batch 900/1077] [D loss: 0.397389] [G loss: 0.169589] [ema: 0.996500] 
[Epoch 1/47] [Batch 1000/1077] [D loss: 0.467843] [G loss: 0.162270] [ema: 0.996668] 
[Epoch 2/47] [Batch 0/1077] [D loss: 0.413075] [G loss: 0.186981] [ema: 0.996787] 
[Epoch 2/47] [Batch 100/1077] [D loss: 0.514181] [G loss: 0.176772] [ema: 0.996930] 
[Epoch 2/47] [Batch 200/1077] [D loss: 0.461758] [G loss: 0.114451] [ema: 0.997060] 
[Epoch 2/47] [Batch 300/1077] [D loss: 0.423654] [G loss: 0.162609] [ema: 0.997179] 
[Epoch 2/47] [Batch 400/1077] [D loss: 0.462032] [G loss: 0.179706] [ema: 0.997290] 
[Epoch 2/47] [Batch 500/1077] [D loss: 0.486453] [G loss: 0.191966] [ema: 0.997392] 
[Epoch 2/47] [Batch 600/1077] [D loss: 0.435008] [G loss: 0.167757] [ema: 0.997486] 
[Epoch 2/47] [Batch 700/1077] [D loss: 0.479980] [G loss: 0.167370] [ema: 0.997574] 
[Epoch 2/47] [Batch 800/1077] [D loss: 0.497478] [G loss: 0.171754] [ema: 0.997656] 
[Epoch 2/47] [Batch 900/1077] [D loss: 0.489214] [G loss: 0.192961] [ema: 0.997733] 
[Epoch 2/47] [Batch 1000/1077] [D loss: 0.396189] [G loss: 0.200489] [ema: 0.997805] 
[Epoch 3/47] [Batch 0/1077] [D loss: 0.487694] [G loss: 0.160324] [ema: 0.997857] 
[Epoch 3/47] [Batch 100/1077] [D loss: 0.501025] [G loss: 0.179323] [ema: 0.997921] 
[Epoch 3/47] [Batch 200/1077] [D loss: 0.479391] [G loss: 0.167778] [ema: 0.997982] 
[Epoch 3/47] [Batch 300/1077] [D loss: 0.556318] [G loss: 0.145255] [ema: 0.998039] 
[Epoch 3/47] [Batch 400/1077] [D loss: 0.514305] [G loss: 0.147339] [ema: 0.998093] 
[Epoch 3/47] [Batch 500/1077] [D loss: 0.499101] [G loss: 0.156349] [ema: 0.998144] 
[Epoch 3/47] [Batch 600/1077] [D loss: 0.487018] [G loss: 0.156276] [ema: 0.998192] 
[Epoch 3/47] [Batch 700/1077] [D loss: 0.456442] [G loss: 0.174293] [ema: 0.998238] 
[Epoch 3/47] [Batch 800/1077] [D loss: 0.416090] [G loss: 0.150042] [ema: 0.998282] 
[Epoch 3/47] [Batch 900/1077] [D loss: 0.434139] [G loss: 0.158598] [ema: 0.998323] 
[Epoch 3/47] [Batch 1000/1077] [D loss: 0.507348] [G loss: 0.130009] [ema: 0.998363] 
[Epoch 4/47] [Batch 0/1077] [D loss: 0.487264] [G loss: 0.152957] [ema: 0.998392] 
[Epoch 4/47] [Batch 100/1077] [D loss: 0.522094] [G loss: 0.128928] [ema: 0.998429] 
[Epoch 4/47] [Batch 200/1077] [D loss: 0.520571] [G loss: 0.142083] [ema: 0.998464] 
[Epoch 4/47] [Batch 300/1077] [D loss: 0.526936] [G loss: 0.130277] [ema: 0.998497] 
[Epoch 4/47] [Batch 400/1077] [D loss: 0.446762] [G loss: 0.187760] [ema: 0.998529] 
[Epoch 4/47] [Batch 500/1077] [D loss: 0.535011] [G loss: 0.109653] [ema: 0.998559] 
[Epoch 4/47] [Batch 600/1077] [D loss: 0.426551] [G loss: 0.191573] [ema: 0.998589] 
[Epoch 4/47] [Batch 700/1077] [D loss: 0.478972] [G loss: 0.144611] [ema: 0.998617] 
[Epoch 4/47] [Batch 800/1077] [D loss: 0.500660] [G loss: 0.156090] [ema: 0.998644] 
[Epoch 4/47] [Batch 900/1077] [D loss: 0.519426] [G loss: 0.149824] [ema: 0.998670] 
[Epoch 4/47] [Batch 1000/1077] [D loss: 0.491811] [G loss: 0.150981] [ema: 0.998695] 
[Epoch 5/47] [Batch 0/1077] [D loss: 0.451295] [G loss: 0.144948] [ema: 0.998714] 
[Epoch 5/47] [Batch 100/1077] [D loss: 0.520467] [G loss: 0.152133] [ema: 0.998737] 
[Epoch 5/47] [Batch 200/1077] [D loss: 0.485171] [G loss: 0.147896] [ema: 0.998760] 
[Epoch 5/47] [Batch 300/1077] [D loss: 0.469756] [G loss: 0.141499] [ema: 0.998781] 
[Epoch 5/47] [Batch 400/1077] [D loss: 0.468697] [G loss: 0.134910] [ema: 0.998803] 
[Epoch 5/47] [Batch 500/1077] [D loss: 0.502126] [G loss: 0.139452] [ema: 0.998823] 
[Epoch 5/47] [Batch 600/1077] [D loss: 0.460044] [G loss: 0.166903] [ema: 0.998843] 
[Epoch 5/47] [Batch 700/1077] [D loss: 0.447333] [G loss: 0.159438] [ema: 0.998862] 
[Epoch 5/47] [Batch 800/1077] [D loss: 0.544534] [G loss: 0.135721] [ema: 0.998880] 
[Epoch 5/47] [Batch 900/1077] [D loss: 0.484080] [G loss: 0.149324] [ema: 0.998898] 
[Epoch 5/47] [Batch 1000/1077] [D loss: 0.456745] [G loss: 0.148163] [ema: 0.998915] 
[Epoch 6/47] [Batch 0/1077] [D loss: 0.465553] [G loss: 0.128926] [ema: 0.998928] 
[Epoch 6/47] [Batch 100/1077] [D loss: 0.525576] [G loss: 0.147596] [ema: 0.998944] 
[Epoch 6/47] [Batch 200/1077] [D loss: 0.511040] [G loss: 0.169157] [ema: 0.998960] 
[Epoch 6/47] [Batch 300/1077] [D loss: 0.466285] [G loss: 0.150615] [ema: 0.998975] 
[Epoch 6/47] [Batch 400/1077] [D loss: 0.515809] [G loss: 0.161514] [ema: 0.998990] 
[Epoch 6/47] [Batch 500/1077] [D loss: 0.515667] [G loss: 0.150698] [ema: 0.999005] 
[Epoch 6/47] [Batch 600/1077] [D loss: 0.476023] [G loss: 0.163361] [ema: 0.999019] 
[Epoch 6/47] [Batch 700/1077] [D loss: 0.507601] [G loss: 0.133076] [ema: 0.999033] 
[Epoch 6/47] [Batch 800/1077] [D loss: 0.452200] [G loss: 0.154240] [ema: 0.999046] 
[Epoch 6/47] [Batch 900/1077] [D loss: 0.471305] [G loss: 0.168860] [ema: 0.999059] 
[Epoch 6/47] [Batch 1000/1077] [D loss: 0.462408] [G loss: 0.136074] [ema: 0.999072] 
[Epoch 7/47] [Batch 0/1077] [D loss: 0.454364] [G loss: 0.161984] [ema: 0.999081] 
[Epoch 7/47] [Batch 100/1077] [D loss: 0.450296] [G loss: 0.163667] [ema: 0.999093] 
[Epoch 7/47] [Batch 200/1077] [D loss: 0.462445] [G loss: 0.135321] [ema: 0.999105] 
[Epoch 7/47] [Batch 300/1077] [D loss: 0.405966] [G loss: 0.168254] [ema: 0.999116] 
[Epoch 7/47] [Batch 400/1077] [D loss: 0.520997] [G loss: 0.163520] [ema: 0.999127] 
[Epoch 7/47] [Batch 500/1077] [D loss: 0.463585] [G loss: 0.146530] [ema: 0.999138] 
[Epoch 7/47] [Batch 600/1077] [D loss: 0.424108] [G loss: 0.167339] [ema: 0.999149] 
[Epoch 7/47] [Batch 700/1077] [D loss: 0.478254] [G loss: 0.157026] [ema: 0.999159] 
[Epoch 7/47] [Batch 800/1077] [D loss: 0.400052] [G loss: 0.170135] [ema: 0.999169] 
[Epoch 7/47] [Batch 900/1077] [D loss: 0.435254] [G loss: 0.144828] [ema: 0.999179] 
[Epoch 7/47] [Batch 1000/1077] [D loss: 0.490743] [G loss: 0.158193] [ema: 0.999189] 
[Epoch 8/47] [Batch 0/1077] [D loss: 0.425395] [G loss: 0.157379] [ema: 0.999196] 
[Epoch 8/47] [Batch 100/1077] [D loss: 0.422379] [G loss: 0.161445] [ema: 0.999205] 
[Epoch 8/47] [Batch 200/1077] [D loss: 0.463584] [G loss: 0.153164] [ema: 0.999214] 
[Epoch 8/47] [Batch 300/1077] [D loss: 0.400289] [G loss: 0.172121] [ema: 0.999223] 
[Epoch 8/47] [Batch 400/1077] [D loss: 0.403016] [G loss: 0.182170] [ema: 0.999231] 
[Epoch 8/47] [Batch 500/1077] [D loss: 0.477655] [G loss: 0.153903] [ema: 0.999240] 
[Epoch 8/47] [Batch 600/1077] [D loss: 0.413879] [G loss: 0.175784] [ema: 0.999248] 
[Epoch 8/47] [Batch 700/1077] [D loss: 0.524078] [G loss: 0.168556] [ema: 0.999256] 
[Epoch 8/47] [Batch 800/1077] [D loss: 0.445895] [G loss: 0.159741] [ema: 0.999264] 
[Epoch 8/47] [Batch 900/1077] [D loss: 0.448344] [G loss: 0.179891] [ema: 0.999272] 
[Epoch 8/47] [Batch 1000/1077] [D loss: 0.445046] [G loss: 0.142975] [ema: 0.999279] 
[Epoch 9/47] [Batch 0/1077] [D loss: 0.389198] [G loss: 0.183061] [ema: 0.999285] 
[Epoch 9/47] [Batch 100/1077] [D loss: 0.448597] [G loss: 0.165579] [ema: 0.999292] 
[Epoch 9/47] [Batch 200/1077] [D loss: 0.485967] [G loss: 0.165570] [ema: 0.999300] 
[Epoch 9/47] [Batch 300/1077] [D loss: 0.452835] [G loss: 0.170390] [ema: 0.999307] 
[Epoch 9/47] [Batch 400/1077] [D loss: 0.427761] [G loss: 0.171445] [ema: 0.999313] 
[Epoch 9/47] [Batch 500/1077] [D loss: 0.439446] [G loss: 0.185868] [ema: 0.999320] 
[Epoch 9/47] [Batch 600/1077] [D loss: 0.429199] [G loss: 0.182079] [ema: 0.999327] 
[Epoch 9/47] [Batch 700/1077] [D loss: 0.395591] [G loss: 0.177973] [ema: 0.999333] 
[Epoch 9/47] [Batch 800/1077] [D loss: 0.469700] [G loss: 0.145224] [ema: 0.999340] 
[Epoch 9/47] [Batch 900/1077] [D loss: 0.477331] [G loss: 0.163096] [ema: 0.999346] 
[Epoch 9/47] [Batch 1000/1077] [D loss: 0.436163] [G loss: 0.164266] [ema: 0.999352] 
[Epoch 10/47] [Batch 0/1077] [D loss: 0.372442] [G loss: 0.164514] [ema: 0.999357] 
[Epoch 10/47] [Batch 100/1077] [D loss: 0.446613] [G loss: 0.166676] [ema: 0.999363] 
[Epoch 10/47] [Batch 200/1077] [D loss: 0.422813] [G loss: 0.165564] [ema: 0.999368] 
[Epoch 10/47] [Batch 300/1077] [D loss: 0.423868] [G loss: 0.199526] [ema: 0.999374] 
[Epoch 10/47] [Batch 400/1077] [D loss: 0.414818] [G loss: 0.155247] [ema: 0.999380] 
[Epoch 10/47] [Batch 500/1077] [D loss: 0.454586] [G loss: 0.158648] [ema: 0.999385] 
[Epoch 10/47] [Batch 600/1077] [D loss: 0.387194] [G loss: 0.178278] [ema: 0.999391] 
[Epoch 10/47] [Batch 700/1077] [D loss: 0.442206] [G loss: 0.187520] [ema: 0.999396] 
[Epoch 10/47] [Batch 800/1077] [D loss: 0.431384] [G loss: 0.164937] [ema: 0.999401] 
[Epoch 10/47] [Batch 900/1077] [D loss: 0.440130] [G loss: 0.166769] [ema: 0.999406] 
[Epoch 10/47] [Batch 1000/1077] [D loss: 0.414390] [G loss: 0.176725] [ema: 0.999411] 
[Epoch 11/47] [Batch 0/1077] [D loss: 0.445384] [G loss: 0.195379] [ema: 0.999415] 
[Epoch 11/47] [Batch 100/1077] [D loss: 0.412467] [G loss: 0.162405] [ema: 0.999420] 
[Epoch 11/47] [Batch 200/1077] [D loss: 0.366332] [G loss: 0.192270] [ema: 0.999425] 
[Epoch 11/47] [Batch 300/1077] [D loss: 0.362758] [G loss: 0.197244] [ema: 0.999430] 
[Epoch 11/47] [Batch 400/1077] [D loss: 0.394329] [G loss: 0.180924] [ema: 0.999434] 
[Epoch 11/47] [Batch 500/1077] [D loss: 0.395382] [G loss: 0.207614] [ema: 0.999439] 
[Epoch 11/47] [Batch 600/1077] [D loss: 0.415492] [G loss: 0.151158] [ema: 0.999443] 
[Epoch 11/47] [Batch 700/1077] [D loss: 0.439571] [G loss: 0.151026] [ema: 0.999448] 
[Epoch 11/47] [Batch 800/1077] [D loss: 0.401106] [G loss: 0.178849] [ema: 0.999452] 
[Epoch 11/47] [Batch 900/1077] [D loss: 0.491454] [G loss: 0.162616] [ema: 0.999456] 
[Epoch 11/47] [Batch 1000/1077] [D loss: 0.431160] [G loss: 0.153150] [ema: 0.999461] 



Saving checkpoint 2 in logs/daghar_50000_30_100/walk_50000_D_30_2024_10_18_02_11_49/Model



[Epoch 12/47] [Batch 0/1077] [D loss: 0.449005] [G loss: 0.173591] [ema: 0.999464] 
[Epoch 12/47] [Batch 100/1077] [D loss: 0.414243] [G loss: 0.153213] [ema: 0.999468] 
[Epoch 12/47] [Batch 200/1077] [D loss: 0.405130] [G loss: 0.171820] [ema: 0.999472] 
[Epoch 12/47] [Batch 300/1077] [D loss: 0.415284] [G loss: 0.135855] [ema: 0.999476] 
[Epoch 12/47] [Batch 400/1077] [D loss: 0.449274] [G loss: 0.191035] [ema: 0.999480] 
[Epoch 12/47] [Batch 500/1077] [D loss: 0.443854] [G loss: 0.184872] [ema: 0.999484] 
[Epoch 12/47] [Batch 600/1077] [D loss: 0.474846] [G loss: 0.170873] [ema: 0.999488] 
[Epoch 12/47] [Batch 700/1077] [D loss: 0.384813] [G loss: 0.194203] [ema: 0.999491] 
[Epoch 12/47] [Batch 800/1077] [D loss: 0.378824] [G loss: 0.168874] [ema: 0.999495] 
[Epoch 12/47] [Batch 900/1077] [D loss: 0.392286] [G loss: 0.185327] [ema: 0.999499] 
[Epoch 12/47] [Batch 1000/1077] [D loss: 0.449253] [G loss: 0.148694] [ema: 0.999502] 
[Epoch 13/47] [Batch 0/1077] [D loss: 0.387124] [G loss: 0.178310] [ema: 0.999505] 
[Epoch 13/47] [Batch 100/1077] [D loss: 0.446080] [G loss: 0.182636] [ema: 0.999509] 
[Epoch 13/47] [Batch 200/1077] [D loss: 0.360023] [G loss: 0.190684] [ema: 0.999512] 
[Epoch 13/47] [Batch 300/1077] [D loss: 0.441801] [G loss: 0.159168] [ema: 0.999515] 
[Epoch 13/47] [Batch 400/1077] [D loss: 0.429398] [G loss: 0.153241] [ema: 0.999519] 
[Epoch 13/47] [Batch 500/1077] [D loss: 0.398036] [G loss: 0.158173] [ema: 0.999522] 
[Epoch 13/47] [Batch 600/1077] [D loss: 0.407764] [G loss: 0.157633] [ema: 0.999525] 
[Epoch 13/47] [Batch 700/1077] [D loss: 0.448824] [G loss: 0.205707] [ema: 0.999529] 
[Epoch 13/47] [Batch 800/1077] [D loss: 0.382541] [G loss: 0.143022] [ema: 0.999532] 
[Epoch 13/47] [Batch 900/1077] [D loss: 0.391423] [G loss: 0.163502] [ema: 0.999535] 
[Epoch 13/47] [Batch 1000/1077] [D loss: 0.406967] [G loss: 0.146851] [ema: 0.999538] 
[Epoch 14/47] [Batch 0/1077] [D loss: 0.442860] [G loss: 0.172966] [ema: 0.999540] 
[Epoch 14/47] [Batch 100/1077] [D loss: 0.400985] [G loss: 0.209103] [ema: 0.999543] 
[Epoch 14/47] [Batch 200/1077] [D loss: 0.465908] [G loss: 0.187864] [ema: 0.999546] 
[Epoch 14/47] [Batch 300/1077] [D loss: 0.412060] [G loss: 0.172064] [ema: 0.999549] 
[Epoch 14/47] [Batch 400/1077] [D loss: 0.387735] [G loss: 0.175543] [ema: 0.999552] 
[Epoch 14/47] [Batch 500/1077] [D loss: 0.391011] [G loss: 0.174823] [ema: 0.999555] 
[Epoch 14/47] [Batch 600/1077] [D loss: 0.435013] [G loss: 0.177865] [ema: 0.999558] 
[Epoch 14/47] [Batch 700/1077] [D loss: 0.404818] [G loss: 0.159517] [ema: 0.999561] 
[Epoch 14/47] [Batch 800/1077] [D loss: 0.491775] [G loss: 0.148012] [ema: 0.999564] 
[Epoch 14/47] [Batch 900/1077] [D loss: 0.443633] [G loss: 0.173638] [ema: 0.999566] 
[Epoch 14/47] [Batch 1000/1077] [D loss: 0.400252] [G loss: 0.203559] [ema: 0.999569] 
[Epoch 15/47] [Batch 0/1077] [D loss: 0.352648] [G loss: 0.179698] [ema: 0.999571] 
[Epoch 15/47] [Batch 100/1077] [D loss: 0.351116] [G loss: 0.175699] [ema: 0.999574] 
[Epoch 15/47] [Batch 200/1077] [D loss: 0.392980] [G loss: 0.178665] [ema: 0.999576] 
[Epoch 15/47] [Batch 300/1077] [D loss: 0.310629] [G loss: 0.185561] [ema: 0.999579] 
[Epoch 15/47] [Batch 400/1077] [D loss: 0.373761] [G loss: 0.172212] [ema: 0.999581] 
[Epoch 15/47] [Batch 500/1077] [D loss: 0.381473] [G loss: 0.167808] [ema: 0.999584] 
[Epoch 15/47] [Batch 600/1077] [D loss: 0.413268] [G loss: 0.190811] [ema: 0.999586] 
[Epoch 15/47] [Batch 700/1077] [D loss: 0.418403] [G loss: 0.174295] [ema: 0.999589] 
[Epoch 15/47] [Batch 800/1077] [D loss: 0.476281] [G loss: 0.144152] [ema: 0.999591] 
[Epoch 15/47] [Batch 900/1077] [D loss: 0.383921] [G loss: 0.180180] [ema: 0.999594] 
[Epoch 15/47] [Batch 1000/1077] [D loss: 0.402322] [G loss: 0.182440] [ema: 0.999596] 
[Epoch 16/47] [Batch 0/1077] [D loss: 0.425327] [G loss: 0.182416] [ema: 0.999598] 
[Epoch 16/47] [Batch 100/1077] [D loss: 0.411108] [G loss: 0.157055] [ema: 0.999600] 
[Epoch 16/47] [Batch 200/1077] [D loss: 0.417998] [G loss: 0.175843] [ema: 0.999602] 
[Epoch 16/47] [Batch 300/1077] [D loss: 0.425471] [G loss: 0.156779] [ema: 0.999605] 
[Epoch 16/47] [Batch 400/1077] [D loss: 0.457949] [G loss: 0.176263] [ema: 0.999607] 
[Epoch 16/47] [Batch 500/1077] [D loss: 0.356102] [G loss: 0.179152] [ema: 0.999609] 
[Epoch 16/47] [Batch 600/1077] [D loss: 0.418084] [G loss: 0.202983] [ema: 0.999611] 
[Epoch 16/47] [Batch 700/1077] [D loss: 0.417917] [G loss: 0.175995] [ema: 0.999614] 
[Epoch 16/47] [Batch 800/1077] [D loss: 0.394588] [G loss: 0.176071] [ema: 0.999616] 
[Epoch 16/47] [Batch 900/1077] [D loss: 0.421742] [G loss: 0.172479] [ema: 0.999618] 
[Epoch 16/47] [Batch 1000/1077] [D loss: 0.461194] [G loss: 0.159348] [ema: 0.999620] 
[Epoch 17/47] [Batch 0/1077] [D loss: 0.416757] [G loss: 0.175008] [ema: 0.999621] 
[Epoch 17/47] [Batch 100/1077] [D loss: 0.416354] [G loss: 0.196996] [ema: 0.999624] 
[Epoch 17/47] [Batch 200/1077] [D loss: 0.427834] [G loss: 0.194145] [ema: 0.999626] 
[Epoch 17/47] [Batch 300/1077] [D loss: 0.361999] [G loss: 0.171081] [ema: 0.999628] 
[Epoch 17/47] [Batch 400/1077] [D loss: 0.408557] [G loss: 0.164914] [ema: 0.999630] 
[Epoch 17/47] [Batch 500/1077] [D loss: 0.419177] [G loss: 0.193420] [ema: 0.999632] 
[Epoch 17/47] [Batch 600/1077] [D loss: 0.402049] [G loss: 0.168933] [ema: 0.999633] 
[Epoch 17/47] [Batch 700/1077] [D loss: 0.368254] [G loss: 0.182084] [ema: 0.999635] 
[Epoch 17/47] [Batch 800/1077] [D loss: 0.457255] [G loss: 0.178407] [ema: 0.999637] 
[Epoch 17/47] [Batch 900/1077] [D loss: 0.364934] [G loss: 0.168080] [ema: 0.999639] 
[Epoch 17/47] [Batch 1000/1077] [D loss: 0.471115] [G loss: 0.160835] [ema: 0.999641] 
[Epoch 18/47] [Batch 0/1077] [D loss: 0.418472] [G loss: 0.139834] [ema: 0.999643] 
[Epoch 18/47] [Batch 100/1077] [D loss: 0.445247] [G loss: 0.151581] [ema: 0.999644] 
[Epoch 18/47] [Batch 200/1077] [D loss: 0.464813] [G loss: 0.178924] [ema: 0.999646] 
[Epoch 18/47] [Batch 300/1077] [D loss: 0.374362] [G loss: 0.171039] [ema: 0.999648] 
[Epoch 18/47] [Batch 400/1077] [D loss: 0.327947] [G loss: 0.188877] [ema: 0.999650] 
[Epoch 18/47] [Batch 500/1077] [D loss: 0.316962] [G loss: 0.176323] [ema: 0.999652] 
[Epoch 18/47] [Batch 600/1077] [D loss: 0.313977] [G loss: 0.192021] [ema: 0.999653] 
[Epoch 18/47] [Batch 700/1077] [D loss: 0.310890] [G loss: 0.208730] [ema: 0.999655] 
[Epoch 18/47] [Batch 800/1077] [D loss: 0.406741] [G loss: 0.170574] [ema: 0.999657] 
[Epoch 18/47] [Batch 900/1077] [D loss: 0.418836] [G loss: 0.158216] [ema: 0.999658] 
[Epoch 18/47] [Batch 1000/1077] [D loss: 0.457348] [G loss: 0.167106] [ema: 0.999660] 
[Epoch 19/47] [Batch 0/1077] [D loss: 0.379196] [G loss: 0.161445] [ema: 0.999661] 
[Epoch 19/47] [Batch 100/1077] [D loss: 0.442187] [G loss: 0.168413] [ema: 0.999663] 
[Epoch 19/47] [Batch 200/1077] [D loss: 0.388173] [G loss: 0.209457] [ema: 0.999665] 
[Epoch 19/47] [Batch 300/1077] [D loss: 0.386787] [G loss: 0.168778] [ema: 0.999666] 
[Epoch 19/47] [Batch 400/1077] [D loss: 0.363712] [G loss: 0.177945] [ema: 0.999668] 
[Epoch 19/47] [Batch 500/1077] [D loss: 0.410010] [G loss: 0.180022] [ema: 0.999669] 
[Epoch 19/47] [Batch 600/1077] [D loss: 0.420110] [G loss: 0.169386] [ema: 0.999671] 
[Epoch 19/47] [Batch 700/1077] [D loss: 0.372700] [G loss: 0.185487] [ema: 0.999673] 
[Epoch 19/47] [Batch 800/1077] [D loss: 0.409543] [G loss: 0.160517] [ema: 0.999674] 
[Epoch 19/47] [Batch 900/1077] [D loss: 0.410662] [G loss: 0.154926] [ema: 0.999676] 
[Epoch 19/47] [Batch 1000/1077] [D loss: 0.427061] [G loss: 0.170009] [ema: 0.999677] 
[Epoch 20/47] [Batch 0/1077] [D loss: 0.481012] [G loss: 0.147210] [ema: 0.999678] 
[Epoch 20/47] [Batch 100/1077] [D loss: 0.382164] [G loss: 0.194861] [ema: 0.999680] 
[Epoch 20/47] [Batch 200/1077] [D loss: 0.419477] [G loss: 0.184828] [ema: 0.999681] 
[Epoch 20/47] [Batch 300/1077] [D loss: 0.443123] [G loss: 0.186471] [ema: 0.999683] 
[Epoch 20/47] [Batch 400/1077] [D loss: 0.453456] [G loss: 0.161434] [ema: 0.999684] 
[Epoch 20/47] [Batch 500/1077] [D loss: 0.413483] [G loss: 0.157674] [ema: 0.999686] 
[Epoch 20/47] [Batch 600/1077] [D loss: 0.511061] [G loss: 0.163938] [ema: 0.999687] 
[Epoch 20/47] [Batch 700/1077] [D loss: 0.431380] [G loss: 0.130612] [ema: 0.999688] 
[Epoch 20/47] [Batch 800/1077] [D loss: 0.418887] [G loss: 0.178425] [ema: 0.999690] 
[Epoch 20/47] [Batch 900/1077] [D loss: 0.469831] [G loss: 0.152717] [ema: 0.999691] 
[Epoch 20/47] [Batch 1000/1077] [D loss: 0.428646] [G loss: 0.169286] [ema: 0.999693] 
[Epoch 21/47] [Batch 0/1077] [D loss: 0.447632] [G loss: 0.140082] [ema: 0.999694] 
[Epoch 21/47] [Batch 100/1077] [D loss: 0.428391] [G loss: 0.163709] [ema: 0.999695] 
[Epoch 21/47] [Batch 200/1077] [D loss: 0.460551] [G loss: 0.152729] [ema: 0.999696] 
[Epoch 21/47] [Batch 300/1077] [D loss: 0.431243] [G loss: 0.180695] [ema: 0.999698] 
[Epoch 21/47] [Batch 400/1077] [D loss: 0.443132] [G loss: 0.136283] [ema: 0.999699] 
[Epoch 21/47] [Batch 500/1077] [D loss: 0.443748] [G loss: 0.154496] [ema: 0.999700] 
[Epoch 21/47] [Batch 600/1077] [D loss: 0.440935] [G loss: 0.136899] [ema: 0.999701] 
[Epoch 21/47] [Batch 700/1077] [D loss: 0.438416] [G loss: 0.142291] [ema: 0.999703] 
[Epoch 21/47] [Batch 800/1077] [D loss: 0.415192] [G loss: 0.154461] [ema: 0.999704] 
[Epoch 21/47] [Batch 900/1077] [D loss: 0.443121] [G loss: 0.151530] [ema: 0.999705] 
[Epoch 21/47] [Batch 1000/1077] [D loss: 0.458545] [G loss: 0.146070] [ema: 0.999707] 
[Epoch 22/47] [Batch 0/1077] [D loss: 0.469981] [G loss: 0.177845] [ema: 0.999708] 
[Epoch 22/47] [Batch 100/1077] [D loss: 0.448935] [G loss: 0.177122] [ema: 0.999709] 
[Epoch 22/47] [Batch 200/1077] [D loss: 0.404751] [G loss: 0.201152] [ema: 0.999710] 
[Epoch 22/47] [Batch 300/1077] [D loss: 0.389071] [G loss: 0.176449] [ema: 0.999711] 
[Epoch 22/47] [Batch 400/1077] [D loss: 0.535592] [G loss: 0.161831] [ema: 0.999712] 
[Epoch 22/47] [Batch 500/1077] [D loss: 0.550388] [G loss: 0.145474] [ema: 0.999714] 
[Epoch 22/47] [Batch 600/1077] [D loss: 0.412131] [G loss: 0.172081] [ema: 0.999715] 
[Epoch 22/47] [Batch 700/1077] [D loss: 0.490471] [G loss: 0.151835] [ema: 0.999716] 
[Epoch 22/47] [Batch 800/1077] [D loss: 0.390661] [G loss: 0.166830] [ema: 0.999717] 
[Epoch 22/47] [Batch 900/1077] [D loss: 0.460226] [G loss: 0.161978] [ema: 0.999718] 
[Epoch 22/47] [Batch 1000/1077] [D loss: 0.443709] [G loss: 0.167683] [ema: 0.999719] 
[Epoch 23/47] [Batch 0/1077] [D loss: 0.424895] [G loss: 0.165010] [ema: 0.999720] 
[Epoch 23/47] [Batch 100/1077] [D loss: 0.487206] [G loss: 0.168405] [ema: 0.999721] 
[Epoch 23/47] [Batch 200/1077] [D loss: 0.460660] [G loss: 0.185905] [ema: 0.999722] 
[Epoch 23/47] [Batch 300/1077] [D loss: 0.422940] [G loss: 0.149746] [ema: 0.999724] 
[Epoch 23/47] [Batch 400/1077] [D loss: 0.468600] [G loss: 0.156750] [ema: 0.999725] 
[Epoch 23/47] [Batch 500/1077] [D loss: 0.460090] [G loss: 0.171150] [ema: 0.999726] 
[Epoch 23/47] [Batch 600/1077] [D loss: 0.485334] [G loss: 0.185064] [ema: 0.999727] 
[Epoch 23/47] [Batch 700/1077] [D loss: 0.444611] [G loss: 0.161888] [ema: 0.999728] 
[Epoch 23/47] [Batch 800/1077] [D loss: 0.394929] [G loss: 0.172069] [ema: 0.999729] 
[Epoch 23/47] [Batch 900/1077] [D loss: 0.471744] [G loss: 0.173079] [ema: 0.999730] 
[Epoch 23/47] [Batch 1000/1077] [D loss: 0.498895] [G loss: 0.162591] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_50000_30_100/walk_50000_D_30_2024_10_18_02_11_49/Model



[Epoch 24/47] [Batch 0/1077] [D loss: 0.478411] [G loss: 0.166383] [ema: 0.999732] 
[Epoch 24/47] [Batch 100/1077] [D loss: 0.476762] [G loss: 0.168270] [ema: 0.999733] 
[Epoch 24/47] [Batch 200/1077] [D loss: 0.412662] [G loss: 0.162038] [ema: 0.999734] 
[Epoch 24/47] [Batch 300/1077] [D loss: 0.421747] [G loss: 0.169133] [ema: 0.999735] 
[Epoch 24/47] [Batch 400/1077] [D loss: 0.467171] [G loss: 0.149346] [ema: 0.999736] 
[Epoch 24/47] [Batch 500/1077] [D loss: 0.444255] [G loss: 0.151899] [ema: 0.999737] 
[Epoch 24/47] [Batch 600/1077] [D loss: 0.471291] [G loss: 0.152448] [ema: 0.999738] 
[Epoch 24/47] [Batch 700/1077] [D loss: 0.477561] [G loss: 0.144583] [ema: 0.999739] 
[Epoch 24/47] [Batch 800/1077] [D loss: 0.452067] [G loss: 0.125018] [ema: 0.999740] 
[Epoch 24/47] [Batch 900/1077] [D loss: 0.453572] [G loss: 0.150035] [ema: 0.999741] 
[Epoch 24/47] [Batch 1000/1077] [D loss: 0.446238] [G loss: 0.149143] [ema: 0.999742] 
[Epoch 25/47] [Batch 0/1077] [D loss: 0.491395] [G loss: 0.149002] [ema: 0.999743] 
[Epoch 25/47] [Batch 100/1077] [D loss: 0.501778] [G loss: 0.157890] [ema: 0.999744] 
[Epoch 25/47] [Batch 200/1077] [D loss: 0.484449] [G loss: 0.148764] [ema: 0.999744] 
[Epoch 25/47] [Batch 300/1077] [D loss: 0.457344] [G loss: 0.139525] [ema: 0.999745] 
[Epoch 25/47] [Batch 400/1077] [D loss: 0.484842] [G loss: 0.148821] [ema: 0.999746] 
[Epoch 25/47] [Batch 500/1077] [D loss: 0.427530] [G loss: 0.170001] [ema: 0.999747] 
[Epoch 25/47] [Batch 600/1077] [D loss: 0.420338] [G loss: 0.159466] [ema: 0.999748] 
[Epoch 25/47] [Batch 700/1077] [D loss: 0.456657] [G loss: 0.135051] [ema: 0.999749] 
[Epoch 25/47] [Batch 800/1077] [D loss: 0.467665] [G loss: 0.140488] [ema: 0.999750] 
[Epoch 25/47] [Batch 900/1077] [D loss: 0.514150] [G loss: 0.151543] [ema: 0.999751] 
[Epoch 25/47] [Batch 1000/1077] [D loss: 0.464895] [G loss: 0.149686] [ema: 0.999752] 
[Epoch 26/47] [Batch 0/1077] [D loss: 0.486419] [G loss: 0.157111] [ema: 0.999752] 
[Epoch 26/47] [Batch 100/1077] [D loss: 0.453475] [G loss: 0.153802] [ema: 0.999753] 
[Epoch 26/47] [Batch 200/1077] [D loss: 0.428493] [G loss: 0.122436] [ema: 0.999754] 
[Epoch 26/47] [Batch 300/1077] [D loss: 0.459099] [G loss: 0.142505] [ema: 0.999755] 
[Epoch 26/47] [Batch 400/1077] [D loss: 0.461168] [G loss: 0.141034] [ema: 0.999756] 
[Epoch 26/47] [Batch 500/1077] [D loss: 0.482315] [G loss: 0.138930] [ema: 0.999757] 
[Epoch 26/47] [Batch 600/1077] [D loss: 0.442068] [G loss: 0.155678] [ema: 0.999758] 
[Epoch 26/47] [Batch 700/1077] [D loss: 0.454017] [G loss: 0.174678] [ema: 0.999759] 
[Epoch 26/47] [Batch 800/1077] [D loss: 0.471134] [G loss: 0.156107] [ema: 0.999759] 
[Epoch 26/47] [Batch 900/1077] [D loss: 0.497083] [G loss: 0.162065] [ema: 0.999760] 
[Epoch 26/47] [Batch 1000/1077] [D loss: 0.413026] [G loss: 0.150329] [ema: 0.999761] 
[Epoch 27/47] [Batch 0/1077] [D loss: 0.497777] [G loss: 0.159058] [ema: 0.999762] 
[Epoch 27/47] [Batch 100/1077] [D loss: 0.448763] [G loss: 0.161723] [ema: 0.999762] 
[Epoch 27/47] [Batch 200/1077] [D loss: 0.491638] [G loss: 0.147761] [ema: 0.999763] 
[Epoch 27/47] [Batch 300/1077] [D loss: 0.470875] [G loss: 0.167776] [ema: 0.999764] 
[Epoch 27/47] [Batch 400/1077] [D loss: 0.463214] [G loss: 0.161538] [ema: 0.999765] 
[Epoch 27/47] [Batch 500/1077] [D loss: 0.450273] [G loss: 0.150127] [ema: 0.999766] 
[Epoch 27/47] [Batch 600/1077] [D loss: 0.523827] [G loss: 0.130094] [ema: 0.999766] 
[Epoch 27/47] [Batch 700/1077] [D loss: 0.443878] [G loss: 0.144484] [ema: 0.999767] 
[Epoch 27/47] [Batch 800/1077] [D loss: 0.442503] [G loss: 0.129786] [ema: 0.999768] 
[Epoch 27/47] [Batch 900/1077] [D loss: 0.516114] [G loss: 0.157583] [ema: 0.999769] 
[Epoch 27/47] [Batch 1000/1077] [D loss: 0.457393] [G loss: 0.152919] [ema: 0.999770] 
[Epoch 28/47] [Batch 0/1077] [D loss: 0.490616] [G loss: 0.158233] [ema: 0.999770] 
[Epoch 28/47] [Batch 100/1077] [D loss: 0.462122] [G loss: 0.180836] [ema: 0.999771] 
[Epoch 28/47] [Batch 200/1077] [D loss: 0.470470] [G loss: 0.140152] [ema: 0.999772] 
[Epoch 28/47] [Batch 300/1077] [D loss: 0.498733] [G loss: 0.152760] [ema: 0.999772] 
[Epoch 28/47] [Batch 400/1077] [D loss: 0.487258] [G loss: 0.145151] [ema: 0.999773] 
[Epoch 28/47] [Batch 500/1077] [D loss: 0.438179] [G loss: 0.149713] [ema: 0.999774] 
[Epoch 28/47] [Batch 600/1077] [D loss: 0.520621] [G loss: 0.153080] [ema: 0.999775] 
[Epoch 28/47] [Batch 700/1077] [D loss: 0.477185] [G loss: 0.137968] [ema: 0.999775] 
[Epoch 28/47] [Batch 800/1077] [D loss: 0.414807] [G loss: 0.159543] [ema: 0.999776] 
[Epoch 28/47] [Batch 900/1077] [D loss: 0.489688] [G loss: 0.142522] [ema: 0.999777] 
[Epoch 28/47] [Batch 1000/1077] [D loss: 0.465811] [G loss: 0.173603] [ema: 0.999778] 
[Epoch 29/47] [Batch 0/1077] [D loss: 0.484436] [G loss: 0.147219] [ema: 0.999778] 
[Epoch 29/47] [Batch 100/1077] [D loss: 0.480659] [G loss: 0.152117] [ema: 0.999779] 
[Epoch 29/47] [Batch 200/1077] [D loss: 0.481808] [G loss: 0.147860] [ema: 0.999780] 
[Epoch 29/47] [Batch 300/1077] [D loss: 0.468913] [G loss: 0.156604] [ema: 0.999780] 
[Epoch 29/47] [Batch 400/1077] [D loss: 0.442411] [G loss: 0.180769] [ema: 0.999781] 
[Epoch 29/47] [Batch 500/1077] [D loss: 0.514296] [G loss: 0.141984] [ema: 0.999782] 
[Epoch 29/47] [Batch 600/1077] [D loss: 0.467127] [G loss: 0.146188] [ema: 0.999782] 
[Epoch 29/47] [Batch 700/1077] [D loss: 0.457670] [G loss: 0.162211] [ema: 0.999783] 
[Epoch 29/47] [Batch 800/1077] [D loss: 0.400953] [G loss: 0.146063] [ema: 0.999784] 
[Epoch 29/47] [Batch 900/1077] [D loss: 0.450846] [G loss: 0.142057] [ema: 0.999784] 
[Epoch 29/47] [Batch 1000/1077] [D loss: 0.504016] [G loss: 0.148802] [ema: 0.999785] 
[Epoch 30/47] [Batch 0/1077] [D loss: 0.468162] [G loss: 0.167310] [ema: 0.999785] 
[Epoch 30/47] [Batch 100/1077] [D loss: 0.363584] [G loss: 0.160689] [ema: 0.999786] 
[Epoch 30/47] [Batch 200/1077] [D loss: 0.438173] [G loss: 0.154185] [ema: 0.999787] 
[Epoch 30/47] [Batch 300/1077] [D loss: 0.489381] [G loss: 0.171816] [ema: 0.999787] 
[Epoch 30/47] [Batch 400/1077] [D loss: 0.514763] [G loss: 0.156913] [ema: 0.999788] 
[Epoch 30/47] [Batch 500/1077] [D loss: 0.471038] [G loss: 0.182168] [ema: 0.999789] 
[Epoch 30/47] [Batch 600/1077] [D loss: 0.407739] [G loss: 0.144776] [ema: 0.999789] 
[Epoch 30/47] [Batch 700/1077] [D loss: 0.445080] [G loss: 0.184541] [ema: 0.999790] 
[Epoch 30/47] [Batch 800/1077] [D loss: 0.477286] [G loss: 0.156692] [ema: 0.999791] 
[Epoch 30/47] [Batch 900/1077] [D loss: 0.422061] [G loss: 0.155914] [ema: 0.999791] 
[Epoch 30/47] [Batch 1000/1077] [D loss: 0.404050] [G loss: 0.167501] [ema: 0.999792] 
[Epoch 31/47] [Batch 0/1077] [D loss: 0.432239] [G loss: 0.172199] [ema: 0.999792] 
[Epoch 31/47] [Batch 100/1077] [D loss: 0.459581] [G loss: 0.159329] [ema: 0.999793] 
[Epoch 31/47] [Batch 200/1077] [D loss: 0.421938] [G loss: 0.171179] [ema: 0.999794] 
[Epoch 31/47] [Batch 300/1077] [D loss: 0.491755] [G loss: 0.164617] [ema: 0.999794] 
[Epoch 31/47] [Batch 400/1077] [D loss: 0.414934] [G loss: 0.167105] [ema: 0.999795] 
[Epoch 31/47] [Batch 500/1077] [D loss: 0.451066] [G loss: 0.163724] [ema: 0.999795] 
[Epoch 31/47] [Batch 600/1077] [D loss: 0.474111] [G loss: 0.137069] [ema: 0.999796] 
[Epoch 31/47] [Batch 700/1077] [D loss: 0.405711] [G loss: 0.172054] [ema: 0.999797] 
[Epoch 31/47] [Batch 800/1077] [D loss: 0.420961] [G loss: 0.172156] [ema: 0.999797] 
[Epoch 31/47] [Batch 900/1077] [D loss: 0.397813] [G loss: 0.159382] [ema: 0.999798] 
[Epoch 31/47] [Batch 1000/1077] [D loss: 0.389367] [G loss: 0.160812] [ema: 0.999798] 
[Epoch 32/47] [Batch 0/1077] [D loss: 0.434512] [G loss: 0.179149] [ema: 0.999799] 
[Epoch 32/47] [Batch 100/1077] [D loss: 0.505737] [G loss: 0.186776] [ema: 0.999799] 
[Epoch 32/47] [Batch 200/1077] [D loss: 0.433086] [G loss: 0.174036] [ema: 0.999800] 
[Epoch 32/47] [Batch 300/1077] [D loss: 0.439390] [G loss: 0.160416] [ema: 0.999801] 
[Epoch 32/47] [Batch 400/1077] [D loss: 0.481613] [G loss: 0.147189] [ema: 0.999801] 
[Epoch 32/47] [Batch 500/1077] [D loss: 0.385005] [G loss: 0.168319] [ema: 0.999802] 
[Epoch 32/47] [Batch 600/1077] [D loss: 0.404136] [G loss: 0.165647] [ema: 0.999802] 
[Epoch 32/47] [Batch 700/1077] [D loss: 0.485426] [G loss: 0.163855] [ema: 0.999803] 
[Epoch 32/47] [Batch 800/1077] [D loss: 0.400168] [G loss: 0.143451] [ema: 0.999803] 
[Epoch 32/47] [Batch 900/1077] [D loss: 0.469389] [G loss: 0.155242] [ema: 0.999804] 
[Epoch 32/47] [Batch 1000/1077] [D loss: 0.419501] [G loss: 0.152607] [ema: 0.999805] 
[Epoch 33/47] [Batch 0/1077] [D loss: 0.464599] [G loss: 0.202395] [ema: 0.999805] 
[Epoch 33/47] [Batch 100/1077] [D loss: 0.441478] [G loss: 0.178814] [ema: 0.999806] 
[Epoch 33/47] [Batch 200/1077] [D loss: 0.443953] [G loss: 0.168402] [ema: 0.999806] 
[Epoch 33/47] [Batch 300/1077] [D loss: 0.422215] [G loss: 0.169715] [ema: 0.999807] 
[Epoch 33/47] [Batch 400/1077] [D loss: 0.438167] [G loss: 0.134333] [ema: 0.999807] 
[Epoch 33/47] [Batch 500/1077] [D loss: 0.418034] [G loss: 0.144119] [ema: 0.999808] 
[Epoch 33/47] [Batch 600/1077] [D loss: 0.432802] [G loss: 0.182058] [ema: 0.999808] 
[Epoch 33/47] [Batch 700/1077] [D loss: 0.376908] [G loss: 0.144849] [ema: 0.999809] 
[Epoch 33/47] [Batch 800/1077] [D loss: 0.465450] [G loss: 0.154559] [ema: 0.999809] 
[Epoch 33/47] [Batch 900/1077] [D loss: 0.435500] [G loss: 0.150267] [ema: 0.999810] 
[Epoch 33/47] [Batch 1000/1077] [D loss: 0.441465] [G loss: 0.157590] [ema: 0.999810] 
[Epoch 34/47] [Batch 0/1077] [D loss: 0.424093] [G loss: 0.160590] [ema: 0.999811] 
[Epoch 34/47] [Batch 100/1077] [D loss: 0.469793] [G loss: 0.193652] [ema: 0.999811] 
[Epoch 34/47] [Batch 200/1077] [D loss: 0.437473] [G loss: 0.146065] [ema: 0.999812] 
[Epoch 34/47] [Batch 300/1077] [D loss: 0.451882] [G loss: 0.141574] [ema: 0.999812] 
[Epoch 34/47] [Batch 400/1077] [D loss: 0.409026] [G loss: 0.186797] [ema: 0.999813] 
[Epoch 34/47] [Batch 500/1077] [D loss: 0.448932] [G loss: 0.175827] [ema: 0.999813] 
[Epoch 34/47] [Batch 600/1077] [D loss: 0.427517] [G loss: 0.164627] [ema: 0.999814] 
[Epoch 34/47] [Batch 700/1077] [D loss: 0.383681] [G loss: 0.182742] [ema: 0.999814] 
[Epoch 34/47] [Batch 800/1077] [D loss: 0.458916] [G loss: 0.190964] [ema: 0.999815] 
[Epoch 34/47] [Batch 900/1077] [D loss: 0.437785] [G loss: 0.188587] [ema: 0.999815] 
[Epoch 34/47] [Batch 1000/1077] [D loss: 0.436392] [G loss: 0.161000] [ema: 0.999816] 
[Epoch 35/47] [Batch 0/1077] [D loss: 0.431262] [G loss: 0.132858] [ema: 0.999816] 
[Epoch 35/47] [Batch 100/1077] [D loss: 0.412163] [G loss: 0.156021] [ema: 0.999817] 
[Epoch 35/47] [Batch 200/1077] [D loss: 0.464009] [G loss: 0.184120] [ema: 0.999817] 
[Epoch 35/47] [Batch 300/1077] [D loss: 0.403744] [G loss: 0.181114] [ema: 0.999818] 
[Epoch 35/47] [Batch 400/1077] [D loss: 0.510908] [G loss: 0.155781] [ema: 0.999818] 
[Epoch 35/47] [Batch 500/1077] [D loss: 0.406059] [G loss: 0.169639] [ema: 0.999819] 
[Epoch 35/47] [Batch 600/1077] [D loss: 0.481358] [G loss: 0.153768] [ema: 0.999819] 
[Epoch 35/47] [Batch 700/1077] [D loss: 0.409297] [G loss: 0.156106] [ema: 0.999819] 
[Epoch 35/47] [Batch 800/1077] [D loss: 0.389002] [G loss: 0.184014] [ema: 0.999820] 
[Epoch 35/47] [Batch 900/1077] [D loss: 0.463361] [G loss: 0.178460] [ema: 0.999820] 
[Epoch 35/47] [Batch 1000/1077] [D loss: 0.432889] [G loss: 0.169976] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_50000_30_100/walk_50000_D_30_2024_10_18_02_11_49/Model



[Epoch 36/47] [Batch 0/1077] [D loss: 0.411121] [G loss: 0.161541] [ema: 0.999821] 
[Epoch 36/47] [Batch 100/1077] [D loss: 0.431477] [G loss: 0.160039] [ema: 0.999822] 
[Epoch 36/47] [Batch 200/1077] [D loss: 0.467470] [G loss: 0.165418] [ema: 0.999822] 
[Epoch 36/47] [Batch 300/1077] [D loss: 0.442690] [G loss: 0.170603] [ema: 0.999823] 
[Epoch 36/47] [Batch 400/1077] [D loss: 0.421218] [G loss: 0.169595] [ema: 0.999823] 
[Epoch 36/47] [Batch 500/1077] [D loss: 0.394517] [G loss: 0.179895] [ema: 0.999824] 
[Epoch 36/47] [Batch 600/1077] [D loss: 0.401729] [G loss: 0.192063] [ema: 0.999824] 
[Epoch 36/47] [Batch 700/1077] [D loss: 0.383017] [G loss: 0.154062] [ema: 0.999824] 
[Epoch 36/47] [Batch 800/1077] [D loss: 0.416650] [G loss: 0.158322] [ema: 0.999825] 
[Epoch 36/47] [Batch 900/1077] [D loss: 0.407972] [G loss: 0.159063] [ema: 0.999825] 
[Epoch 36/47] [Batch 1000/1077] [D loss: 0.369732] [G loss: 0.163456] [ema: 0.999826] 
[Epoch 37/47] [Batch 0/1077] [D loss: 0.393175] [G loss: 0.169260] [ema: 0.999826] 
[Epoch 37/47] [Batch 100/1077] [D loss: 0.450938] [G loss: 0.147364] [ema: 0.999827] 
[Epoch 37/47] [Batch 200/1077] [D loss: 0.448506] [G loss: 0.171607] [ema: 0.999827] 
[Epoch 37/47] [Batch 300/1077] [D loss: 0.425284] [G loss: 0.175117] [ema: 0.999827] 
[Epoch 37/47] [Batch 400/1077] [D loss: 0.507385] [G loss: 0.163055] [ema: 0.999828] 
[Epoch 37/47] [Batch 500/1077] [D loss: 0.415457] [G loss: 0.169138] [ema: 0.999828] 
[Epoch 37/47] [Batch 600/1077] [D loss: 0.451256] [G loss: 0.152068] [ema: 0.999829] 
[Epoch 37/47] [Batch 700/1077] [D loss: 0.437246] [G loss: 0.190718] [ema: 0.999829] 
[Epoch 37/47] [Batch 800/1077] [D loss: 0.486438] [G loss: 0.154750] [ema: 0.999829] 
[Epoch 37/47] [Batch 900/1077] [D loss: 0.445908] [G loss: 0.165369] [ema: 0.999830] 
[Epoch 37/47] [Batch 1000/1077] [D loss: 0.442002] [G loss: 0.165449] [ema: 0.999830] 
[Epoch 38/47] [Batch 0/1077] [D loss: 0.441815] [G loss: 0.140720] [ema: 0.999831] 
[Epoch 38/47] [Batch 100/1077] [D loss: 0.456633] [G loss: 0.138388] [ema: 0.999831] 
[Epoch 38/47] [Batch 200/1077] [D loss: 0.367218] [G loss: 0.195176] [ema: 0.999831] 
[Epoch 38/47] [Batch 300/1077] [D loss: 0.410534] [G loss: 0.152797] [ema: 0.999832] 
[Epoch 38/47] [Batch 400/1077] [D loss: 0.478115] [G loss: 0.153465] [ema: 0.999832] 
[Epoch 38/47] [Batch 500/1077] [D loss: 0.406900] [G loss: 0.161109] [ema: 0.999833] 
[Epoch 38/47] [Batch 600/1077] [D loss: 0.399854] [G loss: 0.147237] [ema: 0.999833] 
[Epoch 38/47] [Batch 700/1077] [D loss: 0.459355] [G loss: 0.159569] [ema: 0.999833] 
[Epoch 38/47] [Batch 800/1077] [D loss: 0.438502] [G loss: 0.143830] [ema: 0.999834] 
[Epoch 38/47] [Batch 900/1077] [D loss: 0.434555] [G loss: 0.174156] [ema: 0.999834] 
[Epoch 38/47] [Batch 1000/1077] [D loss: 0.376749] [G loss: 0.197108] [ema: 0.999835] 
[Epoch 39/47] [Batch 0/1077] [D loss: 0.439548] [G loss: 0.156751] [ema: 0.999835] 
[Epoch 39/47] [Batch 100/1077] [D loss: 0.416556] [G loss: 0.175667] [ema: 0.999835] 
[Epoch 39/47] [Batch 200/1077] [D loss: 0.434491] [G loss: 0.164309] [ema: 0.999836] 
[Epoch 39/47] [Batch 300/1077] [D loss: 0.371604] [G loss: 0.135442] [ema: 0.999836] 
[Epoch 39/47] [Batch 400/1077] [D loss: 0.456796] [G loss: 0.175367] [ema: 0.999837] 
[Epoch 39/47] [Batch 500/1077] [D loss: 0.490012] [G loss: 0.185008] [ema: 0.999837] 
[Epoch 39/47] [Batch 600/1077] [D loss: 0.485537] [G loss: 0.168878] [ema: 0.999837] 
[Epoch 39/47] [Batch 700/1077] [D loss: 0.394039] [G loss: 0.189103] [ema: 0.999838] 
[Epoch 39/47] [Batch 800/1077] [D loss: 0.423050] [G loss: 0.168586] [ema: 0.999838] 
[Epoch 39/47] [Batch 900/1077] [D loss: 0.387290] [G loss: 0.176537] [ema: 0.999838] 
[Epoch 39/47] [Batch 1000/1077] [D loss: 0.413515] [G loss: 0.173231] [ema: 0.999839] 
[Epoch 40/47] [Batch 0/1077] [D loss: 0.381150] [G loss: 0.195432] [ema: 0.999839] 
[Epoch 40/47] [Batch 100/1077] [D loss: 0.466679] [G loss: 0.170069] [ema: 0.999839] 
[Epoch 40/47] [Batch 200/1077] [D loss: 0.421161] [G loss: 0.160326] [ema: 0.999840] 
[Epoch 40/47] [Batch 300/1077] [D loss: 0.381569] [G loss: 0.161108] [ema: 0.999840] 
[Epoch 40/47] [Batch 400/1077] [D loss: 0.435015] [G loss: 0.152535] [ema: 0.999841] 
[Epoch 40/47] [Batch 500/1077] [D loss: 0.481891] [G loss: 0.180738] [ema: 0.999841] 
[Epoch 40/47] [Batch 600/1077] [D loss: 0.392021] [G loss: 0.145186] [ema: 0.999841] 
[Epoch 40/47] [Batch 700/1077] [D loss: 0.457545] [G loss: 0.156510] [ema: 0.999842] 
[Epoch 40/47] [Batch 800/1077] [D loss: 0.465928] [G loss: 0.153003] [ema: 0.999842] 
[Epoch 40/47] [Batch 900/1077] [D loss: 0.467288] [G loss: 0.161940] [ema: 0.999842] 
[Epoch 40/47] [Batch 1000/1077] [D loss: 0.422579] [G loss: 0.180164] [ema: 0.999843] 
[Epoch 41/47] [Batch 0/1077] [D loss: 0.448626] [G loss: 0.174202] [ema: 0.999843] 
[Epoch 41/47] [Batch 100/1077] [D loss: 0.423766] [G loss: 0.145246] [ema: 0.999843] 
[Epoch 41/47] [Batch 200/1077] [D loss: 0.489965] [G loss: 0.172877] [ema: 0.999844] 
[Epoch 41/47] [Batch 300/1077] [D loss: 0.448828] [G loss: 0.188441] [ema: 0.999844] 
[Epoch 41/47] [Batch 400/1077] [D loss: 0.435769] [G loss: 0.148020] [ema: 0.999844] 
[Epoch 41/47] [Batch 500/1077] [D loss: 0.434059] [G loss: 0.155976] [ema: 0.999845] 
[Epoch 41/47] [Batch 600/1077] [D loss: 0.409465] [G loss: 0.160353] [ema: 0.999845] 
[Epoch 41/47] [Batch 700/1077] [D loss: 0.378685] [G loss: 0.186268] [ema: 0.999845] 
[Epoch 41/47] [Batch 800/1077] [D loss: 0.390077] [G loss: 0.156778] [ema: 0.999846] 
[Epoch 41/47] [Batch 900/1077] [D loss: 0.464877] [G loss: 0.177913] [ema: 0.999846] 
[Epoch 41/47] [Batch 1000/1077] [D loss: 0.390729] [G loss: 0.192058] [ema: 0.999847] 
[Epoch 42/47] [Batch 0/1077] [D loss: 0.454187] [G loss: 0.179003] [ema: 0.999847] 
[Epoch 42/47] [Batch 100/1077] [D loss: 0.459521] [G loss: 0.175545] [ema: 0.999847] 
[Epoch 42/47] [Batch 200/1077] [D loss: 0.403782] [G loss: 0.195308] [ema: 0.999847] 
[Epoch 42/47] [Batch 300/1077] [D loss: 0.467731] [G loss: 0.169290] [ema: 0.999848] 
[Epoch 42/47] [Batch 400/1077] [D loss: 0.430514] [G loss: 0.163974] [ema: 0.999848] 
[Epoch 42/47] [Batch 500/1077] [D loss: 0.512216] [G loss: 0.161735] [ema: 0.999848] 
[Epoch 42/47] [Batch 600/1077] [D loss: 0.481331] [G loss: 0.155437] [ema: 0.999849] 
[Epoch 42/47] [Batch 700/1077] [D loss: 0.413110] [G loss: 0.177787] [ema: 0.999849] 
[Epoch 42/47] [Batch 800/1077] [D loss: 0.400412] [G loss: 0.173922] [ema: 0.999849] 
[Epoch 42/47] [Batch 900/1077] [D loss: 0.472406] [G loss: 0.163815] [ema: 0.999850] 
[Epoch 42/47] [Batch 1000/1077] [D loss: 0.438734] [G loss: 0.178295] [ema: 0.999850] 
[Epoch 43/47] [Batch 0/1077] [D loss: 0.431165] [G loss: 0.168311] [ema: 0.999850] 
[Epoch 43/47] [Batch 100/1077] [D loss: 0.434499] [G loss: 0.171629] [ema: 0.999851] 
[Epoch 43/47] [Batch 200/1077] [D loss: 0.457827] [G loss: 0.135249] [ema: 0.999851] 
[Epoch 43/47] [Batch 300/1077] [D loss: 0.444771] [G loss: 0.163211] [ema: 0.999851] 
[Epoch 43/47] [Batch 400/1077] [D loss: 0.393344] [G loss: 0.163873] [ema: 0.999852] 
[Epoch 43/47] [Batch 500/1077] [D loss: 0.487799] [G loss: 0.155692] [ema: 0.999852] 
[Epoch 43/47] [Batch 600/1077] [D loss: 0.438396] [G loss: 0.166098] [ema: 0.999852] 
[Epoch 43/47] [Batch 700/1077] [D loss: 0.395783] [G loss: 0.164356] [ema: 0.999853] 
[Epoch 43/47] [Batch 800/1077] [D loss: 0.393138] [G loss: 0.189675] [ema: 0.999853] 
[Epoch 43/47] [Batch 900/1077] [D loss: 0.424238] [G loss: 0.164151] [ema: 0.999853] 
[Epoch 43/47] [Batch 1000/1077] [D loss: 0.435520] [G loss: 0.179107] [ema: 0.999854] 
[Epoch 44/47] [Batch 0/1077] [D loss: 0.406948] [G loss: 0.148857] [ema: 0.999854] 
[Epoch 44/47] [Batch 100/1077] [D loss: 0.427396] [G loss: 0.169368] [ema: 0.999854] 
[Epoch 44/47] [Batch 200/1077] [D loss: 0.463160] [G loss: 0.158338] [ema: 0.999854] 
[Epoch 44/47] [Batch 300/1077] [D loss: 0.432698] [G loss: 0.134772] [ema: 0.999855] 
[Epoch 44/47] [Batch 400/1077] [D loss: 0.459706] [G loss: 0.144113] [ema: 0.999855] 
[Epoch 44/47] [Batch 500/1077] [D loss: 0.451167] [G loss: 0.172216] [ema: 0.999855] 
[Epoch 44/47] [Batch 600/1077] [D loss: 0.423339] [G loss: 0.145918] [ema: 0.999856] 
[Epoch 44/47] [Batch 700/1077] [D loss: 0.431146] [G loss: 0.176319] [ema: 0.999856] 
[Epoch 44/47] [Batch 800/1077] [D loss: 0.377653] [G loss: 0.146684] [ema: 0.999856] 
[Epoch 44/47] [Batch 900/1077] [D loss: 0.463912] [G loss: 0.153948] [ema: 0.999856] 
[Epoch 44/47] [Batch 1000/1077] [D loss: 0.510496] [G loss: 0.188643] [ema: 0.999857] 
[Epoch 45/47] [Batch 0/1077] [D loss: 0.382603] [G loss: 0.178069] [ema: 0.999857] 
[Epoch 45/47] [Batch 100/1077] [D loss: 0.379786] [G loss: 0.160050] [ema: 0.999857] 
[Epoch 45/47] [Batch 200/1077] [D loss: 0.490097] [G loss: 0.147482] [ema: 0.999858] 
[Epoch 45/47] [Batch 300/1077] [D loss: 0.384655] [G loss: 0.168774] [ema: 0.999858] 
[Epoch 45/47] [Batch 400/1077] [D loss: 0.477243] [G loss: 0.180921] [ema: 0.999858] 
[Epoch 45/47] [Batch 500/1077] [D loss: 0.402030] [G loss: 0.156842] [ema: 0.999858] 
[Epoch 45/47] [Batch 600/1077] [D loss: 0.422698] [G loss: 0.175100] [ema: 0.999859] 
[Epoch 45/47] [Batch 700/1077] [D loss: 0.439293] [G loss: 0.159961] [ema: 0.999859] 
[Epoch 45/47] [Batch 800/1077] [D loss: 0.425387] [G loss: 0.183889] [ema: 0.999859] 
[Epoch 45/47] [Batch 900/1077] [D loss: 0.416240] [G loss: 0.182370] [ema: 0.999860] 
[Epoch 45/47] [Batch 1000/1077] [D loss: 0.413648] [G loss: 0.156316] [ema: 0.999860] 
[Epoch 46/47] [Batch 0/1077] [D loss: 0.401648] [G loss: 0.161226] [ema: 0.999860] 
[Epoch 46/47] [Batch 100/1077] [D loss: 0.462774] [G loss: 0.184290] [ema: 0.999860] 
[Epoch 46/47] [Batch 200/1077] [D loss: 0.402594] [G loss: 0.186720] [ema: 0.999861] 
[Epoch 46/47] [Batch 300/1077] [D loss: 0.413772] [G loss: 0.177387] [ema: 0.999861] 
[Epoch 46/47] [Batch 400/1077] [D loss: 0.436821] [G loss: 0.163924] [ema: 0.999861] 
[Epoch 46/47] [Batch 500/1077] [D loss: 0.455325] [G loss: 0.185146] [ema: 0.999861] 
[Epoch 46/47] [Batch 600/1077] [D loss: 0.419932] [G loss: 0.163064] [ema: 0.999862] 
[Epoch 46/47] [Batch 700/1077] [D loss: 0.477816] [G loss: 0.148342] [ema: 0.999862] 
[Epoch 46/47] [Batch 800/1077] [D loss: 0.440403] [G loss: 0.169646] [ema: 0.999862] 
[Epoch 46/47] [Batch 900/1077] [D loss: 0.432903] [G loss: 0.178168] [ema: 0.999863] 
[Epoch 46/47] [Batch 1000/1077] [D loss: 0.395614] [G loss: 0.178896] [ema: 0.999863] 

----------------------------------------------------------------------------------------------------

 Starting individual training
downstairs training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
downstairs
daghar
return single class data and labels, class is downstairs
data shape is (12854, 3, 1, 30)
label shape is (12854,)
804
Epochs between checkpoint: 16



Saving checkpoint 1 in logs/daghar_50000_30_100/downstairs_50000_D_30_2024_10_18_02_43_48/Model



[Epoch 0/63] [Batch 0/804] [D loss: 1.512122] [G loss: 0.845379] [ema: 0.000000] 
[Epoch 0/63] [Batch 100/804] [D loss: 0.477723] [G loss: 0.179697] [ema: 0.933033] 
[Epoch 0/63] [Batch 200/804] [D loss: 0.366579] [G loss: 0.201812] [ema: 0.965936] 
[Epoch 0/63] [Batch 300/804] [D loss: 0.325529] [G loss: 0.277960] [ema: 0.977160] 
[Epoch 0/63] [Batch 400/804] [D loss: 0.252705] [G loss: 0.224133] [ema: 0.982821] 
[Epoch 0/63] [Batch 500/804] [D loss: 0.360781] [G loss: 0.192956] [ema: 0.986233] 
[Epoch 0/63] [Batch 600/804] [D loss: 0.420590] [G loss: 0.201413] [ema: 0.988514] 
[Epoch 0/63] [Batch 700/804] [D loss: 0.452316] [G loss: 0.194066] [ema: 0.990147] 
[Epoch 0/63] [Batch 800/804] [D loss: 0.469070] [G loss: 0.136752] [ema: 0.991373] 
[Epoch 1/63] [Batch 0/804] [D loss: 0.368260] [G loss: 0.132975] [ema: 0.991416] 
[Epoch 1/63] [Batch 100/804] [D loss: 0.417892] [G loss: 0.163745] [ema: 0.992362] 
[Epoch 1/63] [Batch 200/804] [D loss: 0.390200] [G loss: 0.227046] [ema: 0.993120] 
[Epoch 1/63] [Batch 300/804] [D loss: 0.301816] [G loss: 0.219322] [ema: 0.993741] 
[Epoch 1/63] [Batch 400/804] [D loss: 0.353415] [G loss: 0.225646] [ema: 0.994260] 
[Epoch 1/63] [Batch 500/804] [D loss: 0.404424] [G loss: 0.177841] [ema: 0.994699] 
[Epoch 1/63] [Batch 600/804] [D loss: 0.362315] [G loss: 0.180544] [ema: 0.995075] 
[Epoch 1/63] [Batch 700/804] [D loss: 0.479529] [G loss: 0.143629] [ema: 0.995402] 
[Epoch 1/63] [Batch 800/804] [D loss: 0.467832] [G loss: 0.148607] [ema: 0.995688] 
[Epoch 2/63] [Batch 0/804] [D loss: 0.503887] [G loss: 0.150069] [ema: 0.995699] 
[Epoch 2/63] [Batch 100/804] [D loss: 0.562817] [G loss: 0.160007] [ema: 0.995950] 
[Epoch 2/63] [Batch 200/804] [D loss: 0.471482] [G loss: 0.164136] [ema: 0.996174] 
[Epoch 2/63] [Batch 300/804] [D loss: 0.529312] [G loss: 0.180207] [ema: 0.996374] 
[Epoch 2/63] [Batch 400/804] [D loss: 0.467607] [G loss: 0.166894] [ema: 0.996554] 
[Epoch 2/63] [Batch 500/804] [D loss: 0.417532] [G loss: 0.159146] [ema: 0.996717] 
[Epoch 2/63] [Batch 600/804] [D loss: 0.389684] [G loss: 0.210509] [ema: 0.996866] 
[Epoch 2/63] [Batch 700/804] [D loss: 0.471772] [G loss: 0.146269] [ema: 0.997001] 
[Epoch 2/63] [Batch 800/804] [D loss: 0.397672] [G loss: 0.170943] [ema: 0.997126] 
[Epoch 3/63] [Batch 0/804] [D loss: 0.485328] [G loss: 0.195827] [ema: 0.997130] 
[Epoch 3/63] [Batch 100/804] [D loss: 0.501921] [G loss: 0.177356] [ema: 0.997244] 
[Epoch 3/63] [Batch 200/804] [D loss: 0.399369] [G loss: 0.189840] [ema: 0.997350] 
[Epoch 3/63] [Batch 300/804] [D loss: 0.385060] [G loss: 0.236353] [ema: 0.997447] 
[Epoch 3/63] [Batch 400/804] [D loss: 0.378538] [G loss: 0.194701] [ema: 0.997538] 
[Epoch 3/63] [Batch 500/804] [D loss: 0.366918] [G loss: 0.211961] [ema: 0.997623] 
[Epoch 3/63] [Batch 600/804] [D loss: 0.420950] [G loss: 0.186304] [ema: 0.997701] 
[Epoch 3/63] [Batch 700/804] [D loss: 0.517697] [G loss: 0.139111] [ema: 0.997775] 
[Epoch 3/63] [Batch 800/804] [D loss: 0.516902] [G loss: 0.137862] [ema: 0.997844] 
[Epoch 4/63] [Batch 0/804] [D loss: 0.517532] [G loss: 0.141019] [ema: 0.997847] 
[Epoch 4/63] [Batch 100/804] [D loss: 0.503693] [G loss: 0.157318] [ema: 0.997912] 
[Epoch 4/63] [Batch 200/804] [D loss: 0.523837] [G loss: 0.132217] [ema: 0.997973] 
[Epoch 4/63] [Batch 300/804] [D loss: 0.479247] [G loss: 0.156759] [ema: 0.998031] 
[Epoch 4/63] [Batch 400/804] [D loss: 0.433672] [G loss: 0.184646] [ema: 0.998085] 
[Epoch 4/63] [Batch 500/804] [D loss: 0.406340] [G loss: 0.137082] [ema: 0.998136] 
[Epoch 4/63] [Batch 600/804] [D loss: 0.430580] [G loss: 0.120792] [ema: 0.998185] 
[Epoch 4/63] [Batch 700/804] [D loss: 0.468137] [G loss: 0.151156] [ema: 0.998232] 
[Epoch 4/63] [Batch 800/804] [D loss: 0.483804] [G loss: 0.123309] [ema: 0.998276] 
[Epoch 5/63] [Batch 0/804] [D loss: 0.562496] [G loss: 0.113312] [ema: 0.998277] 
[Epoch 5/63] [Batch 100/804] [D loss: 0.511534] [G loss: 0.142767] [ema: 0.998319] 
[Epoch 5/63] [Batch 200/804] [D loss: 0.523086] [G loss: 0.146952] [ema: 0.998359] 
[Epoch 5/63] [Batch 300/804] [D loss: 0.549893] [G loss: 0.155278] [ema: 0.998397] 
[Epoch 5/63] [Batch 400/804] [D loss: 0.526044] [G loss: 0.141632] [ema: 0.998433] 
[Epoch 5/63] [Batch 500/804] [D loss: 0.514795] [G loss: 0.134601] [ema: 0.998468] 
[Epoch 5/63] [Batch 600/804] [D loss: 0.504227] [G loss: 0.155862] [ema: 0.998501] 
[Epoch 5/63] [Batch 700/804] [D loss: 0.477819] [G loss: 0.165069] [ema: 0.998533] 
[Epoch 5/63] [Batch 800/804] [D loss: 0.467409] [G loss: 0.155070] [ema: 0.998563] 
[Epoch 6/63] [Batch 0/804] [D loss: 0.526122] [G loss: 0.168246] [ema: 0.998564] 
[Epoch 6/63] [Batch 100/804] [D loss: 0.527557] [G loss: 0.186970] [ema: 0.998593] 
[Epoch 6/63] [Batch 200/804] [D loss: 0.487622] [G loss: 0.117733] [ema: 0.998621] 
[Epoch 6/63] [Batch 300/804] [D loss: 0.518412] [G loss: 0.131893] [ema: 0.998648] 
[Epoch 6/63] [Batch 400/804] [D loss: 0.537519] [G loss: 0.133118] [ema: 0.998674] 
[Epoch 6/63] [Batch 500/804] [D loss: 0.466807] [G loss: 0.153329] [ema: 0.998699] 
[Epoch 6/63] [Batch 600/804] [D loss: 0.534869] [G loss: 0.112696] [ema: 0.998723] 
[Epoch 6/63] [Batch 700/804] [D loss: 0.515418] [G loss: 0.137157] [ema: 0.998746] 
[Epoch 6/63] [Batch 800/804] [D loss: 0.452517] [G loss: 0.131310] [ema: 0.998768] 
[Epoch 7/63] [Batch 0/804] [D loss: 0.561813] [G loss: 0.155128] [ema: 0.998769] 
[Epoch 7/63] [Batch 100/804] [D loss: 0.508854] [G loss: 0.153853] [ema: 0.998791] 
[Epoch 7/63] [Batch 200/804] [D loss: 0.430682] [G loss: 0.128063] [ema: 0.998811] 
[Epoch 7/63] [Batch 300/804] [D loss: 0.494931] [G loss: 0.140360] [ema: 0.998831] 
[Epoch 7/63] [Batch 400/804] [D loss: 0.531536] [G loss: 0.144166] [ema: 0.998851] 
[Epoch 7/63] [Batch 500/804] [D loss: 0.511057] [G loss: 0.140354] [ema: 0.998870] 
[Epoch 7/63] [Batch 600/804] [D loss: 0.489056] [G loss: 0.120505] [ema: 0.998888] 
[Epoch 7/63] [Batch 700/804] [D loss: 0.495650] [G loss: 0.134764] [ema: 0.998905] 
[Epoch 7/63] [Batch 800/804] [D loss: 0.541035] [G loss: 0.146958] [ema: 0.998922] 
[Epoch 8/63] [Batch 0/804] [D loss: 0.528636] [G loss: 0.124171] [ema: 0.998923] 
[Epoch 8/63] [Batch 100/804] [D loss: 0.508245] [G loss: 0.157481] [ema: 0.998939] 
[Epoch 8/63] [Batch 200/804] [D loss: 0.495365] [G loss: 0.136517] [ema: 0.998955] 
[Epoch 8/63] [Batch 300/804] [D loss: 0.518767] [G loss: 0.143103] [ema: 0.998971] 
[Epoch 8/63] [Batch 400/804] [D loss: 0.454295] [G loss: 0.146441] [ema: 0.998986] 
[Epoch 8/63] [Batch 500/804] [D loss: 0.458403] [G loss: 0.141368] [ema: 0.999001] 
[Epoch 8/63] [Batch 600/804] [D loss: 0.452529] [G loss: 0.142725] [ema: 0.999015] 
[Epoch 8/63] [Batch 700/804] [D loss: 0.532339] [G loss: 0.142159] [ema: 0.999029] 
[Epoch 8/63] [Batch 800/804] [D loss: 0.510166] [G loss: 0.123202] [ema: 0.999042] 
[Epoch 9/63] [Batch 0/804] [D loss: 0.521307] [G loss: 0.152060] [ema: 0.999043] 
[Epoch 9/63] [Batch 100/804] [D loss: 0.500206] [G loss: 0.157262] [ema: 0.999056] 
[Epoch 9/63] [Batch 200/804] [D loss: 0.517933] [G loss: 0.131889] [ema: 0.999068] 
[Epoch 9/63] [Batch 300/804] [D loss: 0.531192] [G loss: 0.133537] [ema: 0.999081] 
[Epoch 9/63] [Batch 400/804] [D loss: 0.494818] [G loss: 0.144392] [ema: 0.999093] 
[Epoch 9/63] [Batch 500/804] [D loss: 0.508586] [G loss: 0.148929] [ema: 0.999104] 
[Epoch 9/63] [Batch 600/804] [D loss: 0.496773] [G loss: 0.126693] [ema: 0.999116] 
[Epoch 9/63] [Batch 700/804] [D loss: 0.505291] [G loss: 0.116038] [ema: 0.999127] 
[Epoch 9/63] [Batch 800/804] [D loss: 0.478674] [G loss: 0.147995] [ema: 0.999138] 
[Epoch 10/63] [Batch 0/804] [D loss: 0.491850] [G loss: 0.147250] [ema: 0.999138] 
[Epoch 10/63] [Batch 100/804] [D loss: 0.498145] [G loss: 0.153302] [ema: 0.999149] 
[Epoch 10/63] [Batch 200/804] [D loss: 0.499706] [G loss: 0.145847] [ema: 0.999159] 
[Epoch 10/63] [Batch 300/804] [D loss: 0.444555] [G loss: 0.141494] [ema: 0.999169] 
[Epoch 10/63] [Batch 400/804] [D loss: 0.511395] [G loss: 0.127465] [ema: 0.999179] 
[Epoch 10/63] [Batch 500/804] [D loss: 0.540111] [G loss: 0.122646] [ema: 0.999189] 
[Epoch 10/63] [Batch 600/804] [D loss: 0.465778] [G loss: 0.155555] [ema: 0.999198] 
[Epoch 10/63] [Batch 700/804] [D loss: 0.450961] [G loss: 0.142352] [ema: 0.999207] 
[Epoch 10/63] [Batch 800/804] [D loss: 0.521807] [G loss: 0.137544] [ema: 0.999216] 
[Epoch 11/63] [Batch 0/804] [D loss: 0.497690] [G loss: 0.134181] [ema: 0.999217] 
[Epoch 11/63] [Batch 100/804] [D loss: 0.464849] [G loss: 0.153372] [ema: 0.999225] 
[Epoch 11/63] [Batch 200/804] [D loss: 0.462816] [G loss: 0.132800] [ema: 0.999234] 
[Epoch 11/63] [Batch 300/804] [D loss: 0.491094] [G loss: 0.151373] [ema: 0.999242] 
[Epoch 11/63] [Batch 400/804] [D loss: 0.519128] [G loss: 0.129601] [ema: 0.999250] 
[Epoch 11/63] [Batch 500/804] [D loss: 0.486402] [G loss: 0.137956] [ema: 0.999258] 
[Epoch 11/63] [Batch 600/804] [D loss: 0.495232] [G loss: 0.157684] [ema: 0.999266] 
[Epoch 11/63] [Batch 700/804] [D loss: 0.439589] [G loss: 0.134426] [ema: 0.999274] 
[Epoch 11/63] [Batch 800/804] [D loss: 0.466726] [G loss: 0.133672] [ema: 0.999282] 
[Epoch 12/63] [Batch 0/804] [D loss: 0.464005] [G loss: 0.144437] [ema: 0.999282] 
[Epoch 12/63] [Batch 100/804] [D loss: 0.453799] [G loss: 0.170440] [ema: 0.999289] 
[Epoch 12/63] [Batch 200/804] [D loss: 0.458441] [G loss: 0.151514] [ema: 0.999296] 
[Epoch 12/63] [Batch 300/804] [D loss: 0.454505] [G loss: 0.148279] [ema: 0.999303] 
[Epoch 12/63] [Batch 400/804] [D loss: 0.568193] [G loss: 0.138464] [ema: 0.999310] 
[Epoch 12/63] [Batch 500/804] [D loss: 0.501040] [G loss: 0.148073] [ema: 0.999317] 
[Epoch 12/63] [Batch 600/804] [D loss: 0.447682] [G loss: 0.153973] [ema: 0.999324] 
[Epoch 12/63] [Batch 700/804] [D loss: 0.448896] [G loss: 0.137755] [ema: 0.999330] 
[Epoch 12/63] [Batch 800/804] [D loss: 0.513732] [G loss: 0.156293] [ema: 0.999337] 
[Epoch 13/63] [Batch 0/804] [D loss: 0.448985] [G loss: 0.154504] [ema: 0.999337] 
[Epoch 13/63] [Batch 100/804] [D loss: 0.499020] [G loss: 0.151823] [ema: 0.999343] 
[Epoch 13/63] [Batch 200/804] [D loss: 0.480985] [G loss: 0.140469] [ema: 0.999349] 
[Epoch 13/63] [Batch 300/804] [D loss: 0.453806] [G loss: 0.145219] [ema: 0.999356] 
[Epoch 13/63] [Batch 400/804] [D loss: 0.436918] [G loss: 0.160270] [ema: 0.999361] 
[Epoch 13/63] [Batch 500/804] [D loss: 0.513265] [G loss: 0.156860] [ema: 0.999367] 
[Epoch 13/63] [Batch 600/804] [D loss: 0.460738] [G loss: 0.127245] [ema: 0.999373] 
[Epoch 13/63] [Batch 700/804] [D loss: 0.479648] [G loss: 0.129455] [ema: 0.999379] 
[Epoch 13/63] [Batch 800/804] [D loss: 0.429972] [G loss: 0.133451] [ema: 0.999384] 
[Epoch 14/63] [Batch 0/804] [D loss: 0.446355] [G loss: 0.152568] [ema: 0.999384] 
[Epoch 14/63] [Batch 100/804] [D loss: 0.453805] [G loss: 0.144888] [ema: 0.999390] 
[Epoch 14/63] [Batch 200/804] [D loss: 0.430887] [G loss: 0.138620] [ema: 0.999395] 
[Epoch 14/63] [Batch 300/804] [D loss: 0.453020] [G loss: 0.164378] [ema: 0.999400] 
[Epoch 14/63] [Batch 400/804] [D loss: 0.471921] [G loss: 0.134035] [ema: 0.999406] 
[Epoch 14/63] [Batch 500/804] [D loss: 0.450088] [G loss: 0.166043] [ema: 0.999411] 
[Epoch 14/63] [Batch 600/804] [D loss: 0.442431] [G loss: 0.182032] [ema: 0.999416] 
[Epoch 14/63] [Batch 700/804] [D loss: 0.454465] [G loss: 0.140550] [ema: 0.999420] 
[Epoch 14/63] [Batch 800/804] [D loss: 0.495210] [G loss: 0.165852] [ema: 0.999425] 
[Epoch 15/63] [Batch 0/804] [D loss: 0.538676] [G loss: 0.155296] [ema: 0.999425] 
[Epoch 15/63] [Batch 100/804] [D loss: 0.442049] [G loss: 0.177062] [ema: 0.999430] 
[Epoch 15/63] [Batch 200/804] [D loss: 0.405202] [G loss: 0.176371] [ema: 0.999435] 
[Epoch 15/63] [Batch 300/804] [D loss: 0.418185] [G loss: 0.179271] [ema: 0.999439] 
[Epoch 15/63] [Batch 400/804] [D loss: 0.423010] [G loss: 0.158478] [ema: 0.999444] 
[Epoch 15/63] [Batch 500/804] [D loss: 0.455023] [G loss: 0.170299] [ema: 0.999448] 
[Epoch 15/63] [Batch 600/804] [D loss: 0.492496] [G loss: 0.148935] [ema: 0.999453] 
[Epoch 15/63] [Batch 700/804] [D loss: 0.491948] [G loss: 0.136714] [ema: 0.999457] 
[Epoch 15/63] [Batch 800/804] [D loss: 0.434379] [G loss: 0.153842] [ema: 0.999461] 



Saving checkpoint 2 in logs/daghar_50000_30_100/downstairs_50000_D_30_2024_10_18_02_43_48/Model



[Epoch 16/63] [Batch 0/804] [D loss: 0.467707] [G loss: 0.148041] [ema: 0.999461] 
[Epoch 16/63] [Batch 100/804] [D loss: 0.435218] [G loss: 0.171776] [ema: 0.999465] 
[Epoch 16/63] [Batch 200/804] [D loss: 0.466480] [G loss: 0.162961] [ema: 0.999470] 
[Epoch 16/63] [Batch 300/804] [D loss: 0.441844] [G loss: 0.120081] [ema: 0.999474] 
[Epoch 16/63] [Batch 400/804] [D loss: 0.479456] [G loss: 0.152994] [ema: 0.999478] 
[Epoch 16/63] [Batch 500/804] [D loss: 0.403232] [G loss: 0.177620] [ema: 0.999481] 
[Epoch 16/63] [Batch 600/804] [D loss: 0.425130] [G loss: 0.169260] [ema: 0.999485] 
[Epoch 16/63] [Batch 700/804] [D loss: 0.469148] [G loss: 0.141650] [ema: 0.999489] 
[Epoch 16/63] [Batch 800/804] [D loss: 0.450887] [G loss: 0.170720] [ema: 0.999493] 
[Epoch 17/63] [Batch 0/804] [D loss: 0.455876] [G loss: 0.194569] [ema: 0.999493] 
[Epoch 17/63] [Batch 100/804] [D loss: 0.520720] [G loss: 0.140860] [ema: 0.999497] 
[Epoch 17/63] [Batch 200/804] [D loss: 0.444416] [G loss: 0.157226] [ema: 0.999500] 
[Epoch 17/63] [Batch 300/804] [D loss: 0.424014] [G loss: 0.170579] [ema: 0.999504] 
[Epoch 17/63] [Batch 400/804] [D loss: 0.433621] [G loss: 0.139567] [ema: 0.999507] 
[Epoch 17/63] [Batch 500/804] [D loss: 0.433551] [G loss: 0.146915] [ema: 0.999511] 
[Epoch 17/63] [Batch 600/804] [D loss: 0.433556] [G loss: 0.172294] [ema: 0.999514] 
[Epoch 17/63] [Batch 700/804] [D loss: 0.524404] [G loss: 0.172865] [ema: 0.999518] 
[Epoch 17/63] [Batch 800/804] [D loss: 0.421209] [G loss: 0.173572] [ema: 0.999521] 
[Epoch 18/63] [Batch 0/804] [D loss: 0.403071] [G loss: 0.166855] [ema: 0.999521] 
[Epoch 18/63] [Batch 100/804] [D loss: 0.377739] [G loss: 0.136088] [ema: 0.999524] 
[Epoch 18/63] [Batch 200/804] [D loss: 0.412594] [G loss: 0.170094] [ema: 0.999528] 
[Epoch 18/63] [Batch 300/804] [D loss: 0.420910] [G loss: 0.166199] [ema: 0.999531] 
[Epoch 18/63] [Batch 400/804] [D loss: 0.456966] [G loss: 0.160398] [ema: 0.999534] 
[Epoch 18/63] [Batch 500/804] [D loss: 0.398959] [G loss: 0.158339] [ema: 0.999537] 
[Epoch 18/63] [Batch 600/804] [D loss: 0.464727] [G loss: 0.176047] [ema: 0.999540] 
[Epoch 18/63] [Batch 700/804] [D loss: 0.434852] [G loss: 0.156470] [ema: 0.999543] 
[Epoch 18/63] [Batch 800/804] [D loss: 0.451284] [G loss: 0.175809] [ema: 0.999546] 
[Epoch 19/63] [Batch 0/804] [D loss: 0.425623] [G loss: 0.183359] [ema: 0.999546] 
[Epoch 19/63] [Batch 100/804] [D loss: 0.433830] [G loss: 0.187879] [ema: 0.999549] 
[Epoch 19/63] [Batch 200/804] [D loss: 0.419871] [G loss: 0.159139] [ema: 0.999552] 
[Epoch 19/63] [Batch 300/804] [D loss: 0.378667] [G loss: 0.191537] [ema: 0.999555] 
[Epoch 19/63] [Batch 400/804] [D loss: 0.501175] [G loss: 0.168672] [ema: 0.999558] 
[Epoch 19/63] [Batch 500/804] [D loss: 0.406175] [G loss: 0.184205] [ema: 0.999561] 
[Epoch 19/63] [Batch 600/804] [D loss: 0.435563] [G loss: 0.144674] [ema: 0.999563] 
[Epoch 19/63] [Batch 700/804] [D loss: 0.385911] [G loss: 0.184223] [ema: 0.999566] 
[Epoch 19/63] [Batch 800/804] [D loss: 0.471909] [G loss: 0.185761] [ema: 0.999569] 
[Epoch 20/63] [Batch 0/804] [D loss: 0.440513] [G loss: 0.184005] [ema: 0.999569] 
[Epoch 20/63] [Batch 100/804] [D loss: 0.461794] [G loss: 0.176277] [ema: 0.999572] 
[Epoch 20/63] [Batch 200/804] [D loss: 0.403498] [G loss: 0.162021] [ema: 0.999574] 
[Epoch 20/63] [Batch 300/804] [D loss: 0.438927] [G loss: 0.168616] [ema: 0.999577] 
[Epoch 20/63] [Batch 400/804] [D loss: 0.398475] [G loss: 0.170502] [ema: 0.999579] 
[Epoch 20/63] [Batch 500/804] [D loss: 0.437615] [G loss: 0.180214] [ema: 0.999582] 
[Epoch 20/63] [Batch 600/804] [D loss: 0.451041] [G loss: 0.130708] [ema: 0.999585] 
[Epoch 20/63] [Batch 700/804] [D loss: 0.425255] [G loss: 0.166077] [ema: 0.999587] 
[Epoch 20/63] [Batch 800/804] [D loss: 0.373075] [G loss: 0.155433] [ema: 0.999589] 
[Epoch 21/63] [Batch 0/804] [D loss: 0.481016] [G loss: 0.199994] [ema: 0.999590] 
[Epoch 21/63] [Batch 100/804] [D loss: 0.487920] [G loss: 0.191283] [ema: 0.999592] 
[Epoch 21/63] [Batch 200/804] [D loss: 0.426991] [G loss: 0.201625] [ema: 0.999594] 
[Epoch 21/63] [Batch 300/804] [D loss: 0.458924] [G loss: 0.182649] [ema: 0.999597] 
[Epoch 21/63] [Batch 400/804] [D loss: 0.403543] [G loss: 0.162364] [ema: 0.999599] 
[Epoch 21/63] [Batch 500/804] [D loss: 0.355578] [G loss: 0.160798] [ema: 0.999601] 
[Epoch 21/63] [Batch 600/804] [D loss: 0.462166] [G loss: 0.162044] [ema: 0.999604] 
[Epoch 21/63] [Batch 700/804] [D loss: 0.409987] [G loss: 0.157149] [ema: 0.999606] 
[Epoch 21/63] [Batch 800/804] [D loss: 0.423560] [G loss: 0.167238] [ema: 0.999608] 
[Epoch 22/63] [Batch 0/804] [D loss: 0.445767] [G loss: 0.156190] [ema: 0.999608] 
[Epoch 22/63] [Batch 100/804] [D loss: 0.450199] [G loss: 0.165100] [ema: 0.999610] 
[Epoch 22/63] [Batch 200/804] [D loss: 0.453569] [G loss: 0.146397] [ema: 0.999613] 
[Epoch 22/63] [Batch 300/804] [D loss: 0.373184] [G loss: 0.167754] [ema: 0.999615] 
[Epoch 22/63] [Batch 400/804] [D loss: 0.457128] [G loss: 0.143981] [ema: 0.999617] 
[Epoch 22/63] [Batch 500/804] [D loss: 0.468979] [G loss: 0.203854] [ema: 0.999619] 
[Epoch 22/63] [Batch 600/804] [D loss: 0.461770] [G loss: 0.150602] [ema: 0.999621] 
[Epoch 22/63] [Batch 700/804] [D loss: 0.462335] [G loss: 0.186040] [ema: 0.999623] 
[Epoch 22/63] [Batch 800/804] [D loss: 0.412985] [G loss: 0.157662] [ema: 0.999625] 
[Epoch 23/63] [Batch 0/804] [D loss: 0.502832] [G loss: 0.176454] [ema: 0.999625] 
[Epoch 23/63] [Batch 100/804] [D loss: 0.434787] [G loss: 0.168698] [ema: 0.999627] 
[Epoch 23/63] [Batch 200/804] [D loss: 0.435134] [G loss: 0.203959] [ema: 0.999629] 
[Epoch 23/63] [Batch 300/804] [D loss: 0.416780] [G loss: 0.197252] [ema: 0.999631] 
[Epoch 23/63] [Batch 400/804] [D loss: 0.465231] [G loss: 0.169629] [ema: 0.999633] 
[Epoch 23/63] [Batch 500/804] [D loss: 0.452427] [G loss: 0.164564] [ema: 0.999635] 
[Epoch 23/63] [Batch 600/804] [D loss: 0.436838] [G loss: 0.146068] [ema: 0.999637] 
[Epoch 23/63] [Batch 700/804] [D loss: 0.440052] [G loss: 0.167830] [ema: 0.999639] 
[Epoch 23/63] [Batch 800/804] [D loss: 0.523449] [G loss: 0.164290] [ema: 0.999641] 
[Epoch 24/63] [Batch 0/804] [D loss: 0.426133] [G loss: 0.161507] [ema: 0.999641] 
[Epoch 24/63] [Batch 100/804] [D loss: 0.427197] [G loss: 0.166514] [ema: 0.999643] 
[Epoch 24/63] [Batch 200/804] [D loss: 0.457554] [G loss: 0.182670] [ema: 0.999645] 
[Epoch 24/63] [Batch 300/804] [D loss: 0.429644] [G loss: 0.175856] [ema: 0.999646] 
[Epoch 24/63] [Batch 400/804] [D loss: 0.442075] [G loss: 0.154560] [ema: 0.999648] 
[Epoch 24/63] [Batch 500/804] [D loss: 0.478492] [G loss: 0.173903] [ema: 0.999650] 
[Epoch 24/63] [Batch 600/804] [D loss: 0.460883] [G loss: 0.175245] [ema: 0.999652] 
[Epoch 24/63] [Batch 700/804] [D loss: 0.451925] [G loss: 0.191569] [ema: 0.999653] 
[Epoch 24/63] [Batch 800/804] [D loss: 0.511202] [G loss: 0.169577] [ema: 0.999655] 
[Epoch 25/63] [Batch 0/804] [D loss: 0.499136] [G loss: 0.148158] [ema: 0.999655] 
[Epoch 25/63] [Batch 100/804] [D loss: 0.478877] [G loss: 0.166356] [ema: 0.999657] 
[Epoch 25/63] [Batch 200/804] [D loss: 0.493312] [G loss: 0.141077] [ema: 0.999659] 
[Epoch 25/63] [Batch 300/804] [D loss: 0.450100] [G loss: 0.145203] [ema: 0.999660] 
[Epoch 25/63] [Batch 400/804] [D loss: 0.477458] [G loss: 0.161734] [ema: 0.999662] 
[Epoch 25/63] [Batch 500/804] [D loss: 0.437445] [G loss: 0.150301] [ema: 0.999664] 
[Epoch 25/63] [Batch 600/804] [D loss: 0.506852] [G loss: 0.155136] [ema: 0.999665] 
[Epoch 25/63] [Batch 700/804] [D loss: 0.524320] [G loss: 0.120945] [ema: 0.999667] 
[Epoch 25/63] [Batch 800/804] [D loss: 0.460541] [G loss: 0.126718] [ema: 0.999668] 
[Epoch 26/63] [Batch 0/804] [D loss: 0.494327] [G loss: 0.134206] [ema: 0.999668] 
[Epoch 26/63] [Batch 100/804] [D loss: 0.513277] [G loss: 0.133535] [ema: 0.999670] 
[Epoch 26/63] [Batch 200/804] [D loss: 0.517891] [G loss: 0.108436] [ema: 0.999672] 
[Epoch 26/63] [Batch 300/804] [D loss: 0.508656] [G loss: 0.123128] [ema: 0.999673] 
[Epoch 26/63] [Batch 400/804] [D loss: 0.533456] [G loss: 0.146494] [ema: 0.999675] 
[Epoch 26/63] [Batch 500/804] [D loss: 0.507135] [G loss: 0.105001] [ema: 0.999676] 
[Epoch 26/63] [Batch 600/804] [D loss: 0.512292] [G loss: 0.130823] [ema: 0.999678] 
[Epoch 26/63] [Batch 700/804] [D loss: 0.547466] [G loss: 0.122984] [ema: 0.999679] 
[Epoch 26/63] [Batch 800/804] [D loss: 0.496296] [G loss: 0.154114] [ema: 0.999681] 
[Epoch 27/63] [Batch 0/804] [D loss: 0.526220] [G loss: 0.135281] [ema: 0.999681] 
[Epoch 27/63] [Batch 100/804] [D loss: 0.516358] [G loss: 0.138659] [ema: 0.999682] 
[Epoch 27/63] [Batch 200/804] [D loss: 0.539319] [G loss: 0.131483] [ema: 0.999684] 
[Epoch 27/63] [Batch 300/804] [D loss: 0.530058] [G loss: 0.132461] [ema: 0.999685] 
[Epoch 27/63] [Batch 400/804] [D loss: 0.540787] [G loss: 0.113007] [ema: 0.999687] 
[Epoch 27/63] [Batch 500/804] [D loss: 0.464386] [G loss: 0.146439] [ema: 0.999688] 
[Epoch 27/63] [Batch 600/804] [D loss: 0.479130] [G loss: 0.145994] [ema: 0.999689] 
[Epoch 27/63] [Batch 700/804] [D loss: 0.530544] [G loss: 0.164102] [ema: 0.999691] 
[Epoch 27/63] [Batch 800/804] [D loss: 0.466763] [G loss: 0.133080] [ema: 0.999692] 
[Epoch 28/63] [Batch 0/804] [D loss: 0.487814] [G loss: 0.142796] [ema: 0.999692] 
[Epoch 28/63] [Batch 100/804] [D loss: 0.488147] [G loss: 0.130452] [ema: 0.999694] 
[Epoch 28/63] [Batch 200/804] [D loss: 0.517431] [G loss: 0.149900] [ema: 0.999695] 
[Epoch 28/63] [Batch 300/804] [D loss: 0.509845] [G loss: 0.102491] [ema: 0.999696] 
[Epoch 28/63] [Batch 400/804] [D loss: 0.512647] [G loss: 0.126723] [ema: 0.999698] 
[Epoch 28/63] [Batch 500/804] [D loss: 0.523555] [G loss: 0.139064] [ema: 0.999699] 
[Epoch 28/63] [Batch 600/804] [D loss: 0.498272] [G loss: 0.130958] [ema: 0.999700] 
[Epoch 28/63] [Batch 700/804] [D loss: 0.532404] [G loss: 0.126063] [ema: 0.999701] 
[Epoch 28/63] [Batch 800/804] [D loss: 0.530193] [G loss: 0.132581] [ema: 0.999703] 
[Epoch 29/63] [Batch 0/804] [D loss: 0.520754] [G loss: 0.133062] [ema: 0.999703] 
[Epoch 29/63] [Batch 100/804] [D loss: 0.506807] [G loss: 0.128386] [ema: 0.999704] 
[Epoch 29/63] [Batch 200/804] [D loss: 0.517840] [G loss: 0.126363] [ema: 0.999705] 
[Epoch 29/63] [Batch 300/804] [D loss: 0.509221] [G loss: 0.136166] [ema: 0.999707] 
[Epoch 29/63] [Batch 400/804] [D loss: 0.495465] [G loss: 0.141503] [ema: 0.999708] 
[Epoch 29/63] [Batch 500/804] [D loss: 0.544674] [G loss: 0.136734] [ema: 0.999709] 
[Epoch 29/63] [Batch 600/804] [D loss: 0.527110] [G loss: 0.128453] [ema: 0.999710] 
[Epoch 29/63] [Batch 700/804] [D loss: 0.508218] [G loss: 0.138386] [ema: 0.999711] 
[Epoch 29/63] [Batch 800/804] [D loss: 0.541165] [G loss: 0.132901] [ema: 0.999713] 
[Epoch 30/63] [Batch 0/804] [D loss: 0.536327] [G loss: 0.132283] [ema: 0.999713] 
[Epoch 30/63] [Batch 100/804] [D loss: 0.525078] [G loss: 0.122300] [ema: 0.999714] 
[Epoch 30/63] [Batch 200/804] [D loss: 0.518479] [G loss: 0.121087] [ema: 0.999715] 
[Epoch 30/63] [Batch 300/804] [D loss: 0.496776] [G loss: 0.124816] [ema: 0.999716] 
[Epoch 30/63] [Batch 400/804] [D loss: 0.497980] [G loss: 0.120926] [ema: 0.999717] 
[Epoch 30/63] [Batch 500/804] [D loss: 0.543287] [G loss: 0.129827] [ema: 0.999719] 
[Epoch 30/63] [Batch 600/804] [D loss: 0.520013] [G loss: 0.126961] [ema: 0.999720] 
[Epoch 30/63] [Batch 700/804] [D loss: 0.544294] [G loss: 0.119195] [ema: 0.999721] 
[Epoch 30/63] [Batch 800/804] [D loss: 0.475991] [G loss: 0.144554] [ema: 0.999722] 
[Epoch 31/63] [Batch 0/804] [D loss: 0.495176] [G loss: 0.134377] [ema: 0.999722] 
[Epoch 31/63] [Batch 100/804] [D loss: 0.478531] [G loss: 0.121314] [ema: 0.999723] 
[Epoch 31/63] [Batch 200/804] [D loss: 0.501431] [G loss: 0.148046] [ema: 0.999724] 
[Epoch 31/63] [Batch 300/804] [D loss: 0.518675] [G loss: 0.119767] [ema: 0.999725] 
[Epoch 31/63] [Batch 400/804] [D loss: 0.511243] [G loss: 0.122035] [ema: 0.999726] 
[Epoch 31/63] [Batch 500/804] [D loss: 0.524570] [G loss: 0.131360] [ema: 0.999727] 
[Epoch 31/63] [Batch 600/804] [D loss: 0.547290] [G loss: 0.124361] [ema: 0.999728] 
[Epoch 31/63] [Batch 700/804] [D loss: 0.502069] [G loss: 0.146523] [ema: 0.999730] 
[Epoch 31/63] [Batch 800/804] [D loss: 0.512312] [G loss: 0.141712] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_50000_30_100/downstairs_50000_D_30_2024_10_18_02_43_48/Model



[Epoch 32/63] [Batch 0/804] [D loss: 0.474479] [G loss: 0.125889] [ema: 0.999731] 
[Epoch 32/63] [Batch 100/804] [D loss: 0.521931] [G loss: 0.125380] [ema: 0.999732] 
[Epoch 32/63] [Batch 200/804] [D loss: 0.493624] [G loss: 0.129427] [ema: 0.999733] 
[Epoch 32/63] [Batch 300/804] [D loss: 0.504619] [G loss: 0.144270] [ema: 0.999734] 
[Epoch 32/63] [Batch 400/804] [D loss: 0.560552] [G loss: 0.136556] [ema: 0.999735] 
[Epoch 32/63] [Batch 500/804] [D loss: 0.485837] [G loss: 0.133110] [ema: 0.999736] 
[Epoch 32/63] [Batch 600/804] [D loss: 0.521553] [G loss: 0.137267] [ema: 0.999737] 
[Epoch 32/63] [Batch 700/804] [D loss: 0.483877] [G loss: 0.133286] [ema: 0.999738] 
[Epoch 32/63] [Batch 800/804] [D loss: 0.488697] [G loss: 0.127627] [ema: 0.999739] 
[Epoch 33/63] [Batch 0/804] [D loss: 0.507009] [G loss: 0.136050] [ema: 0.999739] 
[Epoch 33/63] [Batch 100/804] [D loss: 0.504683] [G loss: 0.135172] [ema: 0.999740] 
[Epoch 33/63] [Batch 200/804] [D loss: 0.551265] [G loss: 0.137256] [ema: 0.999741] 
[Epoch 33/63] [Batch 300/804] [D loss: 0.472334] [G loss: 0.125946] [ema: 0.999742] 
[Epoch 33/63] [Batch 400/804] [D loss: 0.525256] [G loss: 0.139493] [ema: 0.999743] 
[Epoch 33/63] [Batch 500/804] [D loss: 0.494638] [G loss: 0.133694] [ema: 0.999744] 
[Epoch 33/63] [Batch 600/804] [D loss: 0.474858] [G loss: 0.146104] [ema: 0.999745] 
[Epoch 33/63] [Batch 700/804] [D loss: 0.483662] [G loss: 0.150663] [ema: 0.999745] 
[Epoch 33/63] [Batch 800/804] [D loss: 0.483528] [G loss: 0.117882] [ema: 0.999746] 
[Epoch 34/63] [Batch 0/804] [D loss: 0.530620] [G loss: 0.149310] [ema: 0.999746] 
[Epoch 34/63] [Batch 100/804] [D loss: 0.506872] [G loss: 0.121008] [ema: 0.999747] 
[Epoch 34/63] [Batch 200/804] [D loss: 0.464777] [G loss: 0.121198] [ema: 0.999748] 
[Epoch 34/63] [Batch 300/804] [D loss: 0.468867] [G loss: 0.142735] [ema: 0.999749] 
[Epoch 34/63] [Batch 400/804] [D loss: 0.498922] [G loss: 0.134805] [ema: 0.999750] 
[Epoch 34/63] [Batch 500/804] [D loss: 0.534494] [G loss: 0.136973] [ema: 0.999751] 
[Epoch 34/63] [Batch 600/804] [D loss: 0.460782] [G loss: 0.127754] [ema: 0.999752] 
[Epoch 34/63] [Batch 700/804] [D loss: 0.482213] [G loss: 0.138245] [ema: 0.999753] 
[Epoch 34/63] [Batch 800/804] [D loss: 0.490786] [G loss: 0.105959] [ema: 0.999754] 
[Epoch 35/63] [Batch 0/804] [D loss: 0.493500] [G loss: 0.124729] [ema: 0.999754] 
[Epoch 35/63] [Batch 100/804] [D loss: 0.478948] [G loss: 0.148430] [ema: 0.999755] 
[Epoch 35/63] [Batch 200/804] [D loss: 0.471921] [G loss: 0.117037] [ema: 0.999755] 
[Epoch 35/63] [Batch 300/804] [D loss: 0.452446] [G loss: 0.132795] [ema: 0.999756] 
[Epoch 35/63] [Batch 400/804] [D loss: 0.498406] [G loss: 0.134686] [ema: 0.999757] 
[Epoch 35/63] [Batch 500/804] [D loss: 0.480192] [G loss: 0.110861] [ema: 0.999758] 
[Epoch 35/63] [Batch 600/804] [D loss: 0.518162] [G loss: 0.128069] [ema: 0.999759] 
[Epoch 35/63] [Batch 700/804] [D loss: 0.458778] [G loss: 0.165476] [ema: 0.999760] 
[Epoch 35/63] [Batch 800/804] [D loss: 0.568279] [G loss: 0.132317] [ema: 0.999761] 
[Epoch 36/63] [Batch 0/804] [D loss: 0.561102] [G loss: 0.137861] [ema: 0.999761] 
[Epoch 36/63] [Batch 100/804] [D loss: 0.469782] [G loss: 0.129580] [ema: 0.999761] 
[Epoch 36/63] [Batch 200/804] [D loss: 0.505049] [G loss: 0.145873] [ema: 0.999762] 
[Epoch 36/63] [Batch 300/804] [D loss: 0.553509] [G loss: 0.126642] [ema: 0.999763] 
[Epoch 36/63] [Batch 400/804] [D loss: 0.504271] [G loss: 0.146445] [ema: 0.999764] 
[Epoch 36/63] [Batch 500/804] [D loss: 0.533229] [G loss: 0.121322] [ema: 0.999765] 
[Epoch 36/63] [Batch 600/804] [D loss: 0.493679] [G loss: 0.130901] [ema: 0.999765] 
[Epoch 36/63] [Batch 700/804] [D loss: 0.490016] [G loss: 0.127423] [ema: 0.999766] 
[Epoch 36/63] [Batch 800/804] [D loss: 0.476075] [G loss: 0.116763] [ema: 0.999767] 
[Epoch 37/63] [Batch 0/804] [D loss: 0.457371] [G loss: 0.163295] [ema: 0.999767] 
[Epoch 37/63] [Batch 100/804] [D loss: 0.459066] [G loss: 0.149525] [ema: 0.999768] 
[Epoch 37/63] [Batch 200/804] [D loss: 0.564406] [G loss: 0.143370] [ema: 0.999769] 
[Epoch 37/63] [Batch 300/804] [D loss: 0.541621] [G loss: 0.141248] [ema: 0.999769] 
[Epoch 37/63] [Batch 400/804] [D loss: 0.543820] [G loss: 0.148965] [ema: 0.999770] 
[Epoch 37/63] [Batch 500/804] [D loss: 0.514424] [G loss: 0.133132] [ema: 0.999771] 
[Epoch 37/63] [Batch 600/804] [D loss: 0.469670] [G loss: 0.135693] [ema: 0.999772] 
[Epoch 37/63] [Batch 700/804] [D loss: 0.456195] [G loss: 0.128072] [ema: 0.999772] 
[Epoch 37/63] [Batch 800/804] [D loss: 0.545120] [G loss: 0.169705] [ema: 0.999773] 
[Epoch 38/63] [Batch 0/804] [D loss: 0.500701] [G loss: 0.149500] [ema: 0.999773] 
[Epoch 38/63] [Batch 100/804] [D loss: 0.481241] [G loss: 0.117070] [ema: 0.999774] 
[Epoch 38/63] [Batch 200/804] [D loss: 0.518899] [G loss: 0.159405] [ema: 0.999775] 
[Epoch 38/63] [Batch 300/804] [D loss: 0.455456] [G loss: 0.155560] [ema: 0.999775] 
[Epoch 38/63] [Batch 400/804] [D loss: 0.482785] [G loss: 0.132853] [ema: 0.999776] 
[Epoch 38/63] [Batch 500/804] [D loss: 0.515536] [G loss: 0.138828] [ema: 0.999777] 
[Epoch 38/63] [Batch 600/804] [D loss: 0.481906] [G loss: 0.124188] [ema: 0.999778] 
[Epoch 38/63] [Batch 700/804] [D loss: 0.488902] [G loss: 0.135675] [ema: 0.999778] 
[Epoch 38/63] [Batch 800/804] [D loss: 0.504120] [G loss: 0.152134] [ema: 0.999779] 
[Epoch 39/63] [Batch 0/804] [D loss: 0.478258] [G loss: 0.158553] [ema: 0.999779] 
[Epoch 39/63] [Batch 100/804] [D loss: 0.523355] [G loss: 0.128145] [ema: 0.999780] 
[Epoch 39/63] [Batch 200/804] [D loss: 0.527226] [G loss: 0.144434] [ema: 0.999780] 
[Epoch 39/63] [Batch 300/804] [D loss: 0.478426] [G loss: 0.122459] [ema: 0.999781] 
[Epoch 39/63] [Batch 400/804] [D loss: 0.510624] [G loss: 0.137458] [ema: 0.999782] 
[Epoch 39/63] [Batch 500/804] [D loss: 0.447594] [G loss: 0.120786] [ema: 0.999782] 
[Epoch 39/63] [Batch 600/804] [D loss: 0.466834] [G loss: 0.116851] [ema: 0.999783] 
[Epoch 39/63] [Batch 700/804] [D loss: 0.493642] [G loss: 0.119500] [ema: 0.999784] 
[Epoch 39/63] [Batch 800/804] [D loss: 0.514550] [G loss: 0.119343] [ema: 0.999784] 
[Epoch 40/63] [Batch 0/804] [D loss: 0.461815] [G loss: 0.138231] [ema: 0.999784] 
[Epoch 40/63] [Batch 100/804] [D loss: 0.513551] [G loss: 0.150438] [ema: 0.999785] 
[Epoch 40/63] [Batch 200/804] [D loss: 0.487289] [G loss: 0.164889] [ema: 0.999786] 
[Epoch 40/63] [Batch 300/804] [D loss: 0.452193] [G loss: 0.148066] [ema: 0.999786] 
[Epoch 40/63] [Batch 400/804] [D loss: 0.509579] [G loss: 0.145617] [ema: 0.999787] 
[Epoch 40/63] [Batch 500/804] [D loss: 0.518766] [G loss: 0.147692] [ema: 0.999788] 
[Epoch 40/63] [Batch 600/804] [D loss: 0.484064] [G loss: 0.108319] [ema: 0.999788] 
[Epoch 40/63] [Batch 700/804] [D loss: 0.516897] [G loss: 0.150970] [ema: 0.999789] 
[Epoch 40/63] [Batch 800/804] [D loss: 0.494987] [G loss: 0.144993] [ema: 0.999790] 
[Epoch 41/63] [Batch 0/804] [D loss: 0.564077] [G loss: 0.116916] [ema: 0.999790] 
[Epoch 41/63] [Batch 100/804] [D loss: 0.521498] [G loss: 0.149352] [ema: 0.999790] 
[Epoch 41/63] [Batch 200/804] [D loss: 0.505933] [G loss: 0.148596] [ema: 0.999791] 
[Epoch 41/63] [Batch 300/804] [D loss: 0.439636] [G loss: 0.151127] [ema: 0.999792] 
[Epoch 41/63] [Batch 400/804] [D loss: 0.498904] [G loss: 0.140532] [ema: 0.999792] 
[Epoch 41/63] [Batch 500/804] [D loss: 0.534964] [G loss: 0.133819] [ema: 0.999793] 
[Epoch 41/63] [Batch 600/804] [D loss: 0.532226] [G loss: 0.138365] [ema: 0.999794] 
[Epoch 41/63] [Batch 700/804] [D loss: 0.493490] [G loss: 0.152616] [ema: 0.999794] 
[Epoch 41/63] [Batch 800/804] [D loss: 0.507314] [G loss: 0.124988] [ema: 0.999795] 
[Epoch 42/63] [Batch 0/804] [D loss: 0.507817] [G loss: 0.152711] [ema: 0.999795] 
[Epoch 42/63] [Batch 100/804] [D loss: 0.486597] [G loss: 0.157334] [ema: 0.999795] 
[Epoch 42/63] [Batch 200/804] [D loss: 0.461710] [G loss: 0.127281] [ema: 0.999796] 
[Epoch 42/63] [Batch 300/804] [D loss: 0.468367] [G loss: 0.125116] [ema: 0.999797] 
[Epoch 42/63] [Batch 400/804] [D loss: 0.457384] [G loss: 0.137731] [ema: 0.999797] 
[Epoch 42/63] [Batch 500/804] [D loss: 0.538489] [G loss: 0.136496] [ema: 0.999798] 
[Epoch 42/63] [Batch 600/804] [D loss: 0.486821] [G loss: 0.152772] [ema: 0.999798] 
[Epoch 42/63] [Batch 700/804] [D loss: 0.476698] [G loss: 0.151341] [ema: 0.999799] 
[Epoch 42/63] [Batch 800/804] [D loss: 0.470352] [G loss: 0.141272] [ema: 0.999800] 
[Epoch 43/63] [Batch 0/804] [D loss: 0.500044] [G loss: 0.162605] [ema: 0.999800] 
[Epoch 43/63] [Batch 100/804] [D loss: 0.511855] [G loss: 0.127042] [ema: 0.999800] 
[Epoch 43/63] [Batch 200/804] [D loss: 0.430122] [G loss: 0.151751] [ema: 0.999801] 
[Epoch 43/63] [Batch 300/804] [D loss: 0.552876] [G loss: 0.143142] [ema: 0.999801] 
[Epoch 43/63] [Batch 400/804] [D loss: 0.552570] [G loss: 0.157965] [ema: 0.999802] 
[Epoch 43/63] [Batch 500/804] [D loss: 0.494788] [G loss: 0.142573] [ema: 0.999802] 
[Epoch 43/63] [Batch 600/804] [D loss: 0.464328] [G loss: 0.128444] [ema: 0.999803] 
[Epoch 43/63] [Batch 700/804] [D loss: 0.489719] [G loss: 0.137413] [ema: 0.999804] 
[Epoch 43/63] [Batch 800/804] [D loss: 0.523423] [G loss: 0.138708] [ema: 0.999804] 
[Epoch 44/63] [Batch 0/804] [D loss: 0.470434] [G loss: 0.130930] [ema: 0.999804] 
[Epoch 44/63] [Batch 100/804] [D loss: 0.491569] [G loss: 0.133110] [ema: 0.999805] 
[Epoch 44/63] [Batch 200/804] [D loss: 0.520866] [G loss: 0.137048] [ema: 0.999805] 
[Epoch 44/63] [Batch 300/804] [D loss: 0.457034] [G loss: 0.147477] [ema: 0.999806] 
[Epoch 44/63] [Batch 400/804] [D loss: 0.498412] [G loss: 0.139566] [ema: 0.999806] 
[Epoch 44/63] [Batch 500/804] [D loss: 0.476239] [G loss: 0.160308] [ema: 0.999807] 
[Epoch 44/63] [Batch 600/804] [D loss: 0.577035] [G loss: 0.154489] [ema: 0.999807] 
[Epoch 44/63] [Batch 700/804] [D loss: 0.424681] [G loss: 0.137856] [ema: 0.999808] 
[Epoch 44/63] [Batch 800/804] [D loss: 0.556626] [G loss: 0.165061] [ema: 0.999808] 
[Epoch 45/63] [Batch 0/804] [D loss: 0.461498] [G loss: 0.145439] [ema: 0.999808] 
[Epoch 45/63] [Batch 100/804] [D loss: 0.455839] [G loss: 0.158210] [ema: 0.999809] 
[Epoch 45/63] [Batch 200/804] [D loss: 0.476832] [G loss: 0.134436] [ema: 0.999809] 
[Epoch 45/63] [Batch 300/804] [D loss: 0.533399] [G loss: 0.148023] [ema: 0.999810] 
[Epoch 45/63] [Batch 400/804] [D loss: 0.455203] [G loss: 0.155243] [ema: 0.999811] 
[Epoch 45/63] [Batch 500/804] [D loss: 0.449971] [G loss: 0.176493] [ema: 0.999811] 
[Epoch 45/63] [Batch 600/804] [D loss: 0.471852] [G loss: 0.155989] [ema: 0.999812] 
[Epoch 45/63] [Batch 700/804] [D loss: 0.456832] [G loss: 0.144207] [ema: 0.999812] 
[Epoch 45/63] [Batch 800/804] [D loss: 0.470902] [G loss: 0.153839] [ema: 0.999813] 
[Epoch 46/63] [Batch 0/804] [D loss: 0.492810] [G loss: 0.144403] [ema: 0.999813] 
[Epoch 46/63] [Batch 100/804] [D loss: 0.465762] [G loss: 0.120024] [ema: 0.999813] 
[Epoch 46/63] [Batch 200/804] [D loss: 0.540067] [G loss: 0.138181] [ema: 0.999814] 
[Epoch 46/63] [Batch 300/804] [D loss: 0.533773] [G loss: 0.127500] [ema: 0.999814] 
[Epoch 46/63] [Batch 400/804] [D loss: 0.420427] [G loss: 0.169242] [ema: 0.999815] 
[Epoch 46/63] [Batch 500/804] [D loss: 0.534567] [G loss: 0.138896] [ema: 0.999815] 
[Epoch 46/63] [Batch 600/804] [D loss: 0.510377] [G loss: 0.139475] [ema: 0.999816] 
[Epoch 46/63] [Batch 700/804] [D loss: 0.560933] [G loss: 0.165965] [ema: 0.999816] 
[Epoch 46/63] [Batch 800/804] [D loss: 0.517650] [G loss: 0.153513] [ema: 0.999817] 
[Epoch 47/63] [Batch 0/804] [D loss: 0.469917] [G loss: 0.128769] [ema: 0.999817] 
[Epoch 47/63] [Batch 100/804] [D loss: 0.472237] [G loss: 0.144994] [ema: 0.999817] 
[Epoch 47/63] [Batch 200/804] [D loss: 0.463868] [G loss: 0.151528] [ema: 0.999818] 
[Epoch 47/63] [Batch 300/804] [D loss: 0.528707] [G loss: 0.155949] [ema: 0.999818] 
[Epoch 47/63] [Batch 400/804] [D loss: 0.474782] [G loss: 0.140925] [ema: 0.999819] 
[Epoch 47/63] [Batch 500/804] [D loss: 0.471498] [G loss: 0.164394] [ema: 0.999819] 
[Epoch 47/63] [Batch 600/804] [D loss: 0.501856] [G loss: 0.122943] [ema: 0.999819] 
[Epoch 47/63] [Batch 700/804] [D loss: 0.495638] [G loss: 0.141134] [ema: 0.999820] 
[Epoch 47/63] [Batch 800/804] [D loss: 0.471483] [G loss: 0.142366] [ema: 0.999820] 



Saving checkpoint 4 in logs/daghar_50000_30_100/downstairs_50000_D_30_2024_10_18_02_43_48/Model



[Epoch 48/63] [Batch 0/804] [D loss: 0.425465] [G loss: 0.120517] [ema: 0.999820] 
[Epoch 48/63] [Batch 100/804] [D loss: 0.509760] [G loss: 0.122261] [ema: 0.999821] 
[Epoch 48/63] [Batch 200/804] [D loss: 0.444669] [G loss: 0.149372] [ema: 0.999821] 
[Epoch 48/63] [Batch 300/804] [D loss: 0.423995] [G loss: 0.174341] [ema: 0.999822] 
[Epoch 48/63] [Batch 400/804] [D loss: 0.432338] [G loss: 0.130686] [ema: 0.999822] 
[Epoch 48/63] [Batch 500/804] [D loss: 0.467098] [G loss: 0.156550] [ema: 0.999823] 
[Epoch 48/63] [Batch 600/804] [D loss: 0.456644] [G loss: 0.145645] [ema: 0.999823] 
[Epoch 48/63] [Batch 700/804] [D loss: 0.452343] [G loss: 0.148705] [ema: 0.999824] 
[Epoch 48/63] [Batch 800/804] [D loss: 0.515632] [G loss: 0.135890] [ema: 0.999824] 
[Epoch 49/63] [Batch 0/804] [D loss: 0.444977] [G loss: 0.163505] [ema: 0.999824] 
[Epoch 49/63] [Batch 100/804] [D loss: 0.481390] [G loss: 0.165497] [ema: 0.999825] 
[Epoch 49/63] [Batch 200/804] [D loss: 0.489373] [G loss: 0.173467] [ema: 0.999825] 
[Epoch 49/63] [Batch 300/804] [D loss: 0.427429] [G loss: 0.143532] [ema: 0.999825] 
[Epoch 49/63] [Batch 400/804] [D loss: 0.485096] [G loss: 0.130918] [ema: 0.999826] 
[Epoch 49/63] [Batch 500/804] [D loss: 0.517187] [G loss: 0.116500] [ema: 0.999826] 
[Epoch 49/63] [Batch 600/804] [D loss: 0.486724] [G loss: 0.150473] [ema: 0.999827] 
[Epoch 49/63] [Batch 700/804] [D loss: 0.515021] [G loss: 0.138114] [ema: 0.999827] 
[Epoch 49/63] [Batch 800/804] [D loss: 0.436933] [G loss: 0.158173] [ema: 0.999828] 
[Epoch 50/63] [Batch 0/804] [D loss: 0.459525] [G loss: 0.158621] [ema: 0.999828] 
[Epoch 50/63] [Batch 100/804] [D loss: 0.456224] [G loss: 0.123763] [ema: 0.999828] 
[Epoch 50/63] [Batch 200/804] [D loss: 0.448993] [G loss: 0.162189] [ema: 0.999828] 
[Epoch 50/63] [Batch 300/804] [D loss: 0.539732] [G loss: 0.135909] [ema: 0.999829] 
[Epoch 50/63] [Batch 400/804] [D loss: 0.472878] [G loss: 0.135801] [ema: 0.999829] 
[Epoch 50/63] [Batch 500/804] [D loss: 0.518755] [G loss: 0.167439] [ema: 0.999830] 
[Epoch 50/63] [Batch 600/804] [D loss: 0.531006] [G loss: 0.162572] [ema: 0.999830] 
[Epoch 50/63] [Batch 700/804] [D loss: 0.477584] [G loss: 0.161944] [ema: 0.999831] 
[Epoch 50/63] [Batch 800/804] [D loss: 0.427171] [G loss: 0.159411] [ema: 0.999831] 
[Epoch 51/63] [Batch 0/804] [D loss: 0.474970] [G loss: 0.156444] [ema: 0.999831] 
[Epoch 51/63] [Batch 100/804] [D loss: 0.471595] [G loss: 0.140780] [ema: 0.999831] 
[Epoch 51/63] [Batch 200/804] [D loss: 0.490879] [G loss: 0.137025] [ema: 0.999832] 
[Epoch 51/63] [Batch 300/804] [D loss: 0.503146] [G loss: 0.168816] [ema: 0.999832] 
[Epoch 51/63] [Batch 400/804] [D loss: 0.552129] [G loss: 0.154791] [ema: 0.999833] 
[Epoch 51/63] [Batch 500/804] [D loss: 0.493783] [G loss: 0.127474] [ema: 0.999833] 
[Epoch 51/63] [Batch 600/804] [D loss: 0.452027] [G loss: 0.167643] [ema: 0.999833] 
[Epoch 51/63] [Batch 700/804] [D loss: 0.485821] [G loss: 0.153188] [ema: 0.999834] 
[Epoch 51/63] [Batch 800/804] [D loss: 0.458329] [G loss: 0.157583] [ema: 0.999834] 
[Epoch 52/63] [Batch 0/804] [D loss: 0.469146] [G loss: 0.139111] [ema: 0.999834] 
[Epoch 52/63] [Batch 100/804] [D loss: 0.455664] [G loss: 0.125420] [ema: 0.999835] 
[Epoch 52/63] [Batch 200/804] [D loss: 0.446767] [G loss: 0.144184] [ema: 0.999835] 
[Epoch 52/63] [Batch 300/804] [D loss: 0.424048] [G loss: 0.157139] [ema: 0.999835] 
[Epoch 52/63] [Batch 400/804] [D loss: 0.461675] [G loss: 0.145553] [ema: 0.999836] 
[Epoch 52/63] [Batch 500/804] [D loss: 0.555246] [G loss: 0.165397] [ema: 0.999836] 
[Epoch 52/63] [Batch 600/804] [D loss: 0.487091] [G loss: 0.167331] [ema: 0.999837] 
[Epoch 52/63] [Batch 700/804] [D loss: 0.448727] [G loss: 0.148775] [ema: 0.999837] 
[Epoch 52/63] [Batch 800/804] [D loss: 0.504260] [G loss: 0.160511] [ema: 0.999837] 
[Epoch 53/63] [Batch 0/804] [D loss: 0.464740] [G loss: 0.134063] [ema: 0.999837] 
[Epoch 53/63] [Batch 100/804] [D loss: 0.519079] [G loss: 0.154390] [ema: 0.999838] 
[Epoch 53/63] [Batch 200/804] [D loss: 0.476321] [G loss: 0.168098] [ema: 0.999838] 
[Epoch 53/63] [Batch 300/804] [D loss: 0.460437] [G loss: 0.145036] [ema: 0.999838] 
[Epoch 53/63] [Batch 400/804] [D loss: 0.433889] [G loss: 0.137928] [ema: 0.999839] 
[Epoch 53/63] [Batch 500/804] [D loss: 0.391611] [G loss: 0.158479] [ema: 0.999839] 
[Epoch 53/63] [Batch 600/804] [D loss: 0.451011] [G loss: 0.133036] [ema: 0.999840] 
[Epoch 53/63] [Batch 700/804] [D loss: 0.490580] [G loss: 0.159206] [ema: 0.999840] 
[Epoch 53/63] [Batch 800/804] [D loss: 0.464061] [G loss: 0.143793] [ema: 0.999840] 
[Epoch 54/63] [Batch 0/804] [D loss: 0.504818] [G loss: 0.157572] [ema: 0.999840] 
[Epoch 54/63] [Batch 100/804] [D loss: 0.463050] [G loss: 0.146128] [ema: 0.999841] 
[Epoch 54/63] [Batch 200/804] [D loss: 0.479295] [G loss: 0.176275] [ema: 0.999841] 
[Epoch 54/63] [Batch 300/804] [D loss: 0.430158] [G loss: 0.154583] [ema: 0.999841] 
[Epoch 54/63] [Batch 400/804] [D loss: 0.494323] [G loss: 0.154923] [ema: 0.999842] 
[Epoch 54/63] [Batch 500/804] [D loss: 0.457896] [G loss: 0.151015] [ema: 0.999842] 
[Epoch 54/63] [Batch 600/804] [D loss: 0.474973] [G loss: 0.139730] [ema: 0.999843] 
[Epoch 54/63] [Batch 700/804] [D loss: 0.407416] [G loss: 0.145867] [ema: 0.999843] 
[Epoch 54/63] [Batch 800/804] [D loss: 0.497129] [G loss: 0.150352] [ema: 0.999843] 
[Epoch 55/63] [Batch 0/804] [D loss: 0.500304] [G loss: 0.184167] [ema: 0.999843] 
[Epoch 55/63] [Batch 100/804] [D loss: 0.484443] [G loss: 0.123692] [ema: 0.999844] 
[Epoch 55/63] [Batch 200/804] [D loss: 0.441289] [G loss: 0.171864] [ema: 0.999844] 
[Epoch 55/63] [Batch 300/804] [D loss: 0.503979] [G loss: 0.141336] [ema: 0.999844] 
[Epoch 55/63] [Batch 400/804] [D loss: 0.425443] [G loss: 0.142319] [ema: 0.999845] 
[Epoch 55/63] [Batch 500/804] [D loss: 0.417902] [G loss: 0.133995] [ema: 0.999845] 
[Epoch 55/63] [Batch 600/804] [D loss: 0.434508] [G loss: 0.152192] [ema: 0.999845] 
[Epoch 55/63] [Batch 700/804] [D loss: 0.472668] [G loss: 0.139214] [ema: 0.999846] 
[Epoch 55/63] [Batch 800/804] [D loss: 0.492755] [G loss: 0.141971] [ema: 0.999846] 
[Epoch 56/63] [Batch 0/804] [D loss: 0.511405] [G loss: 0.200500] [ema: 0.999846] 
[Epoch 56/63] [Batch 100/804] [D loss: 0.481854] [G loss: 0.166899] [ema: 0.999846] 
[Epoch 56/63] [Batch 200/804] [D loss: 0.508171] [G loss: 0.128489] [ema: 0.999847] 
[Epoch 56/63] [Batch 300/804] [D loss: 0.499646] [G loss: 0.146870] [ema: 0.999847] 
[Epoch 56/63] [Batch 400/804] [D loss: 0.455308] [G loss: 0.162450] [ema: 0.999847] 
[Epoch 56/63] [Batch 500/804] [D loss: 0.434894] [G loss: 0.139825] [ema: 0.999848] 
[Epoch 56/63] [Batch 600/804] [D loss: 0.435787] [G loss: 0.157446] [ema: 0.999848] 
[Epoch 56/63] [Batch 700/804] [D loss: 0.469346] [G loss: 0.153754] [ema: 0.999848] 
[Epoch 56/63] [Batch 800/804] [D loss: 0.441598] [G loss: 0.145644] [ema: 0.999849] 
[Epoch 57/63] [Batch 0/804] [D loss: 0.434767] [G loss: 0.169739] [ema: 0.999849] 
[Epoch 57/63] [Batch 100/804] [D loss: 0.393827] [G loss: 0.174737] [ema: 0.999849] 
[Epoch 57/63] [Batch 200/804] [D loss: 0.488954] [G loss: 0.170586] [ema: 0.999849] 
[Epoch 57/63] [Batch 300/804] [D loss: 0.423910] [G loss: 0.217532] [ema: 0.999850] 
[Epoch 57/63] [Batch 400/804] [D loss: 0.456511] [G loss: 0.166812] [ema: 0.999850] 
[Epoch 57/63] [Batch 500/804] [D loss: 0.463294] [G loss: 0.135963] [ema: 0.999850] 
[Epoch 57/63] [Batch 600/804] [D loss: 0.517553] [G loss: 0.176413] [ema: 0.999851] 
[Epoch 57/63] [Batch 700/804] [D loss: 0.470122] [G loss: 0.137262] [ema: 0.999851] 
[Epoch 57/63] [Batch 800/804] [D loss: 0.463224] [G loss: 0.148492] [ema: 0.999851] 
[Epoch 58/63] [Batch 0/804] [D loss: 0.403894] [G loss: 0.161792] [ema: 0.999851] 
[Epoch 58/63] [Batch 100/804] [D loss: 0.513399] [G loss: 0.151049] [ema: 0.999852] 
[Epoch 58/63] [Batch 200/804] [D loss: 0.372172] [G loss: 0.167179] [ema: 0.999852] 
[Epoch 58/63] [Batch 300/804] [D loss: 0.403982] [G loss: 0.147976] [ema: 0.999852] 
[Epoch 58/63] [Batch 400/804] [D loss: 0.582043] [G loss: 0.152943] [ema: 0.999853] 
[Epoch 58/63] [Batch 500/804] [D loss: 0.403617] [G loss: 0.150916] [ema: 0.999853] 
[Epoch 58/63] [Batch 600/804] [D loss: 0.450983] [G loss: 0.144834] [ema: 0.999853] 
[Epoch 58/63] [Batch 700/804] [D loss: 0.525899] [G loss: 0.142497] [ema: 0.999854] 
[Epoch 58/63] [Batch 800/804] [D loss: 0.423244] [G loss: 0.166163] [ema: 0.999854] 
[Epoch 59/63] [Batch 0/804] [D loss: 0.469142] [G loss: 0.151561] [ema: 0.999854] 
[Epoch 59/63] [Batch 100/804] [D loss: 0.534567] [G loss: 0.155656] [ema: 0.999854] 
[Epoch 59/63] [Batch 200/804] [D loss: 0.452278] [G loss: 0.181454] [ema: 0.999855] 
[Epoch 59/63] [Batch 300/804] [D loss: 0.490643] [G loss: 0.166474] [ema: 0.999855] 
[Epoch 59/63] [Batch 400/804] [D loss: 0.431879] [G loss: 0.163785] [ema: 0.999855] 
[Epoch 59/63] [Batch 500/804] [D loss: 0.481652] [G loss: 0.143740] [ema: 0.999855] 
[Epoch 59/63] [Batch 600/804] [D loss: 0.552862] [G loss: 0.164007] [ema: 0.999856] 
[Epoch 59/63] [Batch 700/804] [D loss: 0.523400] [G loss: 0.162891] [ema: 0.999856] 
[Epoch 59/63] [Batch 800/804] [D loss: 0.400395] [G loss: 0.142716] [ema: 0.999856] 
[Epoch 60/63] [Batch 0/804] [D loss: 0.456658] [G loss: 0.156602] [ema: 0.999856] 
[Epoch 60/63] [Batch 100/804] [D loss: 0.436687] [G loss: 0.179641] [ema: 0.999857] 
[Epoch 60/63] [Batch 200/804] [D loss: 0.475826] [G loss: 0.146909] [ema: 0.999857] 
[Epoch 60/63] [Batch 300/804] [D loss: 0.470155] [G loss: 0.153860] [ema: 0.999857] 
[Epoch 60/63] [Batch 400/804] [D loss: 0.402247] [G loss: 0.163229] [ema: 0.999858] 
[Epoch 60/63] [Batch 500/804] [D loss: 0.470802] [G loss: 0.133501] [ema: 0.999858] 
[Epoch 60/63] [Batch 600/804] [D loss: 0.446223] [G loss: 0.153275] [ema: 0.999858] 
[Epoch 60/63] [Batch 700/804] [D loss: 0.478564] [G loss: 0.124203] [ema: 0.999858] 
[Epoch 60/63] [Batch 800/804] [D loss: 0.486633] [G loss: 0.157992] [ema: 0.999859] 
[Epoch 61/63] [Batch 0/804] [D loss: 0.420952] [G loss: 0.167380] [ema: 0.999859] 
[Epoch 61/63] [Batch 100/804] [D loss: 0.455755] [G loss: 0.167036] [ema: 0.999859] 
[Epoch 61/63] [Batch 200/804] [D loss: 0.518553] [G loss: 0.130914] [ema: 0.999859] 
[Epoch 61/63] [Batch 300/804] [D loss: 0.461824] [G loss: 0.123700] [ema: 0.999860] 
[Epoch 61/63] [Batch 400/804] [D loss: 0.466254] [G loss: 0.163367] [ema: 0.999860] 
[Epoch 61/63] [Batch 500/804] [D loss: 0.435107] [G loss: 0.171435] [ema: 0.999860] 
[Epoch 61/63] [Batch 600/804] [D loss: 0.445170] [G loss: 0.178092] [ema: 0.999860] 
[Epoch 61/63] [Batch 700/804] [D loss: 0.434640] [G loss: 0.148915] [ema: 0.999861] 
[Epoch 61/63] [Batch 800/804] [D loss: 0.492949] [G loss: 0.147594] [ema: 0.999861] 
[Epoch 62/63] [Batch 0/804] [D loss: 0.418144] [G loss: 0.140137] [ema: 0.999861] 
[Epoch 62/63] [Batch 100/804] [D loss: 0.500442] [G loss: 0.153912] [ema: 0.999861] 
[Epoch 62/63] [Batch 200/804] [D loss: 0.506624] [G loss: 0.153443] [ema: 0.999862] 
[Epoch 62/63] [Batch 300/804] [D loss: 0.487013] [G loss: 0.172846] [ema: 0.999862] 
[Epoch 62/63] [Batch 400/804] [D loss: 0.470140] [G loss: 0.169106] [ema: 0.999862] 
[Epoch 62/63] [Batch 500/804] [D loss: 0.459686] [G loss: 0.137938] [ema: 0.999862] 
[Epoch 62/63] [Batch 600/804] [D loss: 0.480082] [G loss: 0.175554] [ema: 0.999863] 
[Epoch 62/63] [Batch 700/804] [D loss: 0.449705] [G loss: 0.161174] [ema: 0.999863] 
[Epoch 62/63] [Batch 800/804] [D loss: 0.433467] [G loss: 0.176920] [ema: 0.999863] 

----------------------------------------------------------------------------------------------------

 Starting individual training
sit training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
sit
daghar
return single class data and labels, class is sit
data shape is (17228, 3, 1, 30)
label shape is (17228,)
1077
Epochs between checkpoint: 12



Saving checkpoint 1 in logs/daghar_50000_30_100/sit_50000_D_30_2024_10_18_03_16_07/Model



[Epoch 0/47] [Batch 0/1077] [D loss: 1.283707] [G loss: 0.854389] [ema: 0.000000] 
[Epoch 0/47] [Batch 100/1077] [D loss: 0.542048] [G loss: 0.126867] [ema: 0.933033] 
[Epoch 0/47] [Batch 200/1077] [D loss: 0.642570] [G loss: 0.104043] [ema: 0.965936] 
[Epoch 0/47] [Batch 300/1077] [D loss: 0.581181] [G loss: 0.109195] [ema: 0.977160] 
[Epoch 0/47] [Batch 400/1077] [D loss: 0.524517] [G loss: 0.130435] [ema: 0.982821] 
[Epoch 0/47] [Batch 500/1077] [D loss: 0.497634] [G loss: 0.136814] [ema: 0.986233] 
[Epoch 0/47] [Batch 600/1077] [D loss: 0.558200] [G loss: 0.124254] [ema: 0.988514] 
[Epoch 0/47] [Batch 700/1077] [D loss: 0.565397] [G loss: 0.129004] [ema: 0.990147] 
[Epoch 0/47] [Batch 800/1077] [D loss: 0.543334] [G loss: 0.125708] [ema: 0.991373] 
[Epoch 0/47] [Batch 900/1077] [D loss: 0.552674] [G loss: 0.107129] [ema: 0.992328] 
[Epoch 0/47] [Batch 1000/1077] [D loss: 0.599398] [G loss: 0.114332] [ema: 0.993092] 
[Epoch 1/47] [Batch 0/1077] [D loss: 0.493156] [G loss: 0.133921] [ema: 0.993585] 
[Epoch 1/47] [Batch 100/1077] [D loss: 0.571547] [G loss: 0.115524] [ema: 0.994128] 
[Epoch 1/47] [Batch 200/1077] [D loss: 0.570694] [G loss: 0.097468] [ema: 0.994587] 
[Epoch 1/47] [Batch 300/1077] [D loss: 0.591611] [G loss: 0.090719] [ema: 0.994979] 
[Epoch 1/47] [Batch 400/1077] [D loss: 0.523645] [G loss: 0.124026] [ema: 0.995318] 
[Epoch 1/47] [Batch 500/1077] [D loss: 0.564111] [G loss: 0.108069] [ema: 0.995614] 
[Epoch 1/47] [Batch 600/1077] [D loss: 0.547193] [G loss: 0.116069] [ema: 0.995875] 
[Epoch 1/47] [Batch 700/1077] [D loss: 0.520402] [G loss: 0.129555] [ema: 0.996107] 
[Epoch 1/47] [Batch 800/1077] [D loss: 0.525857] [G loss: 0.124131] [ema: 0.996314] 
[Epoch 1/47] [Batch 900/1077] [D loss: 0.521263] [G loss: 0.125585] [ema: 0.996500] 
[Epoch 1/47] [Batch 1000/1077] [D loss: 0.552722] [G loss: 0.121182] [ema: 0.996668] 
[Epoch 2/47] [Batch 0/1077] [D loss: 0.576640] [G loss: 0.109452] [ema: 0.996787] 
[Epoch 2/47] [Batch 100/1077] [D loss: 0.544969] [G loss: 0.112500] [ema: 0.996930] 
[Epoch 2/47] [Batch 200/1077] [D loss: 0.560948] [G loss: 0.109382] [ema: 0.997060] 
[Epoch 2/47] [Batch 300/1077] [D loss: 0.551159] [G loss: 0.118404] [ema: 0.997179] 
[Epoch 2/47] [Batch 400/1077] [D loss: 0.557529] [G loss: 0.107004] [ema: 0.997290] 
[Epoch 2/47] [Batch 500/1077] [D loss: 0.558008] [G loss: 0.118324] [ema: 0.997392] 
[Epoch 2/47] [Batch 600/1077] [D loss: 0.553845] [G loss: 0.111870] [ema: 0.997486] 
[Epoch 2/47] [Batch 700/1077] [D loss: 0.556095] [G loss: 0.111383] [ema: 0.997574] 
[Epoch 2/47] [Batch 800/1077] [D loss: 0.524577] [G loss: 0.117703] [ema: 0.997656] 
[Epoch 2/47] [Batch 900/1077] [D loss: 0.540980] [G loss: 0.113178] [ema: 0.997733] 
[Epoch 2/47] [Batch 1000/1077] [D loss: 0.544526] [G loss: 0.121859] [ema: 0.997805] 
[Epoch 3/47] [Batch 0/1077] [D loss: 0.569705] [G loss: 0.121766] [ema: 0.997857] 
[Epoch 3/47] [Batch 100/1077] [D loss: 0.527249] [G loss: 0.119746] [ema: 0.997921] 
[Epoch 3/47] [Batch 200/1077] [D loss: 0.546393] [G loss: 0.106337] [ema: 0.997982] 
[Epoch 3/47] [Batch 300/1077] [D loss: 0.552110] [G loss: 0.109403] [ema: 0.998039] 
[Epoch 3/47] [Batch 400/1077] [D loss: 0.539071] [G loss: 0.113349] [ema: 0.998093] 
[Epoch 3/47] [Batch 500/1077] [D loss: 0.559962] [G loss: 0.123232] [ema: 0.998144] 
[Epoch 3/47] [Batch 600/1077] [D loss: 0.523753] [G loss: 0.108815] [ema: 0.998192] 
[Epoch 3/47] [Batch 700/1077] [D loss: 0.503303] [G loss: 0.129717] [ema: 0.998238] 
[Epoch 3/47] [Batch 800/1077] [D loss: 0.527448] [G loss: 0.107673] [ema: 0.998282] 
[Epoch 3/47] [Batch 900/1077] [D loss: 0.533909] [G loss: 0.123130] [ema: 0.998323] 
[Epoch 3/47] [Batch 1000/1077] [D loss: 0.552101] [G loss: 0.136841] [ema: 0.998363] 
[Epoch 4/47] [Batch 0/1077] [D loss: 0.497936] [G loss: 0.133663] [ema: 0.998392] 
[Epoch 4/47] [Batch 100/1077] [D loss: 0.504820] [G loss: 0.114250] [ema: 0.998429] 
[Epoch 4/47] [Batch 200/1077] [D loss: 0.516248] [G loss: 0.129618] [ema: 0.998464] 
[Epoch 4/47] [Batch 300/1077] [D loss: 0.479585] [G loss: 0.120946] [ema: 0.998497] 
[Epoch 4/47] [Batch 400/1077] [D loss: 0.537539] [G loss: 0.133611] [ema: 0.998529] 
[Epoch 4/47] [Batch 500/1077] [D loss: 0.539792] [G loss: 0.121071] [ema: 0.998559] 
[Epoch 4/47] [Batch 600/1077] [D loss: 0.536831] [G loss: 0.125825] [ema: 0.998589] 
[Epoch 4/47] [Batch 700/1077] [D loss: 0.533545] [G loss: 0.143408] [ema: 0.998617] 
[Epoch 4/47] [Batch 800/1077] [D loss: 0.544325] [G loss: 0.140324] [ema: 0.998644] 
[Epoch 4/47] [Batch 900/1077] [D loss: 0.581124] [G loss: 0.108291] [ema: 0.998670] 
[Epoch 4/47] [Batch 1000/1077] [D loss: 0.493779] [G loss: 0.149617] [ema: 0.998695] 
[Epoch 5/47] [Batch 0/1077] [D loss: 0.492326] [G loss: 0.151764] [ema: 0.998714] 
[Epoch 5/47] [Batch 100/1077] [D loss: 0.512467] [G loss: 0.139741] [ema: 0.998737] 
[Epoch 5/47] [Batch 200/1077] [D loss: 0.482794] [G loss: 0.132004] [ema: 0.998760] 
[Epoch 5/47] [Batch 300/1077] [D loss: 0.507089] [G loss: 0.132396] [ema: 0.998781] 
[Epoch 5/47] [Batch 400/1077] [D loss: 0.460096] [G loss: 0.128222] [ema: 0.998803] 
[Epoch 5/47] [Batch 500/1077] [D loss: 0.496152] [G loss: 0.142148] [ema: 0.998823] 
[Epoch 5/47] [Batch 600/1077] [D loss: 0.470295] [G loss: 0.141570] [ema: 0.998843] 
[Epoch 5/47] [Batch 700/1077] [D loss: 0.521328] [G loss: 0.147515] [ema: 0.998862] 
[Epoch 5/47] [Batch 800/1077] [D loss: 0.535040] [G loss: 0.117806] [ema: 0.998880] 
[Epoch 5/47] [Batch 900/1077] [D loss: 0.507434] [G loss: 0.150127] [ema: 0.998898] 
[Epoch 5/47] [Batch 1000/1077] [D loss: 0.503560] [G loss: 0.123687] [ema: 0.998915] 
[Epoch 6/47] [Batch 0/1077] [D loss: 0.461504] [G loss: 0.125495] [ema: 0.998928] 
[Epoch 6/47] [Batch 100/1077] [D loss: 0.458694] [G loss: 0.138283] [ema: 0.998944] 
[Epoch 6/47] [Batch 200/1077] [D loss: 0.521644] [G loss: 0.137921] [ema: 0.998960] 
[Epoch 6/47] [Batch 300/1077] [D loss: 0.467975] [G loss: 0.172091] [ema: 0.998975] 
[Epoch 6/47] [Batch 400/1077] [D loss: 0.497538] [G loss: 0.151903] [ema: 0.998990] 
[Epoch 6/47] [Batch 500/1077] [D loss: 0.530243] [G loss: 0.132476] [ema: 0.999005] 
[Epoch 6/47] [Batch 600/1077] [D loss: 0.504315] [G loss: 0.132981] [ema: 0.999019] 
[Epoch 6/47] [Batch 700/1077] [D loss: 0.490961] [G loss: 0.167191] [ema: 0.999033] 
[Epoch 6/47] [Batch 800/1077] [D loss: 0.477340] [G loss: 0.129297] [ema: 0.999046] 
[Epoch 6/47] [Batch 900/1077] [D loss: 0.503536] [G loss: 0.123408] [ema: 0.999059] 
[Epoch 6/47] [Batch 1000/1077] [D loss: 0.435287] [G loss: 0.151760] [ema: 0.999072] 
[Epoch 7/47] [Batch 0/1077] [D loss: 0.464140] [G loss: 0.130234] [ema: 0.999081] 
[Epoch 7/47] [Batch 100/1077] [D loss: 0.490386] [G loss: 0.156705] [ema: 0.999093] 
[Epoch 7/47] [Batch 200/1077] [D loss: 0.500288] [G loss: 0.159304] [ema: 0.999105] 
[Epoch 7/47] [Batch 300/1077] [D loss: 0.459633] [G loss: 0.155137] [ema: 0.999116] 
[Epoch 7/47] [Batch 400/1077] [D loss: 0.481254] [G loss: 0.126127] [ema: 0.999127] 
[Epoch 7/47] [Batch 500/1077] [D loss: 0.441613] [G loss: 0.160912] [ema: 0.999138] 
[Epoch 7/47] [Batch 600/1077] [D loss: 0.465482] [G loss: 0.147879] [ema: 0.999149] 
[Epoch 7/47] [Batch 700/1077] [D loss: 0.456993] [G loss: 0.152089] [ema: 0.999159] 
[Epoch 7/47] [Batch 800/1077] [D loss: 0.525318] [G loss: 0.154399] [ema: 0.999169] 
[Epoch 7/47] [Batch 900/1077] [D loss: 0.457858] [G loss: 0.136606] [ema: 0.999179] 
[Epoch 7/47] [Batch 1000/1077] [D loss: 0.458463] [G loss: 0.145818] [ema: 0.999189] 
[Epoch 8/47] [Batch 0/1077] [D loss: 0.501916] [G loss: 0.136428] [ema: 0.999196] 
[Epoch 8/47] [Batch 100/1077] [D loss: 0.482222] [G loss: 0.159784] [ema: 0.999205] 
[Epoch 8/47] [Batch 200/1077] [D loss: 0.433372] [G loss: 0.128928] [ema: 0.999214] 
[Epoch 8/47] [Batch 300/1077] [D loss: 0.482634] [G loss: 0.170852] [ema: 0.999223] 
[Epoch 8/47] [Batch 400/1077] [D loss: 0.526118] [G loss: 0.172499] [ema: 0.999231] 
[Epoch 8/47] [Batch 500/1077] [D loss: 0.534301] [G loss: 0.157170] [ema: 0.999240] 
[Epoch 8/47] [Batch 600/1077] [D loss: 0.461866] [G loss: 0.126422] [ema: 0.999248] 
[Epoch 8/47] [Batch 700/1077] [D loss: 0.480657] [G loss: 0.137771] [ema: 0.999256] 
[Epoch 8/47] [Batch 800/1077] [D loss: 0.454738] [G loss: 0.127125] [ema: 0.999264] 
[Epoch 8/47] [Batch 900/1077] [D loss: 0.479203] [G loss: 0.143594] [ema: 0.999272] 
[Epoch 8/47] [Batch 1000/1077] [D loss: 0.472591] [G loss: 0.137788] [ema: 0.999279] 
[Epoch 9/47] [Batch 0/1077] [D loss: 0.436628] [G loss: 0.167269] [ema: 0.999285] 
[Epoch 9/47] [Batch 100/1077] [D loss: 0.474917] [G loss: 0.152507] [ema: 0.999292] 
[Epoch 9/47] [Batch 200/1077] [D loss: 0.515118] [G loss: 0.136397] [ema: 0.999300] 
[Epoch 9/47] [Batch 300/1077] [D loss: 0.519748] [G loss: 0.157616] [ema: 0.999307] 
[Epoch 9/47] [Batch 400/1077] [D loss: 0.576153] [G loss: 0.143911] [ema: 0.999313] 
[Epoch 9/47] [Batch 500/1077] [D loss: 0.484196] [G loss: 0.150360] [ema: 0.999320] 
[Epoch 9/47] [Batch 600/1077] [D loss: 0.482562] [G loss: 0.130408] [ema: 0.999327] 
[Epoch 9/47] [Batch 700/1077] [D loss: 0.424673] [G loss: 0.147307] [ema: 0.999333] 
[Epoch 9/47] [Batch 800/1077] [D loss: 0.478005] [G loss: 0.185578] [ema: 0.999340] 
[Epoch 9/47] [Batch 900/1077] [D loss: 0.482645] [G loss: 0.153009] [ema: 0.999346] 
[Epoch 9/47] [Batch 1000/1077] [D loss: 0.500086] [G loss: 0.137670] [ema: 0.999352] 
[Epoch 10/47] [Batch 0/1077] [D loss: 0.470001] [G loss: 0.146994] [ema: 0.999357] 
[Epoch 10/47] [Batch 100/1077] [D loss: 0.499781] [G loss: 0.166621] [ema: 0.999363] 
[Epoch 10/47] [Batch 200/1077] [D loss: 0.461184] [G loss: 0.134381] [ema: 0.999368] 
[Epoch 10/47] [Batch 300/1077] [D loss: 0.506732] [G loss: 0.174427] [ema: 0.999374] 
[Epoch 10/47] [Batch 400/1077] [D loss: 0.462642] [G loss: 0.148785] [ema: 0.999380] 
[Epoch 10/47] [Batch 500/1077] [D loss: 0.482835] [G loss: 0.154538] [ema: 0.999385] 
[Epoch 10/47] [Batch 600/1077] [D loss: 0.439090] [G loss: 0.151397] [ema: 0.999391] 
[Epoch 10/47] [Batch 700/1077] [D loss: 0.510508] [G loss: 0.152941] [ema: 0.999396] 
[Epoch 10/47] [Batch 800/1077] [D loss: 0.475192] [G loss: 0.143643] [ema: 0.999401] 
[Epoch 10/47] [Batch 900/1077] [D loss: 0.498619] [G loss: 0.159355] [ema: 0.999406] 
[Epoch 10/47] [Batch 1000/1077] [D loss: 0.559220] [G loss: 0.130009] [ema: 0.999411] 
[Epoch 11/47] [Batch 0/1077] [D loss: 0.472324] [G loss: 0.163641] [ema: 0.999415] 
[Epoch 11/47] [Batch 100/1077] [D loss: 0.454147] [G loss: 0.148093] [ema: 0.999420] 
[Epoch 11/47] [Batch 200/1077] [D loss: 0.464475] [G loss: 0.138812] [ema: 0.999425] 
[Epoch 11/47] [Batch 300/1077] [D loss: 0.465694] [G loss: 0.139786] [ema: 0.999430] 
[Epoch 11/47] [Batch 400/1077] [D loss: 0.520291] [G loss: 0.169453] [ema: 0.999434] 
[Epoch 11/47] [Batch 500/1077] [D loss: 0.517742] [G loss: 0.159423] [ema: 0.999439] 
[Epoch 11/47] [Batch 600/1077] [D loss: 0.424352] [G loss: 0.156675] [ema: 0.999443] 
[Epoch 11/47] [Batch 700/1077] [D loss: 0.470255] [G loss: 0.129896] [ema: 0.999448] 
[Epoch 11/47] [Batch 800/1077] [D loss: 0.473906] [G loss: 0.126218] [ema: 0.999452] 
[Epoch 11/47] [Batch 900/1077] [D loss: 0.401556] [G loss: 0.172463] [ema: 0.999456] 
[Epoch 11/47] [Batch 1000/1077] [D loss: 0.475140] [G loss: 0.138045] [ema: 0.999461] 



Saving checkpoint 2 in logs/daghar_50000_30_100/sit_50000_D_30_2024_10_18_03_16_07/Model



[Epoch 12/47] [Batch 0/1077] [D loss: 0.545666] [G loss: 0.133928] [ema: 0.999464] 
[Epoch 12/47] [Batch 100/1077] [D loss: 0.466375] [G loss: 0.151516] [ema: 0.999468] 
[Epoch 12/47] [Batch 200/1077] [D loss: 0.455915] [G loss: 0.152293] [ema: 0.999472] 
[Epoch 12/47] [Batch 300/1077] [D loss: 0.495097] [G loss: 0.142408] [ema: 0.999476] 
[Epoch 12/47] [Batch 400/1077] [D loss: 0.451457] [G loss: 0.154780] [ema: 0.999480] 
[Epoch 12/47] [Batch 500/1077] [D loss: 0.476604] [G loss: 0.158305] [ema: 0.999484] 
[Epoch 12/47] [Batch 600/1077] [D loss: 0.461183] [G loss: 0.148715] [ema: 0.999488] 
[Epoch 12/47] [Batch 700/1077] [D loss: 0.453545] [G loss: 0.162205] [ema: 0.999491] 
[Epoch 12/47] [Batch 800/1077] [D loss: 0.469066] [G loss: 0.160973] [ema: 0.999495] 
[Epoch 12/47] [Batch 900/1077] [D loss: 0.477945] [G loss: 0.164405] [ema: 0.999499] 
[Epoch 12/47] [Batch 1000/1077] [D loss: 0.455260] [G loss: 0.145550] [ema: 0.999502] 
[Epoch 13/47] [Batch 0/1077] [D loss: 0.425538] [G loss: 0.168483] [ema: 0.999505] 
[Epoch 13/47] [Batch 100/1077] [D loss: 0.430635] [G loss: 0.148889] [ema: 0.999509] 
[Epoch 13/47] [Batch 200/1077] [D loss: 0.513434] [G loss: 0.151389] [ema: 0.999512] 
[Epoch 13/47] [Batch 300/1077] [D loss: 0.522250] [G loss: 0.163133] [ema: 0.999515] 
[Epoch 13/47] [Batch 400/1077] [D loss: 0.473846] [G loss: 0.129506] [ema: 0.999519] 
[Epoch 13/47] [Batch 500/1077] [D loss: 0.446882] [G loss: 0.140581] [ema: 0.999522] 
[Epoch 13/47] [Batch 600/1077] [D loss: 0.449491] [G loss: 0.147780] [ema: 0.999525] 
[Epoch 13/47] [Batch 700/1077] [D loss: 0.462768] [G loss: 0.169727] [ema: 0.999529] 
[Epoch 13/47] [Batch 800/1077] [D loss: 0.495063] [G loss: 0.152858] [ema: 0.999532] 
[Epoch 13/47] [Batch 900/1077] [D loss: 0.451408] [G loss: 0.153712] [ema: 0.999535] 
[Epoch 13/47] [Batch 1000/1077] [D loss: 0.444230] [G loss: 0.181760] [ema: 0.999538] 
[Epoch 14/47] [Batch 0/1077] [D loss: 0.475049] [G loss: 0.158338] [ema: 0.999540] 
[Epoch 14/47] [Batch 100/1077] [D loss: 0.520760] [G loss: 0.164054] [ema: 0.999543] 
[Epoch 14/47] [Batch 200/1077] [D loss: 0.437261] [G loss: 0.144682] [ema: 0.999546] 
[Epoch 14/47] [Batch 300/1077] [D loss: 0.525236] [G loss: 0.136692] [ema: 0.999549] 
[Epoch 14/47] [Batch 400/1077] [D loss: 0.452863] [G loss: 0.141497] [ema: 0.999552] 
[Epoch 14/47] [Batch 500/1077] [D loss: 0.443802] [G loss: 0.180045] [ema: 0.999555] 
[Epoch 14/47] [Batch 600/1077] [D loss: 0.496749] [G loss: 0.158268] [ema: 0.999558] 
[Epoch 14/47] [Batch 700/1077] [D loss: 0.545099] [G loss: 0.154428] [ema: 0.999561] 
[Epoch 14/47] [Batch 800/1077] [D loss: 0.467589] [G loss: 0.150054] [ema: 0.999564] 
[Epoch 14/47] [Batch 900/1077] [D loss: 0.461733] [G loss: 0.136043] [ema: 0.999566] 
[Epoch 14/47] [Batch 1000/1077] [D loss: 0.444448] [G loss: 0.138248] [ema: 0.999569] 
[Epoch 15/47] [Batch 0/1077] [D loss: 0.515022] [G loss: 0.156678] [ema: 0.999571] 
[Epoch 15/47] [Batch 100/1077] [D loss: 0.485808] [G loss: 0.137048] [ema: 0.999574] 
[Epoch 15/47] [Batch 200/1077] [D loss: 0.548035] [G loss: 0.129095] [ema: 0.999576] 
[Epoch 15/47] [Batch 300/1077] [D loss: 0.476766] [G loss: 0.126738] [ema: 0.999579] 
[Epoch 15/47] [Batch 400/1077] [D loss: 0.463221] [G loss: 0.143458] [ema: 0.999581] 
[Epoch 15/47] [Batch 500/1077] [D loss: 0.439872] [G loss: 0.158061] [ema: 0.999584] 
[Epoch 15/47] [Batch 600/1077] [D loss: 0.444871] [G loss: 0.163955] [ema: 0.999586] 
[Epoch 15/47] [Batch 700/1077] [D loss: 0.477338] [G loss: 0.162232] [ema: 0.999589] 
[Epoch 15/47] [Batch 800/1077] [D loss: 0.434421] [G loss: 0.154168] [ema: 0.999591] 
[Epoch 15/47] [Batch 900/1077] [D loss: 0.597007] [G loss: 0.153502] [ema: 0.999594] 
[Epoch 15/47] [Batch 1000/1077] [D loss: 0.411859] [G loss: 0.124316] [ema: 0.999596] 
[Epoch 16/47] [Batch 0/1077] [D loss: 0.502884] [G loss: 0.175259] [ema: 0.999598] 
[Epoch 16/47] [Batch 100/1077] [D loss: 0.420637] [G loss: 0.156111] [ema: 0.999600] 
[Epoch 16/47] [Batch 200/1077] [D loss: 0.495607] [G loss: 0.150132] [ema: 0.999602] 
[Epoch 16/47] [Batch 300/1077] [D loss: 0.463092] [G loss: 0.160612] [ema: 0.999605] 
[Epoch 16/47] [Batch 400/1077] [D loss: 0.446974] [G loss: 0.139278] [ema: 0.999607] 
[Epoch 16/47] [Batch 500/1077] [D loss: 0.429845] [G loss: 0.188085] [ema: 0.999609] 
[Epoch 16/47] [Batch 600/1077] [D loss: 0.506303] [G loss: 0.141190] [ema: 0.999611] 
[Epoch 16/47] [Batch 700/1077] [D loss: 0.485361] [G loss: 0.160732] [ema: 0.999614] 
[Epoch 16/47] [Batch 800/1077] [D loss: 0.489098] [G loss: 0.153268] [ema: 0.999616] 
[Epoch 16/47] [Batch 900/1077] [D loss: 0.469556] [G loss: 0.173191] [ema: 0.999618] 
[Epoch 16/47] [Batch 1000/1077] [D loss: 0.444566] [G loss: 0.159147] [ema: 0.999620] 
[Epoch 17/47] [Batch 0/1077] [D loss: 0.490659] [G loss: 0.144727] [ema: 0.999621] 
[Epoch 17/47] [Batch 100/1077] [D loss: 0.471032] [G loss: 0.152747] [ema: 0.999624] 
[Epoch 17/47] [Batch 200/1077] [D loss: 0.445459] [G loss: 0.146390] [ema: 0.999626] 
[Epoch 17/47] [Batch 300/1077] [D loss: 0.476327] [G loss: 0.152663] [ema: 0.999628] 
[Epoch 17/47] [Batch 400/1077] [D loss: 0.506879] [G loss: 0.158337] [ema: 0.999630] 
[Epoch 17/47] [Batch 500/1077] [D loss: 0.491839] [G loss: 0.131998] [ema: 0.999632] 
[Epoch 17/47] [Batch 600/1077] [D loss: 0.462027] [G loss: 0.156493] [ema: 0.999633] 
[Epoch 17/47] [Batch 700/1077] [D loss: 0.447096] [G loss: 0.163150] [ema: 0.999635] 
[Epoch 17/47] [Batch 800/1077] [D loss: 0.531418] [G loss: 0.150123] [ema: 0.999637] 
[Epoch 17/47] [Batch 900/1077] [D loss: 0.426659] [G loss: 0.153598] [ema: 0.999639] 
[Epoch 17/47] [Batch 1000/1077] [D loss: 0.503747] [G loss: 0.139596] [ema: 0.999641] 
[Epoch 18/47] [Batch 0/1077] [D loss: 0.476019] [G loss: 0.152956] [ema: 0.999643] 
[Epoch 18/47] [Batch 100/1077] [D loss: 0.439002] [G loss: 0.158910] [ema: 0.999644] 
[Epoch 18/47] [Batch 200/1077] [D loss: 0.441596] [G loss: 0.155894] [ema: 0.999646] 
[Epoch 18/47] [Batch 300/1077] [D loss: 0.531921] [G loss: 0.138247] [ema: 0.999648] 
[Epoch 18/47] [Batch 400/1077] [D loss: 0.515223] [G loss: 0.145453] [ema: 0.999650] 
[Epoch 18/47] [Batch 500/1077] [D loss: 0.474537] [G loss: 0.179637] [ema: 0.999652] 
[Epoch 18/47] [Batch 600/1077] [D loss: 0.462604] [G loss: 0.153734] [ema: 0.999653] 
[Epoch 18/47] [Batch 700/1077] [D loss: 0.514928] [G loss: 0.147874] [ema: 0.999655] 
[Epoch 18/47] [Batch 800/1077] [D loss: 0.468298] [G loss: 0.153531] [ema: 0.999657] 
[Epoch 18/47] [Batch 900/1077] [D loss: 0.475853] [G loss: 0.143128] [ema: 0.999658] 
[Epoch 18/47] [Batch 1000/1077] [D loss: 0.511752] [G loss: 0.145519] [ema: 0.999660] 
[Epoch 19/47] [Batch 0/1077] [D loss: 0.496015] [G loss: 0.146577] [ema: 0.999661] 
[Epoch 19/47] [Batch 100/1077] [D loss: 0.472344] [G loss: 0.173798] [ema: 0.999663] 
[Epoch 19/47] [Batch 200/1077] [D loss: 0.437084] [G loss: 0.155562] [ema: 0.999665] 
[Epoch 19/47] [Batch 300/1077] [D loss: 0.522416] [G loss: 0.163683] [ema: 0.999666] 
[Epoch 19/47] [Batch 400/1077] [D loss: 0.499086] [G loss: 0.164642] [ema: 0.999668] 
[Epoch 19/47] [Batch 500/1077] [D loss: 0.492519] [G loss: 0.139811] [ema: 0.999669] 
[Epoch 19/47] [Batch 600/1077] [D loss: 0.461590] [G loss: 0.143699] [ema: 0.999671] 
[Epoch 19/47] [Batch 700/1077] [D loss: 0.460787] [G loss: 0.146064] [ema: 0.999673] 
[Epoch 19/47] [Batch 800/1077] [D loss: 0.521822] [G loss: 0.138709] [ema: 0.999674] 
[Epoch 19/47] [Batch 900/1077] [D loss: 0.451113] [G loss: 0.163060] [ema: 0.999676] 
[Epoch 19/47] [Batch 1000/1077] [D loss: 0.468866] [G loss: 0.148277] [ema: 0.999677] 
[Epoch 20/47] [Batch 0/1077] [D loss: 0.583097] [G loss: 0.139258] [ema: 0.999678] 
[Epoch 20/47] [Batch 100/1077] [D loss: 0.476316] [G loss: 0.151097] [ema: 0.999680] 
[Epoch 20/47] [Batch 200/1077] [D loss: 0.345516] [G loss: 0.215092] [ema: 0.999681] 
[Epoch 20/47] [Batch 300/1077] [D loss: 0.511708] [G loss: 0.135342] [ema: 0.999683] 
[Epoch 20/47] [Batch 400/1077] [D loss: 0.441591] [G loss: 0.163081] [ema: 0.999684] 
[Epoch 20/47] [Batch 500/1077] [D loss: 0.447944] [G loss: 0.153721] [ema: 0.999686] 
[Epoch 20/47] [Batch 600/1077] [D loss: 0.461161] [G loss: 0.144879] [ema: 0.999687] 
[Epoch 20/47] [Batch 700/1077] [D loss: 0.451982] [G loss: 0.150328] [ema: 0.999688] 
[Epoch 20/47] [Batch 800/1077] [D loss: 0.501948] [G loss: 0.149114] [ema: 0.999690] 
[Epoch 20/47] [Batch 900/1077] [D loss: 0.473338] [G loss: 0.145506] [ema: 0.999691] 
[Epoch 20/47] [Batch 1000/1077] [D loss: 0.526473] [G loss: 0.173409] [ema: 0.999693] 
[Epoch 21/47] [Batch 0/1077] [D loss: 0.510168] [G loss: 0.140381] [ema: 0.999694] 
[Epoch 21/47] [Batch 100/1077] [D loss: 0.442942] [G loss: 0.122725] [ema: 0.999695] 
[Epoch 21/47] [Batch 200/1077] [D loss: 0.515931] [G loss: 0.130727] [ema: 0.999696] 
[Epoch 21/47] [Batch 300/1077] [D loss: 0.495441] [G loss: 0.143920] [ema: 0.999698] 
[Epoch 21/47] [Batch 400/1077] [D loss: 0.545108] [G loss: 0.129746] [ema: 0.999699] 
[Epoch 21/47] [Batch 500/1077] [D loss: 0.482081] [G loss: 0.148614] [ema: 0.999700] 
[Epoch 21/47] [Batch 600/1077] [D loss: 0.446527] [G loss: 0.147471] [ema: 0.999701] 
[Epoch 21/47] [Batch 700/1077] [D loss: 0.505881] [G loss: 0.151942] [ema: 0.999703] 
[Epoch 21/47] [Batch 800/1077] [D loss: 0.440812] [G loss: 0.146785] [ema: 0.999704] 
[Epoch 21/47] [Batch 900/1077] [D loss: 0.340111] [G loss: 0.184654] [ema: 0.999705] 
[Epoch 21/47] [Batch 1000/1077] [D loss: 0.309971] [G loss: 0.229903] [ema: 0.999707] 
[Epoch 22/47] [Batch 0/1077] [D loss: 0.521430] [G loss: 0.154354] [ema: 0.999708] 
[Epoch 22/47] [Batch 100/1077] [D loss: 0.595062] [G loss: 0.135387] [ema: 0.999709] 
[Epoch 22/47] [Batch 200/1077] [D loss: 0.539362] [G loss: 0.104132] [ema: 0.999710] 
[Epoch 22/47] [Batch 300/1077] [D loss: 0.491191] [G loss: 0.134514] [ema: 0.999711] 
[Epoch 22/47] [Batch 400/1077] [D loss: 0.476562] [G loss: 0.135554] [ema: 0.999712] 
[Epoch 22/47] [Batch 500/1077] [D loss: 0.542529] [G loss: 0.134997] [ema: 0.999714] 
[Epoch 22/47] [Batch 600/1077] [D loss: 0.501522] [G loss: 0.139042] [ema: 0.999715] 
[Epoch 22/47] [Batch 700/1077] [D loss: 0.518573] [G loss: 0.134131] [ema: 0.999716] 
[Epoch 22/47] [Batch 800/1077] [D loss: 0.451666] [G loss: 0.183662] [ema: 0.999717] 
[Epoch 22/47] [Batch 900/1077] [D loss: 0.545682] [G loss: 0.112777] [ema: 0.999718] 
[Epoch 22/47] [Batch 1000/1077] [D loss: 0.487475] [G loss: 0.141009] [ema: 0.999719] 
[Epoch 23/47] [Batch 0/1077] [D loss: 0.601011] [G loss: 0.100377] [ema: 0.999720] 
[Epoch 23/47] [Batch 100/1077] [D loss: 0.585997] [G loss: 0.088083] [ema: 0.999721] 
[Epoch 23/47] [Batch 200/1077] [D loss: 0.625577] [G loss: 0.104321] [ema: 0.999722] 
[Epoch 23/47] [Batch 300/1077] [D loss: 0.586684] [G loss: 0.120160] [ema: 0.999724] 
[Epoch 23/47] [Batch 400/1077] [D loss: 0.538314] [G loss: 0.119082] [ema: 0.999725] 
[Epoch 23/47] [Batch 500/1077] [D loss: 0.571158] [G loss: 0.120003] [ema: 0.999726] 
[Epoch 23/47] [Batch 600/1077] [D loss: 0.558176] [G loss: 0.113967] [ema: 0.999727] 
[Epoch 23/47] [Batch 700/1077] [D loss: 0.558321] [G loss: 0.108917] [ema: 0.999728] 
[Epoch 23/47] [Batch 800/1077] [D loss: 0.550867] [G loss: 0.111337] [ema: 0.999729] 
[Epoch 23/47] [Batch 900/1077] [D loss: 0.540950] [G loss: 0.109619] [ema: 0.999730] 
[Epoch 23/47] [Batch 1000/1077] [D loss: 0.563982] [G loss: 0.119770] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_50000_30_100/sit_50000_D_30_2024_10_18_03_16_07/Model



[Epoch 24/47] [Batch 0/1077] [D loss: 0.553162] [G loss: 0.119449] [ema: 0.999732] 
[Epoch 24/47] [Batch 100/1077] [D loss: 0.548031] [G loss: 0.112777] [ema: 0.999733] 
[Epoch 24/47] [Batch 200/1077] [D loss: 0.551236] [G loss: 0.121620] [ema: 0.999734] 
[Epoch 24/47] [Batch 300/1077] [D loss: 0.540629] [G loss: 0.101171] [ema: 0.999735] 
[Epoch 24/47] [Batch 400/1077] [D loss: 0.558093] [G loss: 0.114854] [ema: 0.999736] 
[Epoch 24/47] [Batch 500/1077] [D loss: 0.522855] [G loss: 0.141222] [ema: 0.999737] 
[Epoch 24/47] [Batch 600/1077] [D loss: 0.589058] [G loss: 0.122258] [ema: 0.999738] 
[Epoch 24/47] [Batch 700/1077] [D loss: 0.525890] [G loss: 0.143197] [ema: 0.999739] 
[Epoch 24/47] [Batch 800/1077] [D loss: 0.545553] [G loss: 0.118216] [ema: 0.999740] 
[Epoch 24/47] [Batch 900/1077] [D loss: 0.568100] [G loss: 0.106460] [ema: 0.999741] 
[Epoch 24/47] [Batch 1000/1077] [D loss: 0.499757] [G loss: 0.114626] [ema: 0.999742] 
[Epoch 25/47] [Batch 0/1077] [D loss: 0.439906] [G loss: 0.160409] [ema: 0.999743] 
[Epoch 25/47] [Batch 100/1077] [D loss: 0.513038] [G loss: 0.118019] [ema: 0.999744] 
[Epoch 25/47] [Batch 200/1077] [D loss: 0.517421] [G loss: 0.117446] [ema: 0.999744] 
[Epoch 25/47] [Batch 300/1077] [D loss: 0.520628] [G loss: 0.116862] [ema: 0.999745] 
[Epoch 25/47] [Batch 400/1077] [D loss: 0.488412] [G loss: 0.115206] [ema: 0.999746] 
[Epoch 25/47] [Batch 500/1077] [D loss: 0.434876] [G loss: 0.164276] [ema: 0.999747] 
[Epoch 25/47] [Batch 600/1077] [D loss: 0.491337] [G loss: 0.152683] [ema: 0.999748] 
[Epoch 25/47] [Batch 700/1077] [D loss: 0.523512] [G loss: 0.117637] [ema: 0.999749] 
[Epoch 25/47] [Batch 800/1077] [D loss: 0.520587] [G loss: 0.115559] [ema: 0.999750] 
[Epoch 25/47] [Batch 900/1077] [D loss: 0.510047] [G loss: 0.129996] [ema: 0.999751] 
[Epoch 25/47] [Batch 1000/1077] [D loss: 0.508371] [G loss: 0.147586] [ema: 0.999752] 
[Epoch 26/47] [Batch 0/1077] [D loss: 0.511125] [G loss: 0.131738] [ema: 0.999752] 
[Epoch 26/47] [Batch 100/1077] [D loss: 0.548313] [G loss: 0.126007] [ema: 0.999753] 
[Epoch 26/47] [Batch 200/1077] [D loss: 0.488605] [G loss: 0.153979] [ema: 0.999754] 
[Epoch 26/47] [Batch 300/1077] [D loss: 0.511035] [G loss: 0.124792] [ema: 0.999755] 
[Epoch 26/47] [Batch 400/1077] [D loss: 0.490557] [G loss: 0.147560] [ema: 0.999756] 
[Epoch 26/47] [Batch 500/1077] [D loss: 0.505142] [G loss: 0.141532] [ema: 0.999757] 
[Epoch 26/47] [Batch 600/1077] [D loss: 0.526638] [G loss: 0.110969] [ema: 0.999758] 
[Epoch 26/47] [Batch 700/1077] [D loss: 0.511551] [G loss: 0.144486] [ema: 0.999759] 
[Epoch 26/47] [Batch 800/1077] [D loss: 0.512436] [G loss: 0.128243] [ema: 0.999759] 
[Epoch 26/47] [Batch 900/1077] [D loss: 0.519911] [G loss: 0.156435] [ema: 0.999760] 
[Epoch 26/47] [Batch 1000/1077] [D loss: 0.487227] [G loss: 0.147188] [ema: 0.999761] 
[Epoch 27/47] [Batch 0/1077] [D loss: 0.449513] [G loss: 0.147611] [ema: 0.999762] 
[Epoch 27/47] [Batch 100/1077] [D loss: 0.472595] [G loss: 0.119900] [ema: 0.999762] 
[Epoch 27/47] [Batch 200/1077] [D loss: 0.312084] [G loss: 0.215463] [ema: 0.999763] 
[Epoch 27/47] [Batch 300/1077] [D loss: 0.535321] [G loss: 0.129099] [ema: 0.999764] 
[Epoch 27/47] [Batch 400/1077] [D loss: 0.552137] [G loss: 0.163601] [ema: 0.999765] 
[Epoch 27/47] [Batch 500/1077] [D loss: 0.460326] [G loss: 0.141686] [ema: 0.999766] 
[Epoch 27/47] [Batch 600/1077] [D loss: 0.490500] [G loss: 0.153007] [ema: 0.999766] 
[Epoch 27/47] [Batch 700/1077] [D loss: 0.455865] [G loss: 0.148793] [ema: 0.999767] 
[Epoch 27/47] [Batch 800/1077] [D loss: 0.434582] [G loss: 0.150283] [ema: 0.999768] 
[Epoch 27/47] [Batch 900/1077] [D loss: 0.479486] [G loss: 0.123896] [ema: 0.999769] 
[Epoch 27/47] [Batch 1000/1077] [D loss: 0.474992] [G loss: 0.163639] [ema: 0.999770] 
[Epoch 28/47] [Batch 0/1077] [D loss: 0.450765] [G loss: 0.165623] [ema: 0.999770] 
[Epoch 28/47] [Batch 100/1077] [D loss: 0.528203] [G loss: 0.129783] [ema: 0.999771] 
[Epoch 28/47] [Batch 200/1077] [D loss: 0.476947] [G loss: 0.143594] [ema: 0.999772] 
[Epoch 28/47] [Batch 300/1077] [D loss: 0.475683] [G loss: 0.128063] [ema: 0.999772] 
[Epoch 28/47] [Batch 400/1077] [D loss: 0.448447] [G loss: 0.143758] [ema: 0.999773] 
[Epoch 28/47] [Batch 500/1077] [D loss: 0.461885] [G loss: 0.166164] [ema: 0.999774] 
[Epoch 28/47] [Batch 600/1077] [D loss: 0.504116] [G loss: 0.114402] [ema: 0.999775] 
[Epoch 28/47] [Batch 700/1077] [D loss: 0.502747] [G loss: 0.156579] [ema: 0.999775] 
[Epoch 28/47] [Batch 800/1077] [D loss: 0.436904] [G loss: 0.167053] [ema: 0.999776] 
[Epoch 28/47] [Batch 900/1077] [D loss: 0.523272] [G loss: 0.143701] [ema: 0.999777] 
[Epoch 28/47] [Batch 1000/1077] [D loss: 0.457491] [G loss: 0.140773] [ema: 0.999778] 
[Epoch 29/47] [Batch 0/1077] [D loss: 0.469798] [G loss: 0.138850] [ema: 0.999778] 
[Epoch 29/47] [Batch 100/1077] [D loss: 0.444717] [G loss: 0.150309] [ema: 0.999779] 
[Epoch 29/47] [Batch 200/1077] [D loss: 0.458879] [G loss: 0.153007] [ema: 0.999780] 
[Epoch 29/47] [Batch 300/1077] [D loss: 0.446998] [G loss: 0.154814] [ema: 0.999780] 
[Epoch 29/47] [Batch 400/1077] [D loss: 0.451745] [G loss: 0.139647] [ema: 0.999781] 
[Epoch 29/47] [Batch 500/1077] [D loss: 0.511750] [G loss: 0.157995] [ema: 0.999782] 
[Epoch 29/47] [Batch 600/1077] [D loss: 0.521432] [G loss: 0.130618] [ema: 0.999782] 
[Epoch 29/47] [Batch 700/1077] [D loss: 0.373698] [G loss: 0.144538] [ema: 0.999783] 
[Epoch 29/47] [Batch 800/1077] [D loss: 0.465458] [G loss: 0.174202] [ema: 0.999784] 
[Epoch 29/47] [Batch 900/1077] [D loss: 0.453143] [G loss: 0.143858] [ema: 0.999784] 
[Epoch 29/47] [Batch 1000/1077] [D loss: 0.498020] [G loss: 0.146856] [ema: 0.999785] 
[Epoch 30/47] [Batch 0/1077] [D loss: 0.507314] [G loss: 0.161230] [ema: 0.999785] 
[Epoch 30/47] [Batch 100/1077] [D loss: 0.482615] [G loss: 0.140690] [ema: 0.999786] 
[Epoch 30/47] [Batch 200/1077] [D loss: 0.461911] [G loss: 0.162844] [ema: 0.999787] 
[Epoch 30/47] [Batch 300/1077] [D loss: 0.471730] [G loss: 0.148420] [ema: 0.999787] 
[Epoch 30/47] [Batch 400/1077] [D loss: 0.454665] [G loss: 0.121916] [ema: 0.999788] 
[Epoch 30/47] [Batch 500/1077] [D loss: 0.488359] [G loss: 0.138498] [ema: 0.999789] 
[Epoch 30/47] [Batch 600/1077] [D loss: 0.507941] [G loss: 0.142459] [ema: 0.999789] 
[Epoch 30/47] [Batch 700/1077] [D loss: 0.472410] [G loss: 0.136388] [ema: 0.999790] 
[Epoch 30/47] [Batch 800/1077] [D loss: 0.493383] [G loss: 0.147801] [ema: 0.999791] 
[Epoch 30/47] [Batch 900/1077] [D loss: 0.477072] [G loss: 0.155092] [ema: 0.999791] 
[Epoch 30/47] [Batch 1000/1077] [D loss: 0.468380] [G loss: 0.144107] [ema: 0.999792] 
[Epoch 31/47] [Batch 0/1077] [D loss: 0.459884] [G loss: 0.150405] [ema: 0.999792] 
[Epoch 31/47] [Batch 100/1077] [D loss: 0.532326] [G loss: 0.137472] [ema: 0.999793] 
[Epoch 31/47] [Batch 200/1077] [D loss: 0.477165] [G loss: 0.162574] [ema: 0.999794] 
[Epoch 31/47] [Batch 300/1077] [D loss: 0.495630] [G loss: 0.170679] [ema: 0.999794] 
[Epoch 31/47] [Batch 400/1077] [D loss: 0.485680] [G loss: 0.160622] [ema: 0.999795] 
[Epoch 31/47] [Batch 500/1077] [D loss: 0.474332] [G loss: 0.110362] [ema: 0.999795] 
[Epoch 31/47] [Batch 600/1077] [D loss: 0.459028] [G loss: 0.170908] [ema: 0.999796] 
[Epoch 31/47] [Batch 700/1077] [D loss: 0.488818] [G loss: 0.155504] [ema: 0.999797] 
[Epoch 31/47] [Batch 800/1077] [D loss: 0.508094] [G loss: 0.170821] [ema: 0.999797] 
[Epoch 31/47] [Batch 900/1077] [D loss: 0.474763] [G loss: 0.132374] [ema: 0.999798] 
[Epoch 31/47] [Batch 1000/1077] [D loss: 0.488397] [G loss: 0.133811] [ema: 0.999798] 
[Epoch 32/47] [Batch 0/1077] [D loss: 0.501038] [G loss: 0.140682] [ema: 0.999799] 
[Epoch 32/47] [Batch 100/1077] [D loss: 0.319737] [G loss: 0.245166] [ema: 0.999799] 
[Epoch 32/47] [Batch 200/1077] [D loss: 0.453798] [G loss: 0.192893] [ema: 0.999800] 
[Epoch 32/47] [Batch 300/1077] [D loss: 0.343211] [G loss: 0.189295] [ema: 0.999801] 
[Epoch 32/47] [Batch 400/1077] [D loss: 0.503997] [G loss: 0.162090] [ema: 0.999801] 
[Epoch 32/47] [Batch 500/1077] [D loss: 0.449675] [G loss: 0.165625] [ema: 0.999802] 
[Epoch 32/47] [Batch 600/1077] [D loss: 0.524036] [G loss: 0.143214] [ema: 0.999802] 
[Epoch 32/47] [Batch 700/1077] [D loss: 0.485455] [G loss: 0.140390] [ema: 0.999803] 
[Epoch 32/47] [Batch 800/1077] [D loss: 0.431431] [G loss: 0.149537] [ema: 0.999803] 
[Epoch 32/47] [Batch 900/1077] [D loss: 0.471998] [G loss: 0.130863] [ema: 0.999804] 
[Epoch 32/47] [Batch 1000/1077] [D loss: 0.474652] [G loss: 0.129543] [ema: 0.999805] 
[Epoch 33/47] [Batch 0/1077] [D loss: 0.472789] [G loss: 0.151843] [ema: 0.999805] 
[Epoch 33/47] [Batch 100/1077] [D loss: 0.388950] [G loss: 0.124556] [ema: 0.999806] 
[Epoch 33/47] [Batch 200/1077] [D loss: 0.533987] [G loss: 0.178837] [ema: 0.999806] 
[Epoch 33/47] [Batch 300/1077] [D loss: 0.486975] [G loss: 0.157748] [ema: 0.999807] 
[Epoch 33/47] [Batch 400/1077] [D loss: 0.476444] [G loss: 0.155077] [ema: 0.999807] 
[Epoch 33/47] [Batch 500/1077] [D loss: 0.506000] [G loss: 0.125465] [ema: 0.999808] 
[Epoch 33/47] [Batch 600/1077] [D loss: 0.510645] [G loss: 0.137174] [ema: 0.999808] 
[Epoch 33/47] [Batch 700/1077] [D loss: 0.495982] [G loss: 0.160282] [ema: 0.999809] 
[Epoch 33/47] [Batch 800/1077] [D loss: 0.492252] [G loss: 0.146727] [ema: 0.999809] 
[Epoch 33/47] [Batch 900/1077] [D loss: 0.513032] [G loss: 0.140725] [ema: 0.999810] 
[Epoch 33/47] [Batch 1000/1077] [D loss: 0.498439] [G loss: 0.146628] [ema: 0.999810] 
[Epoch 34/47] [Batch 0/1077] [D loss: 0.423048] [G loss: 0.135899] [ema: 0.999811] 
[Epoch 34/47] [Batch 100/1077] [D loss: 0.569576] [G loss: 0.142700] [ema: 0.999811] 
[Epoch 34/47] [Batch 200/1077] [D loss: 0.522575] [G loss: 0.135671] [ema: 0.999812] 
[Epoch 34/47] [Batch 300/1077] [D loss: 0.544427] [G loss: 0.157779] [ema: 0.999812] 
[Epoch 34/47] [Batch 400/1077] [D loss: 0.495557] [G loss: 0.120654] [ema: 0.999813] 
[Epoch 34/47] [Batch 500/1077] [D loss: 0.493438] [G loss: 0.160086] [ema: 0.999813] 
[Epoch 34/47] [Batch 600/1077] [D loss: 0.494482] [G loss: 0.131371] [ema: 0.999814] 
[Epoch 34/47] [Batch 700/1077] [D loss: 0.502291] [G loss: 0.161275] [ema: 0.999814] 
[Epoch 34/47] [Batch 800/1077] [D loss: 0.493368] [G loss: 0.129206] [ema: 0.999815] 
[Epoch 34/47] [Batch 900/1077] [D loss: 0.458343] [G loss: 0.141381] [ema: 0.999815] 
[Epoch 34/47] [Batch 1000/1077] [D loss: 0.490936] [G loss: 0.118745] [ema: 0.999816] 
[Epoch 35/47] [Batch 0/1077] [D loss: 0.488699] [G loss: 0.150159] [ema: 0.999816] 
[Epoch 35/47] [Batch 100/1077] [D loss: 0.465607] [G loss: 0.137224] [ema: 0.999817] 
[Epoch 35/47] [Batch 200/1077] [D loss: 0.492131] [G loss: 0.140232] [ema: 0.999817] 
[Epoch 35/47] [Batch 300/1077] [D loss: 0.486623] [G loss: 0.138582] [ema: 0.999818] 
[Epoch 35/47] [Batch 400/1077] [D loss: 0.449584] [G loss: 0.141793] [ema: 0.999818] 
[Epoch 35/47] [Batch 500/1077] [D loss: 0.442040] [G loss: 0.170013] [ema: 0.999819] 
[Epoch 35/47] [Batch 600/1077] [D loss: 0.534513] [G loss: 0.145308] [ema: 0.999819] 
[Epoch 35/47] [Batch 700/1077] [D loss: 0.482445] [G loss: 0.138787] [ema: 0.999819] 
[Epoch 35/47] [Batch 800/1077] [D loss: 0.468462] [G loss: 0.154274] [ema: 0.999820] 
[Epoch 35/47] [Batch 900/1077] [D loss: 0.445105] [G loss: 0.144567] [ema: 0.999820] 
[Epoch 35/47] [Batch 1000/1077] [D loss: 0.436608] [G loss: 0.145386] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_50000_30_100/sit_50000_D_30_2024_10_18_03_16_07/Model



[Epoch 36/47] [Batch 0/1077] [D loss: 0.503152] [G loss: 0.139389] [ema: 0.999821] 
[Epoch 36/47] [Batch 100/1077] [D loss: 0.454781] [G loss: 0.147495] [ema: 0.999822] 
[Epoch 36/47] [Batch 200/1077] [D loss: 0.530484] [G loss: 0.128779] [ema: 0.999822] 
[Epoch 36/47] [Batch 300/1077] [D loss: 0.512849] [G loss: 0.147626] [ema: 0.999823] 
[Epoch 36/47] [Batch 400/1077] [D loss: 0.498131] [G loss: 0.129758] [ema: 0.999823] 
[Epoch 36/47] [Batch 500/1077] [D loss: 0.480729] [G loss: 0.132697] [ema: 0.999824] 
[Epoch 36/47] [Batch 600/1077] [D loss: 0.551981] [G loss: 0.133284] [ema: 0.999824] 
[Epoch 36/47] [Batch 700/1077] [D loss: 0.522373] [G loss: 0.136975] [ema: 0.999824] 
[Epoch 36/47] [Batch 800/1077] [D loss: 0.447875] [G loss: 0.136336] [ema: 0.999825] 
[Epoch 36/47] [Batch 900/1077] [D loss: 0.501225] [G loss: 0.153025] [ema: 0.999825] 
[Epoch 36/47] [Batch 1000/1077] [D loss: 0.513797] [G loss: 0.149392] [ema: 0.999826] 
[Epoch 37/47] [Batch 0/1077] [D loss: 0.516996] [G loss: 0.172178] [ema: 0.999826] 
[Epoch 37/47] [Batch 100/1077] [D loss: 0.469538] [G loss: 0.143490] [ema: 0.999827] 
[Epoch 37/47] [Batch 200/1077] [D loss: 0.497358] [G loss: 0.131346] [ema: 0.999827] 
[Epoch 37/47] [Batch 300/1077] [D loss: 0.509953] [G loss: 0.134522] [ema: 0.999827] 
[Epoch 37/47] [Batch 400/1077] [D loss: 0.472983] [G loss: 0.144173] [ema: 0.999828] 
[Epoch 37/47] [Batch 500/1077] [D loss: 0.541206] [G loss: 0.132284] [ema: 0.999828] 
[Epoch 37/47] [Batch 600/1077] [D loss: 0.543239] [G loss: 0.124679] [ema: 0.999829] 
[Epoch 37/47] [Batch 700/1077] [D loss: 0.479337] [G loss: 0.167726] [ema: 0.999829] 
[Epoch 37/47] [Batch 800/1077] [D loss: 0.511827] [G loss: 0.142066] [ema: 0.999829] 
[Epoch 37/47] [Batch 900/1077] [D loss: 0.502749] [G loss: 0.138487] [ema: 0.999830] 
[Epoch 37/47] [Batch 1000/1077] [D loss: 0.475127] [G loss: 0.148127] [ema: 0.999830] 
[Epoch 38/47] [Batch 0/1077] [D loss: 0.431958] [G loss: 0.152017] [ema: 0.999831] 
[Epoch 38/47] [Batch 100/1077] [D loss: 0.462031] [G loss: 0.143602] [ema: 0.999831] 
[Epoch 38/47] [Batch 200/1077] [D loss: 0.462573] [G loss: 0.137952] [ema: 0.999831] 
[Epoch 38/47] [Batch 300/1077] [D loss: 0.452129] [G loss: 0.152780] [ema: 0.999832] 
[Epoch 38/47] [Batch 400/1077] [D loss: 0.486776] [G loss: 0.145955] [ema: 0.999832] 
[Epoch 38/47] [Batch 500/1077] [D loss: 0.535212] [G loss: 0.141135] [ema: 0.999833] 
[Epoch 38/47] [Batch 600/1077] [D loss: 0.489765] [G loss: 0.145512] [ema: 0.999833] 
[Epoch 38/47] [Batch 700/1077] [D loss: 0.512156] [G loss: 0.152857] [ema: 0.999833] 
[Epoch 38/47] [Batch 800/1077] [D loss: 0.504562] [G loss: 0.128523] [ema: 0.999834] 
[Epoch 38/47] [Batch 900/1077] [D loss: 0.517139] [G loss: 0.135130] [ema: 0.999834] 
[Epoch 38/47] [Batch 1000/1077] [D loss: 0.407606] [G loss: 0.184300] [ema: 0.999835] 
[Epoch 39/47] [Batch 0/1077] [D loss: 0.482365] [G loss: 0.130752] [ema: 0.999835] 
[Epoch 39/47] [Batch 100/1077] [D loss: 0.483094] [G loss: 0.213778] [ema: 0.999835] 
[Epoch 39/47] [Batch 200/1077] [D loss: 0.442317] [G loss: 0.144860] [ema: 0.999836] 
[Epoch 39/47] [Batch 300/1077] [D loss: 0.423825] [G loss: 0.173351] [ema: 0.999836] 
[Epoch 39/47] [Batch 400/1077] [D loss: 0.380159] [G loss: 0.223883] [ema: 0.999837] 
[Epoch 39/47] [Batch 500/1077] [D loss: 0.325652] [G loss: 0.197296] [ema: 0.999837] 
[Epoch 39/47] [Batch 600/1077] [D loss: 0.530190] [G loss: 0.126414] [ema: 0.999837] 
[Epoch 39/47] [Batch 700/1077] [D loss: 0.464241] [G loss: 0.145349] [ema: 0.999838] 
[Epoch 39/47] [Batch 800/1077] [D loss: 0.504009] [G loss: 0.127705] [ema: 0.999838] 
[Epoch 39/47] [Batch 900/1077] [D loss: 0.455539] [G loss: 0.154191] [ema: 0.999838] 
[Epoch 39/47] [Batch 1000/1077] [D loss: 0.420937] [G loss: 0.192266] [ema: 0.999839] 
[Epoch 40/47] [Batch 0/1077] [D loss: 0.438703] [G loss: 0.156639] [ema: 0.999839] 
[Epoch 40/47] [Batch 100/1077] [D loss: 0.559660] [G loss: 0.115554] [ema: 0.999839] 
[Epoch 40/47] [Batch 200/1077] [D loss: 0.388539] [G loss: 0.182923] [ema: 0.999840] 
[Epoch 40/47] [Batch 300/1077] [D loss: 0.496620] [G loss: 0.124071] [ema: 0.999840] 
[Epoch 40/47] [Batch 400/1077] [D loss: 0.498138] [G loss: 0.128086] [ema: 0.999841] 
[Epoch 40/47] [Batch 500/1077] [D loss: 0.513980] [G loss: 0.134821] [ema: 0.999841] 
[Epoch 40/47] [Batch 600/1077] [D loss: 0.499612] [G loss: 0.140183] [ema: 0.999841] 
[Epoch 40/47] [Batch 700/1077] [D loss: 0.470601] [G loss: 0.139318] [ema: 0.999842] 
[Epoch 40/47] [Batch 800/1077] [D loss: 0.469841] [G loss: 0.166877] [ema: 0.999842] 
[Epoch 40/47] [Batch 900/1077] [D loss: 0.452202] [G loss: 0.152954] [ema: 0.999842] 
[Epoch 40/47] [Batch 1000/1077] [D loss: 0.468156] [G loss: 0.144274] [ema: 0.999843] 
[Epoch 41/47] [Batch 0/1077] [D loss: 0.473963] [G loss: 0.142525] [ema: 0.999843] 
[Epoch 41/47] [Batch 100/1077] [D loss: 0.461810] [G loss: 0.147995] [ema: 0.999843] 
[Epoch 41/47] [Batch 200/1077] [D loss: 0.506656] [G loss: 0.159354] [ema: 0.999844] 
[Epoch 41/47] [Batch 300/1077] [D loss: 0.486170] [G loss: 0.146840] [ema: 0.999844] 
[Epoch 41/47] [Batch 400/1077] [D loss: 0.303329] [G loss: 0.189734] [ema: 0.999844] 
[Epoch 41/47] [Batch 500/1077] [D loss: 0.509194] [G loss: 0.121827] [ema: 0.999845] 
[Epoch 41/47] [Batch 600/1077] [D loss: 0.480600] [G loss: 0.141117] [ema: 0.999845] 
[Epoch 41/47] [Batch 700/1077] [D loss: 0.474092] [G loss: 0.138075] [ema: 0.999845] 
[Epoch 41/47] [Batch 800/1077] [D loss: 0.524493] [G loss: 0.125083] [ema: 0.999846] 
[Epoch 41/47] [Batch 900/1077] [D loss: 0.504419] [G loss: 0.155256] [ema: 0.999846] 
[Epoch 41/47] [Batch 1000/1077] [D loss: 0.585561] [G loss: 0.130012] [ema: 0.999847] 
[Epoch 42/47] [Batch 0/1077] [D loss: 0.483055] [G loss: 0.158378] [ema: 0.999847] 
[Epoch 42/47] [Batch 100/1077] [D loss: 0.528808] [G loss: 0.136742] [ema: 0.999847] 
[Epoch 42/47] [Batch 200/1077] [D loss: 0.451201] [G loss: 0.142589] [ema: 0.999847] 
[Epoch 42/47] [Batch 300/1077] [D loss: 0.468804] [G loss: 0.134104] [ema: 0.999848] 
[Epoch 42/47] [Batch 400/1077] [D loss: 0.464577] [G loss: 0.154628] [ema: 0.999848] 
[Epoch 42/47] [Batch 500/1077] [D loss: 0.456469] [G loss: 0.147246] [ema: 0.999848] 
[Epoch 42/47] [Batch 600/1077] [D loss: 0.493091] [G loss: 0.148520] [ema: 0.999849] 
[Epoch 42/47] [Batch 700/1077] [D loss: 0.467205] [G loss: 0.146903] [ema: 0.999849] 
[Epoch 42/47] [Batch 800/1077] [D loss: 0.507781] [G loss: 0.109177] [ema: 0.999849] 
[Epoch 42/47] [Batch 900/1077] [D loss: 0.494020] [G loss: 0.154560] [ema: 0.999850] 
[Epoch 42/47] [Batch 1000/1077] [D loss: 0.462272] [G loss: 0.159190] [ema: 0.999850] 
[Epoch 43/47] [Batch 0/1077] [D loss: 0.484435] [G loss: 0.132807] [ema: 0.999850] 
[Epoch 43/47] [Batch 100/1077] [D loss: 0.480714] [G loss: 0.156249] [ema: 0.999851] 
[Epoch 43/47] [Batch 200/1077] [D loss: 0.489354] [G loss: 0.153549] [ema: 0.999851] 
[Epoch 43/47] [Batch 300/1077] [D loss: 0.508940] [G loss: 0.154349] [ema: 0.999851] 
[Epoch 43/47] [Batch 400/1077] [D loss: 0.469310] [G loss: 0.145377] [ema: 0.999852] 
[Epoch 43/47] [Batch 500/1077] [D loss: 0.464201] [G loss: 0.146268] [ema: 0.999852] 
[Epoch 43/47] [Batch 600/1077] [D loss: 0.503119] [G loss: 0.135482] [ema: 0.999852] 
[Epoch 43/47] [Batch 700/1077] [D loss: 0.468916] [G loss: 0.160070] [ema: 0.999853] 
[Epoch 43/47] [Batch 800/1077] [D loss: 0.555087] [G loss: 0.134439] [ema: 0.999853] 
[Epoch 43/47] [Batch 900/1077] [D loss: 0.406432] [G loss: 0.153862] [ema: 0.999853] 
[Epoch 43/47] [Batch 1000/1077] [D loss: 0.487255] [G loss: 0.144647] [ema: 0.999854] 
[Epoch 44/47] [Batch 0/1077] [D loss: 0.509712] [G loss: 0.178253] [ema: 0.999854] 
[Epoch 44/47] [Batch 100/1077] [D loss: 0.411153] [G loss: 0.187521] [ema: 0.999854] 
[Epoch 44/47] [Batch 200/1077] [D loss: 0.560279] [G loss: 0.127804] [ema: 0.999854] 
[Epoch 44/47] [Batch 300/1077] [D loss: 0.515844] [G loss: 0.131600] [ema: 0.999855] 
[Epoch 44/47] [Batch 400/1077] [D loss: 0.471211] [G loss: 0.151131] [ema: 0.999855] 
[Epoch 44/47] [Batch 500/1077] [D loss: 0.485265] [G loss: 0.148813] [ema: 0.999855] 
[Epoch 44/47] [Batch 600/1077] [D loss: 0.499614] [G loss: 0.124629] [ema: 0.999856] 
[Epoch 44/47] [Batch 700/1077] [D loss: 0.478197] [G loss: 0.146611] [ema: 0.999856] 
[Epoch 44/47] [Batch 800/1077] [D loss: 0.491814] [G loss: 0.141571] [ema: 0.999856] 
[Epoch 44/47] [Batch 900/1077] [D loss: 0.415134] [G loss: 0.125062] [ema: 0.999856] 
[Epoch 44/47] [Batch 1000/1077] [D loss: 0.434661] [G loss: 0.128446] [ema: 0.999857] 
[Epoch 45/47] [Batch 0/1077] [D loss: 0.433652] [G loss: 0.171405] [ema: 0.999857] 
[Epoch 45/47] [Batch 100/1077] [D loss: 0.462156] [G loss: 0.145465] [ema: 0.999857] 
[Epoch 45/47] [Batch 200/1077] [D loss: 0.418115] [G loss: 0.165942] [ema: 0.999858] 
[Epoch 45/47] [Batch 300/1077] [D loss: 0.445677] [G loss: 0.141668] [ema: 0.999858] 
[Epoch 45/47] [Batch 400/1077] [D loss: 0.454527] [G loss: 0.135431] [ema: 0.999858] 
[Epoch 45/47] [Batch 500/1077] [D loss: 0.528941] [G loss: 0.169521] [ema: 0.999858] 
[Epoch 45/47] [Batch 600/1077] [D loss: 0.443296] [G loss: 0.143083] [ema: 0.999859] 
[Epoch 45/47] [Batch 700/1077] [D loss: 0.477078] [G loss: 0.159974] [ema: 0.999859] 
[Epoch 45/47] [Batch 800/1077] [D loss: 0.461729] [G loss: 0.157268] [ema: 0.999859] 
[Epoch 45/47] [Batch 900/1077] [D loss: 0.351442] [G loss: 0.191380] [ema: 0.999860] 
[Epoch 45/47] [Batch 1000/1077] [D loss: 0.506346] [G loss: 0.148439] [ema: 0.999860] 
[Epoch 46/47] [Batch 0/1077] [D loss: 0.462264] [G loss: 0.157517] [ema: 0.999860] 
[Epoch 46/47] [Batch 100/1077] [D loss: 0.425344] [G loss: 0.143477] [ema: 0.999860] 
[Epoch 46/47] [Batch 200/1077] [D loss: 0.479790] [G loss: 0.138399] [ema: 0.999861] 
[Epoch 46/47] [Batch 300/1077] [D loss: 0.479607] [G loss: 0.145261] [ema: 0.999861] 
[Epoch 46/47] [Batch 400/1077] [D loss: 0.505630] [G loss: 0.144671] [ema: 0.999861] 
[Epoch 46/47] [Batch 500/1077] [D loss: 0.491329] [G loss: 0.131397] [ema: 0.999861] 
[Epoch 46/47] [Batch 600/1077] [D loss: 0.466424] [G loss: 0.129808] [ema: 0.999862] 
[Epoch 46/47] [Batch 700/1077] [D loss: 0.501962] [G loss: 0.125662] [ema: 0.999862] 
[Epoch 46/47] [Batch 800/1077] [D loss: 0.508782] [G loss: 0.154284] [ema: 0.999862] 
[Epoch 46/47] [Batch 900/1077] [D loss: 0.502204] [G loss: 0.150217] [ema: 0.999863] 
[Epoch 46/47] [Batch 1000/1077] [D loss: 0.454141] [G loss: 0.164149] [ema: 0.999863] 

----------------------------------------------------------------------------------------------------

 Starting individual training
stand training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
stand
daghar
return single class data and labels, class is stand
data shape is (17228, 3, 1, 30)
label shape is (17228,)
1077
Epochs between checkpoint: 12



Saving checkpoint 1 in logs/daghar_50000_30_100/stand_50000_D_30_2024_10_18_03_47_52/Model



[Epoch 0/47] [Batch 0/1077] [D loss: 1.248540] [G loss: 0.856681] [ema: 0.000000] 
[Epoch 0/47] [Batch 100/1077] [D loss: 0.539431] [G loss: 0.127994] [ema: 0.933033] 
[Epoch 0/47] [Batch 200/1077] [D loss: 0.631539] [G loss: 0.104679] [ema: 0.965936] 
[Epoch 0/47] [Batch 300/1077] [D loss: 0.582833] [G loss: 0.109113] [ema: 0.977160] 
[Epoch 0/47] [Batch 400/1077] [D loss: 0.548887] [G loss: 0.128584] [ema: 0.982821] 
[Epoch 0/47] [Batch 500/1077] [D loss: 0.500448] [G loss: 0.136161] [ema: 0.986233] 
[Epoch 0/47] [Batch 600/1077] [D loss: 0.560866] [G loss: 0.122866] [ema: 0.988514] 
[Epoch 0/47] [Batch 700/1077] [D loss: 0.564292] [G loss: 0.130829] [ema: 0.990147] 
[Epoch 0/47] [Batch 800/1077] [D loss: 0.552662] [G loss: 0.124024] [ema: 0.991373] 
[Epoch 0/47] [Batch 900/1077] [D loss: 0.567412] [G loss: 0.105685] [ema: 0.992328] 
[Epoch 0/47] [Batch 1000/1077] [D loss: 0.587552] [G loss: 0.119856] [ema: 0.993092] 
[Epoch 1/47] [Batch 0/1077] [D loss: 0.504245] [G loss: 0.130013] [ema: 0.993585] 
[Epoch 1/47] [Batch 100/1077] [D loss: 0.598121] [G loss: 0.111002] [ema: 0.994128] 
[Epoch 1/47] [Batch 200/1077] [D loss: 0.572826] [G loss: 0.099437] [ema: 0.994587] 
[Epoch 1/47] [Batch 300/1077] [D loss: 0.587419] [G loss: 0.091398] [ema: 0.994979] 
[Epoch 1/47] [Batch 400/1077] [D loss: 0.521834] [G loss: 0.124695] [ema: 0.995318] 
[Epoch 1/47] [Batch 500/1077] [D loss: 0.565149] [G loss: 0.107265] [ema: 0.995614] 
[Epoch 1/47] [Batch 600/1077] [D loss: 0.556828] [G loss: 0.115239] [ema: 0.995875] 
[Epoch 1/47] [Batch 700/1077] [D loss: 0.531277] [G loss: 0.127362] [ema: 0.996107] 
[Epoch 1/47] [Batch 800/1077] [D loss: 0.531875] [G loss: 0.120029] [ema: 0.996314] 
[Epoch 1/47] [Batch 900/1077] [D loss: 0.530532] [G loss: 0.121062] [ema: 0.996500] 
[Epoch 1/47] [Batch 1000/1077] [D loss: 0.547372] [G loss: 0.122974] [ema: 0.996668] 
[Epoch 2/47] [Batch 0/1077] [D loss: 0.553683] [G loss: 0.115519] [ema: 0.996787] 
[Epoch 2/47] [Batch 100/1077] [D loss: 0.544308] [G loss: 0.110259] [ema: 0.996930] 
[Epoch 2/47] [Batch 200/1077] [D loss: 0.573542] [G loss: 0.101175] [ema: 0.997060] 
[Epoch 2/47] [Batch 300/1077] [D loss: 0.557506] [G loss: 0.118724] [ema: 0.997179] 
[Epoch 2/47] [Batch 400/1077] [D loss: 0.561733] [G loss: 0.107954] [ema: 0.997290] 
[Epoch 2/47] [Batch 500/1077] [D loss: 0.558501] [G loss: 0.117128] [ema: 0.997392] 
[Epoch 2/47] [Batch 600/1077] [D loss: 0.567571] [G loss: 0.112667] [ema: 0.997486] 
[Epoch 2/47] [Batch 700/1077] [D loss: 0.549762] [G loss: 0.110186] [ema: 0.997574] 
[Epoch 2/47] [Batch 800/1077] [D loss: 0.546690] [G loss: 0.106442] [ema: 0.997656] 
[Epoch 2/47] [Batch 900/1077] [D loss: 0.549770] [G loss: 0.116022] [ema: 0.997733] 
[Epoch 2/47] [Batch 1000/1077] [D loss: 0.555218] [G loss: 0.122850] [ema: 0.997805] 
[Epoch 3/47] [Batch 0/1077] [D loss: 0.560615] [G loss: 0.113802] [ema: 0.997857] 
[Epoch 3/47] [Batch 100/1077] [D loss: 0.555898] [G loss: 0.114552] [ema: 0.997921] 
[Epoch 3/47] [Batch 200/1077] [D loss: 0.552315] [G loss: 0.109219] [ema: 0.997982] 
[Epoch 3/47] [Batch 300/1077] [D loss: 0.562065] [G loss: 0.115184] [ema: 0.998039] 
[Epoch 3/47] [Batch 400/1077] [D loss: 0.560812] [G loss: 0.105741] [ema: 0.998093] 
[Epoch 3/47] [Batch 500/1077] [D loss: 0.570128] [G loss: 0.116890] [ema: 0.998144] 
[Epoch 3/47] [Batch 600/1077] [D loss: 0.548922] [G loss: 0.108015] [ema: 0.998192] 
[Epoch 3/47] [Batch 700/1077] [D loss: 0.551617] [G loss: 0.111840] [ema: 0.998238] 
[Epoch 3/47] [Batch 800/1077] [D loss: 0.548242] [G loss: 0.118137] [ema: 0.998282] 
[Epoch 3/47] [Batch 900/1077] [D loss: 0.565854] [G loss: 0.110418] [ema: 0.998323] 
[Epoch 3/47] [Batch 1000/1077] [D loss: 0.551983] [G loss: 0.109530] [ema: 0.998363] 
[Epoch 4/47] [Batch 0/1077] [D loss: 0.552787] [G loss: 0.106258] [ema: 0.998392] 
[Epoch 4/47] [Batch 100/1077] [D loss: 0.561424] [G loss: 0.108456] [ema: 0.998429] 
[Epoch 4/47] [Batch 200/1077] [D loss: 0.553878] [G loss: 0.111292] [ema: 0.998464] 
[Epoch 4/47] [Batch 300/1077] [D loss: 0.555979] [G loss: 0.113806] [ema: 0.998497] 
[Epoch 4/47] [Batch 400/1077] [D loss: 0.562571] [G loss: 0.115997] [ema: 0.998529] 
[Epoch 4/47] [Batch 500/1077] [D loss: 0.551763] [G loss: 0.112594] [ema: 0.998559] 
[Epoch 4/47] [Batch 600/1077] [D loss: 0.547022] [G loss: 0.110812] [ema: 0.998589] 
[Epoch 4/47] [Batch 700/1077] [D loss: 0.563313] [G loss: 0.107786] [ema: 0.998617] 
[Epoch 4/47] [Batch 800/1077] [D loss: 0.548775] [G loss: 0.108545] [ema: 0.998644] 
[Epoch 4/47] [Batch 900/1077] [D loss: 0.552507] [G loss: 0.110217] [ema: 0.998670] 
[Epoch 4/47] [Batch 1000/1077] [D loss: 0.560923] [G loss: 0.113323] [ema: 0.998695] 
[Epoch 5/47] [Batch 0/1077] [D loss: 0.549391] [G loss: 0.112700] [ema: 0.998714] 
[Epoch 5/47] [Batch 100/1077] [D loss: 0.534200] [G loss: 0.113841] [ema: 0.998737] 
[Epoch 5/47] [Batch 200/1077] [D loss: 0.540529] [G loss: 0.111347] [ema: 0.998760] 
[Epoch 5/47] [Batch 300/1077] [D loss: 0.556661] [G loss: 0.108774] [ema: 0.998781] 
[Epoch 5/47] [Batch 400/1077] [D loss: 0.541106] [G loss: 0.114176] [ema: 0.998803] 
[Epoch 5/47] [Batch 500/1077] [D loss: 0.558206] [G loss: 0.111102] [ema: 0.998823] 
[Epoch 5/47] [Batch 600/1077] [D loss: 0.558176] [G loss: 0.109447] [ema: 0.998843] 
[Epoch 5/47] [Batch 700/1077] [D loss: 0.553096] [G loss: 0.114873] [ema: 0.998862] 
[Epoch 5/47] [Batch 800/1077] [D loss: 0.555112] [G loss: 0.108468] [ema: 0.998880] 
[Epoch 5/47] [Batch 900/1077] [D loss: 0.564801] [G loss: 0.117346] [ema: 0.998898] 
[Epoch 5/47] [Batch 1000/1077] [D loss: 0.551117] [G loss: 0.112836] [ema: 0.998915] 
[Epoch 6/47] [Batch 0/1077] [D loss: 0.552336] [G loss: 0.118257] [ema: 0.998928] 
[Epoch 6/47] [Batch 100/1077] [D loss: 0.554867] [G loss: 0.114744] [ema: 0.998944] 
[Epoch 6/47] [Batch 200/1077] [D loss: 0.556738] [G loss: 0.113500] [ema: 0.998960] 
[Epoch 6/47] [Batch 300/1077] [D loss: 0.560706] [G loss: 0.114814] [ema: 0.998975] 
[Epoch 6/47] [Batch 400/1077] [D loss: 0.557897] [G loss: 0.107180] [ema: 0.998990] 
[Epoch 6/47] [Batch 500/1077] [D loss: 0.561527] [G loss: 0.110906] [ema: 0.999005] 
[Epoch 6/47] [Batch 600/1077] [D loss: 0.554962] [G loss: 0.113059] [ema: 0.999019] 
[Epoch 6/47] [Batch 700/1077] [D loss: 0.566195] [G loss: 0.112691] [ema: 0.999033] 
[Epoch 6/47] [Batch 800/1077] [D loss: 0.555076] [G loss: 0.115640] [ema: 0.999046] 
[Epoch 6/47] [Batch 900/1077] [D loss: 0.563875] [G loss: 0.114025] [ema: 0.999059] 
[Epoch 6/47] [Batch 1000/1077] [D loss: 0.546227] [G loss: 0.108845] [ema: 0.999072] 
[Epoch 7/47] [Batch 0/1077] [D loss: 0.555229] [G loss: 0.120026] [ema: 0.999081] 
[Epoch 7/47] [Batch 100/1077] [D loss: 0.558724] [G loss: 0.115282] [ema: 0.999093] 
[Epoch 7/47] [Batch 200/1077] [D loss: 0.546527] [G loss: 0.115863] [ema: 0.999105] 
[Epoch 7/47] [Batch 300/1077] [D loss: 0.561151] [G loss: 0.110197] [ema: 0.999116] 
[Epoch 7/47] [Batch 400/1077] [D loss: 0.546676] [G loss: 0.115172] [ema: 0.999127] 
[Epoch 7/47] [Batch 500/1077] [D loss: 0.560584] [G loss: 0.110346] [ema: 0.999138] 
[Epoch 7/47] [Batch 600/1077] [D loss: 0.553332] [G loss: 0.103882] [ema: 0.999149] 
[Epoch 7/47] [Batch 700/1077] [D loss: 0.554296] [G loss: 0.114872] [ema: 0.999159] 
[Epoch 7/47] [Batch 800/1077] [D loss: 0.562760] [G loss: 0.115783] [ema: 0.999169] 
[Epoch 7/47] [Batch 900/1077] [D loss: 0.541353] [G loss: 0.115927] [ema: 0.999179] 
[Epoch 7/47] [Batch 1000/1077] [D loss: 0.569958] [G loss: 0.109446] [ema: 0.999189] 
[Epoch 8/47] [Batch 0/1077] [D loss: 0.551285] [G loss: 0.110623] [ema: 0.999196] 
[Epoch 8/47] [Batch 100/1077] [D loss: 0.562768] [G loss: 0.112048] [ema: 0.999205] 
[Epoch 8/47] [Batch 200/1077] [D loss: 0.539173] [G loss: 0.107874] [ema: 0.999214] 
[Epoch 8/47] [Batch 300/1077] [D loss: 0.560562] [G loss: 0.113983] [ema: 0.999223] 
[Epoch 8/47] [Batch 400/1077] [D loss: 0.569034] [G loss: 0.117132] [ema: 0.999231] 
[Epoch 8/47] [Batch 500/1077] [D loss: 0.578756] [G loss: 0.101945] [ema: 0.999240] 
[Epoch 8/47] [Batch 600/1077] [D loss: 0.562242] [G loss: 0.118001] [ema: 0.999248] 
[Epoch 8/47] [Batch 700/1077] [D loss: 0.552117] [G loss: 0.119082] [ema: 0.999256] 
[Epoch 8/47] [Batch 800/1077] [D loss: 0.555138] [G loss: 0.112727] [ema: 0.999264] 
[Epoch 8/47] [Batch 900/1077] [D loss: 0.548704] [G loss: 0.112740] [ema: 0.999272] 
[Epoch 8/47] [Batch 1000/1077] [D loss: 0.565075] [G loss: 0.108080] [ema: 0.999279] 
[Epoch 9/47] [Batch 0/1077] [D loss: 0.543681] [G loss: 0.112129] [ema: 0.999285] 
[Epoch 9/47] [Batch 100/1077] [D loss: 0.559679] [G loss: 0.109386] [ema: 0.999292] 
[Epoch 9/47] [Batch 200/1077] [D loss: 0.567727] [G loss: 0.113516] [ema: 0.999300] 
[Epoch 9/47] [Batch 300/1077] [D loss: 0.557129] [G loss: 0.111833] [ema: 0.999307] 
[Epoch 9/47] [Batch 400/1077] [D loss: 0.557141] [G loss: 0.113429] [ema: 0.999313] 
[Epoch 9/47] [Batch 500/1077] [D loss: 0.556371] [G loss: 0.115383] [ema: 0.999320] 
[Epoch 9/47] [Batch 600/1077] [D loss: 0.553268] [G loss: 0.114755] [ema: 0.999327] 
[Epoch 9/47] [Batch 700/1077] [D loss: 0.535672] [G loss: 0.122555] [ema: 0.999333] 
[Epoch 9/47] [Batch 800/1077] [D loss: 0.587229] [G loss: 0.115709] [ema: 0.999340] 
[Epoch 9/47] [Batch 900/1077] [D loss: 0.451813] [G loss: 0.263098] [ema: 0.999346] 
[Epoch 9/47] [Batch 1000/1077] [D loss: 0.537084] [G loss: 0.130837] [ema: 0.999352] 
[Epoch 10/47] [Batch 0/1077] [D loss: 0.571548] [G loss: 0.120663] [ema: 0.999357] 
[Epoch 10/47] [Batch 100/1077] [D loss: 0.525998] [G loss: 0.112682] [ema: 0.999363] 
[Epoch 10/47] [Batch 200/1077] [D loss: 0.561910] [G loss: 0.124940] [ema: 0.999368] 
[Epoch 10/47] [Batch 300/1077] [D loss: 0.543726] [G loss: 0.119674] [ema: 0.999374] 
[Epoch 10/47] [Batch 400/1077] [D loss: 0.570436] [G loss: 0.130604] [ema: 0.999380] 
[Epoch 10/47] [Batch 500/1077] [D loss: 0.571456] [G loss: 0.112940] [ema: 0.999385] 
[Epoch 10/47] [Batch 600/1077] [D loss: 0.544409] [G loss: 0.117303] [ema: 0.999391] 
[Epoch 10/47] [Batch 700/1077] [D loss: 0.527446] [G loss: 0.116723] [ema: 0.999396] 
[Epoch 10/47] [Batch 800/1077] [D loss: 0.548518] [G loss: 0.117582] [ema: 0.999401] 
[Epoch 10/47] [Batch 900/1077] [D loss: 0.554594] [G loss: 0.106667] [ema: 0.999406] 
[Epoch 10/47] [Batch 1000/1077] [D loss: 0.546955] [G loss: 0.118505] [ema: 0.999411] 
[Epoch 11/47] [Batch 0/1077] [D loss: 0.548111] [G loss: 0.110075] [ema: 0.999415] 
[Epoch 11/47] [Batch 100/1077] [D loss: 0.544237] [G loss: 0.117906] [ema: 0.999420] 
[Epoch 11/47] [Batch 200/1077] [D loss: 0.522144] [G loss: 0.117869] [ema: 0.999425] 
[Epoch 11/47] [Batch 300/1077] [D loss: 0.536017] [G loss: 0.122135] [ema: 0.999430] 
[Epoch 11/47] [Batch 400/1077] [D loss: 0.533542] [G loss: 0.113626] [ema: 0.999434] 
[Epoch 11/47] [Batch 500/1077] [D loss: 0.546300] [G loss: 0.115724] [ema: 0.999439] 
[Epoch 11/47] [Batch 600/1077] [D loss: 0.538760] [G loss: 0.110849] [ema: 0.999443] 
[Epoch 11/47] [Batch 700/1077] [D loss: 0.500176] [G loss: 0.117355] [ema: 0.999448] 
[Epoch 11/47] [Batch 800/1077] [D loss: 0.531955] [G loss: 0.121492] [ema: 0.999452] 
[Epoch 11/47] [Batch 900/1077] [D loss: 0.517608] [G loss: 0.120163] [ema: 0.999456] 
[Epoch 11/47] [Batch 1000/1077] [D loss: 0.488245] [G loss: 0.138386] [ema: 0.999461] 



Saving checkpoint 2 in logs/daghar_50000_30_100/stand_50000_D_30_2024_10_18_03_47_52/Model



[Epoch 12/47] [Batch 0/1077] [D loss: 0.540705] [G loss: 0.118987] [ema: 0.999464] 
[Epoch 12/47] [Batch 100/1077] [D loss: 0.550107] [G loss: 0.113968] [ema: 0.999468] 
[Epoch 12/47] [Batch 200/1077] [D loss: 0.514741] [G loss: 0.125866] [ema: 0.999472] 
[Epoch 12/47] [Batch 300/1077] [D loss: 0.570162] [G loss: 0.126600] [ema: 0.999476] 
[Epoch 12/47] [Batch 400/1077] [D loss: 0.520141] [G loss: 0.125673] [ema: 0.999480] 
[Epoch 12/47] [Batch 500/1077] [D loss: 0.522091] [G loss: 0.139342] [ema: 0.999484] 
[Epoch 12/47] [Batch 600/1077] [D loss: 0.524597] [G loss: 0.131053] [ema: 0.999488] 
[Epoch 12/47] [Batch 700/1077] [D loss: 0.563990] [G loss: 0.129396] [ema: 0.999491] 
[Epoch 12/47] [Batch 800/1077] [D loss: 0.547852] [G loss: 0.134065] [ema: 0.999495] 
[Epoch 12/47] [Batch 900/1077] [D loss: 0.547385] [G loss: 0.118050] [ema: 0.999499] 
[Epoch 12/47] [Batch 1000/1077] [D loss: 0.531117] [G loss: 0.126844] [ema: 0.999502] 
[Epoch 13/47] [Batch 0/1077] [D loss: 0.513340] [G loss: 0.133315] [ema: 0.999505] 
[Epoch 13/47] [Batch 100/1077] [D loss: 0.541851] [G loss: 0.141087] [ema: 0.999509] 
[Epoch 13/47] [Batch 200/1077] [D loss: 0.496728] [G loss: 0.123026] [ema: 0.999512] 
[Epoch 13/47] [Batch 300/1077] [D loss: 0.528388] [G loss: 0.120397] [ema: 0.999515] 
[Epoch 13/47] [Batch 400/1077] [D loss: 0.527309] [G loss: 0.133711] [ema: 0.999519] 
[Epoch 13/47] [Batch 500/1077] [D loss: 0.543841] [G loss: 0.123663] [ema: 0.999522] 
[Epoch 13/47] [Batch 600/1077] [D loss: 0.518729] [G loss: 0.119551] [ema: 0.999525] 
[Epoch 13/47] [Batch 700/1077] [D loss: 0.510105] [G loss: 0.113452] [ema: 0.999529] 
[Epoch 13/47] [Batch 800/1077] [D loss: 0.561132] [G loss: 0.121463] [ema: 0.999532] 
[Epoch 13/47] [Batch 900/1077] [D loss: 0.494252] [G loss: 0.108157] [ema: 0.999535] 
[Epoch 13/47] [Batch 1000/1077] [D loss: 0.521920] [G loss: 0.119308] [ema: 0.999538] 
[Epoch 14/47] [Batch 0/1077] [D loss: 0.527293] [G loss: 0.120978] [ema: 0.999540] 
[Epoch 14/47] [Batch 100/1077] [D loss: 0.533859] [G loss: 0.137474] [ema: 0.999543] 
[Epoch 14/47] [Batch 200/1077] [D loss: 0.522916] [G loss: 0.142510] [ema: 0.999546] 
[Epoch 14/47] [Batch 300/1077] [D loss: 0.546870] [G loss: 0.144101] [ema: 0.999549] 
[Epoch 14/47] [Batch 400/1077] [D loss: 0.517007] [G loss: 0.123736] [ema: 0.999552] 
[Epoch 14/47] [Batch 500/1077] [D loss: 0.535948] [G loss: 0.136999] [ema: 0.999555] 
[Epoch 14/47] [Batch 600/1077] [D loss: 0.520741] [G loss: 0.124875] [ema: 0.999558] 
[Epoch 14/47] [Batch 700/1077] [D loss: 0.552001] [G loss: 0.134501] [ema: 0.999561] 
[Epoch 14/47] [Batch 800/1077] [D loss: 0.542221] [G loss: 0.128490] [ema: 0.999564] 
[Epoch 14/47] [Batch 900/1077] [D loss: 0.512027] [G loss: 0.128438] [ema: 0.999566] 
[Epoch 14/47] [Batch 1000/1077] [D loss: 0.547354] [G loss: 0.128849] [ema: 0.999569] 
[Epoch 15/47] [Batch 0/1077] [D loss: 0.548357] [G loss: 0.136172] [ema: 0.999571] 
[Epoch 15/47] [Batch 100/1077] [D loss: 0.562869] [G loss: 0.143125] [ema: 0.999574] 
[Epoch 15/47] [Batch 200/1077] [D loss: 0.508614] [G loss: 0.159016] [ema: 0.999576] 
[Epoch 15/47] [Batch 300/1077] [D loss: 0.506825] [G loss: 0.141099] [ema: 0.999579] 
[Epoch 15/47] [Batch 400/1077] [D loss: 0.526944] [G loss: 0.126528] [ema: 0.999581] 
[Epoch 15/47] [Batch 500/1077] [D loss: 0.570873] [G loss: 0.138660] [ema: 0.999584] 
[Epoch 15/47] [Batch 600/1077] [D loss: 0.509079] [G loss: 0.142648] [ema: 0.999586] 
[Epoch 15/47] [Batch 700/1077] [D loss: 0.497589] [G loss: 0.115943] [ema: 0.999589] 
[Epoch 15/47] [Batch 800/1077] [D loss: 0.500986] [G loss: 0.143925] [ema: 0.999591] 
[Epoch 15/47] [Batch 900/1077] [D loss: 0.561520] [G loss: 0.133479] [ema: 0.999594] 
[Epoch 15/47] [Batch 1000/1077] [D loss: 0.577815] [G loss: 0.113588] [ema: 0.999596] 
[Epoch 16/47] [Batch 0/1077] [D loss: 0.475188] [G loss: 0.145783] [ema: 0.999598] 
[Epoch 16/47] [Batch 100/1077] [D loss: 0.522220] [G loss: 0.113641] [ema: 0.999600] 
[Epoch 16/47] [Batch 200/1077] [D loss: 0.556462] [G loss: 0.135572] [ema: 0.999602] 
[Epoch 16/47] [Batch 300/1077] [D loss: 0.495715] [G loss: 0.131343] [ema: 0.999605] 
[Epoch 16/47] [Batch 400/1077] [D loss: 0.473595] [G loss: 0.138698] [ema: 0.999607] 
[Epoch 16/47] [Batch 500/1077] [D loss: 0.489437] [G loss: 0.145680] [ema: 0.999609] 
[Epoch 16/47] [Batch 600/1077] [D loss: 0.512411] [G loss: 0.148412] [ema: 0.999611] 
[Epoch 16/47] [Batch 700/1077] [D loss: 0.486084] [G loss: 0.126331] [ema: 0.999614] 
[Epoch 16/47] [Batch 800/1077] [D loss: 0.588865] [G loss: 0.121974] [ema: 0.999616] 
[Epoch 16/47] [Batch 900/1077] [D loss: 0.506076] [G loss: 0.132263] [ema: 0.999618] 
[Epoch 16/47] [Batch 1000/1077] [D loss: 0.535567] [G loss: 0.130179] [ema: 0.999620] 
[Epoch 17/47] [Batch 0/1077] [D loss: 0.504009] [G loss: 0.158171] [ema: 0.999621] 
[Epoch 17/47] [Batch 100/1077] [D loss: 0.533607] [G loss: 0.124319] [ema: 0.999624] 
[Epoch 17/47] [Batch 200/1077] [D loss: 0.517944] [G loss: 0.147405] [ema: 0.999626] 
[Epoch 17/47] [Batch 300/1077] [D loss: 0.499444] [G loss: 0.121954] [ema: 0.999628] 
[Epoch 17/47] [Batch 400/1077] [D loss: 0.487725] [G loss: 0.143390] [ema: 0.999630] 
[Epoch 17/47] [Batch 500/1077] [D loss: 0.520112] [G loss: 0.125031] [ema: 0.999632] 
[Epoch 17/47] [Batch 600/1077] [D loss: 0.435446] [G loss: 0.154024] [ema: 0.999633] 
[Epoch 17/47] [Batch 700/1077] [D loss: 0.478069] [G loss: 0.136461] [ema: 0.999635] 
[Epoch 17/47] [Batch 800/1077] [D loss: 0.501789] [G loss: 0.118473] [ema: 0.999637] 
[Epoch 17/47] [Batch 900/1077] [D loss: 0.476746] [G loss: 0.121531] [ema: 0.999639] 
[Epoch 17/47] [Batch 1000/1077] [D loss: 0.523270] [G loss: 0.160112] [ema: 0.999641] 
[Epoch 18/47] [Batch 0/1077] [D loss: 0.535993] [G loss: 0.143869] [ema: 0.999643] 
[Epoch 18/47] [Batch 100/1077] [D loss: 0.477118] [G loss: 0.137883] [ema: 0.999644] 
[Epoch 18/47] [Batch 200/1077] [D loss: 0.558981] [G loss: 0.140088] [ema: 0.999646] 
[Epoch 18/47] [Batch 300/1077] [D loss: 0.489784] [G loss: 0.139824] [ema: 0.999648] 
[Epoch 18/47] [Batch 400/1077] [D loss: 0.466781] [G loss: 0.137580] [ema: 0.999650] 
[Epoch 18/47] [Batch 500/1077] [D loss: 0.452026] [G loss: 0.146919] [ema: 0.999652] 
[Epoch 18/47] [Batch 600/1077] [D loss: 0.521721] [G loss: 0.148141] [ema: 0.999653] 
[Epoch 18/47] [Batch 700/1077] [D loss: 0.501597] [G loss: 0.131393] [ema: 0.999655] 
[Epoch 18/47] [Batch 800/1077] [D loss: 0.489139] [G loss: 0.156368] [ema: 0.999657] 
[Epoch 18/47] [Batch 900/1077] [D loss: 0.485729] [G loss: 0.126095] [ema: 0.999658] 
[Epoch 18/47] [Batch 1000/1077] [D loss: 0.487248] [G loss: 0.148723] [ema: 0.999660] 
[Epoch 19/47] [Batch 0/1077] [D loss: 0.524309] [G loss: 0.133310] [ema: 0.999661] 
[Epoch 19/47] [Batch 100/1077] [D loss: 0.490346] [G loss: 0.146996] [ema: 0.999663] 
[Epoch 19/47] [Batch 200/1077] [D loss: 0.495064] [G loss: 0.128762] [ema: 0.999665] 
[Epoch 19/47] [Batch 300/1077] [D loss: 0.460457] [G loss: 0.159726] [ema: 0.999666] 
[Epoch 19/47] [Batch 400/1077] [D loss: 0.519071] [G loss: 0.116881] [ema: 0.999668] 
[Epoch 19/47] [Batch 500/1077] [D loss: 0.535580] [G loss: 0.142876] [ema: 0.999669] 
[Epoch 19/47] [Batch 600/1077] [D loss: 0.493519] [G loss: 0.126504] [ema: 0.999671] 
[Epoch 19/47] [Batch 700/1077] [D loss: 0.515007] [G loss: 0.136863] [ema: 0.999673] 
[Epoch 19/47] [Batch 800/1077] [D loss: 0.509390] [G loss: 0.134702] [ema: 0.999674] 
[Epoch 19/47] [Batch 900/1077] [D loss: 0.512316] [G loss: 0.123338] [ema: 0.999676] 
[Epoch 19/47] [Batch 1000/1077] [D loss: 0.476860] [G loss: 0.128157] [ema: 0.999677] 
[Epoch 20/47] [Batch 0/1077] [D loss: 0.485978] [G loss: 0.152451] [ema: 0.999678] 
[Epoch 20/47] [Batch 100/1077] [D loss: 0.531950] [G loss: 0.130408] [ema: 0.999680] 
[Epoch 20/47] [Batch 200/1077] [D loss: 0.490568] [G loss: 0.124116] [ema: 0.999681] 
[Epoch 20/47] [Batch 300/1077] [D loss: 0.493565] [G loss: 0.137791] [ema: 0.999683] 
[Epoch 20/47] [Batch 400/1077] [D loss: 0.507022] [G loss: 0.143272] [ema: 0.999684] 
[Epoch 20/47] [Batch 500/1077] [D loss: 0.481556] [G loss: 0.131455] [ema: 0.999686] 
[Epoch 20/47] [Batch 600/1077] [D loss: 0.545485] [G loss: 0.147188] [ema: 0.999687] 
[Epoch 20/47] [Batch 700/1077] [D loss: 0.445457] [G loss: 0.126204] [ema: 0.999688] 
[Epoch 20/47] [Batch 800/1077] [D loss: 0.467259] [G loss: 0.143504] [ema: 0.999690] 
[Epoch 20/47] [Batch 900/1077] [D loss: 0.557744] [G loss: 0.136936] [ema: 0.999691] 
[Epoch 20/47] [Batch 1000/1077] [D loss: 0.538259] [G loss: 0.151207] [ema: 0.999693] 
[Epoch 21/47] [Batch 0/1077] [D loss: 0.518302] [G loss: 0.132038] [ema: 0.999694] 
[Epoch 21/47] [Batch 100/1077] [D loss: 0.501097] [G loss: 0.134990] [ema: 0.999695] 
[Epoch 21/47] [Batch 200/1077] [D loss: 0.484860] [G loss: 0.154946] [ema: 0.999696] 
[Epoch 21/47] [Batch 300/1077] [D loss: 0.484932] [G loss: 0.148422] [ema: 0.999698] 
[Epoch 21/47] [Batch 400/1077] [D loss: 0.482824] [G loss: 0.136478] [ema: 0.999699] 
[Epoch 21/47] [Batch 500/1077] [D loss: 0.499307] [G loss: 0.152251] [ema: 0.999700] 
[Epoch 21/47] [Batch 600/1077] [D loss: 0.429145] [G loss: 0.158079] [ema: 0.999701] 
[Epoch 21/47] [Batch 700/1077] [D loss: 0.474699] [G loss: 0.141802] [ema: 0.999703] 
[Epoch 21/47] [Batch 800/1077] [D loss: 0.477431] [G loss: 0.136936] [ema: 0.999704] 
[Epoch 21/47] [Batch 900/1077] [D loss: 0.498571] [G loss: 0.126451] [ema: 0.999705] 
[Epoch 21/47] [Batch 1000/1077] [D loss: 0.447333] [G loss: 0.143659] [ema: 0.999707] 
[Epoch 22/47] [Batch 0/1077] [D loss: 0.479573] [G loss: 0.145827] [ema: 0.999708] 
[Epoch 22/47] [Batch 100/1077] [D loss: 0.487342] [G loss: 0.140362] [ema: 0.999709] 
[Epoch 22/47] [Batch 200/1077] [D loss: 0.516780] [G loss: 0.129841] [ema: 0.999710] 
[Epoch 22/47] [Batch 300/1077] [D loss: 0.507070] [G loss: 0.148065] [ema: 0.999711] 
[Epoch 22/47] [Batch 400/1077] [D loss: 0.502358] [G loss: 0.150736] [ema: 0.999712] 
[Epoch 22/47] [Batch 500/1077] [D loss: 0.484100] [G loss: 0.151350] [ema: 0.999714] 
[Epoch 22/47] [Batch 600/1077] [D loss: 0.511759] [G loss: 0.140696] [ema: 0.999715] 
[Epoch 22/47] [Batch 700/1077] [D loss: 0.538881] [G loss: 0.130102] [ema: 0.999716] 
[Epoch 22/47] [Batch 800/1077] [D loss: 0.462574] [G loss: 0.149434] [ema: 0.999717] 
[Epoch 22/47] [Batch 900/1077] [D loss: 0.505171] [G loss: 0.135748] [ema: 0.999718] 
[Epoch 22/47] [Batch 1000/1077] [D loss: 0.513417] [G loss: 0.138529] [ema: 0.999719] 
[Epoch 23/47] [Batch 0/1077] [D loss: 0.487639] [G loss: 0.137508] [ema: 0.999720] 
[Epoch 23/47] [Batch 100/1077] [D loss: 0.514403] [G loss: 0.133089] [ema: 0.999721] 
[Epoch 23/47] [Batch 200/1077] [D loss: 0.534476] [G loss: 0.113072] [ema: 0.999722] 
[Epoch 23/47] [Batch 300/1077] [D loss: 0.439797] [G loss: 0.144729] [ema: 0.999724] 
[Epoch 23/47] [Batch 400/1077] [D loss: 0.510649] [G loss: 0.139916] [ema: 0.999725] 
[Epoch 23/47] [Batch 500/1077] [D loss: 0.493637] [G loss: 0.129080] [ema: 0.999726] 
[Epoch 23/47] [Batch 600/1077] [D loss: 0.506421] [G loss: 0.135508] [ema: 0.999727] 
[Epoch 23/47] [Batch 700/1077] [D loss: 0.494147] [G loss: 0.143021] [ema: 0.999728] 
[Epoch 23/47] [Batch 800/1077] [D loss: 0.509430] [G loss: 0.138923] [ema: 0.999729] 
[Epoch 23/47] [Batch 900/1077] [D loss: 0.514385] [G loss: 0.130591] [ema: 0.999730] 
[Epoch 23/47] [Batch 1000/1077] [D loss: 0.502467] [G loss: 0.128070] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_50000_30_100/stand_50000_D_30_2024_10_18_03_47_52/Model



[Epoch 24/47] [Batch 0/1077] [D loss: 0.430067] [G loss: 0.153668] [ema: 0.999732] 
[Epoch 24/47] [Batch 100/1077] [D loss: 0.515999] [G loss: 0.152785] [ema: 0.999733] 
[Epoch 24/47] [Batch 200/1077] [D loss: 0.455906] [G loss: 0.151604] [ema: 0.999734] 
[Epoch 24/47] [Batch 300/1077] [D loss: 0.488329] [G loss: 0.145256] [ema: 0.999735] 
[Epoch 24/47] [Batch 400/1077] [D loss: 0.559049] [G loss: 0.135256] [ema: 0.999736] 
[Epoch 24/47] [Batch 500/1077] [D loss: 0.487438] [G loss: 0.150108] [ema: 0.999737] 
[Epoch 24/47] [Batch 600/1077] [D loss: 0.510010] [G loss: 0.135081] [ema: 0.999738] 
[Epoch 24/47] [Batch 700/1077] [D loss: 0.470317] [G loss: 0.127283] [ema: 0.999739] 
[Epoch 24/47] [Batch 800/1077] [D loss: 0.484413] [G loss: 0.141640] [ema: 0.999740] 
[Epoch 24/47] [Batch 900/1077] [D loss: 0.461139] [G loss: 0.130370] [ema: 0.999741] 
[Epoch 24/47] [Batch 1000/1077] [D loss: 0.550183] [G loss: 0.139310] [ema: 0.999742] 
[Epoch 25/47] [Batch 0/1077] [D loss: 0.430115] [G loss: 0.142119] [ema: 0.999743] 
[Epoch 25/47] [Batch 100/1077] [D loss: 0.508534] [G loss: 0.127513] [ema: 0.999744] 
[Epoch 25/47] [Batch 200/1077] [D loss: 0.466138] [G loss: 0.134668] [ema: 0.999744] 
[Epoch 25/47] [Batch 300/1077] [D loss: 0.493768] [G loss: 0.139624] [ema: 0.999745] 
[Epoch 25/47] [Batch 400/1077] [D loss: 0.524683] [G loss: 0.135018] [ema: 0.999746] 
[Epoch 25/47] [Batch 500/1077] [D loss: 0.463705] [G loss: 0.153455] [ema: 0.999747] 
[Epoch 25/47] [Batch 600/1077] [D loss: 0.460612] [G loss: 0.157707] [ema: 0.999748] 
[Epoch 25/47] [Batch 700/1077] [D loss: 0.517346] [G loss: 0.134431] [ema: 0.999749] 
[Epoch 25/47] [Batch 800/1077] [D loss: 0.482012] [G loss: 0.130899] [ema: 0.999750] 
[Epoch 25/47] [Batch 900/1077] [D loss: 0.453897] [G loss: 0.141701] [ema: 0.999751] 
[Epoch 25/47] [Batch 1000/1077] [D loss: 0.481467] [G loss: 0.135959] [ema: 0.999752] 
[Epoch 26/47] [Batch 0/1077] [D loss: 0.497191] [G loss: 0.158188] [ema: 0.999752] 
[Epoch 26/47] [Batch 100/1077] [D loss: 0.442407] [G loss: 0.131701] [ema: 0.999753] 
[Epoch 26/47] [Batch 200/1077] [D loss: 0.484277] [G loss: 0.128913] [ema: 0.999754] 
[Epoch 26/47] [Batch 300/1077] [D loss: 0.434190] [G loss: 0.146973] [ema: 0.999755] 
[Epoch 26/47] [Batch 400/1077] [D loss: 0.515756] [G loss: 0.144230] [ema: 0.999756] 
[Epoch 26/47] [Batch 500/1077] [D loss: 0.501070] [G loss: 0.134214] [ema: 0.999757] 
[Epoch 26/47] [Batch 600/1077] [D loss: 0.461861] [G loss: 0.133490] [ema: 0.999758] 
[Epoch 26/47] [Batch 700/1077] [D loss: 0.522642] [G loss: 0.129035] [ema: 0.999759] 
[Epoch 26/47] [Batch 800/1077] [D loss: 0.500946] [G loss: 0.138169] [ema: 0.999759] 
[Epoch 26/47] [Batch 900/1077] [D loss: 0.488144] [G loss: 0.144989] [ema: 0.999760] 
[Epoch 26/47] [Batch 1000/1077] [D loss: 0.502015] [G loss: 0.138689] [ema: 0.999761] 
[Epoch 27/47] [Batch 0/1077] [D loss: 0.494903] [G loss: 0.133967] [ema: 0.999762] 
[Epoch 27/47] [Batch 100/1077] [D loss: 0.493080] [G loss: 0.133271] [ema: 0.999762] 
[Epoch 27/47] [Batch 200/1077] [D loss: 0.522633] [G loss: 0.142784] [ema: 0.999763] 
[Epoch 27/47] [Batch 300/1077] [D loss: 0.506038] [G loss: 0.151386] [ema: 0.999764] 
[Epoch 27/47] [Batch 400/1077] [D loss: 0.467553] [G loss: 0.146991] [ema: 0.999765] 
[Epoch 27/47] [Batch 500/1077] [D loss: 0.457024] [G loss: 0.153164] [ema: 0.999766] 
[Epoch 27/47] [Batch 600/1077] [D loss: 0.481277] [G loss: 0.124743] [ema: 0.999766] 
[Epoch 27/47] [Batch 700/1077] [D loss: 0.440435] [G loss: 0.134872] [ema: 0.999767] 
[Epoch 27/47] [Batch 800/1077] [D loss: 0.480524] [G loss: 0.148282] [ema: 0.999768] 
[Epoch 27/47] [Batch 900/1077] [D loss: 0.551653] [G loss: 0.120854] [ema: 0.999769] 
[Epoch 27/47] [Batch 1000/1077] [D loss: 0.470561] [G loss: 0.153083] [ema: 0.999770] 
[Epoch 28/47] [Batch 0/1077] [D loss: 0.478658] [G loss: 0.154837] [ema: 0.999770] 
[Epoch 28/47] [Batch 100/1077] [D loss: 0.505143] [G loss: 0.144987] [ema: 0.999771] 
[Epoch 28/47] [Batch 200/1077] [D loss: 0.442545] [G loss: 0.130275] [ema: 0.999772] 
[Epoch 28/47] [Batch 300/1077] [D loss: 0.475981] [G loss: 0.138528] [ema: 0.999772] 
[Epoch 28/47] [Batch 400/1077] [D loss: 0.488956] [G loss: 0.148346] [ema: 0.999773] 
[Epoch 28/47] [Batch 500/1077] [D loss: 0.463063] [G loss: 0.149198] [ema: 0.999774] 
[Epoch 28/47] [Batch 600/1077] [D loss: 0.481278] [G loss: 0.146443] [ema: 0.999775] 
[Epoch 28/47] [Batch 700/1077] [D loss: 0.503746] [G loss: 0.132440] [ema: 0.999775] 
[Epoch 28/47] [Batch 800/1077] [D loss: 0.494296] [G loss: 0.129027] [ema: 0.999776] 
[Epoch 28/47] [Batch 900/1077] [D loss: 0.476499] [G loss: 0.128204] [ema: 0.999777] 
[Epoch 28/47] [Batch 1000/1077] [D loss: 0.458229] [G loss: 0.143353] [ema: 0.999778] 
[Epoch 29/47] [Batch 0/1077] [D loss: 0.493857] [G loss: 0.151000] [ema: 0.999778] 
[Epoch 29/47] [Batch 100/1077] [D loss: 0.444858] [G loss: 0.138798] [ema: 0.999779] 
[Epoch 29/47] [Batch 200/1077] [D loss: 0.528996] [G loss: 0.136477] [ema: 0.999780] 
[Epoch 29/47] [Batch 300/1077] [D loss: 0.458882] [G loss: 0.139349] [ema: 0.999780] 
[Epoch 29/47] [Batch 400/1077] [D loss: 0.445633] [G loss: 0.154916] [ema: 0.999781] 
[Epoch 29/47] [Batch 500/1077] [D loss: 0.471576] [G loss: 0.141454] [ema: 0.999782] 
[Epoch 29/47] [Batch 600/1077] [D loss: 0.520102] [G loss: 0.127858] [ema: 0.999782] 
[Epoch 29/47] [Batch 700/1077] [D loss: 0.475499] [G loss: 0.127103] [ema: 0.999783] 
[Epoch 29/47] [Batch 800/1077] [D loss: 0.496172] [G loss: 0.144337] [ema: 0.999784] 
[Epoch 29/47] [Batch 900/1077] [D loss: 0.475667] [G loss: 0.128529] [ema: 0.999784] 
[Epoch 29/47] [Batch 1000/1077] [D loss: 0.499989] [G loss: 0.135201] [ema: 0.999785] 
[Epoch 30/47] [Batch 0/1077] [D loss: 0.488618] [G loss: 0.140085] [ema: 0.999785] 
[Epoch 30/47] [Batch 100/1077] [D loss: 0.455615] [G loss: 0.143463] [ema: 0.999786] 
[Epoch 30/47] [Batch 200/1077] [D loss: 0.499512] [G loss: 0.148861] [ema: 0.999787] 
[Epoch 30/47] [Batch 300/1077] [D loss: 0.466361] [G loss: 0.157895] [ema: 0.999787] 
[Epoch 30/47] [Batch 400/1077] [D loss: 0.475205] [G loss: 0.139342] [ema: 0.999788] 
[Epoch 30/47] [Batch 500/1077] [D loss: 0.521022] [G loss: 0.141995] [ema: 0.999789] 
[Epoch 30/47] [Batch 600/1077] [D loss: 0.435622] [G loss: 0.166277] [ema: 0.999789] 
[Epoch 30/47] [Batch 700/1077] [D loss: 0.478939] [G loss: 0.163849] [ema: 0.999790] 
[Epoch 30/47] [Batch 800/1077] [D loss: 0.519175] [G loss: 0.152125] [ema: 0.999791] 
[Epoch 30/47] [Batch 900/1077] [D loss: 0.469656] [G loss: 0.125600] [ema: 0.999791] 
[Epoch 30/47] [Batch 1000/1077] [D loss: 0.478751] [G loss: 0.160644] [ema: 0.999792] 
[Epoch 31/47] [Batch 0/1077] [D loss: 0.434415] [G loss: 0.151196] [ema: 0.999792] 
[Epoch 31/47] [Batch 100/1077] [D loss: 0.502209] [G loss: 0.140424] [ema: 0.999793] 
[Epoch 31/47] [Batch 200/1077] [D loss: 0.533240] [G loss: 0.154056] [ema: 0.999794] 
[Epoch 31/47] [Batch 300/1077] [D loss: 0.487131] [G loss: 0.149930] [ema: 0.999794] 
[Epoch 31/47] [Batch 400/1077] [D loss: 0.455500] [G loss: 0.137905] [ema: 0.999795] 
[Epoch 31/47] [Batch 500/1077] [D loss: 0.436491] [G loss: 0.165260] [ema: 0.999795] 
[Epoch 31/47] [Batch 600/1077] [D loss: 0.454248] [G loss: 0.123075] [ema: 0.999796] 
[Epoch 31/47] [Batch 700/1077] [D loss: 0.503440] [G loss: 0.148540] [ema: 0.999797] 
[Epoch 31/47] [Batch 800/1077] [D loss: 0.449726] [G loss: 0.148312] [ema: 0.999797] 
[Epoch 31/47] [Batch 900/1077] [D loss: 0.493234] [G loss: 0.121839] [ema: 0.999798] 
[Epoch 31/47] [Batch 1000/1077] [D loss: 0.435976] [G loss: 0.147593] [ema: 0.999798] 
[Epoch 32/47] [Batch 0/1077] [D loss: 0.506910] [G loss: 0.133642] [ema: 0.999799] 
[Epoch 32/47] [Batch 100/1077] [D loss: 0.544425] [G loss: 0.146584] [ema: 0.999799] 
[Epoch 32/47] [Batch 200/1077] [D loss: 0.507714] [G loss: 0.127276] [ema: 0.999800] 
[Epoch 32/47] [Batch 300/1077] [D loss: 0.493499] [G loss: 0.142065] [ema: 0.999801] 
[Epoch 32/47] [Batch 400/1077] [D loss: 0.535421] [G loss: 0.126122] [ema: 0.999801] 
[Epoch 32/47] [Batch 500/1077] [D loss: 0.516625] [G loss: 0.130916] [ema: 0.999802] 
[Epoch 32/47] [Batch 600/1077] [D loss: 0.541486] [G loss: 0.128111] [ema: 0.999802] 
[Epoch 32/47] [Batch 700/1077] [D loss: 0.503437] [G loss: 0.134441] [ema: 0.999803] 
[Epoch 32/47] [Batch 800/1077] [D loss: 0.495989] [G loss: 0.138086] [ema: 0.999803] 
[Epoch 32/47] [Batch 900/1077] [D loss: 0.439630] [G loss: 0.168180] [ema: 0.999804] 
[Epoch 32/47] [Batch 1000/1077] [D loss: 0.518552] [G loss: 0.134188] [ema: 0.999805] 
[Epoch 33/47] [Batch 0/1077] [D loss: 0.505118] [G loss: 0.136093] [ema: 0.999805] 
[Epoch 33/47] [Batch 100/1077] [D loss: 0.492492] [G loss: 0.131730] [ema: 0.999806] 
[Epoch 33/47] [Batch 200/1077] [D loss: 0.506192] [G loss: 0.126627] [ema: 0.999806] 
[Epoch 33/47] [Batch 300/1077] [D loss: 0.476871] [G loss: 0.141184] [ema: 0.999807] 
[Epoch 33/47] [Batch 400/1077] [D loss: 0.487184] [G loss: 0.116214] [ema: 0.999807] 
[Epoch 33/47] [Batch 500/1077] [D loss: 0.436458] [G loss: 0.160817] [ema: 0.999808] 
[Epoch 33/47] [Batch 600/1077] [D loss: 0.522183] [G loss: 0.123495] [ema: 0.999808] 
[Epoch 33/47] [Batch 700/1077] [D loss: 0.429634] [G loss: 0.154520] [ema: 0.999809] 
[Epoch 33/47] [Batch 800/1077] [D loss: 0.477986] [G loss: 0.137775] [ema: 0.999809] 
[Epoch 33/47] [Batch 900/1077] [D loss: 0.473309] [G loss: 0.133037] [ema: 0.999810] 
[Epoch 33/47] [Batch 1000/1077] [D loss: 0.477801] [G loss: 0.129817] [ema: 0.999810] 
[Epoch 34/47] [Batch 0/1077] [D loss: 0.519824] [G loss: 0.139257] [ema: 0.999811] 
[Epoch 34/47] [Batch 100/1077] [D loss: 0.465719] [G loss: 0.134567] [ema: 0.999811] 
[Epoch 34/47] [Batch 200/1077] [D loss: 0.434731] [G loss: 0.174073] [ema: 0.999812] 
[Epoch 34/47] [Batch 300/1077] [D loss: 0.474374] [G loss: 0.158935] [ema: 0.999812] 
[Epoch 34/47] [Batch 400/1077] [D loss: 0.503419] [G loss: 0.153248] [ema: 0.999813] 
[Epoch 34/47] [Batch 500/1077] [D loss: 0.485128] [G loss: 0.142382] [ema: 0.999813] 
[Epoch 34/47] [Batch 600/1077] [D loss: 0.493595] [G loss: 0.137619] [ema: 0.999814] 
[Epoch 34/47] [Batch 700/1077] [D loss: 0.515138] [G loss: 0.154071] [ema: 0.999814] 
[Epoch 34/47] [Batch 800/1077] [D loss: 0.467903] [G loss: 0.137818] [ema: 0.999815] 
[Epoch 34/47] [Batch 900/1077] [D loss: 0.515387] [G loss: 0.131184] [ema: 0.999815] 
[Epoch 34/47] [Batch 1000/1077] [D loss: 0.456451] [G loss: 0.144758] [ema: 0.999816] 
[Epoch 35/47] [Batch 0/1077] [D loss: 0.516001] [G loss: 0.148370] [ema: 0.999816] 
[Epoch 35/47] [Batch 100/1077] [D loss: 0.438525] [G loss: 0.154909] [ema: 0.999817] 
[Epoch 35/47] [Batch 200/1077] [D loss: 0.469959] [G loss: 0.137728] [ema: 0.999817] 
[Epoch 35/47] [Batch 300/1077] [D loss: 0.480905] [G loss: 0.114471] [ema: 0.999818] 
[Epoch 35/47] [Batch 400/1077] [D loss: 0.533762] [G loss: 0.093810] [ema: 0.999818] 
[Epoch 35/47] [Batch 500/1077] [D loss: 0.521379] [G loss: 0.141687] [ema: 0.999819] 
[Epoch 35/47] [Batch 600/1077] [D loss: 0.534541] [G loss: 0.138872] [ema: 0.999819] 
[Epoch 35/47] [Batch 700/1077] [D loss: 0.491637] [G loss: 0.130709] [ema: 0.999819] 
[Epoch 35/47] [Batch 800/1077] [D loss: 0.443596] [G loss: 0.137232] [ema: 0.999820] 
[Epoch 35/47] [Batch 900/1077] [D loss: 0.489587] [G loss: 0.125796] [ema: 0.999820] 
[Epoch 35/47] [Batch 1000/1077] [D loss: 0.477816] [G loss: 0.134653] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_50000_30_100/stand_50000_D_30_2024_10_18_03_47_52/Model



[Epoch 36/47] [Batch 0/1077] [D loss: 0.601494] [G loss: 0.122879] [ema: 0.999821] 
[Epoch 36/47] [Batch 100/1077] [D loss: 0.527789] [G loss: 0.144015] [ema: 0.999822] 
[Epoch 36/47] [Batch 200/1077] [D loss: 0.516944] [G loss: 0.117518] [ema: 0.999822] 
[Epoch 36/47] [Batch 300/1077] [D loss: 0.512503] [G loss: 0.137123] [ema: 0.999823] 
[Epoch 36/47] [Batch 400/1077] [D loss: 0.514914] [G loss: 0.123125] [ema: 0.999823] 
[Epoch 36/47] [Batch 500/1077] [D loss: 0.557508] [G loss: 0.131323] [ema: 0.999824] 
[Epoch 36/47] [Batch 600/1077] [D loss: 0.539182] [G loss: 0.123570] [ema: 0.999824] 
[Epoch 36/47] [Batch 700/1077] [D loss: 0.508454] [G loss: 0.137645] [ema: 0.999824] 
[Epoch 36/47] [Batch 800/1077] [D loss: 0.533935] [G loss: 0.138581] [ema: 0.999825] 
[Epoch 36/47] [Batch 900/1077] [D loss: 0.532235] [G loss: 0.132653] [ema: 0.999825] 
[Epoch 36/47] [Batch 1000/1077] [D loss: 0.493686] [G loss: 0.150425] [ema: 0.999826] 
[Epoch 37/47] [Batch 0/1077] [D loss: 0.498506] [G loss: 0.134209] [ema: 0.999826] 
[Epoch 37/47] [Batch 100/1077] [D loss: 0.464726] [G loss: 0.155549] [ema: 0.999827] 
[Epoch 37/47] [Batch 200/1077] [D loss: 0.468760] [G loss: 0.126848] [ema: 0.999827] 
[Epoch 37/47] [Batch 300/1077] [D loss: 0.531669] [G loss: 0.130431] [ema: 0.999827] 
[Epoch 37/47] [Batch 400/1077] [D loss: 0.469248] [G loss: 0.150233] [ema: 0.999828] 
[Epoch 37/47] [Batch 500/1077] [D loss: 0.501172] [G loss: 0.136669] [ema: 0.999828] 
[Epoch 37/47] [Batch 600/1077] [D loss: 0.479760] [G loss: 0.138630] [ema: 0.999829] 
[Epoch 37/47] [Batch 700/1077] [D loss: 0.504703] [G loss: 0.149174] [ema: 0.999829] 
[Epoch 37/47] [Batch 800/1077] [D loss: 0.416459] [G loss: 0.156029] [ema: 0.999829] 
[Epoch 37/47] [Batch 900/1077] [D loss: 0.462918] [G loss: 0.144160] [ema: 0.999830] 
[Epoch 37/47] [Batch 1000/1077] [D loss: 0.585641] [G loss: 0.142343] [ema: 0.999830] 
[Epoch 38/47] [Batch 0/1077] [D loss: 0.460970] [G loss: 0.137242] [ema: 0.999831] 
[Epoch 38/47] [Batch 100/1077] [D loss: 0.513104] [G loss: 0.129329] [ema: 0.999831] 
[Epoch 38/47] [Batch 200/1077] [D loss: 0.558078] [G loss: 0.139553] [ema: 0.999831] 
[Epoch 38/47] [Batch 300/1077] [D loss: 0.504929] [G loss: 0.154203] [ema: 0.999832] 
[Epoch 38/47] [Batch 400/1077] [D loss: 0.465508] [G loss: 0.143422] [ema: 0.999832] 
[Epoch 38/47] [Batch 500/1077] [D loss: 0.469351] [G loss: 0.146160] [ema: 0.999833] 
[Epoch 38/47] [Batch 600/1077] [D loss: 0.528761] [G loss: 0.117464] [ema: 0.999833] 
[Epoch 38/47] [Batch 700/1077] [D loss: 0.510768] [G loss: 0.135688] [ema: 0.999833] 
[Epoch 38/47] [Batch 800/1077] [D loss: 0.575463] [G loss: 0.135155] [ema: 0.999834] 
[Epoch 38/47] [Batch 900/1077] [D loss: 0.509181] [G loss: 0.136804] [ema: 0.999834] 
[Epoch 38/47] [Batch 1000/1077] [D loss: 0.506591] [G loss: 0.144945] [ema: 0.999835] 
[Epoch 39/47] [Batch 0/1077] [D loss: 0.535629] [G loss: 0.143322] [ema: 0.999835] 
[Epoch 39/47] [Batch 100/1077] [D loss: 0.497177] [G loss: 0.143928] [ema: 0.999835] 
[Epoch 39/47] [Batch 200/1077] [D loss: 0.513983] [G loss: 0.130654] [ema: 0.999836] 
[Epoch 39/47] [Batch 300/1077] [D loss: 0.544055] [G loss: 0.139782] [ema: 0.999836] 
[Epoch 39/47] [Batch 400/1077] [D loss: 0.495649] [G loss: 0.139832] [ema: 0.999837] 
[Epoch 39/47] [Batch 500/1077] [D loss: 0.478158] [G loss: 0.127925] [ema: 0.999837] 
[Epoch 39/47] [Batch 600/1077] [D loss: 0.498181] [G loss: 0.131820] [ema: 0.999837] 
[Epoch 39/47] [Batch 700/1077] [D loss: 0.503915] [G loss: 0.133896] [ema: 0.999838] 
[Epoch 39/47] [Batch 800/1077] [D loss: 0.496137] [G loss: 0.119725] [ema: 0.999838] 
[Epoch 39/47] [Batch 900/1077] [D loss: 0.506410] [G loss: 0.128373] [ema: 0.999838] 
[Epoch 39/47] [Batch 1000/1077] [D loss: 0.496161] [G loss: 0.115408] [ema: 0.999839] 
[Epoch 40/47] [Batch 0/1077] [D loss: 0.538040] [G loss: 0.124353] [ema: 0.999839] 
[Epoch 40/47] [Batch 100/1077] [D loss: 0.510538] [G loss: 0.124253] [ema: 0.999839] 
[Epoch 40/47] [Batch 200/1077] [D loss: 0.519874] [G loss: 0.134958] [ema: 0.999840] 
[Epoch 40/47] [Batch 300/1077] [D loss: 0.487596] [G loss: 0.124132] [ema: 0.999840] 
[Epoch 40/47] [Batch 400/1077] [D loss: 0.467238] [G loss: 0.153574] [ema: 0.999841] 
[Epoch 40/47] [Batch 500/1077] [D loss: 0.510655] [G loss: 0.129258] [ema: 0.999841] 
[Epoch 40/47] [Batch 600/1077] [D loss: 0.482793] [G loss: 0.127789] [ema: 0.999841] 
[Epoch 40/47] [Batch 700/1077] [D loss: 0.531869] [G loss: 0.133944] [ema: 0.999842] 
[Epoch 40/47] [Batch 800/1077] [D loss: 0.503804] [G loss: 0.128949] [ema: 0.999842] 
[Epoch 40/47] [Batch 900/1077] [D loss: 0.510164] [G loss: 0.126943] [ema: 0.999842] 
[Epoch 40/47] [Batch 1000/1077] [D loss: 0.525618] [G loss: 0.137289] [ema: 0.999843] 
[Epoch 41/47] [Batch 0/1077] [D loss: 0.515069] [G loss: 0.130447] [ema: 0.999843] 
[Epoch 41/47] [Batch 100/1077] [D loss: 0.533759] [G loss: 0.146972] [ema: 0.999843] 
[Epoch 41/47] [Batch 200/1077] [D loss: 0.499773] [G loss: 0.120460] [ema: 0.999844] 
[Epoch 41/47] [Batch 300/1077] [D loss: 0.488514] [G loss: 0.134464] [ema: 0.999844] 
[Epoch 41/47] [Batch 400/1077] [D loss: 0.525565] [G loss: 0.112199] [ema: 0.999844] 
[Epoch 41/47] [Batch 500/1077] [D loss: 0.547212] [G loss: 0.141592] [ema: 0.999845] 
[Epoch 41/47] [Batch 600/1077] [D loss: 0.475966] [G loss: 0.148323] [ema: 0.999845] 
[Epoch 41/47] [Batch 700/1077] [D loss: 0.481767] [G loss: 0.132282] [ema: 0.999845] 
[Epoch 41/47] [Batch 800/1077] [D loss: 0.524916] [G loss: 0.112987] [ema: 0.999846] 
[Epoch 41/47] [Batch 900/1077] [D loss: 0.529795] [G loss: 0.120225] [ema: 0.999846] 
[Epoch 41/47] [Batch 1000/1077] [D loss: 0.538330] [G loss: 0.117854] [ema: 0.999847] 
[Epoch 42/47] [Batch 0/1077] [D loss: 0.527594] [G loss: 0.117554] [ema: 0.999847] 
[Epoch 42/47] [Batch 100/1077] [D loss: 0.535911] [G loss: 0.130557] [ema: 0.999847] 
[Epoch 42/47] [Batch 200/1077] [D loss: 0.592010] [G loss: 0.122247] [ema: 0.999847] 
[Epoch 42/47] [Batch 300/1077] [D loss: 0.531652] [G loss: 0.122593] [ema: 0.999848] 
[Epoch 42/47] [Batch 400/1077] [D loss: 0.532381] [G loss: 0.133519] [ema: 0.999848] 
[Epoch 42/47] [Batch 500/1077] [D loss: 0.497340] [G loss: 0.130518] [ema: 0.999848] 
[Epoch 42/47] [Batch 600/1077] [D loss: 0.512290] [G loss: 0.116133] [ema: 0.999849] 
[Epoch 42/47] [Batch 700/1077] [D loss: 0.548395] [G loss: 0.142937] [ema: 0.999849] 
[Epoch 42/47] [Batch 800/1077] [D loss: 0.577018] [G loss: 0.117740] [ema: 0.999849] 
[Epoch 42/47] [Batch 900/1077] [D loss: 0.553093] [G loss: 0.113932] [ema: 0.999850] 
[Epoch 42/47] [Batch 1000/1077] [D loss: 0.543861] [G loss: 0.134399] [ema: 0.999850] 
[Epoch 43/47] [Batch 0/1077] [D loss: 0.552604] [G loss: 0.117513] [ema: 0.999850] 
[Epoch 43/47] [Batch 100/1077] [D loss: 0.484363] [G loss: 0.129029] [ema: 0.999851] 
[Epoch 43/47] [Batch 200/1077] [D loss: 0.495750] [G loss: 0.126415] [ema: 0.999851] 
[Epoch 43/47] [Batch 300/1077] [D loss: 0.592188] [G loss: 0.147220] [ema: 0.999851] 
[Epoch 43/47] [Batch 400/1077] [D loss: 0.491360] [G loss: 0.117864] [ema: 0.999852] 
[Epoch 43/47] [Batch 500/1077] [D loss: 0.492046] [G loss: 0.132239] [ema: 0.999852] 
[Epoch 43/47] [Batch 600/1077] [D loss: 0.539244] [G loss: 0.122454] [ema: 0.999852] 
[Epoch 43/47] [Batch 700/1077] [D loss: 0.528911] [G loss: 0.108054] [ema: 0.999853] 
[Epoch 43/47] [Batch 800/1077] [D loss: 0.533804] [G loss: 0.118532] [ema: 0.999853] 
[Epoch 43/47] [Batch 900/1077] [D loss: 0.536543] [G loss: 0.125079] [ema: 0.999853] 
[Epoch 43/47] [Batch 1000/1077] [D loss: 0.553086] [G loss: 0.125405] [ema: 0.999854] 
[Epoch 44/47] [Batch 0/1077] [D loss: 0.529959] [G loss: 0.116816] [ema: 0.999854] 
[Epoch 44/47] [Batch 100/1077] [D loss: 0.523623] [G loss: 0.131636] [ema: 0.999854] 
[Epoch 44/47] [Batch 200/1077] [D loss: 0.554727] [G loss: 0.114761] [ema: 0.999854] 
[Epoch 44/47] [Batch 300/1077] [D loss: 0.539816] [G loss: 0.119036] [ema: 0.999855] 
[Epoch 44/47] [Batch 400/1077] [D loss: 0.545200] [G loss: 0.113148] [ema: 0.999855] 
[Epoch 44/47] [Batch 500/1077] [D loss: 0.544731] [G loss: 0.121502] [ema: 0.999855] 
[Epoch 44/47] [Batch 600/1077] [D loss: 0.546717] [G loss: 0.113437] [ema: 0.999856] 
[Epoch 44/47] [Batch 700/1077] [D loss: 0.539151] [G loss: 0.118380] [ema: 0.999856] 
[Epoch 44/47] [Batch 800/1077] [D loss: 0.510022] [G loss: 0.124685] [ema: 0.999856] 
[Epoch 44/47] [Batch 900/1077] [D loss: 0.536349] [G loss: 0.123820] [ema: 0.999856] 
[Epoch 44/47] [Batch 1000/1077] [D loss: 0.525716] [G loss: 0.125039] [ema: 0.999857] 
[Epoch 45/47] [Batch 0/1077] [D loss: 0.544017] [G loss: 0.143033] [ema: 0.999857] 
[Epoch 45/47] [Batch 100/1077] [D loss: 0.509781] [G loss: 0.141665] [ema: 0.999857] 
[Epoch 45/47] [Batch 200/1077] [D loss: 0.475835] [G loss: 0.129091] [ema: 0.999858] 
[Epoch 45/47] [Batch 300/1077] [D loss: 0.538292] [G loss: 0.108962] [ema: 0.999858] 
[Epoch 45/47] [Batch 400/1077] [D loss: 0.518946] [G loss: 0.119605] [ema: 0.999858] 
[Epoch 45/47] [Batch 500/1077] [D loss: 0.474126] [G loss: 0.142670] [ema: 0.999858] 
[Epoch 45/47] [Batch 600/1077] [D loss: 0.571937] [G loss: 0.111920] [ema: 0.999859] 
[Epoch 45/47] [Batch 700/1077] [D loss: 0.472954] [G loss: 0.143135] [ema: 0.999859] 
[Epoch 45/47] [Batch 800/1077] [D loss: 0.533960] [G loss: 0.117928] [ema: 0.999859] 
[Epoch 45/47] [Batch 900/1077] [D loss: 0.515896] [G loss: 0.124621] [ema: 0.999860] 
[Epoch 45/47] [Batch 1000/1077] [D loss: 0.535184] [G loss: 0.117152] [ema: 0.999860] 
[Epoch 46/47] [Batch 0/1077] [D loss: 0.527927] [G loss: 0.117769] [ema: 0.999860] 
[Epoch 46/47] [Batch 100/1077] [D loss: 0.503913] [G loss: 0.131178] [ema: 0.999860] 
[Epoch 46/47] [Batch 200/1077] [D loss: 0.540953] [G loss: 0.125148] [ema: 0.999861] 
[Epoch 46/47] [Batch 300/1077] [D loss: 0.512597] [G loss: 0.112282] [ema: 0.999861] 
[Epoch 46/47] [Batch 400/1077] [D loss: 0.485972] [G loss: 0.122281] [ema: 0.999861] 
[Epoch 46/47] [Batch 500/1077] [D loss: 0.526845] [G loss: 0.113170] [ema: 0.999861] 
[Epoch 46/47] [Batch 600/1077] [D loss: 0.554345] [G loss: 0.113581] [ema: 0.999862] 
[Epoch 46/47] [Batch 700/1077] [D loss: 0.526672] [G loss: 0.128128] [ema: 0.999862] 
[Epoch 46/47] [Batch 800/1077] [D loss: 0.512059] [G loss: 0.127689] [ema: 0.999862] 
[Epoch 46/47] [Batch 900/1077] [D loss: 0.512657] [G loss: 0.119563] [ema: 0.999863] 
[Epoch 46/47] [Batch 1000/1077] [D loss: 0.500358] [G loss: 0.126724] [ema: 0.999863] 
