Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
downstairs
return single class data and labels, class is downstairs
data shape is (12639, 3, 1, 30)
label shape is (12639,)
790
Epochs between ckechpoint: 1




Saving checkpoint 1 in logs/Downstairs_5000_D_30_2024_10_15_12_44_09/Model




[Epoch 0/7] [Batch 0/790] [D loss: 1.213166] [G loss: 0.883360] [ema: 0.000000] 
[Epoch 0/7] [Batch 100/790] [D loss: 0.469353] [G loss: 0.198142] [ema: 0.933033] 
[Epoch 0/7] [Batch 200/790] [D loss: 0.397805] [G loss: 0.188986] [ema: 0.965936] 
[Epoch 0/7] [Batch 300/790] [D loss: 0.272691] [G loss: 0.271859] [ema: 0.977160] 
[Epoch 0/7] [Batch 400/790] [D loss: 0.287708] [G loss: 0.198227] [ema: 0.982821] 
[Epoch 0/7] [Batch 500/790] [D loss: 0.346768] [G loss: 0.171502] [ema: 0.986233] 
[Epoch 0/7] [Batch 600/790] [D loss: 0.424298] [G loss: 0.169059] [ema: 0.988514] 
[Epoch 0/7] [Batch 700/790] [D loss: 0.439032] [G loss: 0.179751] [ema: 0.990147] 




Saving checkpoint 2 in logs/Downstairs_5000_D_30_2024_10_15_12_44_09/Model




[Epoch 1/7] [Batch 0/790] [D loss: 0.460675] [G loss: 0.193481] [ema: 0.991264] 
[Epoch 1/7] [Batch 100/790] [D loss: 0.350185] [G loss: 0.168092] [ema: 0.992242] 
[Epoch 1/7] [Batch 200/790] [D loss: 0.334206] [G loss: 0.195954] [ema: 0.993023] 
[Epoch 1/7] [Batch 300/790] [D loss: 0.356226] [G loss: 0.141512] [ema: 0.993661] 
[Epoch 1/7] [Batch 400/790] [D loss: 0.390108] [G loss: 0.207171] [ema: 0.994192] 
[Epoch 1/7] [Batch 500/790] [D loss: 0.372965] [G loss: 0.197591] [ema: 0.994641] 
[Epoch 1/7] [Batch 600/790] [D loss: 0.459160] [G loss: 0.170054] [ema: 0.995026] 
[Epoch 1/7] [Batch 700/790] [D loss: 0.492950] [G loss: 0.143688] [ema: 0.995359] 




Saving checkpoint 3 in logs/Downstairs_5000_D_30_2024_10_15_12_44_09/Model




[Epoch 2/7] [Batch 0/790] [D loss: 0.565164] [G loss: 0.142363] [ema: 0.995623] 
[Epoch 2/7] [Batch 100/790] [D loss: 0.448459] [G loss: 0.151568] [ema: 0.995883] 
[Epoch 2/7] [Batch 200/790] [D loss: 0.502031] [G loss: 0.169203] [ema: 0.996113] 
[Epoch 2/7] [Batch 300/790] [D loss: 0.528507] [G loss: 0.166953] [ema: 0.996320] 
[Epoch 2/7] [Batch 400/790] [D loss: 0.492805] [G loss: 0.181440] [ema: 0.996505] 
[Epoch 2/7] [Batch 500/790] [D loss: 0.473384] [G loss: 0.162559] [ema: 0.996673] 
[Epoch 2/7] [Batch 600/790] [D loss: 0.377097] [G loss: 0.184461] [ema: 0.996825] 
[Epoch 2/7] [Batch 700/790] [D loss: 0.423039] [G loss: 0.204439] [ema: 0.996964] 




Saving checkpoint 4 in logs/Downstairs_5000_D_30_2024_10_15_12_44_09/Model




[Epoch 3/7] [Batch 0/790] [D loss: 0.465681] [G loss: 0.182214] [ema: 0.997080] 
[Epoch 3/7] [Batch 100/790] [D loss: 0.393599] [G loss: 0.142584] [ema: 0.997198] 
[Epoch 3/7] [Batch 200/790] [D loss: 0.347775] [G loss: 0.160038] [ema: 0.997307] 
[Epoch 3/7] [Batch 300/790] [D loss: 0.398827] [G loss: 0.208795] [ema: 0.997407] 
[Epoch 3/7] [Batch 400/790] [D loss: 0.411164] [G loss: 0.192632] [ema: 0.997501] 
[Epoch 3/7] [Batch 500/790] [D loss: 0.499294] [G loss: 0.120965] [ema: 0.997588] 
[Epoch 3/7] [Batch 600/790] [D loss: 0.512675] [G loss: 0.097070] [ema: 0.997669] 
[Epoch 3/7] [Batch 700/790] [D loss: 0.533163] [G loss: 0.149730] [ema: 0.997745] 




Saving checkpoint 5 in logs/Downstairs_5000_D_30_2024_10_15_12_44_09/Model




[Epoch 4/7] [Batch 0/790] [D loss: 0.466693] [G loss: 0.171283] [ema: 0.997809] 
[Epoch 4/7] [Batch 100/790] [D loss: 0.414903] [G loss: 0.209106] [ema: 0.997876] 
[Epoch 4/7] [Batch 200/790] [D loss: 0.434215] [G loss: 0.170666] [ema: 0.997939] 
[Epoch 4/7] [Batch 300/790] [D loss: 0.462116] [G loss: 0.135140] [ema: 0.997999] 
[Epoch 4/7] [Batch 400/790] [D loss: 0.452724] [G loss: 0.144477] [ema: 0.998055] 
[Epoch 4/7] [Batch 500/790] [D loss: 0.479473] [G loss: 0.137199] [ema: 0.998108] 
[Epoch 4/7] [Batch 600/790] [D loss: 0.490418] [G loss: 0.189134] [ema: 0.998158] 
[Epoch 4/7] [Batch 700/790] [D loss: 0.483321] [G loss: 0.120548] [ema: 0.998206] 




Saving checkpoint 6 in logs/Downstairs_5000_D_30_2024_10_15_12_44_09/Model




[Epoch 5/7] [Batch 0/790] [D loss: 0.535920] [G loss: 0.138306] [ema: 0.998247] 
[Epoch 5/7] [Batch 100/790] [D loss: 0.521814] [G loss: 0.118720] [ema: 0.998290] 
[Epoch 5/7] [Batch 200/790] [D loss: 0.552234] [G loss: 0.109416] [ema: 0.998331] 
[Epoch 5/7] [Batch 300/790] [D loss: 0.545756] [G loss: 0.115902] [ema: 0.998370] 
[Epoch 5/7] [Batch 400/790] [D loss: 0.522613] [G loss: 0.112597] [ema: 0.998408] 
[Epoch 5/7] [Batch 500/790] [D loss: 0.541119] [G loss: 0.119951] [ema: 0.998444] 
[Epoch 5/7] [Batch 600/790] [D loss: 0.533223] [G loss: 0.117081] [ema: 0.998478] 
[Epoch 5/7] [Batch 700/790] [D loss: 0.565617] [G loss: 0.125878] [ema: 0.998510] 




Saving checkpoint 7 in logs/Downstairs_5000_D_30_2024_10_15_12_44_09/Model




[Epoch 6/7] [Batch 0/790] [D loss: 0.533479] [G loss: 0.140779] [ema: 0.998539] 
[Epoch 6/7] [Batch 100/790] [D loss: 0.525479] [G loss: 0.112942] [ema: 0.998569] 
[Epoch 6/7] [Batch 200/790] [D loss: 0.494765] [G loss: 0.115720] [ema: 0.998598] 
[Epoch 6/7] [Batch 300/790] [D loss: 0.546587] [G loss: 0.129397] [ema: 0.998626] 
[Epoch 6/7] [Batch 400/790] [D loss: 0.566103] [G loss: 0.117415] [ema: 0.998652] 
[Epoch 6/7] [Batch 500/790] [D loss: 0.581386] [G loss: 0.128300] [ema: 0.998678] 
[Epoch 6/7] [Batch 600/790] [D loss: 0.522493] [G loss: 0.138763] [ema: 0.998703] 
[Epoch 6/7] [Batch 700/790] [D loss: 0.522262] [G loss: 0.131137] [ema: 0.998727] 
