Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
stand
return single class data and labels, class is stand
data shape is (16940, 3, 1, 30)
label shape is (16940,)
1059
Epochs between ckechpoint: 1




Saving checkpoint 1 in logs/Stand_5000_D_30_2024_10_15_23_55_51/Model




[Epoch 0/5] [Batch 0/1059] [D loss: 1.246002] [G loss: 0.893515] [ema: 0.000000] 
[Epoch 0/5] [Batch 100/1059] [D loss: 0.533717] [G loss: 0.119290] [ema: 0.933033] 
[Epoch 0/5] [Batch 200/1059] [D loss: 0.626516] [G loss: 0.107501] [ema: 0.965936] 
[Epoch 0/5] [Batch 300/1059] [D loss: 0.584517] [G loss: 0.108741] [ema: 0.977160] 
[Epoch 0/5] [Batch 400/1059] [D loss: 0.516785] [G loss: 0.126777] [ema: 0.982821] 
[Epoch 0/5] [Batch 500/1059] [D loss: 0.495834] [G loss: 0.140375] [ema: 0.986233] 
[Epoch 0/5] [Batch 600/1059] [D loss: 0.565821] [G loss: 0.122598] [ema: 0.988514] 
[Epoch 0/5] [Batch 700/1059] [D loss: 0.600904] [G loss: 0.123017] [ema: 0.990147] 
[Epoch 0/5] [Batch 800/1059] [D loss: 0.529738] [G loss: 0.129223] [ema: 0.991373] 
[Epoch 0/5] [Batch 900/1059] [D loss: 0.573973] [G loss: 0.111516] [ema: 0.992328] 
[Epoch 0/5] [Batch 1000/1059] [D loss: 0.609570] [G loss: 0.103903] [ema: 0.993092] 




Saving checkpoint 2 in logs/Stand_5000_D_30_2024_10_15_23_55_51/Model




[Epoch 1/5] [Batch 0/1059] [D loss: 0.514967] [G loss: 0.134302] [ema: 0.993476] 
[Epoch 1/5] [Batch 100/1059] [D loss: 0.566092] [G loss: 0.115522] [ema: 0.994037] 
[Epoch 1/5] [Batch 200/1059] [D loss: 0.563896] [G loss: 0.101831] [ema: 0.994510] 
[Epoch 1/5] [Batch 300/1059] [D loss: 0.589270] [G loss: 0.107302] [ema: 0.994913] 
[Epoch 1/5] [Batch 400/1059] [D loss: 0.523987] [G loss: 0.112740] [ema: 0.995260] 
[Epoch 1/5] [Batch 500/1059] [D loss: 0.534170] [G loss: 0.113232] [ema: 0.995564] 
[Epoch 1/5] [Batch 600/1059] [D loss: 0.526487] [G loss: 0.123394] [ema: 0.995831] 
[Epoch 1/5] [Batch 700/1059] [D loss: 0.517567] [G loss: 0.117108] [ema: 0.996067] 
[Epoch 1/5] [Batch 800/1059] [D loss: 0.541225] [G loss: 0.118164] [ema: 0.996278] 
[Epoch 1/5] [Batch 900/1059] [D loss: 0.537372] [G loss: 0.118797] [ema: 0.996468] 
[Epoch 1/5] [Batch 1000/1059] [D loss: 0.526323] [G loss: 0.113107] [ema: 0.996639] 




Saving checkpoint 3 in logs/Stand_5000_D_30_2024_10_15_23_55_51/Model




[Epoch 2/5] [Batch 0/1059] [D loss: 0.577141] [G loss: 0.116070] [ema: 0.996733] 
[Epoch 2/5] [Batch 100/1059] [D loss: 0.539810] [G loss: 0.112936] [ema: 0.996880] 
[Epoch 2/5] [Batch 200/1059] [D loss: 0.552441] [G loss: 0.122553] [ema: 0.997014] 
[Epoch 2/5] [Batch 300/1059] [D loss: 0.527804] [G loss: 0.108049] [ema: 0.997137] 
[Epoch 2/5] [Batch 400/1059] [D loss: 0.558537] [G loss: 0.121171] [ema: 0.997251] 
[Epoch 2/5] [Batch 500/1059] [D loss: 0.577988] [G loss: 0.119487] [ema: 0.997356] 
[Epoch 2/5] [Batch 600/1059] [D loss: 0.570024] [G loss: 0.110221] [ema: 0.997453] 
[Epoch 2/5] [Batch 700/1059] [D loss: 0.565432] [G loss: 0.125874] [ema: 0.997543] 
[Epoch 2/5] [Batch 800/1059] [D loss: 0.540172] [G loss: 0.107689] [ema: 0.997627] 
[Epoch 2/5] [Batch 900/1059] [D loss: 0.553344] [G loss: 0.112603] [ema: 0.997706] 
[Epoch 2/5] [Batch 1000/1059] [D loss: 0.576880] [G loss: 0.107472] [ema: 0.997779] 




Saving checkpoint 4 in logs/Stand_5000_D_30_2024_10_15_23_55_51/Model




[Epoch 3/5] [Batch 0/1059] [D loss: 0.564430] [G loss: 0.116111] [ema: 0.997821] 
[Epoch 3/5] [Batch 100/1059] [D loss: 0.558052] [G loss: 0.112600] [ema: 0.997887] 
[Epoch 3/5] [Batch 200/1059] [D loss: 0.546596] [G loss: 0.114521] [ema: 0.997950] 
[Epoch 3/5] [Batch 300/1059] [D loss: 0.560534] [G loss: 0.111702] [ema: 0.998008] 
[Epoch 3/5] [Batch 400/1059] [D loss: 0.563595] [G loss: 0.111523] [ema: 0.998064] 
[Epoch 3/5] [Batch 500/1059] [D loss: 0.554122] [G loss: 0.110537] [ema: 0.998117] 
[Epoch 3/5] [Batch 600/1059] [D loss: 0.554803] [G loss: 0.100373] [ema: 0.998167] 
[Epoch 3/5] [Batch 700/1059] [D loss: 0.576128] [G loss: 0.115339] [ema: 0.998214] 
[Epoch 3/5] [Batch 800/1059] [D loss: 0.565280] [G loss: 0.114176] [ema: 0.998259] 
[Epoch 3/5] [Batch 900/1059] [D loss: 0.558360] [G loss: 0.113475] [ema: 0.998301] 
[Epoch 3/5] [Batch 1000/1059] [D loss: 0.568563] [G loss: 0.113966] [ema: 0.998342] 




Saving checkpoint 5 in logs/Stand_5000_D_30_2024_10_15_23_55_51/Model




[Epoch 4/5] [Batch 0/1059] [D loss: 0.538334] [G loss: 0.119125] [ema: 0.998365] 
[Epoch 4/5] [Batch 100/1059] [D loss: 0.558884] [G loss: 0.118467] [ema: 0.998403] 
[Epoch 4/5] [Batch 200/1059] [D loss: 0.549286] [G loss: 0.117850] [ema: 0.998439] 
[Epoch 4/5] [Batch 300/1059] [D loss: 0.552595] [G loss: 0.113567] [ema: 0.998473] 
[Epoch 4/5] [Batch 400/1059] [D loss: 0.555420] [G loss: 0.112989] [ema: 0.998506] 
[Epoch 4/5] [Batch 500/1059] [D loss: 0.559674] [G loss: 0.106902] [ema: 0.998537] 
[Epoch 4/5] [Batch 600/1059] [D loss: 0.553885] [G loss: 0.112022] [ema: 0.998568] 
[Epoch 4/5] [Batch 700/1059] [D loss: 0.559447] [G loss: 0.112957] [ema: 0.998597] 
[Epoch 4/5] [Batch 800/1059] [D loss: 0.550758] [G loss: 0.112498] [ema: 0.998625] 
[Epoch 4/5] [Batch 900/1059] [D loss: 0.556163] [G loss: 0.110770] [ema: 0.998651] 
[Epoch 4/5] [Batch 1000/1059] [D loss: 0.559286] [G loss: 0.114581] [ema: 0.998677] 
