Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
walk
return single class data and labels, class is walk
data shape is (16940, 3, 1, 30)
label shape is (16940,)
1059
Epochs between ckechpoint: 1




Saving checkpoint 1 in logs/walk_5000_D_30_2024_10_15_11_57_54/Model




[Epoch 0/5] [Batch 0/1059] [D loss: 1.163414] [G loss: 0.924447] [ema: 0.000000] 
[Epoch 0/5] [Batch 100/1059] [D loss: 0.406219] [G loss: 0.193059] [ema: 0.933033] 
[Epoch 0/5] [Batch 200/1059] [D loss: 0.361810] [G loss: 0.195976] [ema: 0.965936] 
[Epoch 0/5] [Batch 300/1059] [D loss: 0.313682] [G loss: 0.275845] [ema: 0.977160] 
[Epoch 0/5] [Batch 400/1059] [D loss: 0.300626] [G loss: 0.203168] [ema: 0.982821] 
[Epoch 0/5] [Batch 500/1059] [D loss: 0.323996] [G loss: 0.198172] [ema: 0.986233] 
[Epoch 0/5] [Batch 600/1059] [D loss: 0.375252] [G loss: 0.210085] [ema: 0.988514] 
[Epoch 0/5] [Batch 700/1059] [D loss: 0.403987] [G loss: 0.226621] [ema: 0.990147] 
[Epoch 0/5] [Batch 800/1059] [D loss: 0.410477] [G loss: 0.191821] [ema: 0.991373] 
[Epoch 0/5] [Batch 900/1059] [D loss: 0.308991] [G loss: 0.224334] [ema: 0.992328] 
[Epoch 0/5] [Batch 1000/1059] [D loss: 0.367914] [G loss: 0.196666] [ema: 0.993092] 




Saving checkpoint 2 in logs/walk_5000_D_30_2024_10_15_11_57_54/Model




[Epoch 1/5] [Batch 0/1059] [D loss: 0.349272] [G loss: 0.185020] [ema: 0.993476] 
[Epoch 1/5] [Batch 100/1059] [D loss: 0.303006] [G loss: 0.215137] [ema: 0.994037] 
[Epoch 1/5] [Batch 200/1059] [D loss: 0.326795] [G loss: 0.199698] [ema: 0.994510] 
[Epoch 1/5] [Batch 300/1059] [D loss: 0.345252] [G loss: 0.202358] [ema: 0.994913] 
[Epoch 1/5] [Batch 400/1059] [D loss: 0.413067] [G loss: 0.219007] [ema: 0.995260] 
[Epoch 1/5] [Batch 500/1059] [D loss: 0.448059] [G loss: 0.184354] [ema: 0.995564] 
[Epoch 1/5] [Batch 600/1059] [D loss: 0.505683] [G loss: 0.132438] [ema: 0.995831] 
[Epoch 1/5] [Batch 700/1059] [D loss: 0.408532] [G loss: 0.163683] [ema: 0.996067] 
[Epoch 1/5] [Batch 800/1059] [D loss: 0.389270] [G loss: 0.204659] [ema: 0.996278] 
[Epoch 1/5] [Batch 900/1059] [D loss: 0.419405] [G loss: 0.196513] [ema: 0.996468] 
[Epoch 1/5] [Batch 1000/1059] [D loss: 0.372393] [G loss: 0.224658] [ema: 0.996639] 




Saving checkpoint 3 in logs/walk_5000_D_30_2024_10_15_11_57_54/Model




[Epoch 2/5] [Batch 0/1059] [D loss: 0.347843] [G loss: 0.257525] [ema: 0.996733] 
[Epoch 2/5] [Batch 100/1059] [D loss: 0.384568] [G loss: 0.198239] [ema: 0.996880] 
[Epoch 2/5] [Batch 200/1059] [D loss: 0.401679] [G loss: 0.208250] [ema: 0.997014] 
[Epoch 2/5] [Batch 300/1059] [D loss: 0.436991] [G loss: 0.140661] [ema: 0.997137] 
[Epoch 2/5] [Batch 400/1059] [D loss: 0.504313] [G loss: 0.127364] [ema: 0.997251] 
[Epoch 2/5] [Batch 500/1059] [D loss: 0.546493] [G loss: 0.117632] [ema: 0.997356] 
[Epoch 2/5] [Batch 600/1059] [D loss: 0.549681] [G loss: 0.107653] [ema: 0.997453] 
[Epoch 2/5] [Batch 700/1059] [D loss: 0.511600] [G loss: 0.140719] [ema: 0.997543] 
[Epoch 2/5] [Batch 800/1059] [D loss: 0.468270] [G loss: 0.141824] [ema: 0.997627] 
[Epoch 2/5] [Batch 900/1059] [D loss: 0.449051] [G loss: 0.189092] [ema: 0.997706] 
[Epoch 2/5] [Batch 1000/1059] [D loss: 0.482741] [G loss: 0.126463] [ema: 0.997779] 




Saving checkpoint 4 in logs/walk_5000_D_30_2024_10_15_11_57_54/Model




[Epoch 3/5] [Batch 0/1059] [D loss: 0.496778] [G loss: 0.158111] [ema: 0.997821] 
[Epoch 3/5] [Batch 100/1059] [D loss: 0.454144] [G loss: 0.179364] [ema: 0.997887] 
[Epoch 3/5] [Batch 200/1059] [D loss: 0.459705] [G loss: 0.158360] [ema: 0.997950] 
[Epoch 3/5] [Batch 300/1059] [D loss: 0.440966] [G loss: 0.178849] [ema: 0.998008] 
[Epoch 3/5] [Batch 400/1059] [D loss: 0.512158] [G loss: 0.145484] [ema: 0.998064] 
[Epoch 3/5] [Batch 500/1059] [D loss: 0.454558] [G loss: 0.133131] [ema: 0.998117] 
[Epoch 3/5] [Batch 600/1059] [D loss: 0.497490] [G loss: 0.114184] [ema: 0.998167] 
[Epoch 3/5] [Batch 700/1059] [D loss: 0.469305] [G loss: 0.164267] [ema: 0.998214] 
[Epoch 3/5] [Batch 800/1059] [D loss: 0.476209] [G loss: 0.142220] [ema: 0.998259] 
[Epoch 3/5] [Batch 900/1059] [D loss: 0.471515] [G loss: 0.161446] [ema: 0.998301] 
[Epoch 3/5] [Batch 1000/1059] [D loss: 0.420791] [G loss: 0.116849] [ema: 0.998342] 




Saving checkpoint 5 in logs/walk_5000_D_30_2024_10_15_11_57_54/Model




[Epoch 4/5] [Batch 0/1059] [D loss: 0.406891] [G loss: 0.157480] [ema: 0.998365] 
[Epoch 4/5] [Batch 100/1059] [D loss: 0.441660] [G loss: 0.152110] [ema: 0.998403] 
[Epoch 4/5] [Batch 200/1059] [D loss: 0.476643] [G loss: 0.129764] [ema: 0.998439] 
[Epoch 4/5] [Batch 300/1059] [D loss: 0.428072] [G loss: 0.200967] [ema: 0.998473] 
[Epoch 4/5] [Batch 400/1059] [D loss: 0.416212] [G loss: 0.130338] [ema: 0.998506] 
[Epoch 4/5] [Batch 500/1059] [D loss: 0.464339] [G loss: 0.147274] [ema: 0.998537] 
[Epoch 4/5] [Batch 600/1059] [D loss: 0.506646] [G loss: 0.180487] [ema: 0.998568] 
[Epoch 4/5] [Batch 700/1059] [D loss: 0.478131] [G loss: 0.140158] [ema: 0.998597] 
[Epoch 4/5] [Batch 800/1059] [D loss: 0.468561] [G loss: 0.144872] [ema: 0.998625] 
[Epoch 4/5] [Batch 900/1059] [D loss: 0.455934] [G loss: 0.166208] [ema: 0.998651] 
[Epoch 4/5] [Batch 1000/1059] [D loss: 0.429181] [G loss: 0.159388] [ema: 0.998677] 
