Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
sit
return single class data and labels, class is sit
data shape is (16940, 3, 1, 30)
label shape is (16940,)
1059
Epochs between ckechpoint: 1




Saving checkpoint 1 in logs/Sit_5000_D_30_2024_10_15_23_46_42/Model




[Epoch 0/5] [Batch 0/1059] [D loss: 1.265055] [G loss: 0.891576] [ema: 0.000000] 
[Epoch 0/5] [Batch 100/1059] [D loss: 0.542528] [G loss: 0.116618] [ema: 0.933033] 
[Epoch 0/5] [Batch 200/1059] [D loss: 0.614056] [G loss: 0.107754] [ema: 0.965936] 
[Epoch 0/5] [Batch 300/1059] [D loss: 0.601729] [G loss: 0.107603] [ema: 0.977160] 
[Epoch 0/5] [Batch 400/1059] [D loss: 0.516155] [G loss: 0.123856] [ema: 0.982821] 
[Epoch 0/5] [Batch 500/1059] [D loss: 0.488970] [G loss: 0.141799] [ema: 0.986233] 
[Epoch 0/5] [Batch 600/1059] [D loss: 0.554308] [G loss: 0.125471] [ema: 0.988514] 
[Epoch 0/5] [Batch 700/1059] [D loss: 0.604672] [G loss: 0.122231] [ema: 0.990147] 
[Epoch 0/5] [Batch 800/1059] [D loss: 0.535109] [G loss: 0.129489] [ema: 0.991373] 
[Epoch 0/5] [Batch 900/1059] [D loss: 0.534040] [G loss: 0.112319] [ema: 0.992328] 
[Epoch 0/5] [Batch 1000/1059] [D loss: 0.600270] [G loss: 0.104279] [ema: 0.993092] 




Saving checkpoint 2 in logs/Sit_5000_D_30_2024_10_15_23_46_42/Model




[Epoch 1/5] [Batch 0/1059] [D loss: 0.511733] [G loss: 0.138517] [ema: 0.993476] 
[Epoch 1/5] [Batch 100/1059] [D loss: 0.555507] [G loss: 0.116853] [ema: 0.994037] 
[Epoch 1/5] [Batch 200/1059] [D loss: 0.550909] [G loss: 0.106601] [ema: 0.994510] 
[Epoch 1/5] [Batch 300/1059] [D loss: 0.593014] [G loss: 0.105364] [ema: 0.994913] 
[Epoch 1/5] [Batch 400/1059] [D loss: 0.528100] [G loss: 0.109051] [ema: 0.995260] 
[Epoch 1/5] [Batch 500/1059] [D loss: 0.534445] [G loss: 0.112508] [ema: 0.995564] 
[Epoch 1/5] [Batch 600/1059] [D loss: 0.515613] [G loss: 0.125405] [ema: 0.995831] 
[Epoch 1/5] [Batch 700/1059] [D loss: 0.516105] [G loss: 0.121370] [ema: 0.996067] 
[Epoch 1/5] [Batch 800/1059] [D loss: 0.539221] [G loss: 0.120239] [ema: 0.996278] 
[Epoch 1/5] [Batch 900/1059] [D loss: 0.539733] [G loss: 0.119336] [ema: 0.996468] 
[Epoch 1/5] [Batch 1000/1059] [D loss: 0.522451] [G loss: 0.115979] [ema: 0.996639] 




Saving checkpoint 3 in logs/Sit_5000_D_30_2024_10_15_23_46_42/Model




[Epoch 2/5] [Batch 0/1059] [D loss: 0.578626] [G loss: 0.113448] [ema: 0.996733] 
[Epoch 2/5] [Batch 100/1059] [D loss: 0.529956] [G loss: 0.114290] [ema: 0.996880] 
[Epoch 2/5] [Batch 200/1059] [D loss: 0.547838] [G loss: 0.121194] [ema: 0.997014] 
[Epoch 2/5] [Batch 300/1059] [D loss: 0.511593] [G loss: 0.113050] [ema: 0.997137] 
[Epoch 2/5] [Batch 400/1059] [D loss: 0.565394] [G loss: 0.119272] [ema: 0.997251] 
[Epoch 2/5] [Batch 500/1059] [D loss: 0.580602] [G loss: 0.120159] [ema: 0.997356] 
[Epoch 2/5] [Batch 600/1059] [D loss: 0.571144] [G loss: 0.108398] [ema: 0.997453] 
[Epoch 2/5] [Batch 700/1059] [D loss: 0.570513] [G loss: 0.123349] [ema: 0.997543] 
[Epoch 2/5] [Batch 800/1059] [D loss: 0.534385] [G loss: 0.109409] [ema: 0.997627] 
[Epoch 2/5] [Batch 900/1059] [D loss: 0.533554] [G loss: 0.112802] [ema: 0.997706] 
[Epoch 2/5] [Batch 1000/1059] [D loss: 0.572574] [G loss: 0.106826] [ema: 0.997779] 




Saving checkpoint 4 in logs/Sit_5000_D_30_2024_10_15_23_46_42/Model




[Epoch 3/5] [Batch 0/1059] [D loss: 0.559910] [G loss: 0.114087] [ema: 0.997821] 
[Epoch 3/5] [Batch 100/1059] [D loss: 0.554722] [G loss: 0.114077] [ema: 0.997887] 
[Epoch 3/5] [Batch 200/1059] [D loss: 0.530875] [G loss: 0.111882] [ema: 0.997950] 
[Epoch 3/5] [Batch 300/1059] [D loss: 0.561743] [G loss: 0.114556] [ema: 0.998008] 
[Epoch 3/5] [Batch 400/1059] [D loss: 0.560937] [G loss: 0.103601] [ema: 0.998064] 
[Epoch 3/5] [Batch 500/1059] [D loss: 0.556858] [G loss: 0.102200] [ema: 0.998117] 
[Epoch 3/5] [Batch 600/1059] [D loss: 0.565686] [G loss: 0.099733] [ema: 0.998167] 
[Epoch 3/5] [Batch 700/1059] [D loss: 0.549782] [G loss: 0.117791] [ema: 0.998214] 
[Epoch 3/5] [Batch 800/1059] [D loss: 0.567663] [G loss: 0.118833] [ema: 0.998259] 
[Epoch 3/5] [Batch 900/1059] [D loss: 0.549845] [G loss: 0.115762] [ema: 0.998301] 
[Epoch 3/5] [Batch 1000/1059] [D loss: 0.557251] [G loss: 0.121473] [ema: 0.998342] 




Saving checkpoint 5 in logs/Sit_5000_D_30_2024_10_15_23_46_42/Model




[Epoch 4/5] [Batch 0/1059] [D loss: 0.543452] [G loss: 0.114809] [ema: 0.998365] 
[Epoch 4/5] [Batch 100/1059] [D loss: 0.568619] [G loss: 0.127936] [ema: 0.998403] 
[Epoch 4/5] [Batch 200/1059] [D loss: 0.566932] [G loss: 0.134604] [ema: 0.998439] 
[Epoch 4/5] [Batch 300/1059] [D loss: 0.514156] [G loss: 0.113613] [ema: 0.998473] 
[Epoch 4/5] [Batch 400/1059] [D loss: 0.597790] [G loss: 0.126927] [ema: 0.998506] 
[Epoch 4/5] [Batch 500/1059] [D loss: 0.560134] [G loss: 0.116429] [ema: 0.998537] 
[Epoch 4/5] [Batch 600/1059] [D loss: 0.559378] [G loss: 0.119483] [ema: 0.998568] 
[Epoch 4/5] [Batch 700/1059] [D loss: 0.540135] [G loss: 0.125857] [ema: 0.998597] 
[Epoch 4/5] [Batch 800/1059] [D loss: 0.532556] [G loss: 0.129011] [ema: 0.998625] 
[Epoch 4/5] [Batch 900/1059] [D loss: 0.508564] [G loss: 0.136914] [ema: 0.998651] 
[Epoch 4/5] [Batch 1000/1059] [D loss: 0.554937] [G loss: 0.109785] [ema: 0.998677] 
