Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
upstairs
return single class data and labels, class is upstairs
data shape is (12639, 3, 1, 30)
label shape is (12639,)
790
Epochs between ckechpoint: 1




Saving checkpoint 1 in logs/Upstairs_5000_D_30_2024_10_15_23_59_49/Model




[Epoch 0/7] [Batch 0/790] [D loss: 1.404360] [G loss: 0.880141] [ema: 0.000000] 
[Epoch 0/7] [Batch 100/790] [D loss: 0.429760] [G loss: 0.192999] [ema: 0.933033] 
[Epoch 0/7] [Batch 200/790] [D loss: 0.385523] [G loss: 0.191838] [ema: 0.965936] 
[Epoch 0/7] [Batch 300/790] [D loss: 0.253311] [G loss: 0.280742] [ema: 0.977160] 
[Epoch 0/7] [Batch 400/790] [D loss: 0.287309] [G loss: 0.197574] [ema: 0.982821] 
[Epoch 0/7] [Batch 500/790] [D loss: 0.339648] [G loss: 0.176338] [ema: 0.986233] 
[Epoch 0/7] [Batch 600/790] [D loss: 0.393667] [G loss: 0.188111] [ema: 0.988514] 
[Epoch 0/7] [Batch 700/790] [D loss: 0.403283] [G loss: 0.201703] [ema: 0.990147] 




Saving checkpoint 2 in logs/Upstairs_5000_D_30_2024_10_15_23_59_49/Model




[Epoch 1/7] [Batch 0/790] [D loss: 0.393225] [G loss: 0.224660] [ema: 0.991264] 
[Epoch 1/7] [Batch 100/790] [D loss: 0.293984] [G loss: 0.194483] [ema: 0.992242] 
[Epoch 1/7] [Batch 200/790] [D loss: 0.315461] [G loss: 0.204270] [ema: 0.993023] 
[Epoch 1/7] [Batch 300/790] [D loss: 0.334558] [G loss: 0.169169] [ema: 0.993661] 
[Epoch 1/7] [Batch 400/790] [D loss: 0.313933] [G loss: 0.200667] [ema: 0.994192] 
[Epoch 1/7] [Batch 500/790] [D loss: 0.342908] [G loss: 0.209552] [ema: 0.994641] 
[Epoch 1/7] [Batch 600/790] [D loss: 0.369348] [G loss: 0.205260] [ema: 0.995026] 
[Epoch 1/7] [Batch 700/790] [D loss: 0.443110] [G loss: 0.213462] [ema: 0.995359] 




Saving checkpoint 3 in logs/Upstairs_5000_D_30_2024_10_15_23_59_49/Model




[Epoch 2/7] [Batch 0/790] [D loss: 0.457237] [G loss: 0.164433] [ema: 0.995623] 
[Epoch 2/7] [Batch 100/790] [D loss: 0.425746] [G loss: 0.170392] [ema: 0.995883] 
[Epoch 2/7] [Batch 200/790] [D loss: 0.398575] [G loss: 0.185527] [ema: 0.996113] 
[Epoch 2/7] [Batch 300/790] [D loss: 0.408997] [G loss: 0.207026] [ema: 0.996320] 
[Epoch 2/7] [Batch 400/790] [D loss: 0.432371] [G loss: 0.204711] [ema: 0.996505] 
[Epoch 2/7] [Batch 500/790] [D loss: 0.379223] [G loss: 0.216544] [ema: 0.996673] 
[Epoch 2/7] [Batch 600/790] [D loss: 0.393551] [G loss: 0.191672] [ema: 0.996825] 
[Epoch 2/7] [Batch 700/790] [D loss: 0.454269] [G loss: 0.170579] [ema: 0.996964] 




Saving checkpoint 4 in logs/Upstairs_5000_D_30_2024_10_15_23_59_49/Model




[Epoch 3/7] [Batch 0/790] [D loss: 0.475469] [G loss: 0.139802] [ema: 0.997080] 
[Epoch 3/7] [Batch 100/790] [D loss: 0.511867] [G loss: 0.169761] [ema: 0.997198] 
[Epoch 3/7] [Batch 200/790] [D loss: 0.465778] [G loss: 0.118351] [ema: 0.997307] 
[Epoch 3/7] [Batch 300/790] [D loss: 0.450700] [G loss: 0.160909] [ema: 0.997407] 
[Epoch 3/7] [Batch 400/790] [D loss: 0.393126] [G loss: 0.133523] [ema: 0.997501] 
[Epoch 3/7] [Batch 500/790] [D loss: 0.422390] [G loss: 0.163269] [ema: 0.997588] 
[Epoch 3/7] [Batch 600/790] [D loss: 0.475166] [G loss: 0.181398] [ema: 0.997669] 
[Epoch 3/7] [Batch 700/790] [D loss: 0.568490] [G loss: 0.168817] [ema: 0.997745] 




Saving checkpoint 5 in logs/Upstairs_5000_D_30_2024_10_15_23_59_49/Model




[Epoch 4/7] [Batch 0/790] [D loss: 0.465715] [G loss: 0.146857] [ema: 0.997809] 
[Epoch 4/7] [Batch 100/790] [D loss: 0.440915] [G loss: 0.186202] [ema: 0.997876] 
[Epoch 4/7] [Batch 200/790] [D loss: 0.425224] [G loss: 0.150696] [ema: 0.997939] 
[Epoch 4/7] [Batch 300/790] [D loss: 0.391508] [G loss: 0.168218] [ema: 0.997999] 
[Epoch 4/7] [Batch 400/790] [D loss: 0.453644] [G loss: 0.161543] [ema: 0.998055] 
[Epoch 4/7] [Batch 500/790] [D loss: 0.449959] [G loss: 0.154586] [ema: 0.998108] 
[Epoch 4/7] [Batch 600/790] [D loss: 0.399973] [G loss: 0.161483] [ema: 0.998158] 
[Epoch 4/7] [Batch 700/790] [D loss: 0.440183] [G loss: 0.163465] [ema: 0.998206] 




Saving checkpoint 6 in logs/Upstairs_5000_D_30_2024_10_15_23_59_49/Model




[Epoch 5/7] [Batch 0/790] [D loss: 0.412590] [G loss: 0.176315] [ema: 0.998247] 
[Epoch 5/7] [Batch 100/790] [D loss: 0.408961] [G loss: 0.131719] [ema: 0.998290] 
[Epoch 5/7] [Batch 200/790] [D loss: 0.371930] [G loss: 0.177118] [ema: 0.998331] 
[Epoch 5/7] [Batch 300/790] [D loss: 0.404797] [G loss: 0.201955] [ema: 0.998370] 
[Epoch 5/7] [Batch 400/790] [D loss: 0.420604] [G loss: 0.169228] [ema: 0.998408] 
[Epoch 5/7] [Batch 500/790] [D loss: 0.371100] [G loss: 0.190143] [ema: 0.998444] 
[Epoch 5/7] [Batch 600/790] [D loss: 0.418149] [G loss: 0.198487] [ema: 0.998478] 
[Epoch 5/7] [Batch 700/790] [D loss: 0.443412] [G loss: 0.193297] [ema: 0.998510] 




Saving checkpoint 7 in logs/Upstairs_5000_D_30_2024_10_15_23_59_49/Model




[Epoch 6/7] [Batch 0/790] [D loss: 0.359884] [G loss: 0.156467] [ema: 0.998539] 
[Epoch 6/7] [Batch 100/790] [D loss: 0.401898] [G loss: 0.175449] [ema: 0.998569] 
[Epoch 6/7] [Batch 200/790] [D loss: 0.389884] [G loss: 0.200477] [ema: 0.998598] 
[Epoch 6/7] [Batch 300/790] [D loss: 0.407841] [G loss: 0.191894] [ema: 0.998626] 
[Epoch 6/7] [Batch 400/790] [D loss: 0.479616] [G loss: 0.168561] [ema: 0.998652] 
[Epoch 6/7] [Batch 500/790] [D loss: 0.398581] [G loss: 0.158000] [ema: 0.998678] 
[Epoch 6/7] [Batch 600/790] [D loss: 0.329235] [G loss: 0.205297] [ema: 0.998703] 
[Epoch 6/7] [Batch 700/790] [D loss: 0.446574] [G loss: 0.164070] [ema: 0.998727] 
