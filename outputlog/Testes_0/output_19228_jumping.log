Generator(
  (l1): Linear(in_features=100, out_features=1500, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
x_train shape is (6055, 3, 1, 150), x_test shape is (1524, 3, 1, 150)
y_train shape is (6055,), y_test shape is (1524,)
return single class data and labels, class is Jumping
train_data shape is (600, 3, 1, 150), test_data shape is (146, 3, 1, 150)
train label shape is (600,), test data shape is (146,)
x_train shape is (6055, 3, 1, 150), x_test shape is (1524, 3, 1, 150)
y_train shape is (6055,), y_test shape is (1524,)
return single class data and labels, class is Jumping
train_data shape is (600, 3, 1, 150), test_data shape is (146, 3, 1, 150)
train label shape is (600,), test data shape is (146,)
38
Epochs between ckechpoint: 102




Saving checkpoint 1 in logs/Jumping_2024_10_02_18_34_58/Model




[Epoch 0/506] [Batch 0/38] [D loss: 1.171687] [G loss: 0.484607] [ema: 0.000000] 
[Epoch 1/506] [Batch 0/38] [D loss: 0.434924] [G loss: 0.263207] [ema: 0.833262] 
[Epoch 2/506] [Batch 0/38] [D loss: 0.642551] [G loss: 0.098299] [ema: 0.912832] 
[Epoch 3/506] [Batch 0/38] [D loss: 0.459852] [G loss: 0.185093] [ema: 0.941009] 
[Epoch 4/506] [Batch 0/38] [D loss: 0.426422] [G loss: 0.209906] [ema: 0.955422] 
[Epoch 5/506] [Batch 0/38] [D loss: 0.495488] [G loss: 0.187031] [ema: 0.964176] 
[Epoch 6/506] [Batch 0/38] [D loss: 0.412122] [G loss: 0.179379] [ema: 0.970056] 
[Epoch 7/506] [Batch 0/38] [D loss: 0.371779] [G loss: 0.201947] [ema: 0.974278] 
[Epoch 8/506] [Batch 0/38] [D loss: 0.327737] [G loss: 0.234345] [ema: 0.977457] 
[Epoch 9/506] [Batch 0/38] [D loss: 0.301219] [G loss: 0.254388] [ema: 0.979937] 
[Epoch 10/506] [Batch 0/38] [D loss: 0.357794] [G loss: 0.271704] [ema: 0.981925] 
[Epoch 11/506] [Batch 0/38] [D loss: 0.415879] [G loss: 0.202785] [ema: 0.983554] 
[Epoch 12/506] [Batch 0/38] [D loss: 0.371880] [G loss: 0.182273] [ema: 0.984914] 
[Epoch 13/506] [Batch 0/38] [D loss: 0.320073] [G loss: 0.261729] [ema: 0.986067] 
[Epoch 14/506] [Batch 0/38] [D loss: 0.309013] [G loss: 0.235760] [ema: 0.987055] 
[Epoch 15/506] [Batch 0/38] [D loss: 0.334038] [G loss: 0.247576] [ema: 0.987913] 
[Epoch 16/506] [Batch 0/38] [D loss: 0.327794] [G loss: 0.233780] [ema: 0.988664] 
[Epoch 17/506] [Batch 0/38] [D loss: 0.419595] [G loss: 0.229574] [ema: 0.989328] 
[Epoch 18/506] [Batch 0/38] [D loss: 0.325347] [G loss: 0.249644] [ema: 0.989917] 
[Epoch 19/506] [Batch 0/38] [D loss: 0.401022] [G loss: 0.185639] [ema: 0.990446] 
[Epoch 20/506] [Batch 0/38] [D loss: 0.577780] [G loss: 0.169510] [ema: 0.990921] 
[Epoch 21/506] [Batch 0/38] [D loss: 0.259693] [G loss: 0.270769] [ema: 0.991352] 
[Epoch 22/506] [Batch 0/38] [D loss: 0.398194] [G loss: 0.193877] [ema: 0.991743] 
[Epoch 23/506] [Batch 0/38] [D loss: 0.441244] [G loss: 0.183592] [ema: 0.992101] 
[Epoch 24/506] [Batch 0/38] [D loss: 0.350403] [G loss: 0.225795] [ema: 0.992429] 
[Epoch 25/506] [Batch 0/38] [D loss: 0.359370] [G loss: 0.240342] [ema: 0.992730] 
[Epoch 26/506] [Batch 0/38] [D loss: 0.369790] [G loss: 0.222185] [ema: 0.993009] 
[Epoch 27/506] [Batch 0/38] [D loss: 0.381418] [G loss: 0.201079] [ema: 0.993267] 
[Epoch 28/506] [Batch 0/38] [D loss: 0.407332] [G loss: 0.195163] [ema: 0.993507] 
[Epoch 29/506] [Batch 0/38] [D loss: 0.361901] [G loss: 0.204686] [ema: 0.993730] 
[Epoch 30/506] [Batch 0/38] [D loss: 0.370682] [G loss: 0.187934] [ema: 0.993938] 
[Epoch 31/506] [Batch 0/38] [D loss: 0.493985] [G loss: 0.185053] [ema: 0.994133] 
[Epoch 32/506] [Batch 0/38] [D loss: 0.360560] [G loss: 0.239482] [ema: 0.994316] 
[Epoch 33/506] [Batch 0/38] [D loss: 0.436667] [G loss: 0.165329] [ema: 0.994488] 
[Epoch 34/506] [Batch 0/38] [D loss: 0.530670] [G loss: 0.164774] [ema: 0.994649] 
[Epoch 35/506] [Batch 0/38] [D loss: 0.503635] [G loss: 0.167542] [ema: 0.994802] 
[Epoch 36/506] [Batch 0/38] [D loss: 0.488731] [G loss: 0.168457] [ema: 0.994946] 
[Epoch 37/506] [Batch 0/38] [D loss: 0.395494] [G loss: 0.175269] [ema: 0.995082] 
[Epoch 38/506] [Batch 0/38] [D loss: 0.608252] [G loss: 0.141784] [ema: 0.995211] 
[Epoch 39/506] [Batch 0/38] [D loss: 0.316886] [G loss: 0.236504] [ema: 0.995334] 
[Epoch 40/506] [Batch 0/38] [D loss: 0.490260] [G loss: 0.149145] [ema: 0.995450] 
[Epoch 41/506] [Batch 0/38] [D loss: 0.430232] [G loss: 0.165144] [ema: 0.995561] 
[Epoch 42/506] [Batch 0/38] [D loss: 0.404594] [G loss: 0.228929] [ema: 0.995666] 
[Epoch 43/506] [Batch 0/38] [D loss: 0.430353] [G loss: 0.210953] [ema: 0.995767] 
[Epoch 44/506] [Batch 0/38] [D loss: 0.536694] [G loss: 0.155294] [ema: 0.995863] 
[Epoch 45/506] [Batch 0/38] [D loss: 0.412167] [G loss: 0.206301] [ema: 0.995955] 
[Epoch 46/506] [Batch 0/38] [D loss: 0.495159] [G loss: 0.193300] [ema: 0.996042] 
[Epoch 47/506] [Batch 0/38] [D loss: 0.508914] [G loss: 0.182021] [ema: 0.996127] 
[Epoch 48/506] [Batch 0/38] [D loss: 0.379399] [G loss: 0.211389] [ema: 0.996207] 
[Epoch 49/506] [Batch 0/38] [D loss: 0.398351] [G loss: 0.191018] [ema: 0.996284] 
[Epoch 50/506] [Batch 0/38] [D loss: 0.407910] [G loss: 0.221136] [ema: 0.996359] 
[Epoch 51/506] [Batch 0/38] [D loss: 0.431536] [G loss: 0.176339] [ema: 0.996430] 
[Epoch 52/506] [Batch 0/38] [D loss: 0.451813] [G loss: 0.193316] [ema: 0.996498] 
[Epoch 53/506] [Batch 0/38] [D loss: 0.430713] [G loss: 0.175709] [ema: 0.996564] 
[Epoch 54/506] [Batch 0/38] [D loss: 0.507700] [G loss: 0.160477] [ema: 0.996628] 
[Epoch 55/506] [Batch 0/38] [D loss: 0.388019] [G loss: 0.235260] [ema: 0.996689] 
[Epoch 56/506] [Batch 0/38] [D loss: 0.480567] [G loss: 0.168749] [ema: 0.996748] 
[Epoch 57/506] [Batch 0/38] [D loss: 0.369739] [G loss: 0.221748] [ema: 0.996805] 
[Epoch 58/506] [Batch 0/38] [D loss: 0.430016] [G loss: 0.246836] [ema: 0.996860] 
[Epoch 59/506] [Batch 0/38] [D loss: 0.447000] [G loss: 0.188099] [ema: 0.996913] 
[Epoch 60/506] [Batch 0/38] [D loss: 0.346718] [G loss: 0.204527] [ema: 0.996964] 
[Epoch 61/506] [Batch 0/38] [D loss: 0.366899] [G loss: 0.180926] [ema: 0.997014] 
[Epoch 62/506] [Batch 0/38] [D loss: 0.436342] [G loss: 0.184910] [ema: 0.997062] 
[Epoch 63/506] [Batch 0/38] [D loss: 0.389879] [G loss: 0.198050] [ema: 0.997109] 
[Epoch 64/506] [Batch 0/38] [D loss: 0.374760] [G loss: 0.194040] [ema: 0.997154] 
[Epoch 65/506] [Batch 0/38] [D loss: 0.362941] [G loss: 0.206046] [ema: 0.997198] 
[Epoch 66/506] [Batch 0/38] [D loss: 0.465429] [G loss: 0.220231] [ema: 0.997240] 
[Epoch 67/506] [Batch 0/38] [D loss: 0.445393] [G loss: 0.203215] [ema: 0.997281] 
[Epoch 68/506] [Batch 0/38] [D loss: 0.371544] [G loss: 0.233933] [ema: 0.997321] 
[Epoch 69/506] [Batch 0/38] [D loss: 0.435014] [G loss: 0.174935] [ema: 0.997360] 
[Epoch 70/506] [Batch 0/38] [D loss: 0.342344] [G loss: 0.240159] [ema: 0.997398] 
[Epoch 71/506] [Batch 0/38] [D loss: 0.406432] [G loss: 0.203642] [ema: 0.997434] 
[Epoch 72/506] [Batch 0/38] [D loss: 0.443370] [G loss: 0.275570] [ema: 0.997470] 
[Epoch 73/506] [Batch 0/38] [D loss: 0.422813] [G loss: 0.191206] [ema: 0.997504] 
[Epoch 74/506] [Batch 0/38] [D loss: 0.313663] [G loss: 0.237924] [ema: 0.997538] 
[Epoch 75/506] [Batch 0/38] [D loss: 0.429014] [G loss: 0.188298] [ema: 0.997571] 
[Epoch 76/506] [Batch 0/38] [D loss: 0.395866] [G loss: 0.189145] [ema: 0.997603] 
[Epoch 77/506] [Batch 0/38] [D loss: 0.386858] [G loss: 0.211115] [ema: 0.997634] 
[Epoch 78/506] [Batch 0/38] [D loss: 0.351372] [G loss: 0.195004] [ema: 0.997664] 
[Epoch 79/506] [Batch 0/38] [D loss: 0.349268] [G loss: 0.231809] [ema: 0.997694] 
[Epoch 80/506] [Batch 0/38] [D loss: 0.376601] [G loss: 0.193401] [ema: 0.997723] 
[Epoch 81/506] [Batch 0/38] [D loss: 0.360295] [G loss: 0.190840] [ema: 0.997751] 
[Epoch 82/506] [Batch 0/38] [D loss: 0.370590] [G loss: 0.201840] [ema: 0.997778] 
[Epoch 83/506] [Batch 0/38] [D loss: 0.332142] [G loss: 0.237856] [ema: 0.997805] 
[Epoch 84/506] [Batch 0/38] [D loss: 0.370668] [G loss: 0.200935] [ema: 0.997831] 
[Epoch 85/506] [Batch 0/38] [D loss: 0.389958] [G loss: 0.219299] [ema: 0.997856] 
[Epoch 86/506] [Batch 0/38] [D loss: 0.339032] [G loss: 0.248640] [ema: 0.997881] 
[Epoch 87/506] [Batch 0/38] [D loss: 0.397004] [G loss: 0.259962] [ema: 0.997906] 
[Epoch 88/506] [Batch 0/38] [D loss: 0.382576] [G loss: 0.199622] [ema: 0.997929] 
[Epoch 89/506] [Batch 0/38] [D loss: 0.420058] [G loss: 0.182162] [ema: 0.997953] 
[Epoch 90/506] [Batch 0/38] [D loss: 0.413090] [G loss: 0.189604] [ema: 0.997975] 
[Epoch 91/506] [Batch 0/38] [D loss: 0.294389] [G loss: 0.215887] [ema: 0.997998] 
[Epoch 92/506] [Batch 0/38] [D loss: 0.344186] [G loss: 0.233556] [ema: 0.998019] 
[Epoch 93/506] [Batch 0/38] [D loss: 0.381648] [G loss: 0.230626] [ema: 0.998041] 
[Epoch 94/506] [Batch 0/38] [D loss: 0.352431] [G loss: 0.218017] [ema: 0.998061] 
[Epoch 95/506] [Batch 0/38] [D loss: 0.345718] [G loss: 0.237274] [ema: 0.998082] 
[Epoch 96/506] [Batch 0/38] [D loss: 0.333718] [G loss: 0.215717] [ema: 0.998102] 
[Epoch 97/506] [Batch 0/38] [D loss: 0.372217] [G loss: 0.192064] [ema: 0.998121] 
[Epoch 98/506] [Batch 0/38] [D loss: 0.419151] [G loss: 0.216554] [ema: 0.998140] 
[Epoch 99/506] [Batch 0/38] [D loss: 0.384449] [G loss: 0.206139] [ema: 0.998159] 
[Epoch 100/506] [Batch 0/38] [D loss: 0.399042] [G loss: 0.234903] [ema: 0.998178] 
[Epoch 101/506] [Batch 0/38] [D loss: 0.311252] [G loss: 0.257149] [ema: 0.998196] 




Saving checkpoint 2 in logs/Jumping_2024_10_02_18_34_58/Model




[Epoch 102/506] [Batch 0/38] [D loss: 0.369418] [G loss: 0.206076] [ema: 0.998213] 
[Epoch 103/506] [Batch 0/38] [D loss: 0.339726] [G loss: 0.224975] [ema: 0.998231] 
[Epoch 104/506] [Batch 0/38] [D loss: 0.381711] [G loss: 0.205350] [ema: 0.998248] 
[Epoch 105/506] [Batch 0/38] [D loss: 0.373848] [G loss: 0.209633] [ema: 0.998264] 
[Epoch 106/506] [Batch 0/38] [D loss: 0.332289] [G loss: 0.212612] [ema: 0.998281] 
[Epoch 107/506] [Batch 0/38] [D loss: 0.326894] [G loss: 0.210485] [ema: 0.998297] 
[Epoch 108/506] [Batch 0/38] [D loss: 0.343842] [G loss: 0.203485] [ema: 0.998312] 
[Epoch 109/506] [Batch 0/38] [D loss: 0.339717] [G loss: 0.220157] [ema: 0.998328] 
[Epoch 110/506] [Batch 0/38] [D loss: 0.384682] [G loss: 0.201162] [ema: 0.998343] 
[Epoch 111/506] [Batch 0/38] [D loss: 0.421282] [G loss: 0.203615] [ema: 0.998358] 
[Epoch 112/506] [Batch 0/38] [D loss: 0.339868] [G loss: 0.219228] [ema: 0.998373] 
[Epoch 113/506] [Batch 0/38] [D loss: 0.275812] [G loss: 0.241178] [ema: 0.998387] 
[Epoch 114/506] [Batch 0/38] [D loss: 0.383337] [G loss: 0.225134] [ema: 0.998401] 
[Epoch 115/506] [Batch 0/38] [D loss: 0.314725] [G loss: 0.223467] [ema: 0.998415] 
[Epoch 116/506] [Batch 0/38] [D loss: 0.443788] [G loss: 0.235110] [ema: 0.998429] 
[Epoch 117/506] [Batch 0/38] [D loss: 0.343868] [G loss: 0.209330] [ema: 0.998442] 
[Epoch 118/506] [Batch 0/38] [D loss: 0.354552] [G loss: 0.225685] [ema: 0.998455] 
[Epoch 119/506] [Batch 0/38] [D loss: 0.361979] [G loss: 0.222748] [ema: 0.998468] 
[Epoch 120/506] [Batch 0/38] [D loss: 0.363276] [G loss: 0.182649] [ema: 0.998481] 
[Epoch 121/506] [Batch 0/38] [D loss: 0.360530] [G loss: 0.196974] [ema: 0.998494] 
[Epoch 122/506] [Batch 0/38] [D loss: 0.356234] [G loss: 0.237091] [ema: 0.998506] 
[Epoch 123/506] [Batch 0/38] [D loss: 0.335364] [G loss: 0.215167] [ema: 0.998518] 
[Epoch 124/506] [Batch 0/38] [D loss: 0.354906] [G loss: 0.223944] [ema: 0.998530] 
[Epoch 125/506] [Batch 0/38] [D loss: 0.392487] [G loss: 0.191374] [ema: 0.998542] 
[Epoch 126/506] [Batch 0/38] [D loss: 0.314817] [G loss: 0.223751] [ema: 0.998553] 
[Epoch 127/506] [Batch 0/38] [D loss: 0.366868] [G loss: 0.212865] [ema: 0.998565] 
[Epoch 128/506] [Batch 0/38] [D loss: 0.374974] [G loss: 0.244961] [ema: 0.998576] 
[Epoch 129/506] [Batch 0/38] [D loss: 0.382708] [G loss: 0.225987] [ema: 0.998587] 
[Epoch 130/506] [Batch 0/38] [D loss: 0.360325] [G loss: 0.208878] [ema: 0.998598] 
[Epoch 131/506] [Batch 0/38] [D loss: 0.369722] [G loss: 0.221973] [ema: 0.998609] 
[Epoch 132/506] [Batch 0/38] [D loss: 0.331180] [G loss: 0.216997] [ema: 0.998619] 
[Epoch 133/506] [Batch 0/38] [D loss: 0.347770] [G loss: 0.227820] [ema: 0.998629] 
[Epoch 134/506] [Batch 0/38] [D loss: 0.384721] [G loss: 0.201794] [ema: 0.998640] 
[Epoch 135/506] [Batch 0/38] [D loss: 0.380763] [G loss: 0.254032] [ema: 0.998650] 
[Epoch 136/506] [Batch 0/38] [D loss: 0.351963] [G loss: 0.203860] [ema: 0.998660] 
[Epoch 137/506] [Batch 0/38] [D loss: 0.322637] [G loss: 0.207594] [ema: 0.998669] 
[Epoch 138/506] [Batch 0/38] [D loss: 0.358497] [G loss: 0.199109] [ema: 0.998679] 
[Epoch 139/506] [Batch 0/38] [D loss: 0.305841] [G loss: 0.220775] [ema: 0.998689] 
[Epoch 140/506] [Batch 0/38] [D loss: 0.360268] [G loss: 0.234266] [ema: 0.998698] 
[Epoch 141/506] [Batch 0/38] [D loss: 0.315222] [G loss: 0.203515] [ema: 0.998707] 
[Epoch 142/506] [Batch 0/38] [D loss: 0.328728] [G loss: 0.224140] [ema: 0.998716] 
[Epoch 143/506] [Batch 0/38] [D loss: 0.373643] [G loss: 0.221683] [ema: 0.998725] 
[Epoch 144/506] [Batch 0/38] [D loss: 0.311309] [G loss: 0.182884] [ema: 0.998734] 
[Epoch 145/506] [Batch 0/38] [D loss: 0.303242] [G loss: 0.241230] [ema: 0.998743] 
[Epoch 146/506] [Batch 0/38] [D loss: 0.365319] [G loss: 0.218912] [ema: 0.998751] 
[Epoch 147/506] [Batch 0/38] [D loss: 0.325180] [G loss: 0.211121] [ema: 0.998760] 
[Epoch 148/506] [Batch 0/38] [D loss: 0.326425] [G loss: 0.201310] [ema: 0.998768] 
[Epoch 149/506] [Batch 0/38] [D loss: 0.299971] [G loss: 0.215754] [ema: 0.998777] 
[Epoch 150/506] [Batch 0/38] [D loss: 0.353929] [G loss: 0.216065] [ema: 0.998785] 
[Epoch 151/506] [Batch 0/38] [D loss: 0.324821] [G loss: 0.253554] [ema: 0.998793] 
[Epoch 152/506] [Batch 0/38] [D loss: 0.317059] [G loss: 0.226431] [ema: 0.998801] 
[Epoch 153/506] [Batch 0/38] [D loss: 0.345495] [G loss: 0.195436] [ema: 0.998809] 
[Epoch 154/506] [Batch 0/38] [D loss: 0.328915] [G loss: 0.178547] [ema: 0.998816] 
[Epoch 155/506] [Batch 0/38] [D loss: 0.421162] [G loss: 0.177236] [ema: 0.998824] 
[Epoch 156/506] [Batch 0/38] [D loss: 0.352220] [G loss: 0.224728] [ema: 0.998831] 
[Epoch 157/506] [Batch 0/38] [D loss: 0.327016] [G loss: 0.227615] [ema: 0.998839] 
[Epoch 158/506] [Batch 0/38] [D loss: 0.366650] [G loss: 0.225928] [ema: 0.998846] 
[Epoch 159/506] [Batch 0/38] [D loss: 0.309630] [G loss: 0.219418] [ema: 0.998853] 
[Epoch 160/506] [Batch 0/38] [D loss: 0.383328] [G loss: 0.200622] [ema: 0.998861] 
[Epoch 161/506] [Batch 0/38] [D loss: 0.359076] [G loss: 0.239200] [ema: 0.998868] 
[Epoch 162/506] [Batch 0/38] [D loss: 0.354571] [G loss: 0.229038] [ema: 0.998875] 
[Epoch 163/506] [Batch 0/38] [D loss: 0.313286] [G loss: 0.196550] [ema: 0.998882] 
[Epoch 164/506] [Batch 0/38] [D loss: 0.388866] [G loss: 0.200176] [ema: 0.998888] 
[Epoch 165/506] [Batch 0/38] [D loss: 0.375799] [G loss: 0.211610] [ema: 0.998895] 
[Epoch 166/506] [Batch 0/38] [D loss: 0.286358] [G loss: 0.230854] [ema: 0.998902] 
[Epoch 167/506] [Batch 0/38] [D loss: 0.323302] [G loss: 0.274439] [ema: 0.998908] 
[Epoch 168/506] [Batch 0/38] [D loss: 0.345922] [G loss: 0.206964] [ema: 0.998915] 
[Epoch 169/506] [Batch 0/38] [D loss: 0.332218] [G loss: 0.208969] [ema: 0.998921] 
[Epoch 170/506] [Batch 0/38] [D loss: 0.352637] [G loss: 0.221443] [ema: 0.998928] 
[Epoch 171/506] [Batch 0/38] [D loss: 0.317184] [G loss: 0.265767] [ema: 0.998934] 
[Epoch 172/506] [Batch 0/38] [D loss: 0.290303] [G loss: 0.230585] [ema: 0.998940] 
[Epoch 173/506] [Batch 0/38] [D loss: 0.312473] [G loss: 0.224643] [ema: 0.998946] 
[Epoch 174/506] [Batch 0/38] [D loss: 0.368106] [G loss: 0.220604] [ema: 0.998952] 
[Epoch 175/506] [Batch 0/38] [D loss: 0.307871] [G loss: 0.191621] [ema: 0.998958] 
[Epoch 176/506] [Batch 0/38] [D loss: 0.348388] [G loss: 0.191444] [ema: 0.998964] 
[Epoch 177/506] [Batch 0/38] [D loss: 0.312189] [G loss: 0.203956] [ema: 0.998970] 
[Epoch 178/506] [Batch 0/38] [D loss: 0.346152] [G loss: 0.194059] [ema: 0.998976] 
[Epoch 179/506] [Batch 0/38] [D loss: 0.311212] [G loss: 0.212248] [ema: 0.998981] 
[Epoch 180/506] [Batch 0/38] [D loss: 0.332622] [G loss: 0.222892] [ema: 0.998987] 
[Epoch 181/506] [Batch 0/38] [D loss: 0.377091] [G loss: 0.205791] [ema: 0.998993] 
[Epoch 182/506] [Batch 0/38] [D loss: 0.322381] [G loss: 0.231380] [ema: 0.998998] 
[Epoch 183/506] [Batch 0/38] [D loss: 0.379408] [G loss: 0.224686] [ema: 0.999004] 
[Epoch 184/506] [Batch 0/38] [D loss: 0.283409] [G loss: 0.210585] [ema: 0.999009] 
[Epoch 185/506] [Batch 0/38] [D loss: 0.308926] [G loss: 0.248944] [ema: 0.999015] 
[Epoch 186/506] [Batch 0/38] [D loss: 0.317105] [G loss: 0.238237] [ema: 0.999020] 
[Epoch 187/506] [Batch 0/38] [D loss: 0.358819] [G loss: 0.221994] [ema: 0.999025] 
[Epoch 188/506] [Batch 0/38] [D loss: 0.321166] [G loss: 0.220312] [ema: 0.999030] 
[Epoch 189/506] [Batch 0/38] [D loss: 0.330845] [G loss: 0.237099] [ema: 0.999035] 
[Epoch 190/506] [Batch 0/38] [D loss: 0.298246] [G loss: 0.214249] [ema: 0.999040] 
[Epoch 191/506] [Batch 0/38] [D loss: 0.312831] [G loss: 0.240470] [ema: 0.999045] 
[Epoch 192/506] [Batch 0/38] [D loss: 0.305946] [G loss: 0.205509] [ema: 0.999050] 
[Epoch 193/506] [Batch 0/38] [D loss: 0.275382] [G loss: 0.215602] [ema: 0.999055] 
[Epoch 194/506] [Batch 0/38] [D loss: 0.266417] [G loss: 0.224298] [ema: 0.999060] 
[Epoch 195/506] [Batch 0/38] [D loss: 0.294068] [G loss: 0.220038] [ema: 0.999065] 
[Epoch 196/506] [Batch 0/38] [D loss: 0.329845] [G loss: 0.254632] [ema: 0.999070] 
[Epoch 197/506] [Batch 0/38] [D loss: 0.290943] [G loss: 0.223814] [ema: 0.999075] 
[Epoch 198/506] [Batch 0/38] [D loss: 0.316318] [G loss: 0.233234] [ema: 0.999079] 
[Epoch 199/506] [Batch 0/38] [D loss: 0.307437] [G loss: 0.206322] [ema: 0.999084] 
[Epoch 200/506] [Batch 0/38] [D loss: 0.284530] [G loss: 0.276101] [ema: 0.999088] 
[Epoch 201/506] [Batch 0/38] [D loss: 0.348811] [G loss: 0.218399] [ema: 0.999093] 
[Epoch 202/506] [Batch 0/38] [D loss: 0.327386] [G loss: 0.230452] [ema: 0.999097] 
[Epoch 203/506] [Batch 0/38] [D loss: 0.284875] [G loss: 0.228525] [ema: 0.999102] 




Saving checkpoint 3 in logs/Jumping_2024_10_02_18_34_58/Model




[Epoch 204/506] [Batch 0/38] [D loss: 0.311563] [G loss: 0.222762] [ema: 0.999106] 
[Epoch 205/506] [Batch 0/38] [D loss: 0.281727] [G loss: 0.235570] [ema: 0.999111] 
[Epoch 206/506] [Batch 0/38] [D loss: 0.325756] [G loss: 0.242298] [ema: 0.999115] 
[Epoch 207/506] [Batch 0/38] [D loss: 0.256555] [G loss: 0.244053] [ema: 0.999119] 
[Epoch 208/506] [Batch 0/38] [D loss: 0.312429] [G loss: 0.227961] [ema: 0.999123] 
[Epoch 209/506] [Batch 0/38] [D loss: 0.317694] [G loss: 0.188291] [ema: 0.999128] 
[Epoch 210/506] [Batch 0/38] [D loss: 0.314552] [G loss: 0.227899] [ema: 0.999132] 
[Epoch 211/506] [Batch 0/38] [D loss: 0.279376] [G loss: 0.250738] [ema: 0.999136] 
[Epoch 212/506] [Batch 0/38] [D loss: 0.339714] [G loss: 0.224370] [ema: 0.999140] 
[Epoch 213/506] [Batch 0/38] [D loss: 0.317883] [G loss: 0.227975] [ema: 0.999144] 
[Epoch 214/506] [Batch 0/38] [D loss: 0.328668] [G loss: 0.224933] [ema: 0.999148] 
[Epoch 215/506] [Batch 0/38] [D loss: 0.324308] [G loss: 0.210425] [ema: 0.999152] 
[Epoch 216/506] [Batch 0/38] [D loss: 0.265803] [G loss: 0.206853] [ema: 0.999156] 
[Epoch 217/506] [Batch 0/38] [D loss: 0.332025] [G loss: 0.232893] [ema: 0.999160] 
[Epoch 218/506] [Batch 0/38] [D loss: 0.333195] [G loss: 0.221078] [ema: 0.999164] 
[Epoch 219/506] [Batch 0/38] [D loss: 0.292130] [G loss: 0.240906] [ema: 0.999167] 
[Epoch 220/506] [Batch 0/38] [D loss: 0.310355] [G loss: 0.243294] [ema: 0.999171] 
[Epoch 221/506] [Batch 0/38] [D loss: 0.335559] [G loss: 0.229190] [ema: 0.999175] 
[Epoch 222/506] [Batch 0/38] [D loss: 0.281916] [G loss: 0.265248] [ema: 0.999179] 
[Epoch 223/506] [Batch 0/38] [D loss: 0.310634] [G loss: 0.234006] [ema: 0.999182] 
[Epoch 224/506] [Batch 0/38] [D loss: 0.288561] [G loss: 0.226822] [ema: 0.999186] 
[Epoch 225/506] [Batch 0/38] [D loss: 0.278021] [G loss: 0.228495] [ema: 0.999190] 
[Epoch 226/506] [Batch 0/38] [D loss: 0.337705] [G loss: 0.248140] [ema: 0.999193] 
[Epoch 227/506] [Batch 0/38] [D loss: 0.275062] [G loss: 0.241413] [ema: 0.999197] 
[Epoch 228/506] [Batch 0/38] [D loss: 0.311422] [G loss: 0.209732] [ema: 0.999200] 
[Epoch 229/506] [Batch 0/38] [D loss: 0.311258] [G loss: 0.222977] [ema: 0.999204] 
[Epoch 230/506] [Batch 0/38] [D loss: 0.308580] [G loss: 0.232820] [ema: 0.999207] 
[Epoch 231/506] [Batch 0/38] [D loss: 0.253790] [G loss: 0.270594] [ema: 0.999211] 
[Epoch 232/506] [Batch 0/38] [D loss: 0.304925] [G loss: 0.239261] [ema: 0.999214] 
[Epoch 233/506] [Batch 0/38] [D loss: 0.326502] [G loss: 0.247795] [ema: 0.999217] 
[Epoch 234/506] [Batch 0/38] [D loss: 0.279917] [G loss: 0.239163] [ema: 0.999221] 
[Epoch 235/506] [Batch 0/38] [D loss: 0.255785] [G loss: 0.246272] [ema: 0.999224] 
[Epoch 236/506] [Batch 0/38] [D loss: 0.317409] [G loss: 0.226167] [ema: 0.999227] 
[Epoch 237/506] [Batch 0/38] [D loss: 0.311602] [G loss: 0.231056] [ema: 0.999231] 
[Epoch 238/506] [Batch 0/38] [D loss: 0.316139] [G loss: 0.224257] [ema: 0.999234] 
[Epoch 239/506] [Batch 0/38] [D loss: 0.287494] [G loss: 0.240130] [ema: 0.999237] 
[Epoch 240/506] [Batch 0/38] [D loss: 0.294882] [G loss: 0.233511] [ema: 0.999240] 
[Epoch 241/506] [Batch 0/38] [D loss: 0.314174] [G loss: 0.213409] [ema: 0.999243] 
[Epoch 242/506] [Batch 0/38] [D loss: 0.321451] [G loss: 0.265207] [ema: 0.999247] 
[Epoch 243/506] [Batch 0/38] [D loss: 0.312752] [G loss: 0.191319] [ema: 0.999250] 
[Epoch 244/506] [Batch 0/38] [D loss: 0.270668] [G loss: 0.250875] [ema: 0.999253] 
[Epoch 245/506] [Batch 0/38] [D loss: 0.295128] [G loss: 0.228418] [ema: 0.999256] 
[Epoch 246/506] [Batch 0/38] [D loss: 0.301739] [G loss: 0.238415] [ema: 0.999259] 
[Epoch 247/506] [Batch 0/38] [D loss: 0.300482] [G loss: 0.207674] [ema: 0.999262] 
[Epoch 248/506] [Batch 0/38] [D loss: 0.347111] [G loss: 0.229332] [ema: 0.999265] 
[Epoch 249/506] [Batch 0/38] [D loss: 0.286289] [G loss: 0.222510] [ema: 0.999268] 
[Epoch 250/506] [Batch 0/38] [D loss: 0.301229] [G loss: 0.240590] [ema: 0.999271] 
[Epoch 251/506] [Batch 0/38] [D loss: 0.288731] [G loss: 0.252428] [ema: 0.999274] 
[Epoch 252/506] [Batch 0/38] [D loss: 0.332631] [G loss: 0.248533] [ema: 0.999276] 
[Epoch 253/506] [Batch 0/38] [D loss: 0.272472] [G loss: 0.227888] [ema: 0.999279] 
[Epoch 254/506] [Batch 0/38] [D loss: 0.305011] [G loss: 0.237495] [ema: 0.999282] 
[Epoch 255/506] [Batch 0/38] [D loss: 0.285189] [G loss: 0.260995] [ema: 0.999285] 
[Epoch 256/506] [Batch 0/38] [D loss: 0.294823] [G loss: 0.259018] [ema: 0.999288] 
[Epoch 257/506] [Batch 0/38] [D loss: 0.246813] [G loss: 0.272781] [ema: 0.999290] 
[Epoch 258/506] [Batch 0/38] [D loss: 0.315946] [G loss: 0.239526] [ema: 0.999293] 
[Epoch 259/506] [Batch 0/38] [D loss: 0.315441] [G loss: 0.236955] [ema: 0.999296] 
[Epoch 260/506] [Batch 0/38] [D loss: 0.272943] [G loss: 0.226468] [ema: 0.999299] 
[Epoch 261/506] [Batch 0/38] [D loss: 0.237840] [G loss: 0.245096] [ema: 0.999301] 
[Epoch 262/506] [Batch 0/38] [D loss: 0.369216] [G loss: 0.248085] [ema: 0.999304] 
[Epoch 263/506] [Batch 0/38] [D loss: 0.292964] [G loss: 0.253677] [ema: 0.999307] 
[Epoch 264/506] [Batch 0/38] [D loss: 0.295423] [G loss: 0.270185] [ema: 0.999309] 
[Epoch 265/506] [Batch 0/38] [D loss: 0.275338] [G loss: 0.253072] [ema: 0.999312] 
[Epoch 266/506] [Batch 0/38] [D loss: 0.273928] [G loss: 0.248378] [ema: 0.999314] 
[Epoch 267/506] [Batch 0/38] [D loss: 0.365789] [G loss: 0.219815] [ema: 0.999317] 
[Epoch 268/506] [Batch 0/38] [D loss: 0.320690] [G loss: 0.210604] [ema: 0.999320] 
[Epoch 269/506] [Batch 0/38] [D loss: 0.329609] [G loss: 0.251267] [ema: 0.999322] 
[Epoch 270/506] [Batch 0/38] [D loss: 0.353042] [G loss: 0.218191] [ema: 0.999325] 
[Epoch 271/506] [Batch 0/38] [D loss: 0.331418] [G loss: 0.214887] [ema: 0.999327] 
[Epoch 272/506] [Batch 0/38] [D loss: 0.305742] [G loss: 0.251806] [ema: 0.999330] 
[Epoch 273/506] [Batch 0/38] [D loss: 0.313492] [G loss: 0.233940] [ema: 0.999332] 
[Epoch 274/506] [Batch 0/38] [D loss: 0.311830] [G loss: 0.227992] [ema: 0.999335] 
[Epoch 275/506] [Batch 0/38] [D loss: 0.248094] [G loss: 0.242367] [ema: 0.999337] 
[Epoch 276/506] [Batch 0/38] [D loss: 0.320476] [G loss: 0.236848] [ema: 0.999339] 
[Epoch 277/506] [Batch 0/38] [D loss: 0.321104] [G loss: 0.221734] [ema: 0.999342] 
[Epoch 278/506] [Batch 0/38] [D loss: 0.319083] [G loss: 0.204798] [ema: 0.999344] 
[Epoch 279/506] [Batch 0/38] [D loss: 0.302271] [G loss: 0.267725] [ema: 0.999346] 
[Epoch 280/506] [Batch 0/38] [D loss: 0.283235] [G loss: 0.263399] [ema: 0.999349] 
[Epoch 281/506] [Batch 0/38] [D loss: 0.340006] [G loss: 0.231796] [ema: 0.999351] 
[Epoch 282/506] [Batch 0/38] [D loss: 0.243140] [G loss: 0.261166] [ema: 0.999353] 
[Epoch 283/506] [Batch 0/38] [D loss: 0.430103] [G loss: 0.205919] [ema: 0.999356] 
[Epoch 284/506] [Batch 0/38] [D loss: 0.280657] [G loss: 0.253751] [ema: 0.999358] 
[Epoch 285/506] [Batch 0/38] [D loss: 0.265981] [G loss: 0.253108] [ema: 0.999360] 
[Epoch 286/506] [Batch 0/38] [D loss: 0.305232] [G loss: 0.244977] [ema: 0.999362] 
[Epoch 287/506] [Batch 0/38] [D loss: 0.271600] [G loss: 0.253474] [ema: 0.999365] 
[Epoch 288/506] [Batch 0/38] [D loss: 0.307593] [G loss: 0.223543] [ema: 0.999367] 
[Epoch 289/506] [Batch 0/38] [D loss: 0.297220] [G loss: 0.243640] [ema: 0.999369] 
[Epoch 290/506] [Batch 0/38] [D loss: 0.285754] [G loss: 0.247406] [ema: 0.999371] 
[Epoch 291/506] [Batch 0/38] [D loss: 0.285503] [G loss: 0.230644] [ema: 0.999373] 
[Epoch 292/506] [Batch 0/38] [D loss: 0.279292] [G loss: 0.227355] [ema: 0.999376] 
[Epoch 293/506] [Batch 0/38] [D loss: 0.305107] [G loss: 0.249533] [ema: 0.999378] 
[Epoch 294/506] [Batch 0/38] [D loss: 0.245163] [G loss: 0.251128] [ema: 0.999380] 
[Epoch 295/506] [Batch 0/38] [D loss: 0.324651] [G loss: 0.216056] [ema: 0.999382] 
[Epoch 296/506] [Batch 0/38] [D loss: 0.261477] [G loss: 0.217531] [ema: 0.999384] 
[Epoch 297/506] [Batch 0/38] [D loss: 0.329835] [G loss: 0.220986] [ema: 0.999386] 
[Epoch 298/506] [Batch 0/38] [D loss: 0.291015] [G loss: 0.212323] [ema: 0.999388] 
[Epoch 299/506] [Batch 0/38] [D loss: 0.312012] [G loss: 0.205196] [ema: 0.999390] 
[Epoch 300/506] [Batch 0/38] [D loss: 0.315490] [G loss: 0.220874] [ema: 0.999392] 
[Epoch 301/506] [Batch 0/38] [D loss: 0.314891] [G loss: 0.236264] [ema: 0.999394] 
[Epoch 302/506] [Batch 0/38] [D loss: 0.292276] [G loss: 0.236628] [ema: 0.999396] 
[Epoch 303/506] [Batch 0/38] [D loss: 0.291108] [G loss: 0.249617] [ema: 0.999398] 
[Epoch 304/506] [Batch 0/38] [D loss: 0.279078] [G loss: 0.254289] [ema: 0.999400] 
[Epoch 305/506] [Batch 0/38] [D loss: 0.309552] [G loss: 0.211041] [ema: 0.999402] 




Saving checkpoint 4 in logs/Jumping_2024_10_02_18_34_58/Model




[Epoch 306/506] [Batch 0/38] [D loss: 0.279512] [G loss: 0.211032] [ema: 0.999404] 
[Epoch 307/506] [Batch 0/38] [D loss: 0.297062] [G loss: 0.207027] [ema: 0.999406] 
[Epoch 308/506] [Batch 0/38] [D loss: 0.336137] [G loss: 0.220610] [ema: 0.999408] 
[Epoch 309/506] [Batch 0/38] [D loss: 0.275072] [G loss: 0.238394] [ema: 0.999410] 
[Epoch 310/506] [Batch 0/38] [D loss: 0.329819] [G loss: 0.220343] [ema: 0.999412] 
[Epoch 311/506] [Batch 0/38] [D loss: 0.325304] [G loss: 0.245710] [ema: 0.999414] 
[Epoch 312/506] [Batch 0/38] [D loss: 0.266976] [G loss: 0.253567] [ema: 0.999416] 
[Epoch 313/506] [Batch 0/38] [D loss: 0.261288] [G loss: 0.249359] [ema: 0.999417] 
[Epoch 314/506] [Batch 0/38] [D loss: 0.290724] [G loss: 0.233414] [ema: 0.999419] 
[Epoch 315/506] [Batch 0/38] [D loss: 0.287258] [G loss: 0.229176] [ema: 0.999421] 
[Epoch 316/506] [Batch 0/38] [D loss: 0.275566] [G loss: 0.230596] [ema: 0.999423] 
[Epoch 317/506] [Batch 0/38] [D loss: 0.380551] [G loss: 0.216390] [ema: 0.999425] 
[Epoch 318/506] [Batch 0/38] [D loss: 0.302015] [G loss: 0.228032] [ema: 0.999427] 
[Epoch 319/506] [Batch 0/38] [D loss: 0.271670] [G loss: 0.231700] [ema: 0.999428] 
[Epoch 320/506] [Batch 0/38] [D loss: 0.294018] [G loss: 0.258244] [ema: 0.999430] 
[Epoch 321/506] [Batch 0/38] [D loss: 0.261425] [G loss: 0.253054] [ema: 0.999432] 
[Epoch 322/506] [Batch 0/38] [D loss: 0.350224] [G loss: 0.212619] [ema: 0.999434] 
[Epoch 323/506] [Batch 0/38] [D loss: 0.287939] [G loss: 0.238985] [ema: 0.999435] 
[Epoch 324/506] [Batch 0/38] [D loss: 0.270317] [G loss: 0.222915] [ema: 0.999437] 
[Epoch 325/506] [Batch 0/38] [D loss: 0.278541] [G loss: 0.243270] [ema: 0.999439] 
[Epoch 326/506] [Batch 0/38] [D loss: 0.342889] [G loss: 0.215416] [ema: 0.999441] 
[Epoch 327/506] [Batch 0/38] [D loss: 0.268558] [G loss: 0.240675] [ema: 0.999442] 
[Epoch 328/506] [Batch 0/38] [D loss: 0.269647] [G loss: 0.231291] [ema: 0.999444] 
[Epoch 329/506] [Batch 0/38] [D loss: 0.313591] [G loss: 0.234707] [ema: 0.999446] 
[Epoch 330/506] [Batch 0/38] [D loss: 0.277014] [G loss: 0.247015] [ema: 0.999447] 
[Epoch 331/506] [Batch 0/38] [D loss: 0.299171] [G loss: 0.204160] [ema: 0.999449] 
[Epoch 332/506] [Batch 0/38] [D loss: 0.295948] [G loss: 0.212139] [ema: 0.999451] 
[Epoch 333/506] [Batch 0/38] [D loss: 0.287747] [G loss: 0.255430] [ema: 0.999452] 
[Epoch 334/506] [Batch 0/38] [D loss: 0.314896] [G loss: 0.243225] [ema: 0.999454] 
[Epoch 335/506] [Batch 0/38] [D loss: 0.293616] [G loss: 0.231227] [ema: 0.999456] 
[Epoch 336/506] [Batch 0/38] [D loss: 0.264661] [G loss: 0.255563] [ema: 0.999457] 
[Epoch 337/506] [Batch 0/38] [D loss: 0.292344] [G loss: 0.243180] [ema: 0.999459] 
[Epoch 338/506] [Batch 0/38] [D loss: 0.343472] [G loss: 0.213674] [ema: 0.999460] 
[Epoch 339/506] [Batch 0/38] [D loss: 0.328046] [G loss: 0.211564] [ema: 0.999462] 
[Epoch 340/506] [Batch 0/38] [D loss: 0.286259] [G loss: 0.209311] [ema: 0.999464] 
[Epoch 341/506] [Batch 0/38] [D loss: 0.297990] [G loss: 0.227635] [ema: 0.999465] 
[Epoch 342/506] [Batch 0/38] [D loss: 0.272727] [G loss: 0.230535] [ema: 0.999467] 
[Epoch 343/506] [Batch 0/38] [D loss: 0.313772] [G loss: 0.250674] [ema: 0.999468] 
[Epoch 344/506] [Batch 0/38] [D loss: 0.321905] [G loss: 0.215925] [ema: 0.999470] 
[Epoch 345/506] [Batch 0/38] [D loss: 0.303318] [G loss: 0.208151] [ema: 0.999471] 
[Epoch 346/506] [Batch 0/38] [D loss: 0.251305] [G loss: 0.225081] [ema: 0.999473] 
[Epoch 347/506] [Batch 0/38] [D loss: 0.255733] [G loss: 0.248446] [ema: 0.999474] 
[Epoch 348/506] [Batch 0/38] [D loss: 0.259313] [G loss: 0.222794] [ema: 0.999476] 
[Epoch 349/506] [Batch 0/38] [D loss: 0.288379] [G loss: 0.215050] [ema: 0.999477] 
[Epoch 350/506] [Batch 0/38] [D loss: 0.323755] [G loss: 0.217654] [ema: 0.999479] 
[Epoch 351/506] [Batch 0/38] [D loss: 0.370910] [G loss: 0.227565] [ema: 0.999480] 
[Epoch 352/506] [Batch 0/38] [D loss: 0.281283] [G loss: 0.247817] [ema: 0.999482] 
[Epoch 353/506] [Batch 0/38] [D loss: 0.332713] [G loss: 0.198454] [ema: 0.999483] 
[Epoch 354/506] [Batch 0/38] [D loss: 0.343756] [G loss: 0.227493] [ema: 0.999485] 
[Epoch 355/506] [Batch 0/38] [D loss: 0.322513] [G loss: 0.217700] [ema: 0.999486] 
[Epoch 356/506] [Batch 0/38] [D loss: 0.286410] [G loss: 0.263982] [ema: 0.999488] 
[Epoch 357/506] [Batch 0/38] [D loss: 0.294996] [G loss: 0.254715] [ema: 0.999489] 
[Epoch 358/506] [Batch 0/38] [D loss: 0.253777] [G loss: 0.237848] [ema: 0.999491] 
[Epoch 359/506] [Batch 0/38] [D loss: 0.300119] [G loss: 0.232964] [ema: 0.999492] 
[Epoch 360/506] [Batch 0/38] [D loss: 0.287269] [G loss: 0.223023] [ema: 0.999493] 
[Epoch 361/506] [Batch 0/38] [D loss: 0.258192] [G loss: 0.237386] [ema: 0.999495] 
[Epoch 362/506] [Batch 0/38] [D loss: 0.309065] [G loss: 0.226377] [ema: 0.999496] 
[Epoch 363/506] [Batch 0/38] [D loss: 0.326484] [G loss: 0.236649] [ema: 0.999498] 
[Epoch 364/506] [Batch 0/38] [D loss: 0.289434] [G loss: 0.229450] [ema: 0.999499] 
[Epoch 365/506] [Batch 0/38] [D loss: 0.277915] [G loss: 0.209385] [ema: 0.999500] 
[Epoch 366/506] [Batch 0/38] [D loss: 0.319761] [G loss: 0.251147] [ema: 0.999502] 
[Epoch 367/506] [Batch 0/38] [D loss: 0.307548] [G loss: 0.250571] [ema: 0.999503] 
[Epoch 368/506] [Batch 0/38] [D loss: 0.296048] [G loss: 0.228300] [ema: 0.999504] 
[Epoch 369/506] [Batch 0/38] [D loss: 0.269331] [G loss: 0.229375] [ema: 0.999506] 
[Epoch 370/506] [Batch 0/38] [D loss: 0.260143] [G loss: 0.260377] [ema: 0.999507] 
[Epoch 371/506] [Batch 0/38] [D loss: 0.293926] [G loss: 0.226010] [ema: 0.999508] 
[Epoch 372/506] [Batch 0/38] [D loss: 0.309056] [G loss: 0.270130] [ema: 0.999510] 
[Epoch 373/506] [Batch 0/38] [D loss: 0.328947] [G loss: 0.238397] [ema: 0.999511] 
[Epoch 374/506] [Batch 0/38] [D loss: 0.300782] [G loss: 0.222106] [ema: 0.999512] 
[Epoch 375/506] [Batch 0/38] [D loss: 0.306380] [G loss: 0.247923] [ema: 0.999514] 
[Epoch 376/506] [Batch 0/38] [D loss: 0.291597] [G loss: 0.249092] [ema: 0.999515] 
[Epoch 377/506] [Batch 0/38] [D loss: 0.275020] [G loss: 0.240625] [ema: 0.999516] 
[Epoch 378/506] [Batch 0/38] [D loss: 0.283699] [G loss: 0.225073] [ema: 0.999518] 
[Epoch 379/506] [Batch 0/38] [D loss: 0.254226] [G loss: 0.265194] [ema: 0.999519] 
[Epoch 380/506] [Batch 0/38] [D loss: 0.289110] [G loss: 0.236812] [ema: 0.999520] 
[Epoch 381/506] [Batch 0/38] [D loss: 0.282829] [G loss: 0.223531] [ema: 0.999521] 
[Epoch 382/506] [Batch 0/38] [D loss: 0.272192] [G loss: 0.248660] [ema: 0.999523] 
[Epoch 383/506] [Batch 0/38] [D loss: 0.287243] [G loss: 0.260377] [ema: 0.999524] 
[Epoch 384/506] [Batch 0/38] [D loss: 0.284442] [G loss: 0.252271] [ema: 0.999525] 
[Epoch 385/506] [Batch 0/38] [D loss: 0.263320] [G loss: 0.247643] [ema: 0.999526] 
[Epoch 386/506] [Batch 0/38] [D loss: 0.248810] [G loss: 0.256743] [ema: 0.999528] 
[Epoch 387/506] [Batch 0/38] [D loss: 0.253248] [G loss: 0.245251] [ema: 0.999529] 
[Epoch 388/506] [Batch 0/38] [D loss: 0.304479] [G loss: 0.239760] [ema: 0.999530] 
[Epoch 389/506] [Batch 0/38] [D loss: 0.272583] [G loss: 0.248499] [ema: 0.999531] 
[Epoch 390/506] [Batch 0/38] [D loss: 0.290089] [G loss: 0.241817] [ema: 0.999532] 
[Epoch 391/506] [Batch 0/38] [D loss: 0.315274] [G loss: 0.233054] [ema: 0.999534] 
[Epoch 392/506] [Batch 0/38] [D loss: 0.291412] [G loss: 0.237437] [ema: 0.999535] 
[Epoch 393/506] [Batch 0/38] [D loss: 0.267825] [G loss: 0.247324] [ema: 0.999536] 
[Epoch 394/506] [Batch 0/38] [D loss: 0.308137] [G loss: 0.250491] [ema: 0.999537] 
[Epoch 395/506] [Batch 0/38] [D loss: 0.301162] [G loss: 0.242023] [ema: 0.999538] 
[Epoch 396/506] [Batch 0/38] [D loss: 0.297989] [G loss: 0.221190] [ema: 0.999539] 
[Epoch 397/506] [Batch 0/38] [D loss: 0.295412] [G loss: 0.225089] [ema: 0.999541] 
[Epoch 398/506] [Batch 0/38] [D loss: 0.276782] [G loss: 0.243635] [ema: 0.999542] 
[Epoch 399/506] [Batch 0/38] [D loss: 0.250221] [G loss: 0.257834] [ema: 0.999543] 
[Epoch 400/506] [Batch 0/38] [D loss: 0.297790] [G loss: 0.245580] [ema: 0.999544] 
[Epoch 401/506] [Batch 0/38] [D loss: 0.306277] [G loss: 0.226748] [ema: 0.999545] 
[Epoch 402/506] [Batch 0/38] [D loss: 0.298339] [G loss: 0.231689] [ema: 0.999546] 
[Epoch 403/506] [Batch 0/38] [D loss: 0.325005] [G loss: 0.220398] [ema: 0.999547] 
[Epoch 404/506] [Batch 0/38] [D loss: 0.306685] [G loss: 0.231269] [ema: 0.999549] 
[Epoch 405/506] [Batch 0/38] [D loss: 0.308425] [G loss: 0.259291] [ema: 0.999550] 
[Epoch 406/506] [Batch 0/38] [D loss: 0.247775] [G loss: 0.245859] [ema: 0.999551] 
[Epoch 407/506] [Batch 0/38] [D loss: 0.293826] [G loss: 0.237845] [ema: 0.999552] 




Saving checkpoint 5 in logs/Jumping_2024_10_02_18_34_58/Model




[Epoch 408/506] [Batch 0/38] [D loss: 0.260512] [G loss: 0.251124] [ema: 0.999553] 
[Epoch 409/506] [Batch 0/38] [D loss: 0.251256] [G loss: 0.249714] [ema: 0.999554] 
[Epoch 410/506] [Batch 0/38] [D loss: 0.369002] [G loss: 0.202827] [ema: 0.999555] 
[Epoch 411/506] [Batch 0/38] [D loss: 0.259454] [G loss: 0.251969] [ema: 0.999556] 
[Epoch 412/506] [Batch 0/38] [D loss: 0.270474] [G loss: 0.232562] [ema: 0.999557] 
[Epoch 413/506] [Batch 0/38] [D loss: 0.261550] [G loss: 0.245701] [ema: 0.999558] 
[Epoch 414/506] [Batch 0/38] [D loss: 0.272516] [G loss: 0.242940] [ema: 0.999560] 
[Epoch 415/506] [Batch 0/38] [D loss: 0.277785] [G loss: 0.229573] [ema: 0.999561] 
[Epoch 416/506] [Batch 0/38] [D loss: 0.274231] [G loss: 0.220639] [ema: 0.999562] 
[Epoch 417/506] [Batch 0/38] [D loss: 0.235778] [G loss: 0.278671] [ema: 0.999563] 
[Epoch 418/506] [Batch 0/38] [D loss: 0.273220] [G loss: 0.272635] [ema: 0.999564] 
[Epoch 419/506] [Batch 0/38] [D loss: 0.294797] [G loss: 0.256427] [ema: 0.999565] 
[Epoch 420/506] [Batch 0/38] [D loss: 0.278516] [G loss: 0.258871] [ema: 0.999566] 
[Epoch 421/506] [Batch 0/38] [D loss: 0.286264] [G loss: 0.232910] [ema: 0.999567] 
[Epoch 422/506] [Batch 0/38] [D loss: 0.288355] [G loss: 0.220703] [ema: 0.999568] 
[Epoch 423/506] [Batch 0/38] [D loss: 0.284838] [G loss: 0.232935] [ema: 0.999569] 
[Epoch 424/506] [Batch 0/38] [D loss: 0.326040] [G loss: 0.239890] [ema: 0.999570] 
[Epoch 425/506] [Batch 0/38] [D loss: 0.300068] [G loss: 0.203163] [ema: 0.999571] 
[Epoch 426/506] [Batch 0/38] [D loss: 0.276294] [G loss: 0.252330] [ema: 0.999572] 
[Epoch 427/506] [Batch 0/38] [D loss: 0.262121] [G loss: 0.236330] [ema: 0.999573] 
[Epoch 428/506] [Batch 0/38] [D loss: 0.293131] [G loss: 0.242052] [ema: 0.999574] 
[Epoch 429/506] [Batch 0/38] [D loss: 0.270408] [G loss: 0.249022] [ema: 0.999575] 
[Epoch 430/506] [Batch 0/38] [D loss: 0.308993] [G loss: 0.222962] [ema: 0.999576] 
[Epoch 431/506] [Batch 0/38] [D loss: 0.296689] [G loss: 0.241600] [ema: 0.999577] 
[Epoch 432/506] [Batch 0/38] [D loss: 0.297356] [G loss: 0.259969] [ema: 0.999578] 
[Epoch 433/506] [Batch 0/38] [D loss: 0.286941] [G loss: 0.242836] [ema: 0.999579] 
[Epoch 434/506] [Batch 0/38] [D loss: 0.249426] [G loss: 0.229687] [ema: 0.999580] 
[Epoch 435/506] [Batch 0/38] [D loss: 0.351518] [G loss: 0.224502] [ema: 0.999581] 
[Epoch 436/506] [Batch 0/38] [D loss: 0.301428] [G loss: 0.238628] [ema: 0.999582] 
[Epoch 437/506] [Batch 0/38] [D loss: 0.315598] [G loss: 0.241339] [ema: 0.999583] 
[Epoch 438/506] [Batch 0/38] [D loss: 0.272265] [G loss: 0.257771] [ema: 0.999584] 
[Epoch 439/506] [Batch 0/38] [D loss: 0.298080] [G loss: 0.233399] [ema: 0.999585] 
[Epoch 440/506] [Batch 0/38] [D loss: 0.294984] [G loss: 0.238625] [ema: 0.999586] 
[Epoch 441/506] [Batch 0/38] [D loss: 0.268531] [G loss: 0.267488] [ema: 0.999586] 
[Epoch 442/506] [Batch 0/38] [D loss: 0.274911] [G loss: 0.225673] [ema: 0.999587] 
[Epoch 443/506] [Batch 0/38] [D loss: 0.314447] [G loss: 0.237083] [ema: 0.999588] 
[Epoch 444/506] [Batch 0/38] [D loss: 0.281776] [G loss: 0.217528] [ema: 0.999589] 
[Epoch 445/506] [Batch 0/38] [D loss: 0.230560] [G loss: 0.272200] [ema: 0.999590] 
[Epoch 446/506] [Batch 0/38] [D loss: 0.281394] [G loss: 0.226425] [ema: 0.999591] 
[Epoch 447/506] [Batch 0/38] [D loss: 0.325479] [G loss: 0.211057] [ema: 0.999592] 
[Epoch 448/506] [Batch 0/38] [D loss: 0.260384] [G loss: 0.255436] [ema: 0.999593] 
[Epoch 449/506] [Batch 0/38] [D loss: 0.324881] [G loss: 0.251722] [ema: 0.999594] 
[Epoch 450/506] [Batch 0/38] [D loss: 0.275014] [G loss: 0.220425] [ema: 0.999595] 
[Epoch 451/506] [Batch 0/38] [D loss: 0.266321] [G loss: 0.234200] [ema: 0.999596] 
[Epoch 452/506] [Batch 0/38] [D loss: 0.298397] [G loss: 0.225143] [ema: 0.999597] 
[Epoch 453/506] [Batch 0/38] [D loss: 0.300104] [G loss: 0.242364] [ema: 0.999597] 
[Epoch 454/506] [Batch 0/38] [D loss: 0.270901] [G loss: 0.250707] [ema: 0.999598] 
[Epoch 455/506] [Batch 0/38] [D loss: 0.326216] [G loss: 0.223998] [ema: 0.999599] 
[Epoch 456/506] [Batch 0/38] [D loss: 0.288309] [G loss: 0.222115] [ema: 0.999600] 
[Epoch 457/506] [Batch 0/38] [D loss: 0.286512] [G loss: 0.233248] [ema: 0.999601] 
[Epoch 458/506] [Batch 0/38] [D loss: 0.291754] [G loss: 0.246944] [ema: 0.999602] 
[Epoch 459/506] [Batch 0/38] [D loss: 0.290771] [G loss: 0.240707] [ema: 0.999603] 
[Epoch 460/506] [Batch 0/38] [D loss: 0.257791] [G loss: 0.247246] [ema: 0.999604] 
[Epoch 461/506] [Batch 0/38] [D loss: 0.325185] [G loss: 0.246311] [ema: 0.999604] 
[Epoch 462/506] [Batch 0/38] [D loss: 0.278864] [G loss: 0.216724] [ema: 0.999605] 
[Epoch 463/506] [Batch 0/38] [D loss: 0.263183] [G loss: 0.237969] [ema: 0.999606] 
[Epoch 464/506] [Batch 0/38] [D loss: 0.278914] [G loss: 0.244506] [ema: 0.999607] 
[Epoch 465/506] [Batch 0/38] [D loss: 0.294264] [G loss: 0.258661] [ema: 0.999608] 
[Epoch 466/506] [Batch 0/38] [D loss: 0.289682] [G loss: 0.218985] [ema: 0.999609] 
[Epoch 467/506] [Batch 0/38] [D loss: 0.312490] [G loss: 0.225758] [ema: 0.999609] 
[Epoch 468/506] [Batch 0/38] [D loss: 0.264405] [G loss: 0.240287] [ema: 0.999610] 
[Epoch 469/506] [Batch 0/38] [D loss: 0.381724] [G loss: 0.231702] [ema: 0.999611] 
[Epoch 470/506] [Batch 0/38] [D loss: 0.296496] [G loss: 0.224303] [ema: 0.999612] 
[Epoch 471/506] [Batch 0/38] [D loss: 0.255081] [G loss: 0.251713] [ema: 0.999613] 
[Epoch 472/506] [Batch 0/38] [D loss: 0.293525] [G loss: 0.254798] [ema: 0.999614] 
[Epoch 473/506] [Batch 0/38] [D loss: 0.278975] [G loss: 0.228835] [ema: 0.999614] 
[Epoch 474/506] [Batch 0/38] [D loss: 0.338411] [G loss: 0.218572] [ema: 0.999615] 
[Epoch 475/506] [Batch 0/38] [D loss: 0.295637] [G loss: 0.227170] [ema: 0.999616] 
[Epoch 476/506] [Batch 0/38] [D loss: 0.329451] [G loss: 0.223052] [ema: 0.999617] 
[Epoch 477/506] [Batch 0/38] [D loss: 0.282934] [G loss: 0.240974] [ema: 0.999618] 
[Epoch 478/506] [Batch 0/38] [D loss: 0.293795] [G loss: 0.244710] [ema: 0.999618] 
[Epoch 479/506] [Batch 0/38] [D loss: 0.281549] [G loss: 0.243449] [ema: 0.999619] 
[Epoch 480/506] [Batch 0/38] [D loss: 0.257806] [G loss: 0.245530] [ema: 0.999620] 
[Epoch 481/506] [Batch 0/38] [D loss: 0.294028] [G loss: 0.237161] [ema: 0.999621] 
[Epoch 482/506] [Batch 0/38] [D loss: 0.274733] [G loss: 0.241532] [ema: 0.999622] 
[Epoch 483/506] [Batch 0/38] [D loss: 0.280191] [G loss: 0.245137] [ema: 0.999622] 
[Epoch 484/506] [Batch 0/38] [D loss: 0.275317] [G loss: 0.245900] [ema: 0.999623] 
[Epoch 485/506] [Batch 0/38] [D loss: 0.263759] [G loss: 0.240502] [ema: 0.999624] 
[Epoch 486/506] [Batch 0/38] [D loss: 0.261429] [G loss: 0.242943] [ema: 0.999625] 
[Epoch 487/506] [Batch 0/38] [D loss: 0.276673] [G loss: 0.247460] [ema: 0.999626] 
[Epoch 488/506] [Batch 0/38] [D loss: 0.272906] [G loss: 0.247232] [ema: 0.999626] 
[Epoch 489/506] [Batch 0/38] [D loss: 0.311514] [G loss: 0.227893] [ema: 0.999627] 
[Epoch 490/506] [Batch 0/38] [D loss: 0.335631] [G loss: 0.246231] [ema: 0.999628] 
[Epoch 491/506] [Batch 0/38] [D loss: 0.278876] [G loss: 0.247223] [ema: 0.999629] 
[Epoch 492/506] [Batch 0/38] [D loss: 0.321181] [G loss: 0.265146] [ema: 0.999629] 
[Epoch 493/506] [Batch 0/38] [D loss: 0.295209] [G loss: 0.226833] [ema: 0.999630] 
[Epoch 494/506] [Batch 0/38] [D loss: 0.288760] [G loss: 0.258505] [ema: 0.999631] 
[Epoch 495/506] [Batch 0/38] [D loss: 0.253158] [G loss: 0.248911] [ema: 0.999632] 
[Epoch 496/506] [Batch 0/38] [D loss: 0.276010] [G loss: 0.255951] [ema: 0.999632] 
[Epoch 497/506] [Batch 0/38] [D loss: 0.284477] [G loss: 0.231501] [ema: 0.999633] 
[Epoch 498/506] [Batch 0/38] [D loss: 0.365250] [G loss: 0.232458] [ema: 0.999634] 
[Epoch 499/506] [Batch 0/38] [D loss: 0.280550] [G loss: 0.220403] [ema: 0.999635] 
[Epoch 500/506] [Batch 0/38] [D loss: 0.279146] [G loss: 0.244862] [ema: 0.999635] 
[Epoch 501/506] [Batch 0/38] [D loss: 0.294358] [G loss: 0.213669] [ema: 0.999636] 
[Epoch 502/506] [Batch 0/38] [D loss: 0.275058] [G loss: 0.227862] [ema: 0.999637] 
[Epoch 503/506] [Batch 0/38] [D loss: 0.268247] [G loss: 0.233512] [ema: 0.999637] 
[Epoch 504/506] [Batch 0/38] [D loss: 0.259207] [G loss: 0.240060] [ema: 0.999638] 
[Epoch 505/506] [Batch 0/38] [D loss: 0.284468] [G loss: 0.243734] [ema: 0.999639] 
