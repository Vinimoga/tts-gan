Generator(
  (l1): Linear(in_features=100, out_features=1500, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
x_train shape is (6055, 3, 1, 150), x_test shape is (1524, 3, 1, 150)
y_train shape is (6055,), y_test shape is (1524,)
return single class data and labels, class is Jumping
train_data shape is (600, 3, 1, 150), test_data shape is (146, 3, 1, 150)
train label shape is (600,), test data shape is (146,)
x_train shape is (6055, 3, 1, 150), x_test shape is (1524, 3, 1, 150)
y_train shape is (6055,), y_test shape is (1524,)
return single class data and labels, class is Jumping
train_data shape is (600, 3, 1, 150), test_data shape is (146, 3, 1, 150)
train label shape is (600,), test data shape is (146,)
38
Epochs between ckechpoint: 264




Saving checkpoint 1 in logs/Jumping_2024_10_02_17_29_42/Model




[Epoch 0/1316] [Batch 0/38] [D loss: 1.171687] [G loss: 0.484607] [ema: 0.000000] 
[Epoch 1/1316] [Batch 0/38] [D loss: 0.434924] [G loss: 0.263207] [ema: 0.833262] 
[Epoch 2/1316] [Batch 0/38] [D loss: 0.642551] [G loss: 0.098299] [ema: 0.912832] 
[Epoch 3/1316] [Batch 0/38] [D loss: 0.459852] [G loss: 0.185093] [ema: 0.941009] 
[Epoch 4/1316] [Batch 0/38] [D loss: 0.426422] [G loss: 0.209906] [ema: 0.955422] 
[Epoch 5/1316] [Batch 0/38] [D loss: 0.495488] [G loss: 0.187031] [ema: 0.964176] 
[Epoch 6/1316] [Batch 0/38] [D loss: 0.412122] [G loss: 0.179379] [ema: 0.970056] 
[Epoch 7/1316] [Batch 0/38] [D loss: 0.371779] [G loss: 0.201947] [ema: 0.974278] 
[Epoch 8/1316] [Batch 0/38] [D loss: 0.327737] [G loss: 0.234345] [ema: 0.977457] 
[Epoch 9/1316] [Batch 0/38] [D loss: 0.301219] [G loss: 0.254388] [ema: 0.979937] 
[Epoch 10/1316] [Batch 0/38] [D loss: 0.357794] [G loss: 0.271704] [ema: 0.981925] 
[Epoch 11/1316] [Batch 0/38] [D loss: 0.415879] [G loss: 0.202785] [ema: 0.983554] 
[Epoch 12/1316] [Batch 0/38] [D loss: 0.371880] [G loss: 0.182273] [ema: 0.984914] 
[Epoch 13/1316] [Batch 0/38] [D loss: 0.320073] [G loss: 0.261729] [ema: 0.986067] 
[Epoch 14/1316] [Batch 0/38] [D loss: 0.309013] [G loss: 0.235760] [ema: 0.987055] 
[Epoch 15/1316] [Batch 0/38] [D loss: 0.334038] [G loss: 0.247576] [ema: 0.987913] 
[Epoch 16/1316] [Batch 0/38] [D loss: 0.327794] [G loss: 0.233780] [ema: 0.988664] 
[Epoch 17/1316] [Batch 0/38] [D loss: 0.419595] [G loss: 0.229574] [ema: 0.989328] 
[Epoch 18/1316] [Batch 0/38] [D loss: 0.325347] [G loss: 0.249644] [ema: 0.989917] 
[Epoch 19/1316] [Batch 0/38] [D loss: 0.401022] [G loss: 0.185639] [ema: 0.990446] 
[Epoch 20/1316] [Batch 0/38] [D loss: 0.577780] [G loss: 0.169510] [ema: 0.990921] 
[Epoch 21/1316] [Batch 0/38] [D loss: 0.259693] [G loss: 0.270769] [ema: 0.991352] 
[Epoch 22/1316] [Batch 0/38] [D loss: 0.398194] [G loss: 0.193877] [ema: 0.991743] 
[Epoch 23/1316] [Batch 0/38] [D loss: 0.441244] [G loss: 0.183592] [ema: 0.992101] 
[Epoch 24/1316] [Batch 0/38] [D loss: 0.350403] [G loss: 0.225795] [ema: 0.992429] 
[Epoch 25/1316] [Batch 0/38] [D loss: 0.359370] [G loss: 0.240342] [ema: 0.992730] 
[Epoch 26/1316] [Batch 0/38] [D loss: 0.369790] [G loss: 0.222185] [ema: 0.993009] 
[Epoch 27/1316] [Batch 0/38] [D loss: 0.381418] [G loss: 0.201079] [ema: 0.993267] 
[Epoch 28/1316] [Batch 0/38] [D loss: 0.407332] [G loss: 0.195163] [ema: 0.993507] 
[Epoch 29/1316] [Batch 0/38] [D loss: 0.361901] [G loss: 0.204686] [ema: 0.993730] 
[Epoch 30/1316] [Batch 0/38] [D loss: 0.370682] [G loss: 0.187934] [ema: 0.993938] 
[Epoch 31/1316] [Batch 0/38] [D loss: 0.493985] [G loss: 0.185053] [ema: 0.994133] 
[Epoch 32/1316] [Batch 0/38] [D loss: 0.360560] [G loss: 0.239482] [ema: 0.994316] 
[Epoch 33/1316] [Batch 0/38] [D loss: 0.436667] [G loss: 0.165329] [ema: 0.994488] 
[Epoch 34/1316] [Batch 0/38] [D loss: 0.530670] [G loss: 0.164774] [ema: 0.994649] 
[Epoch 35/1316] [Batch 0/38] [D loss: 0.503635] [G loss: 0.167542] [ema: 0.994802] 
[Epoch 36/1316] [Batch 0/38] [D loss: 0.488731] [G loss: 0.168457] [ema: 0.994946] 
[Epoch 37/1316] [Batch 0/38] [D loss: 0.395494] [G loss: 0.175269] [ema: 0.995082] 
[Epoch 38/1316] [Batch 0/38] [D loss: 0.608252] [G loss: 0.141784] [ema: 0.995211] 
[Epoch 39/1316] [Batch 0/38] [D loss: 0.316886] [G loss: 0.236504] [ema: 0.995334] 
[Epoch 40/1316] [Batch 0/38] [D loss: 0.490260] [G loss: 0.149145] [ema: 0.995450] 
[Epoch 41/1316] [Batch 0/38] [D loss: 0.430232] [G loss: 0.165144] [ema: 0.995561] 
[Epoch 42/1316] [Batch 0/38] [D loss: 0.404594] [G loss: 0.228929] [ema: 0.995666] 
[Epoch 43/1316] [Batch 0/38] [D loss: 0.430353] [G loss: 0.210953] [ema: 0.995767] 
[Epoch 44/1316] [Batch 0/38] [D loss: 0.536694] [G loss: 0.155294] [ema: 0.995863] 
[Epoch 45/1316] [Batch 0/38] [D loss: 0.412167] [G loss: 0.206301] [ema: 0.995955] 
[Epoch 46/1316] [Batch 0/38] [D loss: 0.495159] [G loss: 0.193300] [ema: 0.996042] 
[Epoch 47/1316] [Batch 0/38] [D loss: 0.508914] [G loss: 0.182021] [ema: 0.996127] 
[Epoch 48/1316] [Batch 0/38] [D loss: 0.379399] [G loss: 0.211389] [ema: 0.996207] 
[Epoch 49/1316] [Batch 0/38] [D loss: 0.398351] [G loss: 0.191018] [ema: 0.996284] 
[Epoch 50/1316] [Batch 0/38] [D loss: 0.407910] [G loss: 0.221136] [ema: 0.996359] 
[Epoch 51/1316] [Batch 0/38] [D loss: 0.431536] [G loss: 0.176339] [ema: 0.996430] 
[Epoch 52/1316] [Batch 0/38] [D loss: 0.451813] [G loss: 0.193316] [ema: 0.996498] 
[Epoch 53/1316] [Batch 0/38] [D loss: 0.430713] [G loss: 0.175709] [ema: 0.996564] 
[Epoch 54/1316] [Batch 0/38] [D loss: 0.507700] [G loss: 0.160477] [ema: 0.996628] 
[Epoch 55/1316] [Batch 0/38] [D loss: 0.388019] [G loss: 0.235260] [ema: 0.996689] 
[Epoch 56/1316] [Batch 0/38] [D loss: 0.480567] [G loss: 0.168749] [ema: 0.996748] 
[Epoch 57/1316] [Batch 0/38] [D loss: 0.369739] [G loss: 0.221748] [ema: 0.996805] 
[Epoch 58/1316] [Batch 0/38] [D loss: 0.430016] [G loss: 0.246836] [ema: 0.996860] 
[Epoch 59/1316] [Batch 0/38] [D loss: 0.447000] [G loss: 0.188099] [ema: 0.996913] 
[Epoch 60/1316] [Batch 0/38] [D loss: 0.346718] [G loss: 0.204527] [ema: 0.996964] 
[Epoch 61/1316] [Batch 0/38] [D loss: 0.366899] [G loss: 0.180926] [ema: 0.997014] 
[Epoch 62/1316] [Batch 0/38] [D loss: 0.436342] [G loss: 0.184910] [ema: 0.997062] 
[Epoch 63/1316] [Batch 0/38] [D loss: 0.389879] [G loss: 0.198050] [ema: 0.997109] 
[Epoch 64/1316] [Batch 0/38] [D loss: 0.374760] [G loss: 0.194040] [ema: 0.997154] 
[Epoch 65/1316] [Batch 0/38] [D loss: 0.362941] [G loss: 0.206046] [ema: 0.997198] 
[Epoch 66/1316] [Batch 0/38] [D loss: 0.465429] [G loss: 0.220231] [ema: 0.997240] 
[Epoch 67/1316] [Batch 0/38] [D loss: 0.445393] [G loss: 0.203215] [ema: 0.997281] 
[Epoch 68/1316] [Batch 0/38] [D loss: 0.371544] [G loss: 0.233933] [ema: 0.997321] 
[Epoch 69/1316] [Batch 0/38] [D loss: 0.435014] [G loss: 0.174935] [ema: 0.997360] 
[Epoch 70/1316] [Batch 0/38] [D loss: 0.342344] [G loss: 0.240159] [ema: 0.997398] 
[Epoch 71/1316] [Batch 0/38] [D loss: 0.406432] [G loss: 0.203642] [ema: 0.997434] 
[Epoch 72/1316] [Batch 0/38] [D loss: 0.443370] [G loss: 0.275570] [ema: 0.997470] 
[Epoch 73/1316] [Batch 0/38] [D loss: 0.422813] [G loss: 0.191206] [ema: 0.997504] 
[Epoch 74/1316] [Batch 0/38] [D loss: 0.313663] [G loss: 0.237924] [ema: 0.997538] 
[Epoch 75/1316] [Batch 0/38] [D loss: 0.429014] [G loss: 0.188298] [ema: 0.997571] 
[Epoch 76/1316] [Batch 0/38] [D loss: 0.395866] [G loss: 0.189145] [ema: 0.997603] 
[Epoch 77/1316] [Batch 0/38] [D loss: 0.386858] [G loss: 0.211115] [ema: 0.997634] 
[Epoch 78/1316] [Batch 0/38] [D loss: 0.351372] [G loss: 0.195004] [ema: 0.997664] 
[Epoch 79/1316] [Batch 0/38] [D loss: 0.349268] [G loss: 0.231809] [ema: 0.997694] 
[Epoch 80/1316] [Batch 0/38] [D loss: 0.376601] [G loss: 0.193401] [ema: 0.997723] 
[Epoch 81/1316] [Batch 0/38] [D loss: 0.360295] [G loss: 0.190840] [ema: 0.997751] 
[Epoch 82/1316] [Batch 0/38] [D loss: 0.370590] [G loss: 0.201840] [ema: 0.997778] 
[Epoch 83/1316] [Batch 0/38] [D loss: 0.332142] [G loss: 0.237856] [ema: 0.997805] 
[Epoch 84/1316] [Batch 0/38] [D loss: 0.370668] [G loss: 0.200935] [ema: 0.997831] 
[Epoch 85/1316] [Batch 0/38] [D loss: 0.389958] [G loss: 0.219299] [ema: 0.997856] 
[Epoch 86/1316] [Batch 0/38] [D loss: 0.339032] [G loss: 0.248640] [ema: 0.997881] 
[Epoch 87/1316] [Batch 0/38] [D loss: 0.397004] [G loss: 0.259962] [ema: 0.997906] 
[Epoch 88/1316] [Batch 0/38] [D loss: 0.382576] [G loss: 0.199622] [ema: 0.997929] 
[Epoch 89/1316] [Batch 0/38] [D loss: 0.420058] [G loss: 0.182162] [ema: 0.997953] 
[Epoch 90/1316] [Batch 0/38] [D loss: 0.413090] [G loss: 0.189604] [ema: 0.997975] 
[Epoch 91/1316] [Batch 0/38] [D loss: 0.294389] [G loss: 0.215887] [ema: 0.997998] 
[Epoch 92/1316] [Batch 0/38] [D loss: 0.344186] [G loss: 0.233556] [ema: 0.998019] 
[Epoch 93/1316] [Batch 0/38] [D loss: 0.381648] [G loss: 0.230626] [ema: 0.998041] 
[Epoch 94/1316] [Batch 0/38] [D loss: 0.352431] [G loss: 0.218017] [ema: 0.998061] 
[Epoch 95/1316] [Batch 0/38] [D loss: 0.345718] [G loss: 0.237274] [ema: 0.998082] 
[Epoch 96/1316] [Batch 0/38] [D loss: 0.333718] [G loss: 0.215717] [ema: 0.998102] 
[Epoch 97/1316] [Batch 0/38] [D loss: 0.372217] [G loss: 0.192064] [ema: 0.998121] 
[Epoch 98/1316] [Batch 0/38] [D loss: 0.419151] [G loss: 0.216554] [ema: 0.998140] 
[Epoch 99/1316] [Batch 0/38] [D loss: 0.384449] [G loss: 0.206139] [ema: 0.998159] 
[Epoch 100/1316] [Batch 0/38] [D loss: 0.399042] [G loss: 0.234903] [ema: 0.998178] 
[Epoch 101/1316] [Batch 0/38] [D loss: 0.311252] [G loss: 0.257149] [ema: 0.998196] 
[Epoch 102/1316] [Batch 0/38] [D loss: 0.369418] [G loss: 0.206076] [ema: 0.998213] 
[Epoch 103/1316] [Batch 0/38] [D loss: 0.339726] [G loss: 0.224975] [ema: 0.998231] 
[Epoch 104/1316] [Batch 0/38] [D loss: 0.381711] [G loss: 0.205350] [ema: 0.998248] 
[Epoch 105/1316] [Batch 0/38] [D loss: 0.373848] [G loss: 0.209633] [ema: 0.998264] 
[Epoch 106/1316] [Batch 0/38] [D loss: 0.332289] [G loss: 0.212612] [ema: 0.998281] 
[Epoch 107/1316] [Batch 0/38] [D loss: 0.326894] [G loss: 0.210485] [ema: 0.998297] 
[Epoch 108/1316] [Batch 0/38] [D loss: 0.343842] [G loss: 0.203485] [ema: 0.998312] 
[Epoch 109/1316] [Batch 0/38] [D loss: 0.339717] [G loss: 0.220157] [ema: 0.998328] 
[Epoch 110/1316] [Batch 0/38] [D loss: 0.384682] [G loss: 0.201162] [ema: 0.998343] 
[Epoch 111/1316] [Batch 0/38] [D loss: 0.421282] [G loss: 0.203615] [ema: 0.998358] 
[Epoch 112/1316] [Batch 0/38] [D loss: 0.339868] [G loss: 0.219228] [ema: 0.998373] 
[Epoch 113/1316] [Batch 0/38] [D loss: 0.275812] [G loss: 0.241178] [ema: 0.998387] 
[Epoch 114/1316] [Batch 0/38] [D loss: 0.383337] [G loss: 0.225134] [ema: 0.998401] 
[Epoch 115/1316] [Batch 0/38] [D loss: 0.314725] [G loss: 0.223467] [ema: 0.998415] 
[Epoch 116/1316] [Batch 0/38] [D loss: 0.443788] [G loss: 0.235110] [ema: 0.998429] 
[Epoch 117/1316] [Batch 0/38] [D loss: 0.343868] [G loss: 0.209330] [ema: 0.998442] 
[Epoch 118/1316] [Batch 0/38] [D loss: 0.354552] [G loss: 0.225685] [ema: 0.998455] 
[Epoch 119/1316] [Batch 0/38] [D loss: 0.361979] [G loss: 0.222748] [ema: 0.998468] 
[Epoch 120/1316] [Batch 0/38] [D loss: 0.363276] [G loss: 0.182649] [ema: 0.998481] 
[Epoch 121/1316] [Batch 0/38] [D loss: 0.360530] [G loss: 0.196974] [ema: 0.998494] 
[Epoch 122/1316] [Batch 0/38] [D loss: 0.356234] [G loss: 0.237091] [ema: 0.998506] 
[Epoch 123/1316] [Batch 0/38] [D loss: 0.335364] [G loss: 0.215167] [ema: 0.998518] 
[Epoch 124/1316] [Batch 0/38] [D loss: 0.354906] [G loss: 0.223944] [ema: 0.998530] 
[Epoch 125/1316] [Batch 0/38] [D loss: 0.392487] [G loss: 0.191374] [ema: 0.998542] 
[Epoch 126/1316] [Batch 0/38] [D loss: 0.314817] [G loss: 0.223751] [ema: 0.998553] 
[Epoch 127/1316] [Batch 0/38] [D loss: 0.366868] [G loss: 0.212865] [ema: 0.998565] 
[Epoch 128/1316] [Batch 0/38] [D loss: 0.374974] [G loss: 0.244961] [ema: 0.998576] 
[Epoch 129/1316] [Batch 0/38] [D loss: 0.382708] [G loss: 0.225987] [ema: 0.998587] 
[Epoch 130/1316] [Batch 0/38] [D loss: 0.360325] [G loss: 0.208878] [ema: 0.998598] 
[Epoch 131/1316] [Batch 0/38] [D loss: 0.369722] [G loss: 0.221973] [ema: 0.998609] 
[Epoch 132/1316] [Batch 0/38] [D loss: 0.331180] [G loss: 0.216997] [ema: 0.998619] 
[Epoch 133/1316] [Batch 0/38] [D loss: 0.347770] [G loss: 0.227820] [ema: 0.998629] 
[Epoch 134/1316] [Batch 0/38] [D loss: 0.384721] [G loss: 0.201794] [ema: 0.998640] 
[Epoch 135/1316] [Batch 0/38] [D loss: 0.380763] [G loss: 0.254032] [ema: 0.998650] 
[Epoch 136/1316] [Batch 0/38] [D loss: 0.351963] [G loss: 0.203860] [ema: 0.998660] 
[Epoch 137/1316] [Batch 0/38] [D loss: 0.322637] [G loss: 0.207594] [ema: 0.998669] 
[Epoch 138/1316] [Batch 0/38] [D loss: 0.358497] [G loss: 0.199109] [ema: 0.998679] 
[Epoch 139/1316] [Batch 0/38] [D loss: 0.305841] [G loss: 0.220775] [ema: 0.998689] 
[Epoch 140/1316] [Batch 0/38] [D loss: 0.360268] [G loss: 0.234266] [ema: 0.998698] 
[Epoch 141/1316] [Batch 0/38] [D loss: 0.315222] [G loss: 0.203515] [ema: 0.998707] 
[Epoch 142/1316] [Batch 0/38] [D loss: 0.328728] [G loss: 0.224140] [ema: 0.998716] 
[Epoch 143/1316] [Batch 0/38] [D loss: 0.373643] [G loss: 0.221683] [ema: 0.998725] 
[Epoch 144/1316] [Batch 0/38] [D loss: 0.311309] [G loss: 0.182884] [ema: 0.998734] 
[Epoch 145/1316] [Batch 0/38] [D loss: 0.303242] [G loss: 0.241230] [ema: 0.998743] 
[Epoch 146/1316] [Batch 0/38] [D loss: 0.365319] [G loss: 0.218912] [ema: 0.998751] 
[Epoch 147/1316] [Batch 0/38] [D loss: 0.325180] [G loss: 0.211121] [ema: 0.998760] 
[Epoch 148/1316] [Batch 0/38] [D loss: 0.326425] [G loss: 0.201310] [ema: 0.998768] 
[Epoch 149/1316] [Batch 0/38] [D loss: 0.299971] [G loss: 0.215754] [ema: 0.998777] 
[Epoch 150/1316] [Batch 0/38] [D loss: 0.353929] [G loss: 0.216065] [ema: 0.998785] 
[Epoch 151/1316] [Batch 0/38] [D loss: 0.324821] [G loss: 0.253554] [ema: 0.998793] 
[Epoch 152/1316] [Batch 0/38] [D loss: 0.317059] [G loss: 0.226431] [ema: 0.998801] 
[Epoch 153/1316] [Batch 0/38] [D loss: 0.345495] [G loss: 0.195436] [ema: 0.998809] 
[Epoch 154/1316] [Batch 0/38] [D loss: 0.328915] [G loss: 0.178547] [ema: 0.998816] 
[Epoch 155/1316] [Batch 0/38] [D loss: 0.421162] [G loss: 0.177236] [ema: 0.998824] 
[Epoch 156/1316] [Batch 0/38] [D loss: 0.352220] [G loss: 0.224728] [ema: 0.998831] 
[Epoch 157/1316] [Batch 0/38] [D loss: 0.327016] [G loss: 0.227615] [ema: 0.998839] 
[Epoch 158/1316] [Batch 0/38] [D loss: 0.366650] [G loss: 0.225928] [ema: 0.998846] 
[Epoch 159/1316] [Batch 0/38] [D loss: 0.309630] [G loss: 0.219418] [ema: 0.998853] 
[Epoch 160/1316] [Batch 0/38] [D loss: 0.383328] [G loss: 0.200622] [ema: 0.998861] 
[Epoch 161/1316] [Batch 0/38] [D loss: 0.359076] [G loss: 0.239200] [ema: 0.998868] 
[Epoch 162/1316] [Batch 0/38] [D loss: 0.354571] [G loss: 0.229038] [ema: 0.998875] 
[Epoch 163/1316] [Batch 0/38] [D loss: 0.313286] [G loss: 0.196550] [ema: 0.998882] 
[Epoch 164/1316] [Batch 0/38] [D loss: 0.388866] [G loss: 0.200176] [ema: 0.998888] 
[Epoch 165/1316] [Batch 0/38] [D loss: 0.375799] [G loss: 0.211610] [ema: 0.998895] 
[Epoch 166/1316] [Batch 0/38] [D loss: 0.286358] [G loss: 0.230854] [ema: 0.998902] 
[Epoch 167/1316] [Batch 0/38] [D loss: 0.323302] [G loss: 0.274439] [ema: 0.998908] 
[Epoch 168/1316] [Batch 0/38] [D loss: 0.345922] [G loss: 0.206964] [ema: 0.998915] 
[Epoch 169/1316] [Batch 0/38] [D loss: 0.332218] [G loss: 0.208969] [ema: 0.998921] 
[Epoch 170/1316] [Batch 0/38] [D loss: 0.352637] [G loss: 0.221443] [ema: 0.998928] 
[Epoch 171/1316] [Batch 0/38] [D loss: 0.317184] [G loss: 0.265767] [ema: 0.998934] 
[Epoch 172/1316] [Batch 0/38] [D loss: 0.290303] [G loss: 0.230585] [ema: 0.998940] 
[Epoch 173/1316] [Batch 0/38] [D loss: 0.312473] [G loss: 0.224643] [ema: 0.998946] 
[Epoch 174/1316] [Batch 0/38] [D loss: 0.368106] [G loss: 0.220604] [ema: 0.998952] 
[Epoch 175/1316] [Batch 0/38] [D loss: 0.307871] [G loss: 0.191621] [ema: 0.998958] 
[Epoch 176/1316] [Batch 0/38] [D loss: 0.348388] [G loss: 0.191444] [ema: 0.998964] 
[Epoch 177/1316] [Batch 0/38] [D loss: 0.312189] [G loss: 0.203956] [ema: 0.998970] 
[Epoch 178/1316] [Batch 0/38] [D loss: 0.346152] [G loss: 0.194059] [ema: 0.998976] 
[Epoch 179/1316] [Batch 0/38] [D loss: 0.311212] [G loss: 0.212248] [ema: 0.998981] 
[Epoch 180/1316] [Batch 0/38] [D loss: 0.332622] [G loss: 0.222892] [ema: 0.998987] 
[Epoch 181/1316] [Batch 0/38] [D loss: 0.377091] [G loss: 0.205791] [ema: 0.998993] 
[Epoch 182/1316] [Batch 0/38] [D loss: 0.322381] [G loss: 0.231380] [ema: 0.998998] 
[Epoch 183/1316] [Batch 0/38] [D loss: 0.379408] [G loss: 0.224686] [ema: 0.999004] 
[Epoch 184/1316] [Batch 0/38] [D loss: 0.283409] [G loss: 0.210585] [ema: 0.999009] 
[Epoch 185/1316] [Batch 0/38] [D loss: 0.308926] [G loss: 0.248944] [ema: 0.999015] 
[Epoch 186/1316] [Batch 0/38] [D loss: 0.317105] [G loss: 0.238237] [ema: 0.999020] 
[Epoch 187/1316] [Batch 0/38] [D loss: 0.358819] [G loss: 0.221994] [ema: 0.999025] 
[Epoch 188/1316] [Batch 0/38] [D loss: 0.321166] [G loss: 0.220312] [ema: 0.999030] 
[Epoch 189/1316] [Batch 0/38] [D loss: 0.330845] [G loss: 0.237099] [ema: 0.999035] 
[Epoch 190/1316] [Batch 0/38] [D loss: 0.298246] [G loss: 0.214249] [ema: 0.999040] 
[Epoch 191/1316] [Batch 0/38] [D loss: 0.312831] [G loss: 0.240470] [ema: 0.999045] 
[Epoch 192/1316] [Batch 0/38] [D loss: 0.305946] [G loss: 0.205509] [ema: 0.999050] 
[Epoch 193/1316] [Batch 0/38] [D loss: 0.275382] [G loss: 0.215602] [ema: 0.999055] 
[Epoch 194/1316] [Batch 0/38] [D loss: 0.266417] [G loss: 0.224298] [ema: 0.999060] 
[Epoch 195/1316] [Batch 0/38] [D loss: 0.294068] [G loss: 0.220038] [ema: 0.999065] 
[Epoch 196/1316] [Batch 0/38] [D loss: 0.329845] [G loss: 0.254632] [ema: 0.999070] 
[Epoch 197/1316] [Batch 0/38] [D loss: 0.290943] [G loss: 0.223814] [ema: 0.999075] 
[Epoch 198/1316] [Batch 0/38] [D loss: 0.316318] [G loss: 0.233234] [ema: 0.999079] 
[Epoch 199/1316] [Batch 0/38] [D loss: 0.307437] [G loss: 0.206322] [ema: 0.999084] 
[Epoch 200/1316] [Batch 0/38] [D loss: 0.284530] [G loss: 0.276101] [ema: 0.999088] 
[Epoch 201/1316] [Batch 0/38] [D loss: 0.348811] [G loss: 0.218399] [ema: 0.999093] 
[Epoch 202/1316] [Batch 0/38] [D loss: 0.327386] [G loss: 0.230452] [ema: 0.999097] 
[Epoch 203/1316] [Batch 0/38] [D loss: 0.284875] [G loss: 0.228525] [ema: 0.999102] 
[Epoch 204/1316] [Batch 0/38] [D loss: 0.311563] [G loss: 0.222762] [ema: 0.999106] 
[Epoch 205/1316] [Batch 0/38] [D loss: 0.281727] [G loss: 0.235570] [ema: 0.999111] 
[Epoch 206/1316] [Batch 0/38] [D loss: 0.325756] [G loss: 0.242298] [ema: 0.999115] 
[Epoch 207/1316] [Batch 0/38] [D loss: 0.256555] [G loss: 0.244053] [ema: 0.999119] 
[Epoch 208/1316] [Batch 0/38] [D loss: 0.312429] [G loss: 0.227961] [ema: 0.999123] 
[Epoch 209/1316] [Batch 0/38] [D loss: 0.317694] [G loss: 0.188291] [ema: 0.999128] 
[Epoch 210/1316] [Batch 0/38] [D loss: 0.314552] [G loss: 0.227899] [ema: 0.999132] 
[Epoch 211/1316] [Batch 0/38] [D loss: 0.279376] [G loss: 0.250738] [ema: 0.999136] 
[Epoch 212/1316] [Batch 0/38] [D loss: 0.339714] [G loss: 0.224370] [ema: 0.999140] 
[Epoch 213/1316] [Batch 0/38] [D loss: 0.317883] [G loss: 0.227975] [ema: 0.999144] 
[Epoch 214/1316] [Batch 0/38] [D loss: 0.328668] [G loss: 0.224933] [ema: 0.999148] 
[Epoch 215/1316] [Batch 0/38] [D loss: 0.324308] [G loss: 0.210425] [ema: 0.999152] 
[Epoch 216/1316] [Batch 0/38] [D loss: 0.265803] [G loss: 0.206853] [ema: 0.999156] 
[Epoch 217/1316] [Batch 0/38] [D loss: 0.332025] [G loss: 0.232893] [ema: 0.999160] 
[Epoch 218/1316] [Batch 0/38] [D loss: 0.333195] [G loss: 0.221078] [ema: 0.999164] 
[Epoch 219/1316] [Batch 0/38] [D loss: 0.292130] [G loss: 0.240906] [ema: 0.999167] 
[Epoch 220/1316] [Batch 0/38] [D loss: 0.310355] [G loss: 0.243294] [ema: 0.999171] 
[Epoch 221/1316] [Batch 0/38] [D loss: 0.335559] [G loss: 0.229190] [ema: 0.999175] 
[Epoch 222/1316] [Batch 0/38] [D loss: 0.281916] [G loss: 0.265248] [ema: 0.999179] 
[Epoch 223/1316] [Batch 0/38] [D loss: 0.310634] [G loss: 0.234006] [ema: 0.999182] 
[Epoch 224/1316] [Batch 0/38] [D loss: 0.288561] [G loss: 0.226822] [ema: 0.999186] 
[Epoch 225/1316] [Batch 0/38] [D loss: 0.278021] [G loss: 0.228495] [ema: 0.999190] 
[Epoch 226/1316] [Batch 0/38] [D loss: 0.337705] [G loss: 0.248140] [ema: 0.999193] 
[Epoch 227/1316] [Batch 0/38] [D loss: 0.275062] [G loss: 0.241413] [ema: 0.999197] 
[Epoch 228/1316] [Batch 0/38] [D loss: 0.311422] [G loss: 0.209732] [ema: 0.999200] 
[Epoch 229/1316] [Batch 0/38] [D loss: 0.311258] [G loss: 0.222977] [ema: 0.999204] 
[Epoch 230/1316] [Batch 0/38] [D loss: 0.308580] [G loss: 0.232820] [ema: 0.999207] 
[Epoch 231/1316] [Batch 0/38] [D loss: 0.253790] [G loss: 0.270594] [ema: 0.999211] 
[Epoch 232/1316] [Batch 0/38] [D loss: 0.304925] [G loss: 0.239261] [ema: 0.999214] 
[Epoch 233/1316] [Batch 0/38] [D loss: 0.326502] [G loss: 0.247795] [ema: 0.999217] 
[Epoch 234/1316] [Batch 0/38] [D loss: 0.279917] [G loss: 0.239163] [ema: 0.999221] 
[Epoch 235/1316] [Batch 0/38] [D loss: 0.255785] [G loss: 0.246272] [ema: 0.999224] 
[Epoch 236/1316] [Batch 0/38] [D loss: 0.317409] [G loss: 0.226167] [ema: 0.999227] 
[Epoch 237/1316] [Batch 0/38] [D loss: 0.311602] [G loss: 0.231056] [ema: 0.999231] 
[Epoch 238/1316] [Batch 0/38] [D loss: 0.316139] [G loss: 0.224257] [ema: 0.999234] 
[Epoch 239/1316] [Batch 0/38] [D loss: 0.287494] [G loss: 0.240130] [ema: 0.999237] 
[Epoch 240/1316] [Batch 0/38] [D loss: 0.294882] [G loss: 0.233511] [ema: 0.999240] 
[Epoch 241/1316] [Batch 0/38] [D loss: 0.314174] [G loss: 0.213409] [ema: 0.999243] 
[Epoch 242/1316] [Batch 0/38] [D loss: 0.321451] [G loss: 0.265207] [ema: 0.999247] 
[Epoch 243/1316] [Batch 0/38] [D loss: 0.312752] [G loss: 0.191319] [ema: 0.999250] 
[Epoch 244/1316] [Batch 0/38] [D loss: 0.270668] [G loss: 0.250875] [ema: 0.999253] 
[Epoch 245/1316] [Batch 0/38] [D loss: 0.295128] [G loss: 0.228418] [ema: 0.999256] 
[Epoch 246/1316] [Batch 0/38] [D loss: 0.301739] [G loss: 0.238415] [ema: 0.999259] 
[Epoch 247/1316] [Batch 0/38] [D loss: 0.300482] [G loss: 0.207674] [ema: 0.999262] 
[Epoch 248/1316] [Batch 0/38] [D loss: 0.347111] [G loss: 0.229332] [ema: 0.999265] 
[Epoch 249/1316] [Batch 0/38] [D loss: 0.286289] [G loss: 0.222510] [ema: 0.999268] 
[Epoch 250/1316] [Batch 0/38] [D loss: 0.301229] [G loss: 0.240590] [ema: 0.999271] 
[Epoch 251/1316] [Batch 0/38] [D loss: 0.288731] [G loss: 0.252428] [ema: 0.999274] 
[Epoch 252/1316] [Batch 0/38] [D loss: 0.332631] [G loss: 0.248533] [ema: 0.999276] 
[Epoch 253/1316] [Batch 0/38] [D loss: 0.272472] [G loss: 0.227888] [ema: 0.999279] 
[Epoch 254/1316] [Batch 0/38] [D loss: 0.305011] [G loss: 0.237495] [ema: 0.999282] 
[Epoch 255/1316] [Batch 0/38] [D loss: 0.285189] [G loss: 0.260995] [ema: 0.999285] 
[Epoch 256/1316] [Batch 0/38] [D loss: 0.294823] [G loss: 0.259018] [ema: 0.999288] 
[Epoch 257/1316] [Batch 0/38] [D loss: 0.246813] [G loss: 0.272781] [ema: 0.999290] 
[Epoch 258/1316] [Batch 0/38] [D loss: 0.315946] [G loss: 0.239526] [ema: 0.999293] 
[Epoch 259/1316] [Batch 0/38] [D loss: 0.315441] [G loss: 0.236955] [ema: 0.999296] 
[Epoch 260/1316] [Batch 0/38] [D loss: 0.272943] [G loss: 0.226468] [ema: 0.999299] 
[Epoch 261/1316] [Batch 0/38] [D loss: 0.237840] [G loss: 0.245096] [ema: 0.999301] 
[Epoch 262/1316] [Batch 0/38] [D loss: 0.369216] [G loss: 0.248085] [ema: 0.999304] 
[Epoch 263/1316] [Batch 0/38] [D loss: 0.292964] [G loss: 0.253677] [ema: 0.999307] 




Saving checkpoint 2 in logs/Jumping_2024_10_02_17_29_42/Model




[Epoch 264/1316] [Batch 0/38] [D loss: 0.295423] [G loss: 0.270185] [ema: 0.999309] 
[Epoch 265/1316] [Batch 0/38] [D loss: 0.275338] [G loss: 0.253072] [ema: 0.999312] 
[Epoch 266/1316] [Batch 0/38] [D loss: 0.273928] [G loss: 0.248378] [ema: 0.999314] 
[Epoch 267/1316] [Batch 0/38] [D loss: 0.365789] [G loss: 0.219815] [ema: 0.999317] 
[Epoch 268/1316] [Batch 0/38] [D loss: 0.320690] [G loss: 0.210604] [ema: 0.999320] 
[Epoch 269/1316] [Batch 0/38] [D loss: 0.329609] [G loss: 0.251267] [ema: 0.999322] 
[Epoch 270/1316] [Batch 0/38] [D loss: 0.353042] [G loss: 0.218191] [ema: 0.999325] 
[Epoch 271/1316] [Batch 0/38] [D loss: 0.331418] [G loss: 0.214887] [ema: 0.999327] 
[Epoch 272/1316] [Batch 0/38] [D loss: 0.305742] [G loss: 0.251806] [ema: 0.999330] 
[Epoch 273/1316] [Batch 0/38] [D loss: 0.313492] [G loss: 0.233940] [ema: 0.999332] 
[Epoch 274/1316] [Batch 0/38] [D loss: 0.311830] [G loss: 0.227992] [ema: 0.999335] 
[Epoch 275/1316] [Batch 0/38] [D loss: 0.248094] [G loss: 0.242367] [ema: 0.999337] 
[Epoch 276/1316] [Batch 0/38] [D loss: 0.320476] [G loss: 0.236848] [ema: 0.999339] 
[Epoch 277/1316] [Batch 0/38] [D loss: 0.321104] [G loss: 0.221734] [ema: 0.999342] 
[Epoch 278/1316] [Batch 0/38] [D loss: 0.319083] [G loss: 0.204798] [ema: 0.999344] 
[Epoch 279/1316] [Batch 0/38] [D loss: 0.302271] [G loss: 0.267725] [ema: 0.999346] 
[Epoch 280/1316] [Batch 0/38] [D loss: 0.283235] [G loss: 0.263399] [ema: 0.999349] 
[Epoch 281/1316] [Batch 0/38] [D loss: 0.340006] [G loss: 0.231796] [ema: 0.999351] 
[Epoch 282/1316] [Batch 0/38] [D loss: 0.243140] [G loss: 0.261166] [ema: 0.999353] 
[Epoch 283/1316] [Batch 0/38] [D loss: 0.430103] [G loss: 0.205919] [ema: 0.999356] 
[Epoch 284/1316] [Batch 0/38] [D loss: 0.280657] [G loss: 0.253751] [ema: 0.999358] 
[Epoch 285/1316] [Batch 0/38] [D loss: 0.265981] [G loss: 0.253108] [ema: 0.999360] 
[Epoch 286/1316] [Batch 0/38] [D loss: 0.305232] [G loss: 0.244977] [ema: 0.999362] 
[Epoch 287/1316] [Batch 0/38] [D loss: 0.271600] [G loss: 0.253474] [ema: 0.999365] 
[Epoch 288/1316] [Batch 0/38] [D loss: 0.307593] [G loss: 0.223543] [ema: 0.999367] 
[Epoch 289/1316] [Batch 0/38] [D loss: 0.297220] [G loss: 0.243640] [ema: 0.999369] 
[Epoch 290/1316] [Batch 0/38] [D loss: 0.285754] [G loss: 0.247406] [ema: 0.999371] 
[Epoch 291/1316] [Batch 0/38] [D loss: 0.285503] [G loss: 0.230644] [ema: 0.999373] 
[Epoch 292/1316] [Batch 0/38] [D loss: 0.279292] [G loss: 0.227355] [ema: 0.999376] 
[Epoch 293/1316] [Batch 0/38] [D loss: 0.305107] [G loss: 0.249533] [ema: 0.999378] 
[Epoch 294/1316] [Batch 0/38] [D loss: 0.245163] [G loss: 0.251128] [ema: 0.999380] 
[Epoch 295/1316] [Batch 0/38] [D loss: 0.324651] [G loss: 0.216056] [ema: 0.999382] 
[Epoch 296/1316] [Batch 0/38] [D loss: 0.261477] [G loss: 0.217531] [ema: 0.999384] 
[Epoch 297/1316] [Batch 0/38] [D loss: 0.329835] [G loss: 0.220986] [ema: 0.999386] 
[Epoch 298/1316] [Batch 0/38] [D loss: 0.291015] [G loss: 0.212323] [ema: 0.999388] 
[Epoch 299/1316] [Batch 0/38] [D loss: 0.312012] [G loss: 0.205196] [ema: 0.999390] 
[Epoch 300/1316] [Batch 0/38] [D loss: 0.315490] [G loss: 0.220874] [ema: 0.999392] 
[Epoch 301/1316] [Batch 0/38] [D loss: 0.314891] [G loss: 0.236264] [ema: 0.999394] 
[Epoch 302/1316] [Batch 0/38] [D loss: 0.292276] [G loss: 0.236628] [ema: 0.999396] 
[Epoch 303/1316] [Batch 0/38] [D loss: 0.291108] [G loss: 0.249617] [ema: 0.999398] 
[Epoch 304/1316] [Batch 0/38] [D loss: 0.279078] [G loss: 0.254289] [ema: 0.999400] 
[Epoch 305/1316] [Batch 0/38] [D loss: 0.309552] [G loss: 0.211041] [ema: 0.999402] 
[Epoch 306/1316] [Batch 0/38] [D loss: 0.279512] [G loss: 0.211032] [ema: 0.999404] 
[Epoch 307/1316] [Batch 0/38] [D loss: 0.297062] [G loss: 0.207027] [ema: 0.999406] 
[Epoch 308/1316] [Batch 0/38] [D loss: 0.336137] [G loss: 0.220610] [ema: 0.999408] 
[Epoch 309/1316] [Batch 0/38] [D loss: 0.275072] [G loss: 0.238394] [ema: 0.999410] 
[Epoch 310/1316] [Batch 0/38] [D loss: 0.329819] [G loss: 0.220343] [ema: 0.999412] 
[Epoch 311/1316] [Batch 0/38] [D loss: 0.325304] [G loss: 0.245710] [ema: 0.999414] 
[Epoch 312/1316] [Batch 0/38] [D loss: 0.266976] [G loss: 0.253567] [ema: 0.999416] 
[Epoch 313/1316] [Batch 0/38] [D loss: 0.261288] [G loss: 0.249359] [ema: 0.999417] 
[Epoch 314/1316] [Batch 0/38] [D loss: 0.290724] [G loss: 0.233414] [ema: 0.999419] 
[Epoch 315/1316] [Batch 0/38] [D loss: 0.287258] [G loss: 0.229176] [ema: 0.999421] 
[Epoch 316/1316] [Batch 0/38] [D loss: 0.275566] [G loss: 0.230596] [ema: 0.999423] 
[Epoch 317/1316] [Batch 0/38] [D loss: 0.380551] [G loss: 0.216390] [ema: 0.999425] 
[Epoch 318/1316] [Batch 0/38] [D loss: 0.302015] [G loss: 0.228032] [ema: 0.999427] 
[Epoch 319/1316] [Batch 0/38] [D loss: 0.271670] [G loss: 0.231700] [ema: 0.999428] 
[Epoch 320/1316] [Batch 0/38] [D loss: 0.294018] [G loss: 0.258244] [ema: 0.999430] 
[Epoch 321/1316] [Batch 0/38] [D loss: 0.261425] [G loss: 0.253054] [ema: 0.999432] 
[Epoch 322/1316] [Batch 0/38] [D loss: 0.350224] [G loss: 0.212619] [ema: 0.999434] 
[Epoch 323/1316] [Batch 0/38] [D loss: 0.287939] [G loss: 0.238985] [ema: 0.999435] 
[Epoch 324/1316] [Batch 0/38] [D loss: 0.270317] [G loss: 0.222915] [ema: 0.999437] 
[Epoch 325/1316] [Batch 0/38] [D loss: 0.278541] [G loss: 0.243270] [ema: 0.999439] 
[Epoch 326/1316] [Batch 0/38] [D loss: 0.342889] [G loss: 0.215416] [ema: 0.999441] 
[Epoch 327/1316] [Batch 0/38] [D loss: 0.268558] [G loss: 0.240675] [ema: 0.999442] 
[Epoch 328/1316] [Batch 0/38] [D loss: 0.269647] [G loss: 0.231291] [ema: 0.999444] 
[Epoch 329/1316] [Batch 0/38] [D loss: 0.313591] [G loss: 0.234707] [ema: 0.999446] 
[Epoch 330/1316] [Batch 0/38] [D loss: 0.277014] [G loss: 0.247015] [ema: 0.999447] 
[Epoch 331/1316] [Batch 0/38] [D loss: 0.299171] [G loss: 0.204160] [ema: 0.999449] 
[Epoch 332/1316] [Batch 0/38] [D loss: 0.295948] [G loss: 0.212139] [ema: 0.999451] 
[Epoch 333/1316] [Batch 0/38] [D loss: 0.287747] [G loss: 0.255430] [ema: 0.999452] 
[Epoch 334/1316] [Batch 0/38] [D loss: 0.314896] [G loss: 0.243225] [ema: 0.999454] 
[Epoch 335/1316] [Batch 0/38] [D loss: 0.293616] [G loss: 0.231227] [ema: 0.999456] 
[Epoch 336/1316] [Batch 0/38] [D loss: 0.264661] [G loss: 0.255563] [ema: 0.999457] 
[Epoch 337/1316] [Batch 0/38] [D loss: 0.292344] [G loss: 0.243180] [ema: 0.999459] 
[Epoch 338/1316] [Batch 0/38] [D loss: 0.343472] [G loss: 0.213674] [ema: 0.999460] 
[Epoch 339/1316] [Batch 0/38] [D loss: 0.328046] [G loss: 0.211564] [ema: 0.999462] 
[Epoch 340/1316] [Batch 0/38] [D loss: 0.286259] [G loss: 0.209311] [ema: 0.999464] 
[Epoch 341/1316] [Batch 0/38] [D loss: 0.297990] [G loss: 0.227635] [ema: 0.999465] 
[Epoch 342/1316] [Batch 0/38] [D loss: 0.272727] [G loss: 0.230535] [ema: 0.999467] 
[Epoch 343/1316] [Batch 0/38] [D loss: 0.313772] [G loss: 0.250674] [ema: 0.999468] 
[Epoch 344/1316] [Batch 0/38] [D loss: 0.321905] [G loss: 0.215925] [ema: 0.999470] 
[Epoch 345/1316] [Batch 0/38] [D loss: 0.303318] [G loss: 0.208151] [ema: 0.999471] 
[Epoch 346/1316] [Batch 0/38] [D loss: 0.251305] [G loss: 0.225081] [ema: 0.999473] 
[Epoch 347/1316] [Batch 0/38] [D loss: 0.255733] [G loss: 0.248446] [ema: 0.999474] 
[Epoch 348/1316] [Batch 0/38] [D loss: 0.259313] [G loss: 0.222794] [ema: 0.999476] 
[Epoch 349/1316] [Batch 0/38] [D loss: 0.288379] [G loss: 0.215050] [ema: 0.999477] 
[Epoch 350/1316] [Batch 0/38] [D loss: 0.323755] [G loss: 0.217654] [ema: 0.999479] 
[Epoch 351/1316] [Batch 0/38] [D loss: 0.370910] [G loss: 0.227565] [ema: 0.999480] 
[Epoch 352/1316] [Batch 0/38] [D loss: 0.281283] [G loss: 0.247817] [ema: 0.999482] 
[Epoch 353/1316] [Batch 0/38] [D loss: 0.332713] [G loss: 0.198454] [ema: 0.999483] 
[Epoch 354/1316] [Batch 0/38] [D loss: 0.343756] [G loss: 0.227493] [ema: 0.999485] 
[Epoch 355/1316] [Batch 0/38] [D loss: 0.322513] [G loss: 0.217700] [ema: 0.999486] 
[Epoch 356/1316] [Batch 0/38] [D loss: 0.286410] [G loss: 0.263982] [ema: 0.999488] 
[Epoch 357/1316] [Batch 0/38] [D loss: 0.294996] [G loss: 0.254715] [ema: 0.999489] 
[Epoch 358/1316] [Batch 0/38] [D loss: 0.253777] [G loss: 0.237848] [ema: 0.999491] 
[Epoch 359/1316] [Batch 0/38] [D loss: 0.300119] [G loss: 0.232964] [ema: 0.999492] 
[Epoch 360/1316] [Batch 0/38] [D loss: 0.287269] [G loss: 0.223023] [ema: 0.999493] 
[Epoch 361/1316] [Batch 0/38] [D loss: 0.258192] [G loss: 0.237386] [ema: 0.999495] 
[Epoch 362/1316] [Batch 0/38] [D loss: 0.309065] [G loss: 0.226377] [ema: 0.999496] 
[Epoch 363/1316] [Batch 0/38] [D loss: 0.326484] [G loss: 0.236649] [ema: 0.999498] 
[Epoch 364/1316] [Batch 0/38] [D loss: 0.289434] [G loss: 0.229450] [ema: 0.999499] 
[Epoch 365/1316] [Batch 0/38] [D loss: 0.277915] [G loss: 0.209385] [ema: 0.999500] 
[Epoch 366/1316] [Batch 0/38] [D loss: 0.319761] [G loss: 0.251147] [ema: 0.999502] 
[Epoch 367/1316] [Batch 0/38] [D loss: 0.307548] [G loss: 0.250571] [ema: 0.999503] 
[Epoch 368/1316] [Batch 0/38] [D loss: 0.296048] [G loss: 0.228300] [ema: 0.999504] 
[Epoch 369/1316] [Batch 0/38] [D loss: 0.269331] [G loss: 0.229375] [ema: 0.999506] 
[Epoch 370/1316] [Batch 0/38] [D loss: 0.260143] [G loss: 0.260377] [ema: 0.999507] 
[Epoch 371/1316] [Batch 0/38] [D loss: 0.293926] [G loss: 0.226010] [ema: 0.999508] 
[Epoch 372/1316] [Batch 0/38] [D loss: 0.309056] [G loss: 0.270130] [ema: 0.999510] 
[Epoch 373/1316] [Batch 0/38] [D loss: 0.328947] [G loss: 0.238397] [ema: 0.999511] 
[Epoch 374/1316] [Batch 0/38] [D loss: 0.300782] [G loss: 0.222106] [ema: 0.999512] 
[Epoch 375/1316] [Batch 0/38] [D loss: 0.306380] [G loss: 0.247923] [ema: 0.999514] 
[Epoch 376/1316] [Batch 0/38] [D loss: 0.291597] [G loss: 0.249092] [ema: 0.999515] 
[Epoch 377/1316] [Batch 0/38] [D loss: 0.275020] [G loss: 0.240625] [ema: 0.999516] 
[Epoch 378/1316] [Batch 0/38] [D loss: 0.283699] [G loss: 0.225073] [ema: 0.999518] 
[Epoch 379/1316] [Batch 0/38] [D loss: 0.254226] [G loss: 0.265194] [ema: 0.999519] 
[Epoch 380/1316] [Batch 0/38] [D loss: 0.289110] [G loss: 0.236812] [ema: 0.999520] 
[Epoch 381/1316] [Batch 0/38] [D loss: 0.282829] [G loss: 0.223531] [ema: 0.999521] 
[Epoch 382/1316] [Batch 0/38] [D loss: 0.272192] [G loss: 0.248660] [ema: 0.999523] 
[Epoch 383/1316] [Batch 0/38] [D loss: 0.287243] [G loss: 0.260377] [ema: 0.999524] 
[Epoch 384/1316] [Batch 0/38] [D loss: 0.284442] [G loss: 0.252271] [ema: 0.999525] 
[Epoch 385/1316] [Batch 0/38] [D loss: 0.263320] [G loss: 0.247643] [ema: 0.999526] 
[Epoch 386/1316] [Batch 0/38] [D loss: 0.248810] [G loss: 0.256743] [ema: 0.999528] 
[Epoch 387/1316] [Batch 0/38] [D loss: 0.253248] [G loss: 0.245251] [ema: 0.999529] 
[Epoch 388/1316] [Batch 0/38] [D loss: 0.304479] [G loss: 0.239760] [ema: 0.999530] 
[Epoch 389/1316] [Batch 0/38] [D loss: 0.272583] [G loss: 0.248499] [ema: 0.999531] 
[Epoch 390/1316] [Batch 0/38] [D loss: 0.290089] [G loss: 0.241817] [ema: 0.999532] 
[Epoch 391/1316] [Batch 0/38] [D loss: 0.315274] [G loss: 0.233054] [ema: 0.999534] 
[Epoch 392/1316] [Batch 0/38] [D loss: 0.291412] [G loss: 0.237437] [ema: 0.999535] 
[Epoch 393/1316] [Batch 0/38] [D loss: 0.267825] [G loss: 0.247324] [ema: 0.999536] 
[Epoch 394/1316] [Batch 0/38] [D loss: 0.308137] [G loss: 0.250491] [ema: 0.999537] 
[Epoch 395/1316] [Batch 0/38] [D loss: 0.301162] [G loss: 0.242023] [ema: 0.999538] 
[Epoch 396/1316] [Batch 0/38] [D loss: 0.297989] [G loss: 0.221190] [ema: 0.999539] 
[Epoch 397/1316] [Batch 0/38] [D loss: 0.295412] [G loss: 0.225089] [ema: 0.999541] 
[Epoch 398/1316] [Batch 0/38] [D loss: 0.276782] [G loss: 0.243635] [ema: 0.999542] 
[Epoch 399/1316] [Batch 0/38] [D loss: 0.250221] [G loss: 0.257834] [ema: 0.999543] 
[Epoch 400/1316] [Batch 0/38] [D loss: 0.297790] [G loss: 0.245580] [ema: 0.999544] 
[Epoch 401/1316] [Batch 0/38] [D loss: 0.306277] [G loss: 0.226748] [ema: 0.999545] 
[Epoch 402/1316] [Batch 0/38] [D loss: 0.298339] [G loss: 0.231689] [ema: 0.999546] 
[Epoch 403/1316] [Batch 0/38] [D loss: 0.325005] [G loss: 0.220398] [ema: 0.999547] 
[Epoch 404/1316] [Batch 0/38] [D loss: 0.306685] [G loss: 0.231269] [ema: 0.999549] 
[Epoch 405/1316] [Batch 0/38] [D loss: 0.308425] [G loss: 0.259291] [ema: 0.999550] 
[Epoch 406/1316] [Batch 0/38] [D loss: 0.247775] [G loss: 0.245859] [ema: 0.999551] 
[Epoch 407/1316] [Batch 0/38] [D loss: 0.293826] [G loss: 0.237845] [ema: 0.999552] 
[Epoch 408/1316] [Batch 0/38] [D loss: 0.260512] [G loss: 0.251124] [ema: 0.999553] 
[Epoch 409/1316] [Batch 0/38] [D loss: 0.251256] [G loss: 0.249714] [ema: 0.999554] 
[Epoch 410/1316] [Batch 0/38] [D loss: 0.369002] [G loss: 0.202827] [ema: 0.999555] 
[Epoch 411/1316] [Batch 0/38] [D loss: 0.259454] [G loss: 0.251969] [ema: 0.999556] 
[Epoch 412/1316] [Batch 0/38] [D loss: 0.270474] [G loss: 0.232562] [ema: 0.999557] 
[Epoch 413/1316] [Batch 0/38] [D loss: 0.261550] [G loss: 0.245701] [ema: 0.999558] 
[Epoch 414/1316] [Batch 0/38] [D loss: 0.272516] [G loss: 0.242940] [ema: 0.999560] 
[Epoch 415/1316] [Batch 0/38] [D loss: 0.277785] [G loss: 0.229573] [ema: 0.999561] 
[Epoch 416/1316] [Batch 0/38] [D loss: 0.274231] [G loss: 0.220639] [ema: 0.999562] 
[Epoch 417/1316] [Batch 0/38] [D loss: 0.235778] [G loss: 0.278671] [ema: 0.999563] 
[Epoch 418/1316] [Batch 0/38] [D loss: 0.273220] [G loss: 0.272635] [ema: 0.999564] 
[Epoch 419/1316] [Batch 0/38] [D loss: 0.294797] [G loss: 0.256427] [ema: 0.999565] 
[Epoch 420/1316] [Batch 0/38] [D loss: 0.278516] [G loss: 0.258871] [ema: 0.999566] 
[Epoch 421/1316] [Batch 0/38] [D loss: 0.286264] [G loss: 0.232910] [ema: 0.999567] 
[Epoch 422/1316] [Batch 0/38] [D loss: 0.288355] [G loss: 0.220703] [ema: 0.999568] 
[Epoch 423/1316] [Batch 0/38] [D loss: 0.284838] [G loss: 0.232935] [ema: 0.999569] 
[Epoch 424/1316] [Batch 0/38] [D loss: 0.326040] [G loss: 0.239890] [ema: 0.999570] 
[Epoch 425/1316] [Batch 0/38] [D loss: 0.300068] [G loss: 0.203163] [ema: 0.999571] 
[Epoch 426/1316] [Batch 0/38] [D loss: 0.276294] [G loss: 0.252330] [ema: 0.999572] 
[Epoch 427/1316] [Batch 0/38] [D loss: 0.262121] [G loss: 0.236330] [ema: 0.999573] 
[Epoch 428/1316] [Batch 0/38] [D loss: 0.293131] [G loss: 0.242052] [ema: 0.999574] 
[Epoch 429/1316] [Batch 0/38] [D loss: 0.270408] [G loss: 0.249022] [ema: 0.999575] 
[Epoch 430/1316] [Batch 0/38] [D loss: 0.308993] [G loss: 0.222962] [ema: 0.999576] 
[Epoch 431/1316] [Batch 0/38] [D loss: 0.296689] [G loss: 0.241600] [ema: 0.999577] 
[Epoch 432/1316] [Batch 0/38] [D loss: 0.297356] [G loss: 0.259969] [ema: 0.999578] 
[Epoch 433/1316] [Batch 0/38] [D loss: 0.286941] [G loss: 0.242836] [ema: 0.999579] 
[Epoch 434/1316] [Batch 0/38] [D loss: 0.249426] [G loss: 0.229687] [ema: 0.999580] 
[Epoch 435/1316] [Batch 0/38] [D loss: 0.351518] [G loss: 0.224502] [ema: 0.999581] 
[Epoch 436/1316] [Batch 0/38] [D loss: 0.301428] [G loss: 0.238628] [ema: 0.999582] 
[Epoch 437/1316] [Batch 0/38] [D loss: 0.315598] [G loss: 0.241339] [ema: 0.999583] 
[Epoch 438/1316] [Batch 0/38] [D loss: 0.272265] [G loss: 0.257771] [ema: 0.999584] 
[Epoch 439/1316] [Batch 0/38] [D loss: 0.298080] [G loss: 0.233399] [ema: 0.999585] 
[Epoch 440/1316] [Batch 0/38] [D loss: 0.294984] [G loss: 0.238625] [ema: 0.999586] 
[Epoch 441/1316] [Batch 0/38] [D loss: 0.268531] [G loss: 0.267488] [ema: 0.999586] 
[Epoch 442/1316] [Batch 0/38] [D loss: 0.274911] [G loss: 0.225673] [ema: 0.999587] 
[Epoch 443/1316] [Batch 0/38] [D loss: 0.314447] [G loss: 0.237083] [ema: 0.999588] 
[Epoch 444/1316] [Batch 0/38] [D loss: 0.281776] [G loss: 0.217528] [ema: 0.999589] 
[Epoch 445/1316] [Batch 0/38] [D loss: 0.230560] [G loss: 0.272200] [ema: 0.999590] 
[Epoch 446/1316] [Batch 0/38] [D loss: 0.281394] [G loss: 0.226425] [ema: 0.999591] 
[Epoch 447/1316] [Batch 0/38] [D loss: 0.325479] [G loss: 0.211057] [ema: 0.999592] 
[Epoch 448/1316] [Batch 0/38] [D loss: 0.260384] [G loss: 0.255436] [ema: 0.999593] 
[Epoch 449/1316] [Batch 0/38] [D loss: 0.324881] [G loss: 0.251722] [ema: 0.999594] 
[Epoch 450/1316] [Batch 0/38] [D loss: 0.275014] [G loss: 0.220425] [ema: 0.999595] 
[Epoch 451/1316] [Batch 0/38] [D loss: 0.266321] [G loss: 0.234200] [ema: 0.999596] 
[Epoch 452/1316] [Batch 0/38] [D loss: 0.298397] [G loss: 0.225143] [ema: 0.999597] 
[Epoch 453/1316] [Batch 0/38] [D loss: 0.300104] [G loss: 0.242364] [ema: 0.999597] 
[Epoch 454/1316] [Batch 0/38] [D loss: 0.270901] [G loss: 0.250707] [ema: 0.999598] 
[Epoch 455/1316] [Batch 0/38] [D loss: 0.326216] [G loss: 0.223998] [ema: 0.999599] 
[Epoch 456/1316] [Batch 0/38] [D loss: 0.288309] [G loss: 0.222115] [ema: 0.999600] 
[Epoch 457/1316] [Batch 0/38] [D loss: 0.286512] [G loss: 0.233248] [ema: 0.999601] 
[Epoch 458/1316] [Batch 0/38] [D loss: 0.291754] [G loss: 0.246944] [ema: 0.999602] 
[Epoch 459/1316] [Batch 0/38] [D loss: 0.290771] [G loss: 0.240707] [ema: 0.999603] 
[Epoch 460/1316] [Batch 0/38] [D loss: 0.257791] [G loss: 0.247246] [ema: 0.999604] 
[Epoch 461/1316] [Batch 0/38] [D loss: 0.325185] [G loss: 0.246311] [ema: 0.999604] 
[Epoch 462/1316] [Batch 0/38] [D loss: 0.278864] [G loss: 0.216724] [ema: 0.999605] 
[Epoch 463/1316] [Batch 0/38] [D loss: 0.263183] [G loss: 0.237969] [ema: 0.999606] 
[Epoch 464/1316] [Batch 0/38] [D loss: 0.278914] [G loss: 0.244506] [ema: 0.999607] 
[Epoch 465/1316] [Batch 0/38] [D loss: 0.294264] [G loss: 0.258661] [ema: 0.999608] 
[Epoch 466/1316] [Batch 0/38] [D loss: 0.289682] [G loss: 0.218985] [ema: 0.999609] 
[Epoch 467/1316] [Batch 0/38] [D loss: 0.312490] [G loss: 0.225758] [ema: 0.999609] 
[Epoch 468/1316] [Batch 0/38] [D loss: 0.264405] [G loss: 0.240287] [ema: 0.999610] 
[Epoch 469/1316] [Batch 0/38] [D loss: 0.381724] [G loss: 0.231702] [ema: 0.999611] 
[Epoch 470/1316] [Batch 0/38] [D loss: 0.296496] [G loss: 0.224303] [ema: 0.999612] 
[Epoch 471/1316] [Batch 0/38] [D loss: 0.255081] [G loss: 0.251713] [ema: 0.999613] 
[Epoch 472/1316] [Batch 0/38] [D loss: 0.293525] [G loss: 0.254798] [ema: 0.999614] 
[Epoch 473/1316] [Batch 0/38] [D loss: 0.278975] [G loss: 0.228835] [ema: 0.999614] 
[Epoch 474/1316] [Batch 0/38] [D loss: 0.338411] [G loss: 0.218572] [ema: 0.999615] 
[Epoch 475/1316] [Batch 0/38] [D loss: 0.295637] [G loss: 0.227170] [ema: 0.999616] 
[Epoch 476/1316] [Batch 0/38] [D loss: 0.329451] [G loss: 0.223052] [ema: 0.999617] 
[Epoch 477/1316] [Batch 0/38] [D loss: 0.282934] [G loss: 0.240974] [ema: 0.999618] 
[Epoch 478/1316] [Batch 0/38] [D loss: 0.293795] [G loss: 0.244710] [ema: 0.999618] 
[Epoch 479/1316] [Batch 0/38] [D loss: 0.281549] [G loss: 0.243449] [ema: 0.999619] 
[Epoch 480/1316] [Batch 0/38] [D loss: 0.257806] [G loss: 0.245530] [ema: 0.999620] 
[Epoch 481/1316] [Batch 0/38] [D loss: 0.294028] [G loss: 0.237161] [ema: 0.999621] 
[Epoch 482/1316] [Batch 0/38] [D loss: 0.274733] [G loss: 0.241532] [ema: 0.999622] 
[Epoch 483/1316] [Batch 0/38] [D loss: 0.280191] [G loss: 0.245137] [ema: 0.999622] 
[Epoch 484/1316] [Batch 0/38] [D loss: 0.275317] [G loss: 0.245900] [ema: 0.999623] 
[Epoch 485/1316] [Batch 0/38] [D loss: 0.263759] [G loss: 0.240502] [ema: 0.999624] 
[Epoch 486/1316] [Batch 0/38] [D loss: 0.261429] [G loss: 0.242943] [ema: 0.999625] 
[Epoch 487/1316] [Batch 0/38] [D loss: 0.276673] [G loss: 0.247460] [ema: 0.999626] 
[Epoch 488/1316] [Batch 0/38] [D loss: 0.272906] [G loss: 0.247232] [ema: 0.999626] 
[Epoch 489/1316] [Batch 0/38] [D loss: 0.311514] [G loss: 0.227893] [ema: 0.999627] 
[Epoch 490/1316] [Batch 0/38] [D loss: 0.335631] [G loss: 0.246231] [ema: 0.999628] 
[Epoch 491/1316] [Batch 0/38] [D loss: 0.278876] [G loss: 0.247223] [ema: 0.999629] 
[Epoch 492/1316] [Batch 0/38] [D loss: 0.321181] [G loss: 0.265146] [ema: 0.999629] 
[Epoch 493/1316] [Batch 0/38] [D loss: 0.295209] [G loss: 0.226833] [ema: 0.999630] 
[Epoch 494/1316] [Batch 0/38] [D loss: 0.288760] [G loss: 0.258505] [ema: 0.999631] 
[Epoch 495/1316] [Batch 0/38] [D loss: 0.253158] [G loss: 0.248911] [ema: 0.999632] 
[Epoch 496/1316] [Batch 0/38] [D loss: 0.276010] [G loss: 0.255951] [ema: 0.999632] 
[Epoch 497/1316] [Batch 0/38] [D loss: 0.284477] [G loss: 0.231501] [ema: 0.999633] 
[Epoch 498/1316] [Batch 0/38] [D loss: 0.365250] [G loss: 0.232458] [ema: 0.999634] 
[Epoch 499/1316] [Batch 0/38] [D loss: 0.280550] [G loss: 0.220403] [ema: 0.999635] 
[Epoch 500/1316] [Batch 0/38] [D loss: 0.279146] [G loss: 0.244862] [ema: 0.999635] 
[Epoch 501/1316] [Batch 0/38] [D loss: 0.294358] [G loss: 0.213669] [ema: 0.999636] 
[Epoch 502/1316] [Batch 0/38] [D loss: 0.275058] [G loss: 0.227862] [ema: 0.999637] 
[Epoch 503/1316] [Batch 0/38] [D loss: 0.268247] [G loss: 0.233512] [ema: 0.999637] 
[Epoch 504/1316] [Batch 0/38] [D loss: 0.259207] [G loss: 0.240060] [ema: 0.999638] 
[Epoch 505/1316] [Batch 0/38] [D loss: 0.284468] [G loss: 0.243734] [ema: 0.999639] 
[Epoch 506/1316] [Batch 0/38] [D loss: 0.302875] [G loss: 0.237948] [ema: 0.999640] 
[Epoch 507/1316] [Batch 0/38] [D loss: 0.263806] [G loss: 0.232894] [ema: 0.999640] 
[Epoch 508/1316] [Batch 0/38] [D loss: 0.253941] [G loss: 0.237015] [ema: 0.999641] 
[Epoch 509/1316] [Batch 0/38] [D loss: 0.299815] [G loss: 0.227669] [ema: 0.999642] 
[Epoch 510/1316] [Batch 0/38] [D loss: 0.296173] [G loss: 0.215431] [ema: 0.999642] 
[Epoch 511/1316] [Batch 0/38] [D loss: 0.290649] [G loss: 0.233012] [ema: 0.999643] 
[Epoch 512/1316] [Batch 0/38] [D loss: 0.264902] [G loss: 0.235320] [ema: 0.999644] 
[Epoch 513/1316] [Batch 0/38] [D loss: 0.301152] [G loss: 0.228695] [ema: 0.999644] 
[Epoch 514/1316] [Batch 0/38] [D loss: 0.271580] [G loss: 0.237692] [ema: 0.999645] 
[Epoch 515/1316] [Batch 0/38] [D loss: 0.282364] [G loss: 0.225231] [ema: 0.999646] 
[Epoch 516/1316] [Batch 0/38] [D loss: 0.274780] [G loss: 0.237747] [ema: 0.999647] 
[Epoch 517/1316] [Batch 0/38] [D loss: 0.276203] [G loss: 0.250067] [ema: 0.999647] 
[Epoch 518/1316] [Batch 0/38] [D loss: 0.299644] [G loss: 0.219929] [ema: 0.999648] 
[Epoch 519/1316] [Batch 0/38] [D loss: 0.266911] [G loss: 0.239677] [ema: 0.999649] 
[Epoch 520/1316] [Batch 0/38] [D loss: 0.272651] [G loss: 0.233345] [ema: 0.999649] 
[Epoch 521/1316] [Batch 0/38] [D loss: 0.240019] [G loss: 0.237385] [ema: 0.999650] 
[Epoch 522/1316] [Batch 0/38] [D loss: 0.266141] [G loss: 0.240979] [ema: 0.999651] 
[Epoch 523/1316] [Batch 0/38] [D loss: 0.254185] [G loss: 0.243513] [ema: 0.999651] 
[Epoch 524/1316] [Batch 0/38] [D loss: 0.264084] [G loss: 0.253362] [ema: 0.999652] 
[Epoch 525/1316] [Batch 0/38] [D loss: 0.276960] [G loss: 0.253234] [ema: 0.999653] 
[Epoch 526/1316] [Batch 0/38] [D loss: 0.320934] [G loss: 0.241232] [ema: 0.999653] 
[Epoch 527/1316] [Batch 0/38] [D loss: 0.304238] [G loss: 0.234294] [ema: 0.999654] 




Saving checkpoint 3 in logs/Jumping_2024_10_02_17_29_42/Model




[Epoch 528/1316] [Batch 0/38] [D loss: 0.308411] [G loss: 0.232862] [ema: 0.999655] 
[Epoch 529/1316] [Batch 0/38] [D loss: 0.249526] [G loss: 0.255167] [ema: 0.999655] 
[Epoch 530/1316] [Batch 0/38] [D loss: 0.298925] [G loss: 0.223959] [ema: 0.999656] 
[Epoch 531/1316] [Batch 0/38] [D loss: 0.257677] [G loss: 0.247141] [ema: 0.999657] 
[Epoch 532/1316] [Batch 0/38] [D loss: 0.254308] [G loss: 0.259842] [ema: 0.999657] 
[Epoch 533/1316] [Batch 0/38] [D loss: 0.266404] [G loss: 0.257627] [ema: 0.999658] 
[Epoch 534/1316] [Batch 0/38] [D loss: 0.253019] [G loss: 0.241711] [ema: 0.999658] 
[Epoch 535/1316] [Batch 0/38] [D loss: 0.290286] [G loss: 0.248899] [ema: 0.999659] 
[Epoch 536/1316] [Batch 0/38] [D loss: 0.262876] [G loss: 0.242658] [ema: 0.999660] 
[Epoch 537/1316] [Batch 0/38] [D loss: 0.254528] [G loss: 0.253366] [ema: 0.999660] 
[Epoch 538/1316] [Batch 0/38] [D loss: 0.248891] [G loss: 0.237915] [ema: 0.999661] 
[Epoch 539/1316] [Batch 0/38] [D loss: 0.263141] [G loss: 0.246259] [ema: 0.999662] 
[Epoch 540/1316] [Batch 0/38] [D loss: 0.282856] [G loss: 0.243396] [ema: 0.999662] 
[Epoch 541/1316] [Batch 0/38] [D loss: 0.263835] [G loss: 0.237638] [ema: 0.999663] 
[Epoch 542/1316] [Batch 0/38] [D loss: 0.295226] [G loss: 0.225018] [ema: 0.999664] 
[Epoch 543/1316] [Batch 0/38] [D loss: 0.282235] [G loss: 0.240870] [ema: 0.999664] 
[Epoch 544/1316] [Batch 0/38] [D loss: 0.305392] [G loss: 0.254601] [ema: 0.999665] 
[Epoch 545/1316] [Batch 0/38] [D loss: 0.271839] [G loss: 0.236358] [ema: 0.999665] 
[Epoch 546/1316] [Batch 0/38] [D loss: 0.293718] [G loss: 0.232126] [ema: 0.999666] 
[Epoch 547/1316] [Batch 0/38] [D loss: 0.276715] [G loss: 0.237406] [ema: 0.999667] 
[Epoch 548/1316] [Batch 0/38] [D loss: 0.272285] [G loss: 0.243804] [ema: 0.999667] 
[Epoch 549/1316] [Batch 0/38] [D loss: 0.296951] [G loss: 0.222634] [ema: 0.999668] 
[Epoch 550/1316] [Batch 0/38] [D loss: 0.332332] [G loss: 0.228204] [ema: 0.999668] 
[Epoch 551/1316] [Batch 0/38] [D loss: 0.275426] [G loss: 0.239264] [ema: 0.999669] 
[Epoch 552/1316] [Batch 0/38] [D loss: 0.300303] [G loss: 0.235681] [ema: 0.999670] 
[Epoch 553/1316] [Batch 0/38] [D loss: 0.283521] [G loss: 0.247293] [ema: 0.999670] 
[Epoch 554/1316] [Batch 0/38] [D loss: 0.262653] [G loss: 0.249038] [ema: 0.999671] 
[Epoch 555/1316] [Batch 0/38] [D loss: 0.269456] [G loss: 0.246567] [ema: 0.999671] 
[Epoch 556/1316] [Batch 0/38] [D loss: 0.288983] [G loss: 0.224778] [ema: 0.999672] 
[Epoch 557/1316] [Batch 0/38] [D loss: 0.310109] [G loss: 0.245496] [ema: 0.999673] 
[Epoch 558/1316] [Batch 0/38] [D loss: 0.277088] [G loss: 0.249396] [ema: 0.999673] 
[Epoch 559/1316] [Batch 0/38] [D loss: 0.279867] [G loss: 0.251443] [ema: 0.999674] 
[Epoch 560/1316] [Batch 0/38] [D loss: 0.276480] [G loss: 0.240747] [ema: 0.999674] 
[Epoch 561/1316] [Batch 0/38] [D loss: 0.345007] [G loss: 0.205717] [ema: 0.999675] 
[Epoch 562/1316] [Batch 0/38] [D loss: 0.248565] [G loss: 0.222525] [ema: 0.999675] 
[Epoch 563/1316] [Batch 0/38] [D loss: 0.325168] [G loss: 0.237194] [ema: 0.999676] 
[Epoch 564/1316] [Batch 0/38] [D loss: 0.269113] [G loss: 0.249215] [ema: 0.999677] 
[Epoch 565/1316] [Batch 0/38] [D loss: 0.268055] [G loss: 0.224110] [ema: 0.999677] 
[Epoch 566/1316] [Batch 0/38] [D loss: 0.321428] [G loss: 0.234077] [ema: 0.999678] 
[Epoch 567/1316] [Batch 0/38] [D loss: 0.269779] [G loss: 0.256393] [ema: 0.999678] 
[Epoch 568/1316] [Batch 0/38] [D loss: 0.297049] [G loss: 0.248276] [ema: 0.999679] 
[Epoch 569/1316] [Batch 0/38] [D loss: 0.274618] [G loss: 0.254928] [ema: 0.999679] 
[Epoch 570/1316] [Batch 0/38] [D loss: 0.248860] [G loss: 0.253054] [ema: 0.999680] 
[Epoch 571/1316] [Batch 0/38] [D loss: 0.282536] [G loss: 0.243349] [ema: 0.999681] 
[Epoch 572/1316] [Batch 0/38] [D loss: 0.264912] [G loss: 0.249193] [ema: 0.999681] 
[Epoch 573/1316] [Batch 0/38] [D loss: 0.263530] [G loss: 0.248715] [ema: 0.999682] 
[Epoch 574/1316] [Batch 0/38] [D loss: 0.316273] [G loss: 0.238692] [ema: 0.999682] 
[Epoch 575/1316] [Batch 0/38] [D loss: 0.250037] [G loss: 0.243704] [ema: 0.999683] 
[Epoch 576/1316] [Batch 0/38] [D loss: 0.315301] [G loss: 0.220940] [ema: 0.999683] 
[Epoch 577/1316] [Batch 0/38] [D loss: 0.280803] [G loss: 0.242129] [ema: 0.999684] 
[Epoch 578/1316] [Batch 0/38] [D loss: 0.290523] [G loss: 0.237998] [ema: 0.999684] 
[Epoch 579/1316] [Batch 0/38] [D loss: 0.292751] [G loss: 0.218771] [ema: 0.999685] 
[Epoch 580/1316] [Batch 0/38] [D loss: 0.299516] [G loss: 0.262982] [ema: 0.999686] 
[Epoch 581/1316] [Batch 0/38] [D loss: 0.260033] [G loss: 0.250804] [ema: 0.999686] 
[Epoch 582/1316] [Batch 0/38] [D loss: 0.306807] [G loss: 0.244679] [ema: 0.999687] 
[Epoch 583/1316] [Batch 0/38] [D loss: 0.297389] [G loss: 0.253524] [ema: 0.999687] 
[Epoch 584/1316] [Batch 0/38] [D loss: 0.328158] [G loss: 0.252947] [ema: 0.999688] 
[Epoch 585/1316] [Batch 0/38] [D loss: 0.263613] [G loss: 0.254572] [ema: 0.999688] 
[Epoch 586/1316] [Batch 0/38] [D loss: 0.348317] [G loss: 0.235525] [ema: 0.999689] 
[Epoch 587/1316] [Batch 0/38] [D loss: 0.321287] [G loss: 0.231783] [ema: 0.999689] 
[Epoch 588/1316] [Batch 0/38] [D loss: 0.283006] [G loss: 0.230077] [ema: 0.999690] 
[Epoch 589/1316] [Batch 0/38] [D loss: 0.306941] [G loss: 0.235661] [ema: 0.999690] 
[Epoch 590/1316] [Batch 0/38] [D loss: 0.311648] [G loss: 0.253395] [ema: 0.999691] 
[Epoch 591/1316] [Batch 0/38] [D loss: 0.282171] [G loss: 0.231658] [ema: 0.999691] 
[Epoch 592/1316] [Batch 0/38] [D loss: 0.296609] [G loss: 0.240482] [ema: 0.999692] 
[Epoch 593/1316] [Batch 0/38] [D loss: 0.276670] [G loss: 0.236525] [ema: 0.999692] 
[Epoch 594/1316] [Batch 0/38] [D loss: 0.265034] [G loss: 0.236771] [ema: 0.999693] 
[Epoch 595/1316] [Batch 0/38] [D loss: 0.292402] [G loss: 0.230217] [ema: 0.999693] 
[Epoch 596/1316] [Batch 0/38] [D loss: 0.265931] [G loss: 0.252806] [ema: 0.999694] 
[Epoch 597/1316] [Batch 0/38] [D loss: 0.260527] [G loss: 0.248794] [ema: 0.999695] 
[Epoch 598/1316] [Batch 0/38] [D loss: 0.267589] [G loss: 0.260406] [ema: 0.999695] 
[Epoch 599/1316] [Batch 0/38] [D loss: 0.287385] [G loss: 0.235948] [ema: 0.999696] 
[Epoch 600/1316] [Batch 0/38] [D loss: 0.260074] [G loss: 0.269683] [ema: 0.999696] 
[Epoch 601/1316] [Batch 0/38] [D loss: 0.281503] [G loss: 0.251223] [ema: 0.999697] 
[Epoch 602/1316] [Batch 0/38] [D loss: 0.277090] [G loss: 0.240298] [ema: 0.999697] 
[Epoch 603/1316] [Batch 0/38] [D loss: 0.283975] [G loss: 0.232721] [ema: 0.999698] 
[Epoch 604/1316] [Batch 0/38] [D loss: 0.261434] [G loss: 0.249956] [ema: 0.999698] 
[Epoch 605/1316] [Batch 0/38] [D loss: 0.309599] [G loss: 0.233194] [ema: 0.999699] 
[Epoch 606/1316] [Batch 0/38] [D loss: 0.262923] [G loss: 0.246303] [ema: 0.999699] 
[Epoch 607/1316] [Batch 0/38] [D loss: 0.262275] [G loss: 0.244067] [ema: 0.999700] 
[Epoch 608/1316] [Batch 0/38] [D loss: 0.266487] [G loss: 0.246619] [ema: 0.999700] 
[Epoch 609/1316] [Batch 0/38] [D loss: 0.280509] [G loss: 0.238743] [ema: 0.999701] 
[Epoch 610/1316] [Batch 0/38] [D loss: 0.258419] [G loss: 0.254795] [ema: 0.999701] 
[Epoch 611/1316] [Batch 0/38] [D loss: 0.296605] [G loss: 0.226703] [ema: 0.999702] 
[Epoch 612/1316] [Batch 0/38] [D loss: 0.260478] [G loss: 0.238067] [ema: 0.999702] 
[Epoch 613/1316] [Batch 0/38] [D loss: 0.284104] [G loss: 0.242319] [ema: 0.999702] 
[Epoch 614/1316] [Batch 0/38] [D loss: 0.268052] [G loss: 0.254219] [ema: 0.999703] 
[Epoch 615/1316] [Batch 0/38] [D loss: 0.253663] [G loss: 0.260904] [ema: 0.999703] 
[Epoch 616/1316] [Batch 0/38] [D loss: 0.257599] [G loss: 0.249888] [ema: 0.999704] 
[Epoch 617/1316] [Batch 0/38] [D loss: 0.244634] [G loss: 0.265498] [ema: 0.999704] 
[Epoch 618/1316] [Batch 0/38] [D loss: 0.270111] [G loss: 0.243269] [ema: 0.999705] 
[Epoch 619/1316] [Batch 0/38] [D loss: 0.337089] [G loss: 0.229915] [ema: 0.999705] 
[Epoch 620/1316] [Batch 0/38] [D loss: 0.266039] [G loss: 0.245043] [ema: 0.999706] 
[Epoch 621/1316] [Batch 0/38] [D loss: 0.295545] [G loss: 0.249590] [ema: 0.999706] 
[Epoch 622/1316] [Batch 0/38] [D loss: 0.275097] [G loss: 0.223341] [ema: 0.999707] 
[Epoch 623/1316] [Batch 0/38] [D loss: 0.253354] [G loss: 0.253373] [ema: 0.999707] 
[Epoch 624/1316] [Batch 0/38] [D loss: 0.270677] [G loss: 0.249231] [ema: 0.999708] 
[Epoch 625/1316] [Batch 0/38] [D loss: 0.293225] [G loss: 0.233712] [ema: 0.999708] 
[Epoch 626/1316] [Batch 0/38] [D loss: 0.271572] [G loss: 0.247008] [ema: 0.999709] 
[Epoch 627/1316] [Batch 0/38] [D loss: 0.274785] [G loss: 0.261935] [ema: 0.999709] 
[Epoch 628/1316] [Batch 0/38] [D loss: 0.277106] [G loss: 0.263116] [ema: 0.999710] 
[Epoch 629/1316] [Batch 0/38] [D loss: 0.306690] [G loss: 0.242573] [ema: 0.999710] 
[Epoch 630/1316] [Batch 0/38] [D loss: 0.329189] [G loss: 0.228656] [ema: 0.999711] 
[Epoch 631/1316] [Batch 0/38] [D loss: 0.271508] [G loss: 0.233341] [ema: 0.999711] 
[Epoch 632/1316] [Batch 0/38] [D loss: 0.267499] [G loss: 0.236924] [ema: 0.999711] 
[Epoch 633/1316] [Batch 0/38] [D loss: 0.273851] [G loss: 0.236360] [ema: 0.999712] 
[Epoch 634/1316] [Batch 0/38] [D loss: 0.303618] [G loss: 0.246845] [ema: 0.999712] 
[Epoch 635/1316] [Batch 0/38] [D loss: 0.296201] [G loss: 0.240019] [ema: 0.999713] 
[Epoch 636/1316] [Batch 0/38] [D loss: 0.264060] [G loss: 0.239977] [ema: 0.999713] 
[Epoch 637/1316] [Batch 0/38] [D loss: 0.282560] [G loss: 0.233031] [ema: 0.999714] 
[Epoch 638/1316] [Batch 0/38] [D loss: 0.271469] [G loss: 0.236919] [ema: 0.999714] 
[Epoch 639/1316] [Batch 0/38] [D loss: 0.250139] [G loss: 0.239593] [ema: 0.999715] 
[Epoch 640/1316] [Batch 0/38] [D loss: 0.256412] [G loss: 0.248782] [ema: 0.999715] 
[Epoch 641/1316] [Batch 0/38] [D loss: 0.244748] [G loss: 0.245753] [ema: 0.999715] 
[Epoch 642/1316] [Batch 0/38] [D loss: 0.248041] [G loss: 0.247915] [ema: 0.999716] 
[Epoch 643/1316] [Batch 0/38] [D loss: 0.299958] [G loss: 0.242683] [ema: 0.999716] 
[Epoch 644/1316] [Batch 0/38] [D loss: 0.256989] [G loss: 0.236731] [ema: 0.999717] 
[Epoch 645/1316] [Batch 0/38] [D loss: 0.286838] [G loss: 0.237115] [ema: 0.999717] 
[Epoch 646/1316] [Batch 0/38] [D loss: 0.315834] [G loss: 0.252967] [ema: 0.999718] 
[Epoch 647/1316] [Batch 0/38] [D loss: 0.252012] [G loss: 0.263048] [ema: 0.999718] 
[Epoch 648/1316] [Batch 0/38] [D loss: 0.262491] [G loss: 0.237232] [ema: 0.999719] 
[Epoch 649/1316] [Batch 0/38] [D loss: 0.275359] [G loss: 0.254509] [ema: 0.999719] 
[Epoch 650/1316] [Batch 0/38] [D loss: 0.286944] [G loss: 0.251237] [ema: 0.999719] 
[Epoch 651/1316] [Batch 0/38] [D loss: 0.280925] [G loss: 0.237693] [ema: 0.999720] 
[Epoch 652/1316] [Batch 0/38] [D loss: 0.276364] [G loss: 0.239686] [ema: 0.999720] 
[Epoch 653/1316] [Batch 0/38] [D loss: 0.274559] [G loss: 0.248213] [ema: 0.999721] 
[Epoch 654/1316] [Batch 0/38] [D loss: 0.325035] [G loss: 0.240883] [ema: 0.999721] 
[Epoch 655/1316] [Batch 0/38] [D loss: 0.307599] [G loss: 0.230711] [ema: 0.999722] 
[Epoch 656/1316] [Batch 0/38] [D loss: 0.303675] [G loss: 0.257839] [ema: 0.999722] 
[Epoch 657/1316] [Batch 0/38] [D loss: 0.285337] [G loss: 0.246117] [ema: 0.999722] 
[Epoch 658/1316] [Batch 0/38] [D loss: 0.252316] [G loss: 0.255526] [ema: 0.999723] 
[Epoch 659/1316] [Batch 0/38] [D loss: 0.236641] [G loss: 0.248487] [ema: 0.999723] 
[Epoch 660/1316] [Batch 0/38] [D loss: 0.336037] [G loss: 0.233404] [ema: 0.999724] 
[Epoch 661/1316] [Batch 0/38] [D loss: 0.281395] [G loss: 0.237111] [ema: 0.999724] 
[Epoch 662/1316] [Batch 0/38] [D loss: 0.263819] [G loss: 0.229042] [ema: 0.999724] 
[Epoch 663/1316] [Batch 0/38] [D loss: 0.258567] [G loss: 0.244604] [ema: 0.999725] 
[Epoch 664/1316] [Batch 0/38] [D loss: 0.303520] [G loss: 0.253794] [ema: 0.999725] 
[Epoch 665/1316] [Batch 0/38] [D loss: 0.259806] [G loss: 0.268768] [ema: 0.999726] 
[Epoch 666/1316] [Batch 0/38] [D loss: 0.273944] [G loss: 0.243123] [ema: 0.999726] 
[Epoch 667/1316] [Batch 0/38] [D loss: 0.279037] [G loss: 0.242433] [ema: 0.999727] 
[Epoch 668/1316] [Batch 0/38] [D loss: 0.275059] [G loss: 0.241636] [ema: 0.999727] 
[Epoch 669/1316] [Batch 0/38] [D loss: 0.275966] [G loss: 0.232067] [ema: 0.999727] 
[Epoch 670/1316] [Batch 0/38] [D loss: 0.250828] [G loss: 0.238356] [ema: 0.999728] 
[Epoch 671/1316] [Batch 0/38] [D loss: 0.249069] [G loss: 0.234007] [ema: 0.999728] 
[Epoch 672/1316] [Batch 0/38] [D loss: 0.276769] [G loss: 0.241377] [ema: 0.999729] 
[Epoch 673/1316] [Batch 0/38] [D loss: 0.251913] [G loss: 0.250933] [ema: 0.999729] 
[Epoch 674/1316] [Batch 0/38] [D loss: 0.251536] [G loss: 0.242774] [ema: 0.999729] 
[Epoch 675/1316] [Batch 0/38] [D loss: 0.248883] [G loss: 0.249450] [ema: 0.999730] 
[Epoch 676/1316] [Batch 0/38] [D loss: 0.283098] [G loss: 0.239611] [ema: 0.999730] 
[Epoch 677/1316] [Batch 0/38] [D loss: 0.299283] [G loss: 0.243762] [ema: 0.999731] 
[Epoch 678/1316] [Batch 0/38] [D loss: 0.274446] [G loss: 0.235711] [ema: 0.999731] 
[Epoch 679/1316] [Batch 0/38] [D loss: 0.346966] [G loss: 0.240270] [ema: 0.999731] 
[Epoch 680/1316] [Batch 0/38] [D loss: 0.258486] [G loss: 0.244393] [ema: 0.999732] 
[Epoch 681/1316] [Batch 0/38] [D loss: 0.281609] [G loss: 0.245637] [ema: 0.999732] 
[Epoch 682/1316] [Batch 0/38] [D loss: 0.276808] [G loss: 0.253373] [ema: 0.999733] 
[Epoch 683/1316] [Batch 0/38] [D loss: 0.249226] [G loss: 0.253592] [ema: 0.999733] 
[Epoch 684/1316] [Batch 0/38] [D loss: 0.278013] [G loss: 0.247442] [ema: 0.999733] 
[Epoch 685/1316] [Batch 0/38] [D loss: 0.244600] [G loss: 0.257329] [ema: 0.999734] 
[Epoch 686/1316] [Batch 0/38] [D loss: 0.285393] [G loss: 0.230607] [ema: 0.999734] 
[Epoch 687/1316] [Batch 0/38] [D loss: 0.263990] [G loss: 0.234119] [ema: 0.999735] 
[Epoch 688/1316] [Batch 0/38] [D loss: 0.290843] [G loss: 0.238863] [ema: 0.999735] 
[Epoch 689/1316] [Batch 0/38] [D loss: 0.283761] [G loss: 0.231347] [ema: 0.999735] 
[Epoch 690/1316] [Batch 0/38] [D loss: 0.245697] [G loss: 0.257792] [ema: 0.999736] 
[Epoch 691/1316] [Batch 0/38] [D loss: 0.256079] [G loss: 0.230537] [ema: 0.999736] 
[Epoch 692/1316] [Batch 0/38] [D loss: 0.278542] [G loss: 0.227222] [ema: 0.999736] 
[Epoch 693/1316] [Batch 0/38] [D loss: 0.274397] [G loss: 0.252477] [ema: 0.999737] 
[Epoch 694/1316] [Batch 0/38] [D loss: 0.251794] [G loss: 0.237122] [ema: 0.999737] 
[Epoch 695/1316] [Batch 0/38] [D loss: 0.254683] [G loss: 0.245600] [ema: 0.999738] 
[Epoch 696/1316] [Batch 0/38] [D loss: 0.253888] [G loss: 0.243022] [ema: 0.999738] 
[Epoch 697/1316] [Batch 0/38] [D loss: 0.257555] [G loss: 0.249308] [ema: 0.999738] 
[Epoch 698/1316] [Batch 0/38] [D loss: 0.264398] [G loss: 0.251078] [ema: 0.999739] 
[Epoch 699/1316] [Batch 0/38] [D loss: 0.285896] [G loss: 0.240924] [ema: 0.999739] 
[Epoch 700/1316] [Batch 0/38] [D loss: 0.285304] [G loss: 0.233445] [ema: 0.999739] 
[Epoch 701/1316] [Batch 0/38] [D loss: 0.270695] [G loss: 0.248075] [ema: 0.999740] 
[Epoch 702/1316] [Batch 0/38] [D loss: 0.268505] [G loss: 0.238865] [ema: 0.999740] 
[Epoch 703/1316] [Batch 0/38] [D loss: 0.296905] [G loss: 0.243265] [ema: 0.999741] 
[Epoch 704/1316] [Batch 0/38] [D loss: 0.313993] [G loss: 0.225827] [ema: 0.999741] 
[Epoch 705/1316] [Batch 0/38] [D loss: 0.268913] [G loss: 0.234870] [ema: 0.999741] 
[Epoch 706/1316] [Batch 0/38] [D loss: 0.262862] [G loss: 0.243092] [ema: 0.999742] 
[Epoch 707/1316] [Batch 0/38] [D loss: 0.296244] [G loss: 0.241524] [ema: 0.999742] 
[Epoch 708/1316] [Batch 0/38] [D loss: 0.276812] [G loss: 0.239092] [ema: 0.999742] 
[Epoch 709/1316] [Batch 0/38] [D loss: 0.278629] [G loss: 0.246612] [ema: 0.999743] 
[Epoch 710/1316] [Batch 0/38] [D loss: 0.278737] [G loss: 0.254771] [ema: 0.999743] 
[Epoch 711/1316] [Batch 0/38] [D loss: 0.262564] [G loss: 0.248479] [ema: 0.999743] 
[Epoch 712/1316] [Batch 0/38] [D loss: 0.301433] [G loss: 0.233468] [ema: 0.999744] 
[Epoch 713/1316] [Batch 0/38] [D loss: 0.263364] [G loss: 0.236508] [ema: 0.999744] 
[Epoch 714/1316] [Batch 0/38] [D loss: 0.265301] [G loss: 0.244124] [ema: 0.999745] 
[Epoch 715/1316] [Batch 0/38] [D loss: 0.258702] [G loss: 0.251461] [ema: 0.999745] 
[Epoch 716/1316] [Batch 0/38] [D loss: 0.249671] [G loss: 0.244669] [ema: 0.999745] 
[Epoch 717/1316] [Batch 0/38] [D loss: 0.257924] [G loss: 0.223248] [ema: 0.999746] 
[Epoch 718/1316] [Batch 0/38] [D loss: 0.277214] [G loss: 0.243069] [ema: 0.999746] 
[Epoch 719/1316] [Batch 0/38] [D loss: 0.259632] [G loss: 0.237047] [ema: 0.999746] 
[Epoch 720/1316] [Batch 0/38] [D loss: 0.266733] [G loss: 0.235795] [ema: 0.999747] 
[Epoch 721/1316] [Batch 0/38] [D loss: 0.260299] [G loss: 0.252908] [ema: 0.999747] 
[Epoch 722/1316] [Batch 0/38] [D loss: 0.299703] [G loss: 0.253461] [ema: 0.999747] 
[Epoch 723/1316] [Batch 0/38] [D loss: 0.294569] [G loss: 0.223607] [ema: 0.999748] 
[Epoch 724/1316] [Batch 0/38] [D loss: 0.266349] [G loss: 0.235637] [ema: 0.999748] 
[Epoch 725/1316] [Batch 0/38] [D loss: 0.253810] [G loss: 0.248639] [ema: 0.999748] 
[Epoch 726/1316] [Batch 0/38] [D loss: 0.255321] [G loss: 0.251755] [ema: 0.999749] 
[Epoch 727/1316] [Batch 0/38] [D loss: 0.260248] [G loss: 0.237551] [ema: 0.999749] 
[Epoch 728/1316] [Batch 0/38] [D loss: 0.247464] [G loss: 0.250665] [ema: 0.999749] 
[Epoch 729/1316] [Batch 0/38] [D loss: 0.263819] [G loss: 0.230361] [ema: 0.999750] 
[Epoch 730/1316] [Batch 0/38] [D loss: 0.283787] [G loss: 0.233846] [ema: 0.999750] 
[Epoch 731/1316] [Batch 0/38] [D loss: 0.259483] [G loss: 0.246798] [ema: 0.999751] 
[Epoch 732/1316] [Batch 0/38] [D loss: 0.302948] [G loss: 0.240571] [ema: 0.999751] 
[Epoch 733/1316] [Batch 0/38] [D loss: 0.264722] [G loss: 0.253796] [ema: 0.999751] 
[Epoch 734/1316] [Batch 0/38] [D loss: 0.255506] [G loss: 0.257564] [ema: 0.999752] 
[Epoch 735/1316] [Batch 0/38] [D loss: 0.281665] [G loss: 0.230107] [ema: 0.999752] 
[Epoch 736/1316] [Batch 0/38] [D loss: 0.270523] [G loss: 0.243162] [ema: 0.999752] 
[Epoch 737/1316] [Batch 0/38] [D loss: 0.282069] [G loss: 0.244538] [ema: 0.999753] 
[Epoch 738/1316] [Batch 0/38] [D loss: 0.244719] [G loss: 0.256242] [ema: 0.999753] 
[Epoch 739/1316] [Batch 0/38] [D loss: 0.256044] [G loss: 0.250893] [ema: 0.999753] 
[Epoch 740/1316] [Batch 0/38] [D loss: 0.287031] [G loss: 0.244916] [ema: 0.999754] 
[Epoch 741/1316] [Batch 0/38] [D loss: 0.279775] [G loss: 0.233049] [ema: 0.999754] 
[Epoch 742/1316] [Batch 0/38] [D loss: 0.294451] [G loss: 0.244771] [ema: 0.999754] 
[Epoch 743/1316] [Batch 0/38] [D loss: 0.276166] [G loss: 0.236485] [ema: 0.999755] 
[Epoch 744/1316] [Batch 0/38] [D loss: 0.257199] [G loss: 0.239481] [ema: 0.999755] 
[Epoch 745/1316] [Batch 0/38] [D loss: 0.286610] [G loss: 0.244690] [ema: 0.999755] 
[Epoch 746/1316] [Batch 0/38] [D loss: 0.260262] [G loss: 0.258343] [ema: 0.999756] 
[Epoch 747/1316] [Batch 0/38] [D loss: 0.289947] [G loss: 0.227017] [ema: 0.999756] 
[Epoch 748/1316] [Batch 0/38] [D loss: 0.272360] [G loss: 0.221443] [ema: 0.999756] 
[Epoch 749/1316] [Batch 0/38] [D loss: 0.331824] [G loss: 0.233863] [ema: 0.999756] 
[Epoch 750/1316] [Batch 0/38] [D loss: 0.274725] [G loss: 0.243328] [ema: 0.999757] 
[Epoch 751/1316] [Batch 0/38] [D loss: 0.258712] [G loss: 0.242385] [ema: 0.999757] 
[Epoch 752/1316] [Batch 0/38] [D loss: 0.294517] [G loss: 0.214611] [ema: 0.999757] 
[Epoch 753/1316] [Batch 0/38] [D loss: 0.290412] [G loss: 0.233899] [ema: 0.999758] 
[Epoch 754/1316] [Batch 0/38] [D loss: 0.265482] [G loss: 0.255981] [ema: 0.999758] 
[Epoch 755/1316] [Batch 0/38] [D loss: 0.263973] [G loss: 0.240214] [ema: 0.999758] 
[Epoch 756/1316] [Batch 0/38] [D loss: 0.324845] [G loss: 0.223165] [ema: 0.999759] 
[Epoch 757/1316] [Batch 0/38] [D loss: 0.244317] [G loss: 0.250155] [ema: 0.999759] 
[Epoch 758/1316] [Batch 0/38] [D loss: 0.268319] [G loss: 0.243481] [ema: 0.999759] 
[Epoch 759/1316] [Batch 0/38] [D loss: 0.257000] [G loss: 0.250195] [ema: 0.999760] 
[Epoch 760/1316] [Batch 0/38] [D loss: 0.301045] [G loss: 0.246175] [ema: 0.999760] 
[Epoch 761/1316] [Batch 0/38] [D loss: 0.258929] [G loss: 0.239075] [ema: 0.999760] 
[Epoch 762/1316] [Batch 0/38] [D loss: 0.248365] [G loss: 0.245274] [ema: 0.999761] 
[Epoch 763/1316] [Batch 0/38] [D loss: 0.274478] [G loss: 0.241444] [ema: 0.999761] 
[Epoch 764/1316] [Batch 0/38] [D loss: 0.245072] [G loss: 0.240969] [ema: 0.999761] 
[Epoch 765/1316] [Batch 0/38] [D loss: 0.257193] [G loss: 0.240524] [ema: 0.999762] 
[Epoch 766/1316] [Batch 0/38] [D loss: 0.273810] [G loss: 0.224812] [ema: 0.999762] 
[Epoch 767/1316] [Batch 0/38] [D loss: 0.267119] [G loss: 0.239848] [ema: 0.999762] 
[Epoch 768/1316] [Batch 0/38] [D loss: 0.254571] [G loss: 0.239915] [ema: 0.999763] 
[Epoch 769/1316] [Batch 0/38] [D loss: 0.268367] [G loss: 0.238888] [ema: 0.999763] 
[Epoch 770/1316] [Batch 0/38] [D loss: 0.261922] [G loss: 0.250021] [ema: 0.999763] 
[Epoch 771/1316] [Batch 0/38] [D loss: 0.256592] [G loss: 0.259844] [ema: 0.999763] 
[Epoch 772/1316] [Batch 0/38] [D loss: 0.265225] [G loss: 0.236536] [ema: 0.999764] 
[Epoch 773/1316] [Batch 0/38] [D loss: 0.273896] [G loss: 0.239066] [ema: 0.999764] 
[Epoch 774/1316] [Batch 0/38] [D loss: 0.289767] [G loss: 0.244318] [ema: 0.999764] 
[Epoch 775/1316] [Batch 0/38] [D loss: 0.280878] [G loss: 0.234627] [ema: 0.999765] 
[Epoch 776/1316] [Batch 0/38] [D loss: 0.256638] [G loss: 0.252480] [ema: 0.999765] 
[Epoch 777/1316] [Batch 0/38] [D loss: 0.264724] [G loss: 0.233872] [ema: 0.999765] 
[Epoch 778/1316] [Batch 0/38] [D loss: 0.285367] [G loss: 0.242867] [ema: 0.999766] 
[Epoch 779/1316] [Batch 0/38] [D loss: 0.254927] [G loss: 0.253020] [ema: 0.999766] 
[Epoch 780/1316] [Batch 0/38] [D loss: 0.270465] [G loss: 0.242833] [ema: 0.999766] 
[Epoch 781/1316] [Batch 0/38] [D loss: 0.262333] [G loss: 0.245650] [ema: 0.999766] 
[Epoch 782/1316] [Batch 0/38] [D loss: 0.325669] [G loss: 0.249676] [ema: 0.999767] 
[Epoch 783/1316] [Batch 0/38] [D loss: 0.253286] [G loss: 0.242854] [ema: 0.999767] 
[Epoch 784/1316] [Batch 0/38] [D loss: 0.294384] [G loss: 0.226987] [ema: 0.999767] 
[Epoch 785/1316] [Batch 0/38] [D loss: 0.261768] [G loss: 0.234076] [ema: 0.999768] 
[Epoch 786/1316] [Batch 0/38] [D loss: 0.277472] [G loss: 0.247721] [ema: 0.999768] 
[Epoch 787/1316] [Batch 0/38] [D loss: 0.245421] [G loss: 0.252910] [ema: 0.999768] 
[Epoch 788/1316] [Batch 0/38] [D loss: 0.273100] [G loss: 0.236036] [ema: 0.999769] 
[Epoch 789/1316] [Batch 0/38] [D loss: 0.252182] [G loss: 0.250506] [ema: 0.999769] 
[Epoch 790/1316] [Batch 0/38] [D loss: 0.287373] [G loss: 0.233353] [ema: 0.999769] 
[Epoch 791/1316] [Batch 0/38] [D loss: 0.264545] [G loss: 0.258731] [ema: 0.999769] 




Saving checkpoint 4 in logs/Jumping_2024_10_02_17_29_42/Model




[Epoch 792/1316] [Batch 0/38] [D loss: 0.256398] [G loss: 0.243316] [ema: 0.999770] 
[Epoch 793/1316] [Batch 0/38] [D loss: 0.270883] [G loss: 0.239065] [ema: 0.999770] 
[Epoch 794/1316] [Batch 0/38] [D loss: 0.261797] [G loss: 0.220336] [ema: 0.999770] 
[Epoch 795/1316] [Batch 0/38] [D loss: 0.263282] [G loss: 0.239199] [ema: 0.999771] 
[Epoch 796/1316] [Batch 0/38] [D loss: 0.280825] [G loss: 0.234401] [ema: 0.999771] 
[Epoch 797/1316] [Batch 0/38] [D loss: 0.255019] [G loss: 0.264159] [ema: 0.999771] 
[Epoch 798/1316] [Batch 0/38] [D loss: 0.270448] [G loss: 0.241757] [ema: 0.999771] 
[Epoch 799/1316] [Batch 0/38] [D loss: 0.249438] [G loss: 0.259198] [ema: 0.999772] 
[Epoch 800/1316] [Batch 0/38] [D loss: 0.274477] [G loss: 0.227682] [ema: 0.999772] 
[Epoch 801/1316] [Batch 0/38] [D loss: 0.285096] [G loss: 0.256338] [ema: 0.999772] 
[Epoch 802/1316] [Batch 0/38] [D loss: 0.287651] [G loss: 0.249179] [ema: 0.999773] 
[Epoch 803/1316] [Batch 0/38] [D loss: 0.261661] [G loss: 0.240596] [ema: 0.999773] 
[Epoch 804/1316] [Batch 0/38] [D loss: 0.311831] [G loss: 0.234641] [ema: 0.999773] 
[Epoch 805/1316] [Batch 0/38] [D loss: 0.270271] [G loss: 0.238483] [ema: 0.999773] 
[Epoch 806/1316] [Batch 0/38] [D loss: 0.244400] [G loss: 0.260152] [ema: 0.999774] 
[Epoch 807/1316] [Batch 0/38] [D loss: 0.264641] [G loss: 0.225262] [ema: 0.999774] 
[Epoch 808/1316] [Batch 0/38] [D loss: 0.251051] [G loss: 0.256289] [ema: 0.999774] 
[Epoch 809/1316] [Batch 0/38] [D loss: 0.324286] [G loss: 0.250909] [ema: 0.999775] 
[Epoch 810/1316] [Batch 0/38] [D loss: 0.319388] [G loss: 0.232207] [ema: 0.999775] 
[Epoch 811/1316] [Batch 0/38] [D loss: 0.255388] [G loss: 0.245234] [ema: 0.999775] 
[Epoch 812/1316] [Batch 0/38] [D loss: 0.278804] [G loss: 0.246684] [ema: 0.999775] 
[Epoch 813/1316] [Batch 0/38] [D loss: 0.249162] [G loss: 0.257000] [ema: 0.999776] 
[Epoch 814/1316] [Batch 0/38] [D loss: 0.290034] [G loss: 0.239259] [ema: 0.999776] 
[Epoch 815/1316] [Batch 0/38] [D loss: 0.304623] [G loss: 0.264426] [ema: 0.999776] 
[Epoch 816/1316] [Batch 0/38] [D loss: 0.258512] [G loss: 0.245935] [ema: 0.999776] 
[Epoch 817/1316] [Batch 0/38] [D loss: 0.290609] [G loss: 0.241939] [ema: 0.999777] 
[Epoch 818/1316] [Batch 0/38] [D loss: 0.251372] [G loss: 0.247713] [ema: 0.999777] 
[Epoch 819/1316] [Batch 0/38] [D loss: 0.254470] [G loss: 0.249349] [ema: 0.999777] 
[Epoch 820/1316] [Batch 0/38] [D loss: 0.251239] [G loss: 0.249819] [ema: 0.999778] 
[Epoch 821/1316] [Batch 0/38] [D loss: 0.275338] [G loss: 0.235321] [ema: 0.999778] 
[Epoch 822/1316] [Batch 0/38] [D loss: 0.292758] [G loss: 0.243928] [ema: 0.999778] 
[Epoch 823/1316] [Batch 0/38] [D loss: 0.266812] [G loss: 0.240660] [ema: 0.999778] 
[Epoch 824/1316] [Batch 0/38] [D loss: 0.298221] [G loss: 0.244580] [ema: 0.999779] 
[Epoch 825/1316] [Batch 0/38] [D loss: 0.304995] [G loss: 0.235211] [ema: 0.999779] 
[Epoch 826/1316] [Batch 0/38] [D loss: 0.294636] [G loss: 0.253169] [ema: 0.999779] 
[Epoch 827/1316] [Batch 0/38] [D loss: 0.264137] [G loss: 0.237205] [ema: 0.999779] 
[Epoch 828/1316] [Batch 0/38] [D loss: 0.284807] [G loss: 0.240990] [ema: 0.999780] 
[Epoch 829/1316] [Batch 0/38] [D loss: 0.237008] [G loss: 0.254587] [ema: 0.999780] 
[Epoch 830/1316] [Batch 0/38] [D loss: 0.262054] [G loss: 0.238637] [ema: 0.999780] 
[Epoch 831/1316] [Batch 0/38] [D loss: 0.240350] [G loss: 0.241000] [ema: 0.999781] 
[Epoch 832/1316] [Batch 0/38] [D loss: 0.266586] [G loss: 0.236636] [ema: 0.999781] 
[Epoch 833/1316] [Batch 0/38] [D loss: 0.285867] [G loss: 0.235840] [ema: 0.999781] 
[Epoch 834/1316] [Batch 0/38] [D loss: 0.266345] [G loss: 0.254318] [ema: 0.999781] 
[Epoch 835/1316] [Batch 0/38] [D loss: 0.264316] [G loss: 0.255603] [ema: 0.999782] 
[Epoch 836/1316] [Batch 0/38] [D loss: 0.264953] [G loss: 0.228537] [ema: 0.999782] 
[Epoch 837/1316] [Batch 0/38] [D loss: 0.275753] [G loss: 0.236591] [ema: 0.999782] 
[Epoch 838/1316] [Batch 0/38] [D loss: 0.284079] [G loss: 0.248846] [ema: 0.999782] 
[Epoch 839/1316] [Batch 0/38] [D loss: 0.244391] [G loss: 0.237051] [ema: 0.999783] 
[Epoch 840/1316] [Batch 0/38] [D loss: 0.268599] [G loss: 0.240894] [ema: 0.999783] 
[Epoch 841/1316] [Batch 0/38] [D loss: 0.268737] [G loss: 0.265441] [ema: 0.999783] 
[Epoch 842/1316] [Batch 0/38] [D loss: 0.267793] [G loss: 0.249110] [ema: 0.999783] 
[Epoch 843/1316] [Batch 0/38] [D loss: 0.305286] [G loss: 0.242097] [ema: 0.999784] 
[Epoch 844/1316] [Batch 0/38] [D loss: 0.262965] [G loss: 0.256798] [ema: 0.999784] 
[Epoch 845/1316] [Batch 0/38] [D loss: 0.254930] [G loss: 0.249130] [ema: 0.999784] 
[Epoch 846/1316] [Batch 0/38] [D loss: 0.285040] [G loss: 0.247205] [ema: 0.999784] 
[Epoch 847/1316] [Batch 0/38] [D loss: 0.276462] [G loss: 0.230746] [ema: 0.999785] 
[Epoch 848/1316] [Batch 0/38] [D loss: 0.289325] [G loss: 0.245495] [ema: 0.999785] 
[Epoch 849/1316] [Batch 0/38] [D loss: 0.316915] [G loss: 0.219732] [ema: 0.999785] 
[Epoch 850/1316] [Batch 0/38] [D loss: 0.245335] [G loss: 0.238632] [ema: 0.999785] 
[Epoch 851/1316] [Batch 0/38] [D loss: 0.263798] [G loss: 0.236483] [ema: 0.999786] 
[Epoch 852/1316] [Batch 0/38] [D loss: 0.270148] [G loss: 0.246076] [ema: 0.999786] 
[Epoch 853/1316] [Batch 0/38] [D loss: 0.275674] [G loss: 0.228547] [ema: 0.999786] 
[Epoch 854/1316] [Batch 0/38] [D loss: 0.281676] [G loss: 0.230700] [ema: 0.999786] 
[Epoch 855/1316] [Batch 0/38] [D loss: 0.253714] [G loss: 0.212783] [ema: 0.999787] 
[Epoch 856/1316] [Batch 0/38] [D loss: 0.286057] [G loss: 0.231655] [ema: 0.999787] 
[Epoch 857/1316] [Batch 0/38] [D loss: 0.281237] [G loss: 0.237295] [ema: 0.999787] 
[Epoch 858/1316] [Batch 0/38] [D loss: 0.259559] [G loss: 0.244118] [ema: 0.999787] 
[Epoch 859/1316] [Batch 0/38] [D loss: 0.269612] [G loss: 0.244537] [ema: 0.999788] 
[Epoch 860/1316] [Batch 0/38] [D loss: 0.284753] [G loss: 0.237334] [ema: 0.999788] 
[Epoch 861/1316] [Batch 0/38] [D loss: 0.250152] [G loss: 0.233422] [ema: 0.999788] 
[Epoch 862/1316] [Batch 0/38] [D loss: 0.284741] [G loss: 0.243979] [ema: 0.999788] 
[Epoch 863/1316] [Batch 0/38] [D loss: 0.256808] [G loss: 0.246287] [ema: 0.999789] 
[Epoch 864/1316] [Batch 0/38] [D loss: 0.255013] [G loss: 0.250458] [ema: 0.999789] 
[Epoch 865/1316] [Batch 0/38] [D loss: 0.261118] [G loss: 0.266189] [ema: 0.999789] 
[Epoch 866/1316] [Batch 0/38] [D loss: 0.243602] [G loss: 0.250662] [ema: 0.999789] 
[Epoch 867/1316] [Batch 0/38] [D loss: 0.271736] [G loss: 0.233968] [ema: 0.999790] 
[Epoch 868/1316] [Batch 0/38] [D loss: 0.289308] [G loss: 0.255205] [ema: 0.999790] 
[Epoch 869/1316] [Batch 0/38] [D loss: 0.260063] [G loss: 0.248953] [ema: 0.999790] 
[Epoch 870/1316] [Batch 0/38] [D loss: 0.267368] [G loss: 0.259016] [ema: 0.999790] 
[Epoch 871/1316] [Batch 0/38] [D loss: 0.275003] [G loss: 0.248305] [ema: 0.999791] 
[Epoch 872/1316] [Batch 0/38] [D loss: 0.283579] [G loss: 0.245314] [ema: 0.999791] 
[Epoch 873/1316] [Batch 0/38] [D loss: 0.259817] [G loss: 0.247397] [ema: 0.999791] 
[Epoch 874/1316] [Batch 0/38] [D loss: 0.261999] [G loss: 0.237065] [ema: 0.999791] 
[Epoch 875/1316] [Batch 0/38] [D loss: 0.289138] [G loss: 0.247087] [ema: 0.999792] 
[Epoch 876/1316] [Batch 0/38] [D loss: 0.269287] [G loss: 0.241487] [ema: 0.999792] 
[Epoch 877/1316] [Batch 0/38] [D loss: 0.274965] [G loss: 0.242733] [ema: 0.999792] 
[Epoch 878/1316] [Batch 0/38] [D loss: 0.248078] [G loss: 0.230964] [ema: 0.999792] 
[Epoch 879/1316] [Batch 0/38] [D loss: 0.279504] [G loss: 0.236785] [ema: 0.999793] 
[Epoch 880/1316] [Batch 0/38] [D loss: 0.280807] [G loss: 0.256442] [ema: 0.999793] 
[Epoch 881/1316] [Batch 0/38] [D loss: 0.268980] [G loss: 0.242224] [ema: 0.999793] 
[Epoch 882/1316] [Batch 0/38] [D loss: 0.257959] [G loss: 0.247339] [ema: 0.999793] 
[Epoch 883/1316] [Batch 0/38] [D loss: 0.263298] [G loss: 0.246193] [ema: 0.999793] 
[Epoch 884/1316] [Batch 0/38] [D loss: 0.287256] [G loss: 0.248447] [ema: 0.999794] 
[Epoch 885/1316] [Batch 0/38] [D loss: 0.260416] [G loss: 0.249747] [ema: 0.999794] 
[Epoch 886/1316] [Batch 0/38] [D loss: 0.254008] [G loss: 0.247675] [ema: 0.999794] 
[Epoch 887/1316] [Batch 0/38] [D loss: 0.280231] [G loss: 0.237693] [ema: 0.999794] 
[Epoch 888/1316] [Batch 0/38] [D loss: 0.305828] [G loss: 0.241029] [ema: 0.999795] 
[Epoch 889/1316] [Batch 0/38] [D loss: 0.274641] [G loss: 0.245850] [ema: 0.999795] 
[Epoch 890/1316] [Batch 0/38] [D loss: 0.254770] [G loss: 0.249852] [ema: 0.999795] 
[Epoch 891/1316] [Batch 0/38] [D loss: 0.249377] [G loss: 0.256536] [ema: 0.999795] 
[Epoch 892/1316] [Batch 0/38] [D loss: 0.237426] [G loss: 0.255065] [ema: 0.999796] 
[Epoch 893/1316] [Batch 0/38] [D loss: 0.303652] [G loss: 0.231404] [ema: 0.999796] 
[Epoch 894/1316] [Batch 0/38] [D loss: 0.290730] [G loss: 0.240044] [ema: 0.999796] 
[Epoch 895/1316] [Batch 0/38] [D loss: 0.271191] [G loss: 0.234936] [ema: 0.999796] 
[Epoch 896/1316] [Batch 0/38] [D loss: 0.258921] [G loss: 0.244293] [ema: 0.999796] 
[Epoch 897/1316] [Batch 0/38] [D loss: 0.265823] [G loss: 0.248006] [ema: 0.999797] 
[Epoch 898/1316] [Batch 0/38] [D loss: 0.256064] [G loss: 0.246759] [ema: 0.999797] 
[Epoch 899/1316] [Batch 0/38] [D loss: 0.267416] [G loss: 0.244538] [ema: 0.999797] 
[Epoch 900/1316] [Batch 0/38] [D loss: 0.255181] [G loss: 0.247707] [ema: 0.999797] 
[Epoch 901/1316] [Batch 0/38] [D loss: 0.260443] [G loss: 0.226223] [ema: 0.999798] 
[Epoch 902/1316] [Batch 0/38] [D loss: 0.276200] [G loss: 0.237488] [ema: 0.999798] 
[Epoch 903/1316] [Batch 0/38] [D loss: 0.255344] [G loss: 0.233346] [ema: 0.999798] 
[Epoch 904/1316] [Batch 0/38] [D loss: 0.263182] [G loss: 0.248452] [ema: 0.999798] 
[Epoch 905/1316] [Batch 0/38] [D loss: 0.271685] [G loss: 0.243057] [ema: 0.999798] 
[Epoch 906/1316] [Batch 0/38] [D loss: 0.266911] [G loss: 0.248370] [ema: 0.999799] 
[Epoch 907/1316] [Batch 0/38] [D loss: 0.298890] [G loss: 0.216220] [ema: 0.999799] 
[Epoch 908/1316] [Batch 0/38] [D loss: 0.264578] [G loss: 0.240767] [ema: 0.999799] 
[Epoch 909/1316] [Batch 0/38] [D loss: 0.268596] [G loss: 0.251816] [ema: 0.999799] 
[Epoch 910/1316] [Batch 0/38] [D loss: 0.295966] [G loss: 0.239863] [ema: 0.999800] 
[Epoch 911/1316] [Batch 0/38] [D loss: 0.266257] [G loss: 0.217283] [ema: 0.999800] 
[Epoch 912/1316] [Batch 0/38] [D loss: 0.263092] [G loss: 0.248112] [ema: 0.999800] 
[Epoch 913/1316] [Batch 0/38] [D loss: 0.268764] [G loss: 0.255658] [ema: 0.999800] 
[Epoch 914/1316] [Batch 0/38] [D loss: 0.257678] [G loss: 0.249323] [ema: 0.999800] 
[Epoch 915/1316] [Batch 0/38] [D loss: 0.277718] [G loss: 0.227796] [ema: 0.999801] 
[Epoch 916/1316] [Batch 0/38] [D loss: 0.285657] [G loss: 0.250001] [ema: 0.999801] 
[Epoch 917/1316] [Batch 0/38] [D loss: 0.275625] [G loss: 0.244448] [ema: 0.999801] 
[Epoch 918/1316] [Batch 0/38] [D loss: 0.312070] [G loss: 0.240307] [ema: 0.999801] 
[Epoch 919/1316] [Batch 0/38] [D loss: 0.255075] [G loss: 0.238222] [ema: 0.999802] 
[Epoch 920/1316] [Batch 0/38] [D loss: 0.260283] [G loss: 0.257746] [ema: 0.999802] 
[Epoch 921/1316] [Batch 0/38] [D loss: 0.251425] [G loss: 0.236699] [ema: 0.999802] 
[Epoch 922/1316] [Batch 0/38] [D loss: 0.314103] [G loss: 0.238054] [ema: 0.999802] 
[Epoch 923/1316] [Batch 0/38] [D loss: 0.277118] [G loss: 0.249340] [ema: 0.999802] 
[Epoch 924/1316] [Batch 0/38] [D loss: 0.253463] [G loss: 0.248629] [ema: 0.999803] 
[Epoch 925/1316] [Batch 0/38] [D loss: 0.253098] [G loss: 0.255804] [ema: 0.999803] 
[Epoch 926/1316] [Batch 0/38] [D loss: 0.249754] [G loss: 0.241116] [ema: 0.999803] 
[Epoch 927/1316] [Batch 0/38] [D loss: 0.265500] [G loss: 0.253063] [ema: 0.999803] 
[Epoch 928/1316] [Batch 0/38] [D loss: 0.259471] [G loss: 0.246063] [ema: 0.999803] 
[Epoch 929/1316] [Batch 0/38] [D loss: 0.302449] [G loss: 0.250291] [ema: 0.999804] 
[Epoch 930/1316] [Batch 0/38] [D loss: 0.261163] [G loss: 0.240521] [ema: 0.999804] 
[Epoch 931/1316] [Batch 0/38] [D loss: 0.254394] [G loss: 0.249527] [ema: 0.999804] 
[Epoch 932/1316] [Batch 0/38] [D loss: 0.265323] [G loss: 0.259974] [ema: 0.999804] 
[Epoch 933/1316] [Batch 0/38] [D loss: 0.293936] [G loss: 0.243976] [ema: 0.999805] 
[Epoch 934/1316] [Batch 0/38] [D loss: 0.312815] [G loss: 0.234671] [ema: 0.999805] 
[Epoch 935/1316] [Batch 0/38] [D loss: 0.275374] [G loss: 0.239008] [ema: 0.999805] 
[Epoch 936/1316] [Batch 0/38] [D loss: 0.251855] [G loss: 0.245703] [ema: 0.999805] 
[Epoch 937/1316] [Batch 0/38] [D loss: 0.249703] [G loss: 0.249815] [ema: 0.999805] 
[Epoch 938/1316] [Batch 0/38] [D loss: 0.269549] [G loss: 0.226880] [ema: 0.999806] 
[Epoch 939/1316] [Batch 0/38] [D loss: 0.262393] [G loss: 0.254071] [ema: 0.999806] 
[Epoch 940/1316] [Batch 0/38] [D loss: 0.272291] [G loss: 0.220272] [ema: 0.999806] 
[Epoch 941/1316] [Batch 0/38] [D loss: 0.279084] [G loss: 0.260534] [ema: 0.999806] 
[Epoch 942/1316] [Batch 0/38] [D loss: 0.293691] [G loss: 0.243261] [ema: 0.999806] 
[Epoch 943/1316] [Batch 0/38] [D loss: 0.307971] [G loss: 0.249457] [ema: 0.999807] 
[Epoch 944/1316] [Batch 0/38] [D loss: 0.270345] [G loss: 0.229802] [ema: 0.999807] 
[Epoch 945/1316] [Batch 0/38] [D loss: 0.245129] [G loss: 0.250258] [ema: 0.999807] 
[Epoch 946/1316] [Batch 0/38] [D loss: 0.265542] [G loss: 0.248567] [ema: 0.999807] 
[Epoch 947/1316] [Batch 0/38] [D loss: 0.250093] [G loss: 0.236192] [ema: 0.999807] 
[Epoch 948/1316] [Batch 0/38] [D loss: 0.272554] [G loss: 0.251360] [ema: 0.999808] 
[Epoch 949/1316] [Batch 0/38] [D loss: 0.248914] [G loss: 0.254149] [ema: 0.999808] 
[Epoch 950/1316] [Batch 0/38] [D loss: 0.252326] [G loss: 0.247060] [ema: 0.999808] 
[Epoch 951/1316] [Batch 0/38] [D loss: 0.263297] [G loss: 0.240853] [ema: 0.999808] 
[Epoch 952/1316] [Batch 0/38] [D loss: 0.262798] [G loss: 0.240209] [ema: 0.999808] 
[Epoch 953/1316] [Batch 0/38] [D loss: 0.259558] [G loss: 0.247366] [ema: 0.999809] 
[Epoch 954/1316] [Batch 0/38] [D loss: 0.272847] [G loss: 0.258973] [ema: 0.999809] 
[Epoch 955/1316] [Batch 0/38] [D loss: 0.291087] [G loss: 0.251076] [ema: 0.999809] 
[Epoch 956/1316] [Batch 0/38] [D loss: 0.255742] [G loss: 0.251980] [ema: 0.999809] 
[Epoch 957/1316] [Batch 0/38] [D loss: 0.276996] [G loss: 0.253168] [ema: 0.999809] 
[Epoch 958/1316] [Batch 0/38] [D loss: 0.270372] [G loss: 0.240979] [ema: 0.999810] 
[Epoch 959/1316] [Batch 0/38] [D loss: 0.281100] [G loss: 0.252440] [ema: 0.999810] 
[Epoch 960/1316] [Batch 0/38] [D loss: 0.251953] [G loss: 0.247037] [ema: 0.999810] 
[Epoch 961/1316] [Batch 0/38] [D loss: 0.284649] [G loss: 0.238145] [ema: 0.999810] 
[Epoch 962/1316] [Batch 0/38] [D loss: 0.254636] [G loss: 0.251132] [ema: 0.999810] 
[Epoch 963/1316] [Batch 0/38] [D loss: 0.251617] [G loss: 0.250453] [ema: 0.999811] 
[Epoch 964/1316] [Batch 0/38] [D loss: 0.265896] [G loss: 0.251346] [ema: 0.999811] 
[Epoch 965/1316] [Batch 0/38] [D loss: 0.259081] [G loss: 0.244715] [ema: 0.999811] 
[Epoch 966/1316] [Batch 0/38] [D loss: 0.259881] [G loss: 0.253368] [ema: 0.999811] 
[Epoch 967/1316] [Batch 0/38] [D loss: 0.246070] [G loss: 0.236996] [ema: 0.999811] 
[Epoch 968/1316] [Batch 0/38] [D loss: 0.277604] [G loss: 0.240283] [ema: 0.999812] 
[Epoch 969/1316] [Batch 0/38] [D loss: 0.249261] [G loss: 0.246693] [ema: 0.999812] 
[Epoch 970/1316] [Batch 0/38] [D loss: 0.265619] [G loss: 0.244588] [ema: 0.999812] 
[Epoch 971/1316] [Batch 0/38] [D loss: 0.266037] [G loss: 0.253680] [ema: 0.999812] 
[Epoch 972/1316] [Batch 0/38] [D loss: 0.268051] [G loss: 0.252043] [ema: 0.999812] 
[Epoch 973/1316] [Batch 0/38] [D loss: 0.262640] [G loss: 0.246765] [ema: 0.999813] 
[Epoch 974/1316] [Batch 0/38] [D loss: 0.256244] [G loss: 0.239104] [ema: 0.999813] 
[Epoch 975/1316] [Batch 0/38] [D loss: 0.254367] [G loss: 0.243491] [ema: 0.999813] 
[Epoch 976/1316] [Batch 0/38] [D loss: 0.256190] [G loss: 0.253148] [ema: 0.999813] 
[Epoch 977/1316] [Batch 0/38] [D loss: 0.304034] [G loss: 0.249536] [ema: 0.999813] 
[Epoch 978/1316] [Batch 0/38] [D loss: 0.286088] [G loss: 0.231817] [ema: 0.999814] 
[Epoch 979/1316] [Batch 0/38] [D loss: 0.250444] [G loss: 0.261202] [ema: 0.999814] 
[Epoch 980/1316] [Batch 0/38] [D loss: 0.277093] [G loss: 0.261063] [ema: 0.999814] 
[Epoch 981/1316] [Batch 0/38] [D loss: 0.288226] [G loss: 0.244331] [ema: 0.999814] 
[Epoch 982/1316] [Batch 0/38] [D loss: 0.248663] [G loss: 0.260251] [ema: 0.999814] 
[Epoch 983/1316] [Batch 0/38] [D loss: 0.257657] [G loss: 0.254090] [ema: 0.999814] 
[Epoch 984/1316] [Batch 0/38] [D loss: 0.270394] [G loss: 0.241681] [ema: 0.999815] 
[Epoch 985/1316] [Batch 0/38] [D loss: 0.261514] [G loss: 0.248544] [ema: 0.999815] 
[Epoch 986/1316] [Batch 0/38] [D loss: 0.290775] [G loss: 0.260570] [ema: 0.999815] 
[Epoch 987/1316] [Batch 0/38] [D loss: 0.269984] [G loss: 0.240575] [ema: 0.999815] 
[Epoch 988/1316] [Batch 0/38] [D loss: 0.310209] [G loss: 0.218917] [ema: 0.999815] 
[Epoch 989/1316] [Batch 0/38] [D loss: 0.266822] [G loss: 0.231603] [ema: 0.999816] 
[Epoch 990/1316] [Batch 0/38] [D loss: 0.246206] [G loss: 0.258847] [ema: 0.999816] 
[Epoch 991/1316] [Batch 0/38] [D loss: 0.251641] [G loss: 0.239077] [ema: 0.999816] 
[Epoch 992/1316] [Batch 0/38] [D loss: 0.262541] [G loss: 0.256408] [ema: 0.999816] 
[Epoch 993/1316] [Batch 0/38] [D loss: 0.248864] [G loss: 0.261195] [ema: 0.999816] 
[Epoch 994/1316] [Batch 0/38] [D loss: 0.263740] [G loss: 0.242396] [ema: 0.999817] 
[Epoch 995/1316] [Batch 0/38] [D loss: 0.270568] [G loss: 0.229213] [ema: 0.999817] 
[Epoch 996/1316] [Batch 0/38] [D loss: 0.312565] [G loss: 0.217084] [ema: 0.999817] 
[Epoch 997/1316] [Batch 0/38] [D loss: 0.249072] [G loss: 0.248686] [ema: 0.999817] 
[Epoch 998/1316] [Batch 0/38] [D loss: 0.246817] [G loss: 0.240914] [ema: 0.999817] 
[Epoch 999/1316] [Batch 0/38] [D loss: 0.255812] [G loss: 0.255158] [ema: 0.999817] 
[Epoch 1000/1316] [Batch 0/38] [D loss: 0.258837] [G loss: 0.257493] [ema: 0.999818] 
[Epoch 1001/1316] [Batch 0/38] [D loss: 0.278298] [G loss: 0.245282] [ema: 0.999818] 
[Epoch 1002/1316] [Batch 0/38] [D loss: 0.272503] [G loss: 0.241996] [ema: 0.999818] 
[Epoch 1003/1316] [Batch 0/38] [D loss: 0.284569] [G loss: 0.237448] [ema: 0.999818] 
[Epoch 1004/1316] [Batch 0/38] [D loss: 0.266413] [G loss: 0.251954] [ema: 0.999818] 
[Epoch 1005/1316] [Batch 0/38] [D loss: 0.267415] [G loss: 0.241745] [ema: 0.999819] 
[Epoch 1006/1316] [Batch 0/38] [D loss: 0.286137] [G loss: 0.215995] [ema: 0.999819] 
[Epoch 1007/1316] [Batch 0/38] [D loss: 0.266395] [G loss: 0.242861] [ema: 0.999819] 
[Epoch 1008/1316] [Batch 0/38] [D loss: 0.249682] [G loss: 0.248834] [ema: 0.999819] 
[Epoch 1009/1316] [Batch 0/38] [D loss: 0.278220] [G loss: 0.248842] [ema: 0.999819] 
[Epoch 1010/1316] [Batch 0/38] [D loss: 0.261568] [G loss: 0.245998] [ema: 0.999819] 
[Epoch 1011/1316] [Batch 0/38] [D loss: 0.313296] [G loss: 0.230875] [ema: 0.999820] 
[Epoch 1012/1316] [Batch 0/38] [D loss: 0.256011] [G loss: 0.256441] [ema: 0.999820] 
[Epoch 1013/1316] [Batch 0/38] [D loss: 0.285142] [G loss: 0.259819] [ema: 0.999820] 
[Epoch 1014/1316] [Batch 0/38] [D loss: 0.246308] [G loss: 0.266811] [ema: 0.999820] 
[Epoch 1015/1316] [Batch 0/38] [D loss: 0.267774] [G loss: 0.247661] [ema: 0.999820] 
[Epoch 1016/1316] [Batch 0/38] [D loss: 0.255624] [G loss: 0.257920] [ema: 0.999820] 
[Epoch 1017/1316] [Batch 0/38] [D loss: 0.266078] [G loss: 0.242725] [ema: 0.999821] 
[Epoch 1018/1316] [Batch 0/38] [D loss: 0.265919] [G loss: 0.239138] [ema: 0.999821] 
[Epoch 1019/1316] [Batch 0/38] [D loss: 0.358355] [G loss: 0.243509] [ema: 0.999821] 
[Epoch 1020/1316] [Batch 0/38] [D loss: 0.248940] [G loss: 0.251636] [ema: 0.999821] 
[Epoch 1021/1316] [Batch 0/38] [D loss: 0.288752] [G loss: 0.264501] [ema: 0.999821] 
[Epoch 1022/1316] [Batch 0/38] [D loss: 0.258711] [G loss: 0.246262] [ema: 0.999822] 
[Epoch 1023/1316] [Batch 0/38] [D loss: 0.257152] [G loss: 0.235651] [ema: 0.999822] 
[Epoch 1024/1316] [Batch 0/38] [D loss: 0.307107] [G loss: 0.233212] [ema: 0.999822] 
[Epoch 1025/1316] [Batch 0/38] [D loss: 0.247969] [G loss: 0.244995] [ema: 0.999822] 
[Epoch 1026/1316] [Batch 0/38] [D loss: 0.268199] [G loss: 0.230056] [ema: 0.999822] 
[Epoch 1027/1316] [Batch 0/38] [D loss: 0.316270] [G loss: 0.253109] [ema: 0.999822] 
[Epoch 1028/1316] [Batch 0/38] [D loss: 0.262365] [G loss: 0.247853] [ema: 0.999823] 
[Epoch 1029/1316] [Batch 0/38] [D loss: 0.280096] [G loss: 0.250449] [ema: 0.999823] 
[Epoch 1030/1316] [Batch 0/38] [D loss: 0.277797] [G loss: 0.256809] [ema: 0.999823] 
[Epoch 1031/1316] [Batch 0/38] [D loss: 0.269706] [G loss: 0.255691] [ema: 0.999823] 
[Epoch 1032/1316] [Batch 0/38] [D loss: 0.245719] [G loss: 0.243544] [ema: 0.999823] 
[Epoch 1033/1316] [Batch 0/38] [D loss: 0.275152] [G loss: 0.240332] [ema: 0.999823] 
[Epoch 1034/1316] [Batch 0/38] [D loss: 0.250070] [G loss: 0.239735] [ema: 0.999824] 
[Epoch 1035/1316] [Batch 0/38] [D loss: 0.277285] [G loss: 0.239526] [ema: 0.999824] 
[Epoch 1036/1316] [Batch 0/38] [D loss: 0.252979] [G loss: 0.248433] [ema: 0.999824] 
[Epoch 1037/1316] [Batch 0/38] [D loss: 0.261104] [G loss: 0.248088] [ema: 0.999824] 
[Epoch 1038/1316] [Batch 0/38] [D loss: 0.241936] [G loss: 0.253397] [ema: 0.999824] 
[Epoch 1039/1316] [Batch 0/38] [D loss: 0.274612] [G loss: 0.234804] [ema: 0.999824] 
[Epoch 1040/1316] [Batch 0/38] [D loss: 0.294862] [G loss: 0.263678] [ema: 0.999825] 
[Epoch 1041/1316] [Batch 0/38] [D loss: 0.260522] [G loss: 0.250131] [ema: 0.999825] 
[Epoch 1042/1316] [Batch 0/38] [D loss: 0.259202] [G loss: 0.240246] [ema: 0.999825] 
[Epoch 1043/1316] [Batch 0/38] [D loss: 0.285783] [G loss: 0.239762] [ema: 0.999825] 
[Epoch 1044/1316] [Batch 0/38] [D loss: 0.251680] [G loss: 0.256076] [ema: 0.999825] 
[Epoch 1045/1316] [Batch 0/38] [D loss: 0.276409] [G loss: 0.251843] [ema: 0.999825] 
[Epoch 1046/1316] [Batch 0/38] [D loss: 0.289451] [G loss: 0.239307] [ema: 0.999826] 
[Epoch 1047/1316] [Batch 0/38] [D loss: 0.259917] [G loss: 0.242688] [ema: 0.999826] 
[Epoch 1048/1316] [Batch 0/38] [D loss: 0.248943] [G loss: 0.247537] [ema: 0.999826] 
[Epoch 1049/1316] [Batch 0/38] [D loss: 0.258740] [G loss: 0.213345] [ema: 0.999826] 
[Epoch 1050/1316] [Batch 0/38] [D loss: 0.295357] [G loss: 0.227014] [ema: 0.999826] 
[Epoch 1051/1316] [Batch 0/38] [D loss: 0.262830] [G loss: 0.245660] [ema: 0.999826] 
[Epoch 1052/1316] [Batch 0/38] [D loss: 0.277546] [G loss: 0.251355] [ema: 0.999827] 
[Epoch 1053/1316] [Batch 0/38] [D loss: 0.250651] [G loss: 0.244825] [ema: 0.999827] 
[Epoch 1054/1316] [Batch 0/38] [D loss: 0.255070] [G loss: 0.228878] [ema: 0.999827] 
[Epoch 1055/1316] [Batch 0/38] [D loss: 0.270113] [G loss: 0.240331] [ema: 0.999827] 




Saving checkpoint 5 in logs/Jumping_2024_10_02_17_29_42/Model




[Epoch 1056/1316] [Batch 0/38] [D loss: 0.292904] [G loss: 0.243750] [ema: 0.999827] 
[Epoch 1057/1316] [Batch 0/38] [D loss: 0.246049] [G loss: 0.238223] [ema: 0.999827] 
[Epoch 1058/1316] [Batch 0/38] [D loss: 0.271966] [G loss: 0.219828] [ema: 0.999828] 
[Epoch 1059/1316] [Batch 0/38] [D loss: 0.257357] [G loss: 0.260121] [ema: 0.999828] 
[Epoch 1060/1316] [Batch 0/38] [D loss: 0.270437] [G loss: 0.237566] [ema: 0.999828] 
[Epoch 1061/1316] [Batch 0/38] [D loss: 0.259189] [G loss: 0.246991] [ema: 0.999828] 
[Epoch 1062/1316] [Batch 0/38] [D loss: 0.267728] [G loss: 0.252689] [ema: 0.999828] 
[Epoch 1063/1316] [Batch 0/38] [D loss: 0.253873] [G loss: 0.249730] [ema: 0.999828] 
[Epoch 1064/1316] [Batch 0/38] [D loss: 0.265874] [G loss: 0.251131] [ema: 0.999829] 
[Epoch 1065/1316] [Batch 0/38] [D loss: 0.274568] [G loss: 0.263166] [ema: 0.999829] 
[Epoch 1066/1316] [Batch 0/38] [D loss: 0.284124] [G loss: 0.242731] [ema: 0.999829] 
[Epoch 1067/1316] [Batch 0/38] [D loss: 0.273902] [G loss: 0.235581] [ema: 0.999829] 
[Epoch 1068/1316] [Batch 0/38] [D loss: 0.311024] [G loss: 0.251159] [ema: 0.999829] 
[Epoch 1069/1316] [Batch 0/38] [D loss: 0.321099] [G loss: 0.235959] [ema: 0.999829] 
[Epoch 1070/1316] [Batch 0/38] [D loss: 0.271203] [G loss: 0.240457] [ema: 0.999830] 
[Epoch 1071/1316] [Batch 0/38] [D loss: 0.246022] [G loss: 0.262987] [ema: 0.999830] 
[Epoch 1072/1316] [Batch 0/38] [D loss: 0.269567] [G loss: 0.245774] [ema: 0.999830] 
[Epoch 1073/1316] [Batch 0/38] [D loss: 0.255913] [G loss: 0.241629] [ema: 0.999830] 
[Epoch 1074/1316] [Batch 0/38] [D loss: 0.250982] [G loss: 0.246836] [ema: 0.999830] 
[Epoch 1075/1316] [Batch 0/38] [D loss: 0.262590] [G loss: 0.250139] [ema: 0.999830] 
[Epoch 1076/1316] [Batch 0/38] [D loss: 0.259764] [G loss: 0.252583] [ema: 0.999830] 
[Epoch 1077/1316] [Batch 0/38] [D loss: 0.265768] [G loss: 0.241926] [ema: 0.999831] 
[Epoch 1078/1316] [Batch 0/38] [D loss: 0.284500] [G loss: 0.252483] [ema: 0.999831] 
[Epoch 1079/1316] [Batch 0/38] [D loss: 0.250194] [G loss: 0.254852] [ema: 0.999831] 
[Epoch 1080/1316] [Batch 0/38] [D loss: 0.261660] [G loss: 0.232667] [ema: 0.999831] 
[Epoch 1081/1316] [Batch 0/38] [D loss: 0.260110] [G loss: 0.252676] [ema: 0.999831] 
[Epoch 1082/1316] [Batch 0/38] [D loss: 0.273401] [G loss: 0.248161] [ema: 0.999831] 
[Epoch 1083/1316] [Batch 0/38] [D loss: 0.264356] [G loss: 0.237831] [ema: 0.999832] 
[Epoch 1084/1316] [Batch 0/38] [D loss: 0.271556] [G loss: 0.241688] [ema: 0.999832] 
[Epoch 1085/1316] [Batch 0/38] [D loss: 0.248200] [G loss: 0.248709] [ema: 0.999832] 
[Epoch 1086/1316] [Batch 0/38] [D loss: 0.300785] [G loss: 0.238112] [ema: 0.999832] 
[Epoch 1087/1316] [Batch 0/38] [D loss: 0.273867] [G loss: 0.238568] [ema: 0.999832] 
[Epoch 1088/1316] [Batch 0/38] [D loss: 0.273831] [G loss: 0.239989] [ema: 0.999832] 
[Epoch 1089/1316] [Batch 0/38] [D loss: 0.255372] [G loss: 0.233830] [ema: 0.999833] 
[Epoch 1090/1316] [Batch 0/38] [D loss: 0.262796] [G loss: 0.237098] [ema: 0.999833] 
[Epoch 1091/1316] [Batch 0/38] [D loss: 0.254137] [G loss: 0.243957] [ema: 0.999833] 
[Epoch 1092/1316] [Batch 0/38] [D loss: 0.261372] [G loss: 0.242240] [ema: 0.999833] 
[Epoch 1093/1316] [Batch 0/38] [D loss: 0.266046] [G loss: 0.242428] [ema: 0.999833] 
[Epoch 1094/1316] [Batch 0/38] [D loss: 0.266426] [G loss: 0.239523] [ema: 0.999833] 
[Epoch 1095/1316] [Batch 0/38] [D loss: 0.270626] [G loss: 0.241334] [ema: 0.999833] 
[Epoch 1096/1316] [Batch 0/38] [D loss: 0.258000] [G loss: 0.245950] [ema: 0.999834] 
[Epoch 1097/1316] [Batch 0/38] [D loss: 0.272064] [G loss: 0.244614] [ema: 0.999834] 
[Epoch 1098/1316] [Batch 0/38] [D loss: 0.265277] [G loss: 0.248276] [ema: 0.999834] 
[Epoch 1099/1316] [Batch 0/38] [D loss: 0.296774] [G loss: 0.249324] [ema: 0.999834] 
[Epoch 1100/1316] [Batch 0/38] [D loss: 0.262768] [G loss: 0.245068] [ema: 0.999834] 
[Epoch 1101/1316] [Batch 0/38] [D loss: 0.257396] [G loss: 0.250781] [ema: 0.999834] 
[Epoch 1102/1316] [Batch 0/38] [D loss: 0.276076] [G loss: 0.236509] [ema: 0.999834] 
[Epoch 1103/1316] [Batch 0/38] [D loss: 0.248681] [G loss: 0.257056] [ema: 0.999835] 
[Epoch 1104/1316] [Batch 0/38] [D loss: 0.278923] [G loss: 0.234349] [ema: 0.999835] 
[Epoch 1105/1316] [Batch 0/38] [D loss: 0.276204] [G loss: 0.237480] [ema: 0.999835] 
[Epoch 1106/1316] [Batch 0/38] [D loss: 0.259996] [G loss: 0.255340] [ema: 0.999835] 
[Epoch 1107/1316] [Batch 0/38] [D loss: 0.262067] [G loss: 0.246192] [ema: 0.999835] 
[Epoch 1108/1316] [Batch 0/38] [D loss: 0.256578] [G loss: 0.232368] [ema: 0.999835] 
[Epoch 1109/1316] [Batch 0/38] [D loss: 0.270156] [G loss: 0.245208] [ema: 0.999836] 
[Epoch 1110/1316] [Batch 0/38] [D loss: 0.261035] [G loss: 0.246075] [ema: 0.999836] 
[Epoch 1111/1316] [Batch 0/38] [D loss: 0.252365] [G loss: 0.256471] [ema: 0.999836] 
[Epoch 1112/1316] [Batch 0/38] [D loss: 0.250413] [G loss: 0.251643] [ema: 0.999836] 
[Epoch 1113/1316] [Batch 0/38] [D loss: 0.269606] [G loss: 0.261838] [ema: 0.999836] 
[Epoch 1114/1316] [Batch 0/38] [D loss: 0.241163] [G loss: 0.246564] [ema: 0.999836] 
[Epoch 1115/1316] [Batch 0/38] [D loss: 0.268453] [G loss: 0.244349] [ema: 0.999836] 
[Epoch 1116/1316] [Batch 0/38] [D loss: 0.254463] [G loss: 0.242120] [ema: 0.999837] 
[Epoch 1117/1316] [Batch 0/38] [D loss: 0.254259] [G loss: 0.233978] [ema: 0.999837] 
[Epoch 1118/1316] [Batch 0/38] [D loss: 0.249115] [G loss: 0.248791] [ema: 0.999837] 
[Epoch 1119/1316] [Batch 0/38] [D loss: 0.257656] [G loss: 0.245945] [ema: 0.999837] 
[Epoch 1120/1316] [Batch 0/38] [D loss: 0.255688] [G loss: 0.253350] [ema: 0.999837] 
[Epoch 1121/1316] [Batch 0/38] [D loss: 0.272999] [G loss: 0.239032] [ema: 0.999837] 
[Epoch 1122/1316] [Batch 0/38] [D loss: 0.256639] [G loss: 0.240157] [ema: 0.999837] 
[Epoch 1123/1316] [Batch 0/38] [D loss: 0.303809] [G loss: 0.259360] [ema: 0.999838] 
[Epoch 1124/1316] [Batch 0/38] [D loss: 0.252685] [G loss: 0.247962] [ema: 0.999838] 
[Epoch 1125/1316] [Batch 0/38] [D loss: 0.263949] [G loss: 0.237355] [ema: 0.999838] 
[Epoch 1126/1316] [Batch 0/38] [D loss: 0.263671] [G loss: 0.244944] [ema: 0.999838] 
[Epoch 1127/1316] [Batch 0/38] [D loss: 0.295948] [G loss: 0.246104] [ema: 0.999838] 
[Epoch 1128/1316] [Batch 0/38] [D loss: 0.250849] [G loss: 0.245703] [ema: 0.999838] 
[Epoch 1129/1316] [Batch 0/38] [D loss: 0.260301] [G loss: 0.238061] [ema: 0.999838] 
[Epoch 1130/1316] [Batch 0/38] [D loss: 0.294213] [G loss: 0.223623] [ema: 0.999839] 
[Epoch 1131/1316] [Batch 0/38] [D loss: 0.260341] [G loss: 0.238226] [ema: 0.999839] 
[Epoch 1132/1316] [Batch 0/38] [D loss: 0.269901] [G loss: 0.241260] [ema: 0.999839] 
[Epoch 1133/1316] [Batch 0/38] [D loss: 0.278919] [G loss: 0.226659] [ema: 0.999839] 
[Epoch 1134/1316] [Batch 0/38] [D loss: 0.249861] [G loss: 0.253978] [ema: 0.999839] 
[Epoch 1135/1316] [Batch 0/38] [D loss: 0.277129] [G loss: 0.239721] [ema: 0.999839] 
[Epoch 1136/1316] [Batch 0/38] [D loss: 0.261815] [G loss: 0.264888] [ema: 0.999839] 
[Epoch 1137/1316] [Batch 0/38] [D loss: 0.258675] [G loss: 0.241332] [ema: 0.999840] 
[Epoch 1138/1316] [Batch 0/38] [D loss: 0.263608] [G loss: 0.235347] [ema: 0.999840] 
[Epoch 1139/1316] [Batch 0/38] [D loss: 0.294378] [G loss: 0.238266] [ema: 0.999840] 
[Epoch 1140/1316] [Batch 0/38] [D loss: 0.271211] [G loss: 0.246597] [ema: 0.999840] 
[Epoch 1141/1316] [Batch 0/38] [D loss: 0.261795] [G loss: 0.241742] [ema: 0.999840] 
[Epoch 1142/1316] [Batch 0/38] [D loss: 0.250518] [G loss: 0.245432] [ema: 0.999840] 
[Epoch 1143/1316] [Batch 0/38] [D loss: 0.262670] [G loss: 0.242656] [ema: 0.999840] 
[Epoch 1144/1316] [Batch 0/38] [D loss: 0.250404] [G loss: 0.247414] [ema: 0.999841] 
[Epoch 1145/1316] [Batch 0/38] [D loss: 0.261456] [G loss: 0.244375] [ema: 0.999841] 
[Epoch 1146/1316] [Batch 0/38] [D loss: 0.244826] [G loss: 0.261742] [ema: 0.999841] 
[Epoch 1147/1316] [Batch 0/38] [D loss: 0.255156] [G loss: 0.254429] [ema: 0.999841] 
[Epoch 1148/1316] [Batch 0/38] [D loss: 0.248795] [G loss: 0.256216] [ema: 0.999841] 
[Epoch 1149/1316] [Batch 0/38] [D loss: 0.261112] [G loss: 0.233098] [ema: 0.999841] 
[Epoch 1150/1316] [Batch 0/38] [D loss: 0.240376] [G loss: 0.254295] [ema: 0.999841] 
[Epoch 1151/1316] [Batch 0/38] [D loss: 0.252889] [G loss: 0.242619] [ema: 0.999842] 
[Epoch 1152/1316] [Batch 0/38] [D loss: 0.294870] [G loss: 0.259979] [ema: 0.999842] 
[Epoch 1153/1316] [Batch 0/38] [D loss: 0.256829] [G loss: 0.236507] [ema: 0.999842] 
[Epoch 1154/1316] [Batch 0/38] [D loss: 0.260083] [G loss: 0.245707] [ema: 0.999842] 
[Epoch 1155/1316] [Batch 0/38] [D loss: 0.263586] [G loss: 0.238950] [ema: 0.999842] 
[Epoch 1156/1316] [Batch 0/38] [D loss: 0.246845] [G loss: 0.256167] [ema: 0.999842] 
[Epoch 1157/1316] [Batch 0/38] [D loss: 0.268694] [G loss: 0.234758] [ema: 0.999842] 
[Epoch 1158/1316] [Batch 0/38] [D loss: 0.283872] [G loss: 0.207670] [ema: 0.999842] 
[Epoch 1159/1316] [Batch 0/38] [D loss: 0.258801] [G loss: 0.242036] [ema: 0.999843] 
[Epoch 1160/1316] [Batch 0/38] [D loss: 0.256801] [G loss: 0.243090] [ema: 0.999843] 
[Epoch 1161/1316] [Batch 0/38] [D loss: 0.287704] [G loss: 0.234175] [ema: 0.999843] 
[Epoch 1162/1316] [Batch 0/38] [D loss: 0.265158] [G loss: 0.220631] [ema: 0.999843] 
[Epoch 1163/1316] [Batch 0/38] [D loss: 0.256242] [G loss: 0.248002] [ema: 0.999843] 
[Epoch 1164/1316] [Batch 0/38] [D loss: 0.252161] [G loss: 0.252313] [ema: 0.999843] 
[Epoch 1165/1316] [Batch 0/38] [D loss: 0.266702] [G loss: 0.238677] [ema: 0.999843] 
[Epoch 1166/1316] [Batch 0/38] [D loss: 0.294774] [G loss: 0.248263] [ema: 0.999844] 
[Epoch 1167/1316] [Batch 0/38] [D loss: 0.275733] [G loss: 0.235576] [ema: 0.999844] 
[Epoch 1168/1316] [Batch 0/38] [D loss: 0.261186] [G loss: 0.247853] [ema: 0.999844] 
[Epoch 1169/1316] [Batch 0/38] [D loss: 0.278302] [G loss: 0.250223] [ema: 0.999844] 
[Epoch 1170/1316] [Batch 0/38] [D loss: 0.251770] [G loss: 0.247522] [ema: 0.999844] 
[Epoch 1171/1316] [Batch 0/38] [D loss: 0.289967] [G loss: 0.251034] [ema: 0.999844] 
[Epoch 1172/1316] [Batch 0/38] [D loss: 0.263238] [G loss: 0.248171] [ema: 0.999844] 
[Epoch 1173/1316] [Batch 0/38] [D loss: 0.262358] [G loss: 0.256111] [ema: 0.999845] 
[Epoch 1174/1316] [Batch 0/38] [D loss: 0.251699] [G loss: 0.250027] [ema: 0.999845] 
[Epoch 1175/1316] [Batch 0/38] [D loss: 0.254325] [G loss: 0.247282] [ema: 0.999845] 
[Epoch 1176/1316] [Batch 0/38] [D loss: 0.247379] [G loss: 0.243127] [ema: 0.999845] 
[Epoch 1177/1316] [Batch 0/38] [D loss: 0.275088] [G loss: 0.236036] [ema: 0.999845] 
[Epoch 1178/1316] [Batch 0/38] [D loss: 0.260294] [G loss: 0.236629] [ema: 0.999845] 
[Epoch 1179/1316] [Batch 0/38] [D loss: 0.248511] [G loss: 0.245917] [ema: 0.999845] 
[Epoch 1180/1316] [Batch 0/38] [D loss: 0.254075] [G loss: 0.249880] [ema: 0.999845] 
[Epoch 1181/1316] [Batch 0/38] [D loss: 0.268574] [G loss: 0.252598] [ema: 0.999846] 
[Epoch 1182/1316] [Batch 0/38] [D loss: 0.264355] [G loss: 0.254975] [ema: 0.999846] 
[Epoch 1183/1316] [Batch 0/38] [D loss: 0.252092] [G loss: 0.246581] [ema: 0.999846] 
[Epoch 1184/1316] [Batch 0/38] [D loss: 0.278018] [G loss: 0.226855] [ema: 0.999846] 
[Epoch 1185/1316] [Batch 0/38] [D loss: 0.269324] [G loss: 0.231135] [ema: 0.999846] 
[Epoch 1186/1316] [Batch 0/38] [D loss: 0.331973] [G loss: 0.242377] [ema: 0.999846] 
[Epoch 1187/1316] [Batch 0/38] [D loss: 0.281147] [G loss: 0.238291] [ema: 0.999846] 
[Epoch 1188/1316] [Batch 0/38] [D loss: 0.263086] [G loss: 0.248890] [ema: 0.999846] 
[Epoch 1189/1316] [Batch 0/38] [D loss: 0.256505] [G loss: 0.236309] [ema: 0.999847] 
[Epoch 1190/1316] [Batch 0/38] [D loss: 0.256118] [G loss: 0.247467] [ema: 0.999847] 
[Epoch 1191/1316] [Batch 0/38] [D loss: 0.255083] [G loss: 0.245985] [ema: 0.999847] 
[Epoch 1192/1316] [Batch 0/38] [D loss: 0.275624] [G loss: 0.241102] [ema: 0.999847] 
[Epoch 1193/1316] [Batch 0/38] [D loss: 0.268927] [G loss: 0.251191] [ema: 0.999847] 
[Epoch 1194/1316] [Batch 0/38] [D loss: 0.254584] [G loss: 0.232920] [ema: 0.999847] 
[Epoch 1195/1316] [Batch 0/38] [D loss: 0.305903] [G loss: 0.247328] [ema: 0.999847] 
[Epoch 1196/1316] [Batch 0/38] [D loss: 0.288198] [G loss: 0.235249] [ema: 0.999847] 
[Epoch 1197/1316] [Batch 0/38] [D loss: 0.265153] [G loss: 0.243728] [ema: 0.999848] 
[Epoch 1198/1316] [Batch 0/38] [D loss: 0.255832] [G loss: 0.240666] [ema: 0.999848] 
[Epoch 1199/1316] [Batch 0/38] [D loss: 0.279466] [G loss: 0.248098] [ema: 0.999848] 
[Epoch 1200/1316] [Batch 0/38] [D loss: 0.272133] [G loss: 0.248965] [ema: 0.999848] 
[Epoch 1201/1316] [Batch 0/38] [D loss: 0.268799] [G loss: 0.244755] [ema: 0.999848] 
[Epoch 1202/1316] [Batch 0/38] [D loss: 0.262729] [G loss: 0.237092] [ema: 0.999848] 
[Epoch 1203/1316] [Batch 0/38] [D loss: 0.251373] [G loss: 0.245286] [ema: 0.999848] 
[Epoch 1204/1316] [Batch 0/38] [D loss: 0.248818] [G loss: 0.247936] [ema: 0.999849] 
[Epoch 1205/1316] [Batch 0/38] [D loss: 0.250775] [G loss: 0.254738] [ema: 0.999849] 
[Epoch 1206/1316] [Batch 0/38] [D loss: 0.264700] [G loss: 0.257495] [ema: 0.999849] 
[Epoch 1207/1316] [Batch 0/38] [D loss: 0.268015] [G loss: 0.240387] [ema: 0.999849] 
[Epoch 1208/1316] [Batch 0/38] [D loss: 0.258487] [G loss: 0.234099] [ema: 0.999849] 
[Epoch 1209/1316] [Batch 0/38] [D loss: 0.270652] [G loss: 0.234538] [ema: 0.999849] 
[Epoch 1210/1316] [Batch 0/38] [D loss: 0.337326] [G loss: 0.235254] [ema: 0.999849] 
[Epoch 1211/1316] [Batch 0/38] [D loss: 0.259504] [G loss: 0.240724] [ema: 0.999849] 
[Epoch 1212/1316] [Batch 0/38] [D loss: 0.291533] [G loss: 0.252600] [ema: 0.999850] 
[Epoch 1213/1316] [Batch 0/38] [D loss: 0.301944] [G loss: 0.248323] [ema: 0.999850] 
[Epoch 1214/1316] [Batch 0/38] [D loss: 0.280178] [G loss: 0.247351] [ema: 0.999850] 
[Epoch 1215/1316] [Batch 0/38] [D loss: 0.291517] [G loss: 0.254054] [ema: 0.999850] 
[Epoch 1216/1316] [Batch 0/38] [D loss: 0.273948] [G loss: 0.242859] [ema: 0.999850] 
[Epoch 1217/1316] [Batch 0/38] [D loss: 0.274607] [G loss: 0.249652] [ema: 0.999850] 
[Epoch 1218/1316] [Batch 0/38] [D loss: 0.293759] [G loss: 0.235161] [ema: 0.999850] 
[Epoch 1219/1316] [Batch 0/38] [D loss: 0.257832] [G loss: 0.254218] [ema: 0.999850] 
[Epoch 1220/1316] [Batch 0/38] [D loss: 0.307202] [G loss: 0.261120] [ema: 0.999850] 
[Epoch 1221/1316] [Batch 0/38] [D loss: 0.251310] [G loss: 0.246218] [ema: 0.999851] 
[Epoch 1222/1316] [Batch 0/38] [D loss: 0.268989] [G loss: 0.246587] [ema: 0.999851] 
[Epoch 1223/1316] [Batch 0/38] [D loss: 0.273616] [G loss: 0.224080] [ema: 0.999851] 
[Epoch 1224/1316] [Batch 0/38] [D loss: 0.258484] [G loss: 0.237208] [ema: 0.999851] 
[Epoch 1225/1316] [Batch 0/38] [D loss: 0.288940] [G loss: 0.255293] [ema: 0.999851] 
[Epoch 1226/1316] [Batch 0/38] [D loss: 0.261752] [G loss: 0.240064] [ema: 0.999851] 
[Epoch 1227/1316] [Batch 0/38] [D loss: 0.289204] [G loss: 0.245331] [ema: 0.999851] 
[Epoch 1228/1316] [Batch 0/38] [D loss: 0.262927] [G loss: 0.259902] [ema: 0.999851] 
[Epoch 1229/1316] [Batch 0/38] [D loss: 0.302920] [G loss: 0.241106] [ema: 0.999852] 
[Epoch 1230/1316] [Batch 0/38] [D loss: 0.319782] [G loss: 0.227839] [ema: 0.999852] 
[Epoch 1231/1316] [Batch 0/38] [D loss: 0.244904] [G loss: 0.260330] [ema: 0.999852] 
[Epoch 1232/1316] [Batch 0/38] [D loss: 0.307291] [G loss: 0.246384] [ema: 0.999852] 
[Epoch 1233/1316] [Batch 0/38] [D loss: 0.257925] [G loss: 0.241299] [ema: 0.999852] 
[Epoch 1234/1316] [Batch 0/38] [D loss: 0.261367] [G loss: 0.245290] [ema: 0.999852] 
[Epoch 1235/1316] [Batch 0/38] [D loss: 0.258476] [G loss: 0.258237] [ema: 0.999852] 
[Epoch 1236/1316] [Batch 0/38] [D loss: 0.248906] [G loss: 0.248648] [ema: 0.999852] 
[Epoch 1237/1316] [Batch 0/38] [D loss: 0.250949] [G loss: 0.256084] [ema: 0.999853] 
[Epoch 1238/1316] [Batch 0/38] [D loss: 0.247840] [G loss: 0.245076] [ema: 0.999853] 
[Epoch 1239/1316] [Batch 0/38] [D loss: 0.302329] [G loss: 0.248736] [ema: 0.999853] 
[Epoch 1240/1316] [Batch 0/38] [D loss: 0.268580] [G loss: 0.246840] [ema: 0.999853] 
[Epoch 1241/1316] [Batch 0/38] [D loss: 0.270980] [G loss: 0.237270] [ema: 0.999853] 
[Epoch 1242/1316] [Batch 0/38] [D loss: 0.283043] [G loss: 0.224802] [ema: 0.999853] 
[Epoch 1243/1316] [Batch 0/38] [D loss: 0.263523] [G loss: 0.235962] [ema: 0.999853] 
[Epoch 1244/1316] [Batch 0/38] [D loss: 0.296518] [G loss: 0.255134] [ema: 0.999853] 
[Epoch 1245/1316] [Batch 0/38] [D loss: 0.253744] [G loss: 0.248128] [ema: 0.999853] 
[Epoch 1246/1316] [Batch 0/38] [D loss: 0.263532] [G loss: 0.240835] [ema: 0.999854] 
[Epoch 1247/1316] [Batch 0/38] [D loss: 0.242921] [G loss: 0.254409] [ema: 0.999854] 
[Epoch 1248/1316] [Batch 0/38] [D loss: 0.264852] [G loss: 0.243964] [ema: 0.999854] 
[Epoch 1249/1316] [Batch 0/38] [D loss: 0.255817] [G loss: 0.248095] [ema: 0.999854] 
[Epoch 1250/1316] [Batch 0/38] [D loss: 0.264832] [G loss: 0.243821] [ema: 0.999854] 
[Epoch 1251/1316] [Batch 0/38] [D loss: 0.252802] [G loss: 0.250265] [ema: 0.999854] 
[Epoch 1252/1316] [Batch 0/38] [D loss: 0.259983] [G loss: 0.244830] [ema: 0.999854] 
[Epoch 1253/1316] [Batch 0/38] [D loss: 0.270474] [G loss: 0.243441] [ema: 0.999854] 
[Epoch 1254/1316] [Batch 0/38] [D loss: 0.259069] [G loss: 0.248922] [ema: 0.999855] 
[Epoch 1255/1316] [Batch 0/38] [D loss: 0.262402] [G loss: 0.241376] [ema: 0.999855] 
[Epoch 1256/1316] [Batch 0/38] [D loss: 0.266372] [G loss: 0.263716] [ema: 0.999855] 
[Epoch 1257/1316] [Batch 0/38] [D loss: 0.248386] [G loss: 0.252909] [ema: 0.999855] 
[Epoch 1258/1316] [Batch 0/38] [D loss: 0.298663] [G loss: 0.238767] [ema: 0.999855] 
[Epoch 1259/1316] [Batch 0/38] [D loss: 0.259684] [G loss: 0.245107] [ema: 0.999855] 
[Epoch 1260/1316] [Batch 0/38] [D loss: 0.278245] [G loss: 0.239161] [ema: 0.999855] 
[Epoch 1261/1316] [Batch 0/38] [D loss: 0.246886] [G loss: 0.251752] [ema: 0.999855] 
[Epoch 1262/1316] [Batch 0/38] [D loss: 0.248300] [G loss: 0.246670] [ema: 0.999855] 
[Epoch 1263/1316] [Batch 0/38] [D loss: 0.268735] [G loss: 0.239279] [ema: 0.999856] 
[Epoch 1264/1316] [Batch 0/38] [D loss: 0.258236] [G loss: 0.249095] [ema: 0.999856] 
[Epoch 1265/1316] [Batch 0/38] [D loss: 0.301331] [G loss: 0.232646] [ema: 0.999856] 
[Epoch 1266/1316] [Batch 0/38] [D loss: 0.328561] [G loss: 0.248799] [ema: 0.999856] 
[Epoch 1267/1316] [Batch 0/38] [D loss: 0.322807] [G loss: 0.237935] [ema: 0.999856] 
[Epoch 1268/1316] [Batch 0/38] [D loss: 0.291961] [G loss: 0.231358] [ema: 0.999856] 
[Epoch 1269/1316] [Batch 0/38] [D loss: 0.314517] [G loss: 0.243052] [ema: 0.999856] 
[Epoch 1270/1316] [Batch 0/38] [D loss: 0.275106] [G loss: 0.250766] [ema: 0.999856] 
[Epoch 1271/1316] [Batch 0/38] [D loss: 0.254782] [G loss: 0.249289] [ema: 0.999856] 
[Epoch 1272/1316] [Batch 0/38] [D loss: 0.254646] [G loss: 0.245228] [ema: 0.999857] 
[Epoch 1273/1316] [Batch 0/38] [D loss: 0.253511] [G loss: 0.240434] [ema: 0.999857] 
[Epoch 1274/1316] [Batch 0/38] [D loss: 0.252267] [G loss: 0.240529] [ema: 0.999857] 
[Epoch 1275/1316] [Batch 0/38] [D loss: 0.296843] [G loss: 0.257976] [ema: 0.999857] 
[Epoch 1276/1316] [Batch 0/38] [D loss: 0.276888] [G loss: 0.238518] [ema: 0.999857] 
[Epoch 1277/1316] [Batch 0/38] [D loss: 0.257656] [G loss: 0.249760] [ema: 0.999857] 
[Epoch 1278/1316] [Batch 0/38] [D loss: 0.270833] [G loss: 0.231046] [ema: 0.999857] 
[Epoch 1279/1316] [Batch 0/38] [D loss: 0.263519] [G loss: 0.241595] [ema: 0.999857] 
[Epoch 1280/1316] [Batch 0/38] [D loss: 0.253573] [G loss: 0.241478] [ema: 0.999858] 
[Epoch 1281/1316] [Batch 0/38] [D loss: 0.251100] [G loss: 0.251299] [ema: 0.999858] 
[Epoch 1282/1316] [Batch 0/38] [D loss: 0.249222] [G loss: 0.261447] [ema: 0.999858] 
[Epoch 1283/1316] [Batch 0/38] [D loss: 0.258253] [G loss: 0.251271] [ema: 0.999858] 
[Epoch 1284/1316] [Batch 0/38] [D loss: 0.264403] [G loss: 0.245178] [ema: 0.999858] 
[Epoch 1285/1316] [Batch 0/38] [D loss: 0.268992] [G loss: 0.247300] [ema: 0.999858] 
[Epoch 1286/1316] [Batch 0/38] [D loss: 0.274289] [G loss: 0.251167] [ema: 0.999858] 
[Epoch 1287/1316] [Batch 0/38] [D loss: 0.262975] [G loss: 0.259829] [ema: 0.999858] 
[Epoch 1288/1316] [Batch 0/38] [D loss: 0.292592] [G loss: 0.240815] [ema: 0.999858] 
[Epoch 1289/1316] [Batch 0/38] [D loss: 0.269405] [G loss: 0.244952] [ema: 0.999858] 
[Epoch 1290/1316] [Batch 0/38] [D loss: 0.277580] [G loss: 0.244349] [ema: 0.999859] 
[Epoch 1291/1316] [Batch 0/38] [D loss: 0.266296] [G loss: 0.232521] [ema: 0.999859] 
[Epoch 1292/1316] [Batch 0/38] [D loss: 0.247599] [G loss: 0.253543] [ema: 0.999859] 
[Epoch 1293/1316] [Batch 0/38] [D loss: 0.275286] [G loss: 0.251943] [ema: 0.999859] 
[Epoch 1294/1316] [Batch 0/38] [D loss: 0.298818] [G loss: 0.241393] [ema: 0.999859] 
[Epoch 1295/1316] [Batch 0/38] [D loss: 0.248124] [G loss: 0.249261] [ema: 0.999859] 
[Epoch 1296/1316] [Batch 0/38] [D loss: 0.275400] [G loss: 0.239953] [ema: 0.999859] 
[Epoch 1297/1316] [Batch 0/38] [D loss: 0.259274] [G loss: 0.252891] [ema: 0.999859] 
[Epoch 1298/1316] [Batch 0/38] [D loss: 0.244848] [G loss: 0.252474] [ema: 0.999859] 
[Epoch 1299/1316] [Batch 0/38] [D loss: 0.270199] [G loss: 0.237980] [ema: 0.999860] 
[Epoch 1300/1316] [Batch 0/38] [D loss: 0.289914] [G loss: 0.248834] [ema: 0.999860] 
[Epoch 1301/1316] [Batch 0/38] [D loss: 0.252357] [G loss: 0.236358] [ema: 0.999860] 
[Epoch 1302/1316] [Batch 0/38] [D loss: 0.252479] [G loss: 0.245703] [ema: 0.999860] 
[Epoch 1303/1316] [Batch 0/38] [D loss: 0.254873] [G loss: 0.233301] [ema: 0.999860] 
[Epoch 1304/1316] [Batch 0/38] [D loss: 0.272089] [G loss: 0.242264] [ema: 0.999860] 
[Epoch 1305/1316] [Batch 0/38] [D loss: 0.236689] [G loss: 0.265313] [ema: 0.999860] 
[Epoch 1306/1316] [Batch 0/38] [D loss: 0.261005] [G loss: 0.234063] [ema: 0.999860] 
[Epoch 1307/1316] [Batch 0/38] [D loss: 0.259484] [G loss: 0.237828] [ema: 0.999860] 
[Epoch 1308/1316] [Batch 0/38] [D loss: 0.254592] [G loss: 0.250522] [ema: 0.999861] 
[Epoch 1309/1316] [Batch 0/38] [D loss: 0.257400] [G loss: 0.244251] [ema: 0.999861] 
[Epoch 1310/1316] [Batch 0/38] [D loss: 0.263309] [G loss: 0.254085] [ema: 0.999861] 
[Epoch 1311/1316] [Batch 0/38] [D loss: 0.287086] [G loss: 0.233077] [ema: 0.999861] 
[Epoch 1312/1316] [Batch 0/38] [D loss: 0.266312] [G loss: 0.243753] [ema: 0.999861] 
[Epoch 1313/1316] [Batch 0/38] [D loss: 0.272108] [G loss: 0.243354] [ema: 0.999861] 
[Epoch 1314/1316] [Batch 0/38] [D loss: 0.295238] [G loss: 0.257090] [ema: 0.999861] 
[Epoch 1315/1316] [Batch 0/38] [D loss: 0.257457] [G loss: 0.239137] [ema: 0.999861] 
