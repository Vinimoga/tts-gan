
 Starting training
Total of classes being trained: 6

['MotionSense_DAGHAR_Multiclass.csv', 'RealWorld_thigh_DAGHAR_Multiclass.csv', 'WISDM_DAGHAR_Multiclass.csv', 'UCI_DAGHAR_Multiclass.csv', 'RealWorld_waist_DAGHAR_Multiclass.csv', 'KuHar_DAGHAR_Multiclass.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
MotionSense_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
MotionSense_DAGHAR_Multiclass
daghar
return single class data and labels, class is MotionSense_DAGHAR_Multiclass
data shape is (7116, 3, 1, 30)
label shape is (7116,)
445
Epochs between checkpoint: 29



Saving checkpoint 1 in logs/daghar_split_dataset_50000_30_100/MotionSense_DAGHAR_Multiclass_50000_D_30_2024_10_25_00_00_24/Model



[Epoch 0/113] [Batch 0/445] [D loss: 1.143372] [G loss: 0.921842] [ema: 0.000000] 
[Epoch 0/113] [Batch 100/445] [D loss: 0.564218] [G loss: 0.179172] [ema: 0.933033] 
[Epoch 0/113] [Batch 200/445] [D loss: 0.489522] [G loss: 0.153745] [ema: 0.965936] 
[Epoch 0/113] [Batch 300/445] [D loss: 0.437970] [G loss: 0.197351] [ema: 0.977160] 
[Epoch 0/113] [Batch 400/445] [D loss: 0.368930] [G loss: 0.183860] [ema: 0.982821] 
[Epoch 1/113] [Batch 0/445] [D loss: 0.439881] [G loss: 0.217918] [ema: 0.984544] 
[Epoch 1/113] [Batch 100/445] [D loss: 0.485127] [G loss: 0.223945] [ema: 0.987362] 
[Epoch 1/113] [Batch 200/445] [D loss: 0.521551] [G loss: 0.153264] [ema: 0.989311] 
[Epoch 1/113] [Batch 300/445] [D loss: 0.388806] [G loss: 0.194146] [ema: 0.990739] 
[Epoch 1/113] [Batch 400/445] [D loss: 0.495402] [G loss: 0.153443] [ema: 0.991831] 
[Epoch 2/113] [Batch 0/445] [D loss: 0.453579] [G loss: 0.158256] [ema: 0.992242] 
[Epoch 2/113] [Batch 100/445] [D loss: 0.425940] [G loss: 0.140157] [ema: 0.993023] 
[Epoch 2/113] [Batch 200/445] [D loss: 0.424822] [G loss: 0.140082] [ema: 0.993661] 
[Epoch 2/113] [Batch 300/445] [D loss: 0.445922] [G loss: 0.180716] [ema: 0.994192] 
[Epoch 2/113] [Batch 400/445] [D loss: 0.488232] [G loss: 0.169868] [ema: 0.994641] 
[Epoch 3/113] [Batch 0/445] [D loss: 0.376348] [G loss: 0.190569] [ema: 0.994821] 
[Epoch 3/113] [Batch 100/445] [D loss: 0.395377] [G loss: 0.181677] [ema: 0.995181] 
[Epoch 3/113] [Batch 200/445] [D loss: 0.413843] [G loss: 0.180773] [ema: 0.995495] 
[Epoch 3/113] [Batch 300/445] [D loss: 0.357482] [G loss: 0.197936] [ema: 0.995770] 
[Epoch 3/113] [Batch 400/445] [D loss: 0.390619] [G loss: 0.149085] [ema: 0.996013] 
[Epoch 4/113] [Batch 0/445] [D loss: 0.389871] [G loss: 0.182620] [ema: 0.996113] 
[Epoch 4/113] [Batch 100/445] [D loss: 0.386466] [G loss: 0.185541] [ema: 0.996320] 
[Epoch 4/113] [Batch 200/445] [D loss: 0.328707] [G loss: 0.217478] [ema: 0.996505] 
[Epoch 4/113] [Batch 300/445] [D loss: 0.354553] [G loss: 0.198781] [ema: 0.996673] 
[Epoch 4/113] [Batch 400/445] [D loss: 0.363205] [G loss: 0.228209] [ema: 0.996825] 
[Epoch 5/113] [Batch 0/445] [D loss: 0.319960] [G loss: 0.202387] [ema: 0.996890] 
[Epoch 5/113] [Batch 100/445] [D loss: 0.446954] [G loss: 0.194701] [ema: 0.997023] 
[Epoch 5/113] [Batch 200/445] [D loss: 0.383433] [G loss: 0.162825] [ema: 0.997146] 
[Epoch 5/113] [Batch 300/445] [D loss: 0.341497] [G loss: 0.199915] [ema: 0.997259] 
[Epoch 5/113] [Batch 400/445] [D loss: 0.308456] [G loss: 0.241363] [ema: 0.997363] 
[Epoch 6/113] [Batch 0/445] [D loss: 0.359376] [G loss: 0.210524] [ema: 0.997407] 
[Epoch 6/113] [Batch 100/445] [D loss: 0.331380] [G loss: 0.220895] [ema: 0.997501] 
[Epoch 6/113] [Batch 200/445] [D loss: 0.344701] [G loss: 0.196891] [ema: 0.997588] 
[Epoch 6/113] [Batch 300/445] [D loss: 0.356109] [G loss: 0.237104] [ema: 0.997669] 
[Epoch 6/113] [Batch 400/445] [D loss: 0.374408] [G loss: 0.229033] [ema: 0.997745] 
[Epoch 7/113] [Batch 0/445] [D loss: 0.376094] [G loss: 0.165579] [ema: 0.997777] 
[Epoch 7/113] [Batch 100/445] [D loss: 0.397998] [G loss: 0.191038] [ema: 0.997846] 
[Epoch 7/113] [Batch 200/445] [D loss: 0.377278] [G loss: 0.204576] [ema: 0.997911] 
[Epoch 7/113] [Batch 300/445] [D loss: 0.410348] [G loss: 0.201884] [ema: 0.997972] 
[Epoch 7/113] [Batch 400/445] [D loss: 0.473505] [G loss: 0.178226] [ema: 0.998030] 
[Epoch 8/113] [Batch 0/445] [D loss: 0.452844] [G loss: 0.155062] [ema: 0.998055] 
[Epoch 8/113] [Batch 100/445] [D loss: 0.511468] [G loss: 0.133966] [ema: 0.998108] 
[Epoch 8/113] [Batch 200/445] [D loss: 0.457257] [G loss: 0.174079] [ema: 0.998158] 
[Epoch 8/113] [Batch 300/445] [D loss: 0.465421] [G loss: 0.135565] [ema: 0.998206] 
[Epoch 8/113] [Batch 400/445] [D loss: 0.414172] [G loss: 0.127441] [ema: 0.998251] 
[Epoch 9/113] [Batch 0/445] [D loss: 0.409720] [G loss: 0.184857] [ema: 0.998271] 
[Epoch 9/113] [Batch 100/445] [D loss: 0.468598] [G loss: 0.157300] [ema: 0.998313] 
[Epoch 9/113] [Batch 200/445] [D loss: 0.422249] [G loss: 0.159103] [ema: 0.998353] 
[Epoch 9/113] [Batch 300/445] [D loss: 0.390699] [G loss: 0.189503] [ema: 0.998391] 
[Epoch 9/113] [Batch 400/445] [D loss: 0.437732] [G loss: 0.164783] [ema: 0.998428] 
[Epoch 10/113] [Batch 0/445] [D loss: 0.388240] [G loss: 0.205776] [ema: 0.998444] 
[Epoch 10/113] [Batch 100/445] [D loss: 0.388437] [G loss: 0.200411] [ema: 0.998478] 
[Epoch 10/113] [Batch 200/445] [D loss: 0.403614] [G loss: 0.129385] [ema: 0.998510] 
[Epoch 10/113] [Batch 300/445] [D loss: 0.469071] [G loss: 0.178169] [ema: 0.998542] 
[Epoch 10/113] [Batch 400/445] [D loss: 0.459371] [G loss: 0.159791] [ema: 0.998572] 
[Epoch 11/113] [Batch 0/445] [D loss: 0.352462] [G loss: 0.198632] [ema: 0.998585] 
[Epoch 11/113] [Batch 100/445] [D loss: 0.346581] [G loss: 0.201835] [ema: 0.998613] 
[Epoch 11/113] [Batch 200/445] [D loss: 0.321868] [G loss: 0.185370] [ema: 0.998640] 
[Epoch 11/113] [Batch 300/445] [D loss: 0.394894] [G loss: 0.182394] [ema: 0.998667] 
[Epoch 11/113] [Batch 400/445] [D loss: 0.410081] [G loss: 0.168477] [ema: 0.998692] 
[Epoch 12/113] [Batch 0/445] [D loss: 0.422714] [G loss: 0.188131] [ema: 0.998703] 
[Epoch 12/113] [Batch 100/445] [D loss: 0.440829] [G loss: 0.180673] [ema: 0.998727] 
[Epoch 12/113] [Batch 200/445] [D loss: 0.413853] [G loss: 0.176613] [ema: 0.998750] 
[Epoch 12/113] [Batch 300/445] [D loss: 0.412669] [G loss: 0.152123] [ema: 0.998772] 
[Epoch 12/113] [Batch 400/445] [D loss: 0.363097] [G loss: 0.185241] [ema: 0.998793] 
[Epoch 13/113] [Batch 0/445] [D loss: 0.418884] [G loss: 0.202886] [ema: 0.998803] 
[Epoch 13/113] [Batch 100/445] [D loss: 0.387068] [G loss: 0.218215] [ema: 0.998823] 
[Epoch 13/113] [Batch 200/445] [D loss: 0.399054] [G loss: 0.197185] [ema: 0.998843] 
[Epoch 13/113] [Batch 300/445] [D loss: 0.327351] [G loss: 0.215759] [ema: 0.998862] 
[Epoch 13/113] [Batch 400/445] [D loss: 0.351216] [G loss: 0.180315] [ema: 0.998880] 
[Epoch 14/113] [Batch 0/445] [D loss: 0.382165] [G loss: 0.159440] [ema: 0.998888] 
[Epoch 14/113] [Batch 100/445] [D loss: 0.434630] [G loss: 0.199839] [ema: 0.998906] 
[Epoch 14/113] [Batch 200/445] [D loss: 0.400120] [G loss: 0.180329] [ema: 0.998923] 
[Epoch 14/113] [Batch 300/445] [D loss: 0.324239] [G loss: 0.183910] [ema: 0.998939] 
[Epoch 14/113] [Batch 400/445] [D loss: 0.365367] [G loss: 0.185444] [ema: 0.998955] 
[Epoch 15/113] [Batch 0/445] [D loss: 0.367023] [G loss: 0.201255] [ema: 0.998962] 
[Epoch 15/113] [Batch 100/445] [D loss: 0.422078] [G loss: 0.172883] [ema: 0.998977] 
[Epoch 15/113] [Batch 200/445] [D loss: 0.389940] [G loss: 0.195960] [ema: 0.998992] 
[Epoch 15/113] [Batch 300/445] [D loss: 0.379738] [G loss: 0.169290] [ema: 0.999007] 
[Epoch 15/113] [Batch 400/445] [D loss: 0.351881] [G loss: 0.182545] [ema: 0.999021] 
[Epoch 16/113] [Batch 0/445] [D loss: 0.393726] [G loss: 0.191068] [ema: 0.999027] 
[Epoch 16/113] [Batch 100/445] [D loss: 0.343973] [G loss: 0.220339] [ema: 0.999040] 
[Epoch 16/113] [Batch 200/445] [D loss: 0.342748] [G loss: 0.187587] [ema: 0.999054] 
[Epoch 16/113] [Batch 300/445] [D loss: 0.390968] [G loss: 0.222783] [ema: 0.999066] 
[Epoch 16/113] [Batch 400/445] [D loss: 0.401622] [G loss: 0.184245] [ema: 0.999079] 
[Epoch 17/113] [Batch 0/445] [D loss: 0.414554] [G loss: 0.178919] [ema: 0.999084] 
[Epoch 17/113] [Batch 100/445] [D loss: 0.400599] [G loss: 0.205753] [ema: 0.999096] 
[Epoch 17/113] [Batch 200/445] [D loss: 0.396842] [G loss: 0.173300] [ema: 0.999108] 
[Epoch 17/113] [Batch 300/445] [D loss: 0.358622] [G loss: 0.206524] [ema: 0.999119] 
[Epoch 17/113] [Batch 400/445] [D loss: 0.313772] [G loss: 0.178544] [ema: 0.999130] 
[Epoch 18/113] [Batch 0/445] [D loss: 0.348437] [G loss: 0.199505] [ema: 0.999135] 
[Epoch 18/113] [Batch 100/445] [D loss: 0.366136] [G loss: 0.180522] [ema: 0.999146] 
[Epoch 18/113] [Batch 200/445] [D loss: 0.357726] [G loss: 0.213722] [ema: 0.999156] 
[Epoch 18/113] [Batch 300/445] [D loss: 0.377633] [G loss: 0.184323] [ema: 0.999166] 
[Epoch 18/113] [Batch 400/445] [D loss: 0.324534] [G loss: 0.170063] [ema: 0.999176] 
[Epoch 19/113] [Batch 0/445] [D loss: 0.379149] [G loss: 0.192211] [ema: 0.999181] 
[Epoch 19/113] [Batch 100/445] [D loss: 0.354021] [G loss: 0.163911] [ema: 0.999190] 
[Epoch 19/113] [Batch 200/445] [D loss: 0.428775] [G loss: 0.184657] [ema: 0.999199] 
[Epoch 19/113] [Batch 300/445] [D loss: 0.381369] [G loss: 0.203337] [ema: 0.999209] 
[Epoch 19/113] [Batch 400/445] [D loss: 0.312979] [G loss: 0.170100] [ema: 0.999218] 
[Epoch 20/113] [Batch 0/445] [D loss: 0.346537] [G loss: 0.221386] [ema: 0.999221] 
[Epoch 20/113] [Batch 100/445] [D loss: 0.335508] [G loss: 0.181997] [ema: 0.999230] 
[Epoch 20/113] [Batch 200/445] [D loss: 0.293415] [G loss: 0.215350] [ema: 0.999239] 
[Epoch 20/113] [Batch 300/445] [D loss: 0.323240] [G loss: 0.204228] [ema: 0.999247] 
[Epoch 20/113] [Batch 400/445] [D loss: 0.310734] [G loss: 0.231269] [ema: 0.999255] 
[Epoch 21/113] [Batch 0/445] [D loss: 0.348804] [G loss: 0.233656] [ema: 0.999259] 
[Epoch 21/113] [Batch 100/445] [D loss: 0.300328] [G loss: 0.212051] [ema: 0.999266] 
[Epoch 21/113] [Batch 200/445] [D loss: 0.282029] [G loss: 0.245305] [ema: 0.999274] 
[Epoch 21/113] [Batch 300/445] [D loss: 0.306697] [G loss: 0.207604] [ema: 0.999282] 
[Epoch 21/113] [Batch 400/445] [D loss: 0.333276] [G loss: 0.225033] [ema: 0.999289] 
[Epoch 22/113] [Batch 0/445] [D loss: 0.328978] [G loss: 0.218593] [ema: 0.999292] 
[Epoch 22/113] [Batch 100/445] [D loss: 0.336163] [G loss: 0.176466] [ema: 0.999299] 
[Epoch 22/113] [Batch 200/445] [D loss: 0.347750] [G loss: 0.194715] [ema: 0.999306] 
[Epoch 22/113] [Batch 300/445] [D loss: 0.400139] [G loss: 0.197918] [ema: 0.999313] 
[Epoch 22/113] [Batch 400/445] [D loss: 0.338395] [G loss: 0.206924] [ema: 0.999320] 
[Epoch 23/113] [Batch 0/445] [D loss: 0.366442] [G loss: 0.202337] [ema: 0.999323] 
[Epoch 23/113] [Batch 100/445] [D loss: 0.368077] [G loss: 0.204934] [ema: 0.999330] 
[Epoch 23/113] [Batch 200/445] [D loss: 0.396383] [G loss: 0.198386] [ema: 0.999336] 
[Epoch 23/113] [Batch 300/445] [D loss: 0.333860] [G loss: 0.182720] [ema: 0.999342] 
[Epoch 23/113] [Batch 400/445] [D loss: 0.333678] [G loss: 0.201130] [ema: 0.999348] 
[Epoch 24/113] [Batch 0/445] [D loss: 0.409944] [G loss: 0.197856] [ema: 0.999351] 
[Epoch 24/113] [Batch 100/445] [D loss: 0.322395] [G loss: 0.215743] [ema: 0.999357] 
[Epoch 24/113] [Batch 200/445] [D loss: 0.374729] [G loss: 0.235443] [ema: 0.999363] 
[Epoch 24/113] [Batch 300/445] [D loss: 0.366532] [G loss: 0.206659] [ema: 0.999369] 
[Epoch 24/113] [Batch 400/445] [D loss: 0.349508] [G loss: 0.175030] [ema: 0.999375] 
[Epoch 25/113] [Batch 0/445] [D loss: 0.369687] [G loss: 0.210774] [ema: 0.999377] 
[Epoch 25/113] [Batch 100/445] [D loss: 0.375037] [G loss: 0.192416] [ema: 0.999383] 
[Epoch 25/113] [Batch 200/445] [D loss: 0.418203] [G loss: 0.189810] [ema: 0.999388] 
[Epoch 25/113] [Batch 300/445] [D loss: 0.347588] [G loss: 0.216209] [ema: 0.999393] 
[Epoch 25/113] [Batch 400/445] [D loss: 0.391891] [G loss: 0.170588] [ema: 0.999399] 
[Epoch 26/113] [Batch 0/445] [D loss: 0.331876] [G loss: 0.208675] [ema: 0.999401] 
[Epoch 26/113] [Batch 100/445] [D loss: 0.432375] [G loss: 0.186157] [ema: 0.999406] 
[Epoch 26/113] [Batch 200/445] [D loss: 0.344649] [G loss: 0.183649] [ema: 0.999411] 
[Epoch 26/113] [Batch 300/445] [D loss: 0.377003] [G loss: 0.200320] [ema: 0.999416] 
[Epoch 26/113] [Batch 400/445] [D loss: 0.383890] [G loss: 0.201269] [ema: 0.999421] 
[Epoch 27/113] [Batch 0/445] [D loss: 0.314389] [G loss: 0.190587] [ema: 0.999423] 
[Epoch 27/113] [Batch 100/445] [D loss: 0.344142] [G loss: 0.181412] [ema: 0.999428] 
[Epoch 27/113] [Batch 200/445] [D loss: 0.389122] [G loss: 0.180002] [ema: 0.999433] 
[Epoch 27/113] [Batch 300/445] [D loss: 0.419093] [G loss: 0.206533] [ema: 0.999437] 
[Epoch 27/113] [Batch 400/445] [D loss: 0.409211] [G loss: 0.199504] [ema: 0.999442] 
[Epoch 28/113] [Batch 0/445] [D loss: 0.363929] [G loss: 0.171742] [ema: 0.999444] 
[Epoch 28/113] [Batch 100/445] [D loss: 0.375822] [G loss: 0.198114] [ema: 0.999448] 
[Epoch 28/113] [Batch 200/445] [D loss: 0.347165] [G loss: 0.198211] [ema: 0.999453] 
[Epoch 28/113] [Batch 300/445] [D loss: 0.373523] [G loss: 0.176906] [ema: 0.999457] 
[Epoch 28/113] [Batch 400/445] [D loss: 0.349286] [G loss: 0.201443] [ema: 0.999461] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_30_100/MotionSense_DAGHAR_Multiclass_50000_D_30_2024_10_25_00_00_24/Model



[Epoch 29/113] [Batch 0/445] [D loss: 0.358400] [G loss: 0.197848] [ema: 0.999463] 
[Epoch 29/113] [Batch 100/445] [D loss: 0.362359] [G loss: 0.199021] [ema: 0.999467] 
[Epoch 29/113] [Batch 200/445] [D loss: 0.305618] [G loss: 0.239152] [ema: 0.999471] 
[Epoch 29/113] [Batch 300/445] [D loss: 0.295321] [G loss: 0.215336] [ema: 0.999475] 
[Epoch 29/113] [Batch 400/445] [D loss: 0.380548] [G loss: 0.221123] [ema: 0.999479] 
[Epoch 30/113] [Batch 0/445] [D loss: 0.358654] [G loss: 0.204272] [ema: 0.999481] 
[Epoch 30/113] [Batch 100/445] [D loss: 0.380698] [G loss: 0.193649] [ema: 0.999485] 
[Epoch 30/113] [Batch 200/445] [D loss: 0.292130] [G loss: 0.213743] [ema: 0.999489] 
[Epoch 30/113] [Batch 300/445] [D loss: 0.286904] [G loss: 0.255054] [ema: 0.999492] 
[Epoch 30/113] [Batch 400/445] [D loss: 0.317854] [G loss: 0.226577] [ema: 0.999496] 
[Epoch 31/113] [Batch 0/445] [D loss: 0.321389] [G loss: 0.219569] [ema: 0.999498] 
[Epoch 31/113] [Batch 100/445] [D loss: 0.283355] [G loss: 0.238190] [ema: 0.999501] 
[Epoch 31/113] [Batch 200/445] [D loss: 0.346364] [G loss: 0.222418] [ema: 0.999505] 
[Epoch 31/113] [Batch 300/445] [D loss: 0.332201] [G loss: 0.214064] [ema: 0.999508] 
[Epoch 31/113] [Batch 400/445] [D loss: 0.377895] [G loss: 0.207263] [ema: 0.999512] 
[Epoch 32/113] [Batch 0/445] [D loss: 0.378479] [G loss: 0.176302] [ema: 0.999513] 
[Epoch 32/113] [Batch 100/445] [D loss: 0.388972] [G loss: 0.196612] [ema: 0.999517] 
[Epoch 32/113] [Batch 200/445] [D loss: 0.372444] [G loss: 0.206702] [ema: 0.999520] 
[Epoch 32/113] [Batch 300/445] [D loss: 0.340473] [G loss: 0.219762] [ema: 0.999523] 
[Epoch 32/113] [Batch 400/445] [D loss: 0.447869] [G loss: 0.172163] [ema: 0.999527] 
[Epoch 33/113] [Batch 0/445] [D loss: 0.393650] [G loss: 0.163413] [ema: 0.999528] 
[Epoch 33/113] [Batch 100/445] [D loss: 0.380953] [G loss: 0.180477] [ema: 0.999531] 
[Epoch 33/113] [Batch 200/445] [D loss: 0.370025] [G loss: 0.198255] [ema: 0.999534] 
[Epoch 33/113] [Batch 300/445] [D loss: 0.381336] [G loss: 0.195904] [ema: 0.999538] 
[Epoch 33/113] [Batch 400/445] [D loss: 0.402341] [G loss: 0.179384] [ema: 0.999541] 
[Epoch 34/113] [Batch 0/445] [D loss: 0.369229] [G loss: 0.211918] [ema: 0.999542] 
[Epoch 34/113] [Batch 100/445] [D loss: 0.374961] [G loss: 0.205984] [ema: 0.999545] 
[Epoch 34/113] [Batch 200/445] [D loss: 0.396008] [G loss: 0.191547] [ema: 0.999548] 
[Epoch 34/113] [Batch 300/445] [D loss: 0.352924] [G loss: 0.193355] [ema: 0.999551] 
[Epoch 34/113] [Batch 400/445] [D loss: 0.421477] [G loss: 0.203929] [ema: 0.999554] 
[Epoch 35/113] [Batch 0/445] [D loss: 0.339928] [G loss: 0.184158] [ema: 0.999555] 
[Epoch 35/113] [Batch 100/445] [D loss: 0.372601] [G loss: 0.213088] [ema: 0.999558] 
[Epoch 35/113] [Batch 200/445] [D loss: 0.423831] [G loss: 0.179865] [ema: 0.999561] 
[Epoch 35/113] [Batch 300/445] [D loss: 0.407925] [G loss: 0.180088] [ema: 0.999563] 
[Epoch 35/113] [Batch 400/445] [D loss: 0.348207] [G loss: 0.198052] [ema: 0.999566] 
[Epoch 36/113] [Batch 0/445] [D loss: 0.351778] [G loss: 0.223260] [ema: 0.999567] 
[Epoch 36/113] [Batch 100/445] [D loss: 0.372611] [G loss: 0.186353] [ema: 0.999570] 
[Epoch 36/113] [Batch 200/445] [D loss: 0.392313] [G loss: 0.197386] [ema: 0.999573] 
[Epoch 36/113] [Batch 300/445] [D loss: 0.419555] [G loss: 0.153364] [ema: 0.999575] 
[Epoch 36/113] [Batch 400/445] [D loss: 0.441447] [G loss: 0.184022] [ema: 0.999578] 
[Epoch 37/113] [Batch 0/445] [D loss: 0.393583] [G loss: 0.179333] [ema: 0.999579] 
[Epoch 37/113] [Batch 100/445] [D loss: 0.401858] [G loss: 0.175325] [ema: 0.999582] 
[Epoch 37/113] [Batch 200/445] [D loss: 0.416106] [G loss: 0.178135] [ema: 0.999584] 
[Epoch 37/113] [Batch 300/445] [D loss: 0.386976] [G loss: 0.169574] [ema: 0.999587] 
[Epoch 37/113] [Batch 400/445] [D loss: 0.426497] [G loss: 0.196673] [ema: 0.999589] 
[Epoch 38/113] [Batch 0/445] [D loss: 0.417413] [G loss: 0.190371] [ema: 0.999590] 
[Epoch 38/113] [Batch 100/445] [D loss: 0.376363] [G loss: 0.172133] [ema: 0.999593] 
[Epoch 38/113] [Batch 200/445] [D loss: 0.494088] [G loss: 0.166188] [ema: 0.999595] 
[Epoch 38/113] [Batch 300/445] [D loss: 0.351047] [G loss: 0.179061] [ema: 0.999597] 
[Epoch 38/113] [Batch 400/445] [D loss: 0.412333] [G loss: 0.169141] [ema: 0.999600] 
[Epoch 39/113] [Batch 0/445] [D loss: 0.418782] [G loss: 0.196746] [ema: 0.999601] 
[Epoch 39/113] [Batch 100/445] [D loss: 0.406554] [G loss: 0.172077] [ema: 0.999603] 
[Epoch 39/113] [Batch 200/445] [D loss: 0.416631] [G loss: 0.142795] [ema: 0.999605] 
[Epoch 39/113] [Batch 300/445] [D loss: 0.437795] [G loss: 0.145679] [ema: 0.999607] 
[Epoch 39/113] [Batch 400/445] [D loss: 0.368717] [G loss: 0.190667] [ema: 0.999610] 
[Epoch 40/113] [Batch 0/445] [D loss: 0.439773] [G loss: 0.192475] [ema: 0.999611] 
[Epoch 40/113] [Batch 100/445] [D loss: 0.379565] [G loss: 0.165944] [ema: 0.999613] 
[Epoch 40/113] [Batch 200/445] [D loss: 0.403032] [G loss: 0.167887] [ema: 0.999615] 
[Epoch 40/113] [Batch 300/445] [D loss: 0.401069] [G loss: 0.167519] [ema: 0.999617] 
[Epoch 40/113] [Batch 400/445] [D loss: 0.409858] [G loss: 0.175629] [ema: 0.999619] 
[Epoch 41/113] [Batch 0/445] [D loss: 0.424505] [G loss: 0.203384] [ema: 0.999620] 
[Epoch 41/113] [Batch 100/445] [D loss: 0.368711] [G loss: 0.186341] [ema: 0.999622] 
[Epoch 41/113] [Batch 200/445] [D loss: 0.387285] [G loss: 0.167810] [ema: 0.999624] 
[Epoch 41/113] [Batch 300/445] [D loss: 0.394480] [G loss: 0.186457] [ema: 0.999626] 
[Epoch 41/113] [Batch 400/445] [D loss: 0.508003] [G loss: 0.169336] [ema: 0.999628] 
[Epoch 42/113] [Batch 0/445] [D loss: 0.418912] [G loss: 0.186703] [ema: 0.999629] 
[Epoch 42/113] [Batch 100/445] [D loss: 0.419518] [G loss: 0.179176] [ema: 0.999631] 
[Epoch 42/113] [Batch 200/445] [D loss: 0.398245] [G loss: 0.207621] [ema: 0.999633] 
[Epoch 42/113] [Batch 300/445] [D loss: 0.371334] [G loss: 0.202191] [ema: 0.999635] 
[Epoch 42/113] [Batch 400/445] [D loss: 0.349782] [G loss: 0.186276] [ema: 0.999637] 
[Epoch 43/113] [Batch 0/445] [D loss: 0.398449] [G loss: 0.199856] [ema: 0.999638] 
[Epoch 43/113] [Batch 100/445] [D loss: 0.418228] [G loss: 0.175955] [ema: 0.999640] 
[Epoch 43/113] [Batch 200/445] [D loss: 0.445408] [G loss: 0.205417] [ema: 0.999642] 
[Epoch 43/113] [Batch 300/445] [D loss: 0.389904] [G loss: 0.210240] [ema: 0.999643] 
[Epoch 43/113] [Batch 400/445] [D loss: 0.383923] [G loss: 0.168527] [ema: 0.999645] 
[Epoch 44/113] [Batch 0/445] [D loss: 0.416808] [G loss: 0.179226] [ema: 0.999646] 
[Epoch 44/113] [Batch 100/445] [D loss: 0.468529] [G loss: 0.187078] [ema: 0.999648] 
[Epoch 44/113] [Batch 200/445] [D loss: 0.474943] [G loss: 0.181198] [ema: 0.999650] 
[Epoch 44/113] [Batch 300/445] [D loss: 0.475469] [G loss: 0.158510] [ema: 0.999651] 
[Epoch 44/113] [Batch 400/445] [D loss: 0.388027] [G loss: 0.170996] [ema: 0.999653] 
[Epoch 45/113] [Batch 0/445] [D loss: 0.392464] [G loss: 0.182903] [ema: 0.999654] 
[Epoch 45/113] [Batch 100/445] [D loss: 0.439784] [G loss: 0.173628] [ema: 0.999656] 
[Epoch 45/113] [Batch 200/445] [D loss: 0.411304] [G loss: 0.161232] [ema: 0.999657] 
[Epoch 45/113] [Batch 300/445] [D loss: 0.409581] [G loss: 0.171360] [ema: 0.999659] 
[Epoch 45/113] [Batch 400/445] [D loss: 0.391084] [G loss: 0.178364] [ema: 0.999661] 
[Epoch 46/113] [Batch 0/445] [D loss: 0.369886] [G loss: 0.186285] [ema: 0.999661] 
[Epoch 46/113] [Batch 100/445] [D loss: 0.431916] [G loss: 0.177401] [ema: 0.999663] 
[Epoch 46/113] [Batch 200/445] [D loss: 0.441453] [G loss: 0.177423] [ema: 0.999665] 
[Epoch 46/113] [Batch 300/445] [D loss: 0.429139] [G loss: 0.158923] [ema: 0.999666] 
[Epoch 46/113] [Batch 400/445] [D loss: 0.402124] [G loss: 0.153019] [ema: 0.999668] 
[Epoch 47/113] [Batch 0/445] [D loss: 0.431040] [G loss: 0.188531] [ema: 0.999669] 
[Epoch 47/113] [Batch 100/445] [D loss: 0.453925] [G loss: 0.162983] [ema: 0.999670] 
[Epoch 47/113] [Batch 200/445] [D loss: 0.415306] [G loss: 0.202808] [ema: 0.999672] 
[Epoch 47/113] [Batch 300/445] [D loss: 0.388213] [G loss: 0.172117] [ema: 0.999673] 
[Epoch 47/113] [Batch 400/445] [D loss: 0.474353] [G loss: 0.174345] [ema: 0.999675] 
[Epoch 48/113] [Batch 0/445] [D loss: 0.375671] [G loss: 0.195021] [ema: 0.999676] 
[Epoch 48/113] [Batch 100/445] [D loss: 0.455135] [G loss: 0.187841] [ema: 0.999677] 
[Epoch 48/113] [Batch 200/445] [D loss: 0.445274] [G loss: 0.167067] [ema: 0.999679] 
[Epoch 48/113] [Batch 300/445] [D loss: 0.465638] [G loss: 0.187790] [ema: 0.999680] 
[Epoch 48/113] [Batch 400/445] [D loss: 0.403322] [G loss: 0.155008] [ema: 0.999682] 
[Epoch 49/113] [Batch 0/445] [D loss: 0.394510] [G loss: 0.164236] [ema: 0.999682] 
[Epoch 49/113] [Batch 100/445] [D loss: 0.469033] [G loss: 0.174953] [ema: 0.999684] 
[Epoch 49/113] [Batch 200/445] [D loss: 0.482686] [G loss: 0.223084] [ema: 0.999685] 
[Epoch 49/113] [Batch 300/445] [D loss: 0.430334] [G loss: 0.159254] [ema: 0.999686] 
[Epoch 49/113] [Batch 400/445] [D loss: 0.407673] [G loss: 0.159088] [ema: 0.999688] 
[Epoch 50/113] [Batch 0/445] [D loss: 0.365051] [G loss: 0.190470] [ema: 0.999689] 
[Epoch 50/113] [Batch 100/445] [D loss: 0.345068] [G loss: 0.193368] [ema: 0.999690] 
[Epoch 50/113] [Batch 200/445] [D loss: 0.389413] [G loss: 0.193424] [ema: 0.999691] 
[Epoch 50/113] [Batch 300/445] [D loss: 0.373023] [G loss: 0.198550] [ema: 0.999693] 
[Epoch 50/113] [Batch 400/445] [D loss: 0.369018] [G loss: 0.171527] [ema: 0.999694] 
[Epoch 51/113] [Batch 0/445] [D loss: 0.379382] [G loss: 0.180799] [ema: 0.999695] 
[Epoch 51/113] [Batch 100/445] [D loss: 0.435888] [G loss: 0.175951] [ema: 0.999696] 
[Epoch 51/113] [Batch 200/445] [D loss: 0.435460] [G loss: 0.181559] [ema: 0.999697] 
[Epoch 51/113] [Batch 300/445] [D loss: 0.447991] [G loss: 0.185835] [ema: 0.999699] 
[Epoch 51/113] [Batch 400/445] [D loss: 0.417968] [G loss: 0.179220] [ema: 0.999700] 
[Epoch 52/113] [Batch 0/445] [D loss: 0.374702] [G loss: 0.197697] [ema: 0.999700] 
[Epoch 52/113] [Batch 100/445] [D loss: 0.427252] [G loss: 0.175292] [ema: 0.999702] 
[Epoch 52/113] [Batch 200/445] [D loss: 0.433863] [G loss: 0.165887] [ema: 0.999703] 
[Epoch 52/113] [Batch 300/445] [D loss: 0.385459] [G loss: 0.165506] [ema: 0.999704] 
[Epoch 52/113] [Batch 400/445] [D loss: 0.366335] [G loss: 0.168501] [ema: 0.999706] 
[Epoch 53/113] [Batch 0/445] [D loss: 0.382406] [G loss: 0.195099] [ema: 0.999706] 
[Epoch 53/113] [Batch 100/445] [D loss: 0.397632] [G loss: 0.165496] [ema: 0.999707] 
[Epoch 53/113] [Batch 200/445] [D loss: 0.392680] [G loss: 0.146491] [ema: 0.999709] 
[Epoch 53/113] [Batch 300/445] [D loss: 0.465794] [G loss: 0.180466] [ema: 0.999710] 
[Epoch 53/113] [Batch 400/445] [D loss: 0.374672] [G loss: 0.189968] [ema: 0.999711] 
[Epoch 54/113] [Batch 0/445] [D loss: 0.450980] [G loss: 0.170287] [ema: 0.999712] 
[Epoch 54/113] [Batch 100/445] [D loss: 0.388289] [G loss: 0.150658] [ema: 0.999713] 
[Epoch 54/113] [Batch 200/445] [D loss: 0.424870] [G loss: 0.185712] [ema: 0.999714] 
[Epoch 54/113] [Batch 300/445] [D loss: 0.410036] [G loss: 0.133927] [ema: 0.999715] 
[Epoch 54/113] [Batch 400/445] [D loss: 0.405870] [G loss: 0.188417] [ema: 0.999716] 
[Epoch 55/113] [Batch 0/445] [D loss: 0.435339] [G loss: 0.175254] [ema: 0.999717] 
[Epoch 55/113] [Batch 100/445] [D loss: 0.439361] [G loss: 0.169751] [ema: 0.999718] 
[Epoch 55/113] [Batch 200/445] [D loss: 0.364495] [G loss: 0.186810] [ema: 0.999719] 
[Epoch 55/113] [Batch 300/445] [D loss: 0.416138] [G loss: 0.165577] [ema: 0.999720] 
[Epoch 55/113] [Batch 400/445] [D loss: 0.429813] [G loss: 0.153696] [ema: 0.999721] 
[Epoch 56/113] [Batch 0/445] [D loss: 0.398720] [G loss: 0.173977] [ema: 0.999722] 
[Epoch 56/113] [Batch 100/445] [D loss: 0.435025] [G loss: 0.168379] [ema: 0.999723] 
[Epoch 56/113] [Batch 200/445] [D loss: 0.416930] [G loss: 0.201292] [ema: 0.999724] 
[Epoch 56/113] [Batch 300/445] [D loss: 0.413569] [G loss: 0.153655] [ema: 0.999725] 
[Epoch 56/113] [Batch 400/445] [D loss: 0.406839] [G loss: 0.186156] [ema: 0.999726] 
[Epoch 57/113] [Batch 0/445] [D loss: 0.393035] [G loss: 0.173365] [ema: 0.999727] 
[Epoch 57/113] [Batch 100/445] [D loss: 0.444675] [G loss: 0.149448] [ema: 0.999728] 
[Epoch 57/113] [Batch 200/445] [D loss: 0.405646] [G loss: 0.191829] [ema: 0.999729] 
[Epoch 57/113] [Batch 300/445] [D loss: 0.425568] [G loss: 0.151397] [ema: 0.999730] 
[Epoch 57/113] [Batch 400/445] [D loss: 0.409811] [G loss: 0.139398] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_30_100/MotionSense_DAGHAR_Multiclass_50000_D_30_2024_10_25_00_00_24/Model



[Epoch 58/113] [Batch 0/445] [D loss: 0.437757] [G loss: 0.183528] [ema: 0.999731] 
[Epoch 58/113] [Batch 100/445] [D loss: 0.410172] [G loss: 0.180099] [ema: 0.999733] 
[Epoch 58/113] [Batch 200/445] [D loss: 0.400712] [G loss: 0.172717] [ema: 0.999734] 
[Epoch 58/113] [Batch 300/445] [D loss: 0.418974] [G loss: 0.186728] [ema: 0.999735] 
[Epoch 58/113] [Batch 400/445] [D loss: 0.382205] [G loss: 0.172491] [ema: 0.999736] 
[Epoch 59/113] [Batch 0/445] [D loss: 0.411665] [G loss: 0.171539] [ema: 0.999736] 
[Epoch 59/113] [Batch 100/445] [D loss: 0.375892] [G loss: 0.183948] [ema: 0.999737] 
[Epoch 59/113] [Batch 200/445] [D loss: 0.409651] [G loss: 0.146275] [ema: 0.999738] 
[Epoch 59/113] [Batch 300/445] [D loss: 0.423833] [G loss: 0.162302] [ema: 0.999739] 
[Epoch 59/113] [Batch 400/445] [D loss: 0.379358] [G loss: 0.163557] [ema: 0.999740] 
[Epoch 60/113] [Batch 0/445] [D loss: 0.385500] [G loss: 0.175638] [ema: 0.999740] 
[Epoch 60/113] [Batch 100/445] [D loss: 0.413464] [G loss: 0.164366] [ema: 0.999741] 
[Epoch 60/113] [Batch 200/445] [D loss: 0.401185] [G loss: 0.187858] [ema: 0.999742] 
[Epoch 60/113] [Batch 300/445] [D loss: 0.411258] [G loss: 0.181144] [ema: 0.999743] 
[Epoch 60/113] [Batch 400/445] [D loss: 0.390795] [G loss: 0.177328] [ema: 0.999744] 
[Epoch 61/113] [Batch 0/445] [D loss: 0.404242] [G loss: 0.188803] [ema: 0.999745] 
[Epoch 61/113] [Batch 100/445] [D loss: 0.362280] [G loss: 0.198511] [ema: 0.999746] 
[Epoch 61/113] [Batch 200/445] [D loss: 0.378885] [G loss: 0.182863] [ema: 0.999747] 
[Epoch 61/113] [Batch 300/445] [D loss: 0.481644] [G loss: 0.189209] [ema: 0.999747] 
[Epoch 61/113] [Batch 400/445] [D loss: 0.357387] [G loss: 0.189066] [ema: 0.999748] 
[Epoch 62/113] [Batch 0/445] [D loss: 0.436442] [G loss: 0.169533] [ema: 0.999749] 
[Epoch 62/113] [Batch 100/445] [D loss: 0.392373] [G loss: 0.183888] [ema: 0.999750] 
[Epoch 62/113] [Batch 200/445] [D loss: 0.446666] [G loss: 0.178358] [ema: 0.999751] 
[Epoch 62/113] [Batch 300/445] [D loss: 0.425148] [G loss: 0.167834] [ema: 0.999752] 
[Epoch 62/113] [Batch 400/445] [D loss: 0.393281] [G loss: 0.192186] [ema: 0.999752] 
[Epoch 63/113] [Batch 0/445] [D loss: 0.400463] [G loss: 0.182886] [ema: 0.999753] 
[Epoch 63/113] [Batch 100/445] [D loss: 0.395262] [G loss: 0.179458] [ema: 0.999754] 
[Epoch 63/113] [Batch 200/445] [D loss: 0.378570] [G loss: 0.175386] [ema: 0.999755] 
[Epoch 63/113] [Batch 300/445] [D loss: 0.406718] [G loss: 0.200144] [ema: 0.999755] 
[Epoch 63/113] [Batch 400/445] [D loss: 0.358653] [G loss: 0.160933] [ema: 0.999756] 
[Epoch 64/113] [Batch 0/445] [D loss: 0.380338] [G loss: 0.147650] [ema: 0.999757] 
[Epoch 64/113] [Batch 100/445] [D loss: 0.460329] [G loss: 0.163358] [ema: 0.999758] 
[Epoch 64/113] [Batch 200/445] [D loss: 0.427428] [G loss: 0.162012] [ema: 0.999758] 
[Epoch 64/113] [Batch 300/445] [D loss: 0.416762] [G loss: 0.193901] [ema: 0.999759] 
[Epoch 64/113] [Batch 400/445] [D loss: 0.402772] [G loss: 0.188336] [ema: 0.999760] 
[Epoch 65/113] [Batch 0/445] [D loss: 0.477341] [G loss: 0.182524] [ema: 0.999760] 
[Epoch 65/113] [Batch 100/445] [D loss: 0.365517] [G loss: 0.171936] [ema: 0.999761] 
[Epoch 65/113] [Batch 200/445] [D loss: 0.376363] [G loss: 0.180048] [ema: 0.999762] 
[Epoch 65/113] [Batch 300/445] [D loss: 0.496650] [G loss: 0.169827] [ema: 0.999763] 
[Epoch 65/113] [Batch 400/445] [D loss: 0.406946] [G loss: 0.177626] [ema: 0.999764] 
[Epoch 66/113] [Batch 0/445] [D loss: 0.358927] [G loss: 0.164823] [ema: 0.999764] 
[Epoch 66/113] [Batch 100/445] [D loss: 0.396746] [G loss: 0.166379] [ema: 0.999765] 
[Epoch 66/113] [Batch 200/445] [D loss: 0.362164] [G loss: 0.176262] [ema: 0.999766] 
[Epoch 66/113] [Batch 300/445] [D loss: 0.417203] [G loss: 0.190114] [ema: 0.999766] 
[Epoch 66/113] [Batch 400/445] [D loss: 0.362440] [G loss: 0.166386] [ema: 0.999767] 
[Epoch 67/113] [Batch 0/445] [D loss: 0.385329] [G loss: 0.188024] [ema: 0.999768] 
[Epoch 67/113] [Batch 100/445] [D loss: 0.459866] [G loss: 0.152585] [ema: 0.999768] 
[Epoch 67/113] [Batch 200/445] [D loss: 0.333601] [G loss: 0.185641] [ema: 0.999769] 
[Epoch 67/113] [Batch 300/445] [D loss: 0.406065] [G loss: 0.178797] [ema: 0.999770] 
[Epoch 67/113] [Batch 400/445] [D loss: 0.413952] [G loss: 0.178188] [ema: 0.999771] 
[Epoch 68/113] [Batch 0/445] [D loss: 0.368623] [G loss: 0.183203] [ema: 0.999771] 
[Epoch 68/113] [Batch 100/445] [D loss: 0.392345] [G loss: 0.207625] [ema: 0.999772] 
[Epoch 68/113] [Batch 200/445] [D loss: 0.414730] [G loss: 0.206896] [ema: 0.999772] 
[Epoch 68/113] [Batch 300/445] [D loss: 0.403604] [G loss: 0.176868] [ema: 0.999773] 
[Epoch 68/113] [Batch 400/445] [D loss: 0.349647] [G loss: 0.193428] [ema: 0.999774] 
[Epoch 69/113] [Batch 0/445] [D loss: 0.428541] [G loss: 0.186652] [ema: 0.999774] 
[Epoch 69/113] [Batch 100/445] [D loss: 0.394932] [G loss: 0.196380] [ema: 0.999775] 
[Epoch 69/113] [Batch 200/445] [D loss: 0.363189] [G loss: 0.170797] [ema: 0.999776] 
[Epoch 69/113] [Batch 300/445] [D loss: 0.381150] [G loss: 0.188993] [ema: 0.999776] 
[Epoch 69/113] [Batch 400/445] [D loss: 0.399114] [G loss: 0.173417] [ema: 0.999777] 
[Epoch 70/113] [Batch 0/445] [D loss: 0.411706] [G loss: 0.204804] [ema: 0.999778] 
[Epoch 70/113] [Batch 100/445] [D loss: 0.369479] [G loss: 0.203172] [ema: 0.999778] 
[Epoch 70/113] [Batch 200/445] [D loss: 0.355853] [G loss: 0.184427] [ema: 0.999779] 
[Epoch 70/113] [Batch 300/445] [D loss: 0.386922] [G loss: 0.174126] [ema: 0.999780] 
[Epoch 70/113] [Batch 400/445] [D loss: 0.409752] [G loss: 0.176918] [ema: 0.999780] 
[Epoch 71/113] [Batch 0/445] [D loss: 0.412282] [G loss: 0.179996] [ema: 0.999781] 
[Epoch 71/113] [Batch 100/445] [D loss: 0.407178] [G loss: 0.173646] [ema: 0.999781] 
[Epoch 71/113] [Batch 200/445] [D loss: 0.445780] [G loss: 0.181898] [ema: 0.999782] 
[Epoch 71/113] [Batch 300/445] [D loss: 0.406661] [G loss: 0.166201] [ema: 0.999783] 
[Epoch 71/113] [Batch 400/445] [D loss: 0.430469] [G loss: 0.206635] [ema: 0.999783] 
[Epoch 72/113] [Batch 0/445] [D loss: 0.421137] [G loss: 0.191671] [ema: 0.999784] 
[Epoch 72/113] [Batch 100/445] [D loss: 0.390284] [G loss: 0.179563] [ema: 0.999784] 
[Epoch 72/113] [Batch 200/445] [D loss: 0.379244] [G loss: 0.186141] [ema: 0.999785] 
[Epoch 72/113] [Batch 300/445] [D loss: 0.398619] [G loss: 0.173623] [ema: 0.999786] 
[Epoch 72/113] [Batch 400/445] [D loss: 0.446909] [G loss: 0.195884] [ema: 0.999786] 
[Epoch 73/113] [Batch 0/445] [D loss: 0.423549] [G loss: 0.210553] [ema: 0.999787] 
[Epoch 73/113] [Batch 100/445] [D loss: 0.415762] [G loss: 0.185550] [ema: 0.999787] 
[Epoch 73/113] [Batch 200/445] [D loss: 0.432086] [G loss: 0.190524] [ema: 0.999788] 
[Epoch 73/113] [Batch 300/445] [D loss: 0.365049] [G loss: 0.161041] [ema: 0.999789] 
[Epoch 73/113] [Batch 400/445] [D loss: 0.397328] [G loss: 0.175372] [ema: 0.999789] 
[Epoch 74/113] [Batch 0/445] [D loss: 0.422941] [G loss: 0.170561] [ema: 0.999790] 
[Epoch 74/113] [Batch 100/445] [D loss: 0.366830] [G loss: 0.191979] [ema: 0.999790] 
[Epoch 74/113] [Batch 200/445] [D loss: 0.363453] [G loss: 0.155870] [ema: 0.999791] 
[Epoch 74/113] [Batch 300/445] [D loss: 0.422575] [G loss: 0.158703] [ema: 0.999791] 
[Epoch 74/113] [Batch 400/445] [D loss: 0.409338] [G loss: 0.162506] [ema: 0.999792] 
[Epoch 75/113] [Batch 0/445] [D loss: 0.356989] [G loss: 0.178065] [ema: 0.999792] 
[Epoch 75/113] [Batch 100/445] [D loss: 0.382531] [G loss: 0.175819] [ema: 0.999793] 
[Epoch 75/113] [Batch 200/445] [D loss: 0.373278] [G loss: 0.188025] [ema: 0.999794] 
[Epoch 75/113] [Batch 300/445] [D loss: 0.441413] [G loss: 0.181808] [ema: 0.999794] 
[Epoch 75/113] [Batch 400/445] [D loss: 0.390785] [G loss: 0.151841] [ema: 0.999795] 
[Epoch 76/113] [Batch 0/445] [D loss: 0.371638] [G loss: 0.196762] [ema: 0.999795] 
[Epoch 76/113] [Batch 100/445] [D loss: 0.396653] [G loss: 0.166161] [ema: 0.999796] 
[Epoch 76/113] [Batch 200/445] [D loss: 0.369191] [G loss: 0.197336] [ema: 0.999796] 
[Epoch 76/113] [Batch 300/445] [D loss: 0.351324] [G loss: 0.208272] [ema: 0.999797] 
[Epoch 76/113] [Batch 400/445] [D loss: 0.358200] [G loss: 0.201530] [ema: 0.999797] 
[Epoch 77/113] [Batch 0/445] [D loss: 0.383547] [G loss: 0.212955] [ema: 0.999798] 
[Epoch 77/113] [Batch 100/445] [D loss: 0.326367] [G loss: 0.188394] [ema: 0.999798] 
[Epoch 77/113] [Batch 200/445] [D loss: 0.413074] [G loss: 0.159552] [ema: 0.999799] 
[Epoch 77/113] [Batch 300/445] [D loss: 0.370029] [G loss: 0.151144] [ema: 0.999799] 
[Epoch 77/113] [Batch 400/445] [D loss: 0.377948] [G loss: 0.201242] [ema: 0.999800] 
[Epoch 78/113] [Batch 0/445] [D loss: 0.362547] [G loss: 0.190291] [ema: 0.999800] 
[Epoch 78/113] [Batch 100/445] [D loss: 0.418708] [G loss: 0.189157] [ema: 0.999801] 
[Epoch 78/113] [Batch 200/445] [D loss: 0.426888] [G loss: 0.172249] [ema: 0.999801] 
[Epoch 78/113] [Batch 300/445] [D loss: 0.400554] [G loss: 0.173622] [ema: 0.999802] 
[Epoch 78/113] [Batch 400/445] [D loss: 0.423435] [G loss: 0.172255] [ema: 0.999803] 
[Epoch 79/113] [Batch 0/445] [D loss: 0.358156] [G loss: 0.190750] [ema: 0.999803] 
[Epoch 79/113] [Batch 100/445] [D loss: 0.386169] [G loss: 0.173089] [ema: 0.999803] 
[Epoch 79/113] [Batch 200/445] [D loss: 0.353223] [G loss: 0.192490] [ema: 0.999804] 
[Epoch 79/113] [Batch 300/445] [D loss: 0.412124] [G loss: 0.189299] [ema: 0.999805] 
[Epoch 79/113] [Batch 400/445] [D loss: 0.365523] [G loss: 0.203152] [ema: 0.999805] 
[Epoch 80/113] [Batch 0/445] [D loss: 0.391443] [G loss: 0.178129] [ema: 0.999805] 
[Epoch 80/113] [Batch 100/445] [D loss: 0.414922] [G loss: 0.181397] [ema: 0.999806] 
[Epoch 80/113] [Batch 200/445] [D loss: 0.412093] [G loss: 0.182323] [ema: 0.999806] 
[Epoch 80/113] [Batch 300/445] [D loss: 0.462119] [G loss: 0.164553] [ema: 0.999807] 
[Epoch 80/113] [Batch 400/445] [D loss: 0.382466] [G loss: 0.198124] [ema: 0.999807] 
[Epoch 81/113] [Batch 0/445] [D loss: 0.322559] [G loss: 0.220318] [ema: 0.999808] 
[Epoch 81/113] [Batch 100/445] [D loss: 0.334808] [G loss: 0.207509] [ema: 0.999808] 
[Epoch 81/113] [Batch 200/445] [D loss: 0.380548] [G loss: 0.192508] [ema: 0.999809] 
[Epoch 81/113] [Batch 300/445] [D loss: 0.401338] [G loss: 0.176190] [ema: 0.999809] 
[Epoch 81/113] [Batch 400/445] [D loss: 0.365230] [G loss: 0.187634] [ema: 0.999810] 
[Epoch 82/113] [Batch 0/445] [D loss: 0.320731] [G loss: 0.217001] [ema: 0.999810] 
[Epoch 82/113] [Batch 100/445] [D loss: 0.392049] [G loss: 0.192847] [ema: 0.999811] 
[Epoch 82/113] [Batch 200/445] [D loss: 0.339058] [G loss: 0.178151] [ema: 0.999811] 
[Epoch 82/113] [Batch 300/445] [D loss: 0.372264] [G loss: 0.183947] [ema: 0.999812] 
[Epoch 82/113] [Batch 400/445] [D loss: 0.328799] [G loss: 0.183134] [ema: 0.999812] 
[Epoch 83/113] [Batch 0/445] [D loss: 0.344125] [G loss: 0.180295] [ema: 0.999812] 
[Epoch 83/113] [Batch 100/445] [D loss: 0.398862] [G loss: 0.162835] [ema: 0.999813] 
[Epoch 83/113] [Batch 200/445] [D loss: 0.371288] [G loss: 0.199052] [ema: 0.999813] 
[Epoch 83/113] [Batch 300/445] [D loss: 0.366080] [G loss: 0.202293] [ema: 0.999814] 
[Epoch 83/113] [Batch 400/445] [D loss: 0.390976] [G loss: 0.179578] [ema: 0.999814] 
[Epoch 84/113] [Batch 0/445] [D loss: 0.405302] [G loss: 0.157571] [ema: 0.999815] 
[Epoch 84/113] [Batch 100/445] [D loss: 0.481592] [G loss: 0.185627] [ema: 0.999815] 
[Epoch 84/113] [Batch 200/445] [D loss: 0.377036] [G loss: 0.202639] [ema: 0.999816] 
[Epoch 84/113] [Batch 300/445] [D loss: 0.365659] [G loss: 0.186924] [ema: 0.999816] 
[Epoch 84/113] [Batch 400/445] [D loss: 0.427654] [G loss: 0.190684] [ema: 0.999817] 
[Epoch 85/113] [Batch 0/445] [D loss: 0.402258] [G loss: 0.199366] [ema: 0.999817] 
[Epoch 85/113] [Batch 100/445] [D loss: 0.395375] [G loss: 0.206591] [ema: 0.999817] 
[Epoch 85/113] [Batch 200/445] [D loss: 0.344835] [G loss: 0.179071] [ema: 0.999818] 
[Epoch 85/113] [Batch 300/445] [D loss: 0.414997] [G loss: 0.187286] [ema: 0.999818] 
[Epoch 85/113] [Batch 400/445] [D loss: 0.365883] [G loss: 0.174257] [ema: 0.999819] 
[Epoch 86/113] [Batch 0/445] [D loss: 0.397003] [G loss: 0.180402] [ema: 0.999819] 
[Epoch 86/113] [Batch 100/445] [D loss: 0.392690] [G loss: 0.200611] [ema: 0.999819] 
[Epoch 86/113] [Batch 200/445] [D loss: 0.457789] [G loss: 0.199928] [ema: 0.999820] 
[Epoch 86/113] [Batch 300/445] [D loss: 0.472371] [G loss: 0.208605] [ema: 0.999820] 
[Epoch 86/113] [Batch 400/445] [D loss: 0.317674] [G loss: 0.213547] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_30_100/MotionSense_DAGHAR_Multiclass_50000_D_30_2024_10_25_00_00_24/Model



[Epoch 87/113] [Batch 0/445] [D loss: 0.376126] [G loss: 0.169795] [ema: 0.999821] 
[Epoch 87/113] [Batch 100/445] [D loss: 0.409990] [G loss: 0.203007] [ema: 0.999821] 
[Epoch 87/113] [Batch 200/445] [D loss: 0.417999] [G loss: 0.175415] [ema: 0.999822] 
[Epoch 87/113] [Batch 300/445] [D loss: 0.426004] [G loss: 0.189270] [ema: 0.999822] 
[Epoch 87/113] [Batch 400/445] [D loss: 0.357351] [G loss: 0.188821] [ema: 0.999823] 
[Epoch 88/113] [Batch 0/445] [D loss: 0.379107] [G loss: 0.179941] [ema: 0.999823] 
[Epoch 88/113] [Batch 100/445] [D loss: 0.352517] [G loss: 0.191997] [ema: 0.999823] 
[Epoch 88/113] [Batch 200/445] [D loss: 0.404329] [G loss: 0.184688] [ema: 0.999824] 
[Epoch 88/113] [Batch 300/445] [D loss: 0.355614] [G loss: 0.178458] [ema: 0.999824] 
[Epoch 88/113] [Batch 400/445] [D loss: 0.403263] [G loss: 0.188240] [ema: 0.999825] 
[Epoch 89/113] [Batch 0/445] [D loss: 0.330020] [G loss: 0.225313] [ema: 0.999825] 
[Epoch 89/113] [Batch 100/445] [D loss: 0.321856] [G loss: 0.193647] [ema: 0.999825] 
[Epoch 89/113] [Batch 200/445] [D loss: 0.364914] [G loss: 0.196581] [ema: 0.999826] 
[Epoch 89/113] [Batch 300/445] [D loss: 0.342852] [G loss: 0.199683] [ema: 0.999826] 
[Epoch 89/113] [Batch 400/445] [D loss: 0.316160] [G loss: 0.193161] [ema: 0.999827] 
[Epoch 90/113] [Batch 0/445] [D loss: 0.343929] [G loss: 0.219688] [ema: 0.999827] 
[Epoch 90/113] [Batch 100/445] [D loss: 0.410841] [G loss: 0.203771] [ema: 0.999827] 
[Epoch 90/113] [Batch 200/445] [D loss: 0.402432] [G loss: 0.223329] [ema: 0.999828] 
[Epoch 90/113] [Batch 300/445] [D loss: 0.401443] [G loss: 0.192851] [ema: 0.999828] 
[Epoch 90/113] [Batch 400/445] [D loss: 0.394562] [G loss: 0.178075] [ema: 0.999829] 
[Epoch 91/113] [Batch 0/445] [D loss: 0.400111] [G loss: 0.196049] [ema: 0.999829] 
[Epoch 91/113] [Batch 100/445] [D loss: 0.389187] [G loss: 0.198045] [ema: 0.999829] 
[Epoch 91/113] [Batch 200/445] [D loss: 0.332632] [G loss: 0.181283] [ema: 0.999830] 
[Epoch 91/113] [Batch 300/445] [D loss: 0.378128] [G loss: 0.186764] [ema: 0.999830] 
[Epoch 91/113] [Batch 400/445] [D loss: 0.421249] [G loss: 0.198516] [ema: 0.999831] 
[Epoch 92/113] [Batch 0/445] [D loss: 0.380029] [G loss: 0.212458] [ema: 0.999831] 
[Epoch 92/113] [Batch 100/445] [D loss: 0.365672] [G loss: 0.198046] [ema: 0.999831] 
[Epoch 92/113] [Batch 200/445] [D loss: 0.336924] [G loss: 0.185978] [ema: 0.999832] 
[Epoch 92/113] [Batch 300/445] [D loss: 0.351931] [G loss: 0.181029] [ema: 0.999832] 
[Epoch 92/113] [Batch 400/445] [D loss: 0.368038] [G loss: 0.178759] [ema: 0.999832] 
[Epoch 93/113] [Batch 0/445] [D loss: 0.370244] [G loss: 0.213857] [ema: 0.999833] 
[Epoch 93/113] [Batch 100/445] [D loss: 0.353755] [G loss: 0.202732] [ema: 0.999833] 
[Epoch 93/113] [Batch 200/445] [D loss: 0.363317] [G loss: 0.183717] [ema: 0.999833] 
[Epoch 93/113] [Batch 300/445] [D loss: 0.372830] [G loss: 0.173806] [ema: 0.999834] 
[Epoch 93/113] [Batch 400/445] [D loss: 0.371747] [G loss: 0.198343] [ema: 0.999834] 
[Epoch 94/113] [Batch 0/445] [D loss: 0.366743] [G loss: 0.191623] [ema: 0.999834] 
[Epoch 94/113] [Batch 100/445] [D loss: 0.393907] [G loss: 0.188667] [ema: 0.999835] 
[Epoch 94/113] [Batch 200/445] [D loss: 0.366479] [G loss: 0.178255] [ema: 0.999835] 
[Epoch 94/113] [Batch 300/445] [D loss: 0.404238] [G loss: 0.197689] [ema: 0.999835] 
[Epoch 94/113] [Batch 400/445] [D loss: 0.441338] [G loss: 0.215851] [ema: 0.999836] 
[Epoch 95/113] [Batch 0/445] [D loss: 0.391588] [G loss: 0.214421] [ema: 0.999836] 
[Epoch 95/113] [Batch 100/445] [D loss: 0.340742] [G loss: 0.179029] [ema: 0.999836] 
[Epoch 95/113] [Batch 200/445] [D loss: 0.399123] [G loss: 0.209558] [ema: 0.999837] 
[Epoch 95/113] [Batch 300/445] [D loss: 0.353543] [G loss: 0.189685] [ema: 0.999837] 
[Epoch 95/113] [Batch 400/445] [D loss: 0.396231] [G loss: 0.187807] [ema: 0.999838] 
[Epoch 96/113] [Batch 0/445] [D loss: 0.376706] [G loss: 0.182758] [ema: 0.999838] 
[Epoch 96/113] [Batch 100/445] [D loss: 0.343594] [G loss: 0.217983] [ema: 0.999838] 
[Epoch 96/113] [Batch 200/445] [D loss: 0.298462] [G loss: 0.184419] [ema: 0.999839] 
[Epoch 96/113] [Batch 300/445] [D loss: 0.389379] [G loss: 0.210093] [ema: 0.999839] 
[Epoch 96/113] [Batch 400/445] [D loss: 0.375163] [G loss: 0.219739] [ema: 0.999839] 
[Epoch 97/113] [Batch 0/445] [D loss: 0.440765] [G loss: 0.210479] [ema: 0.999839] 
[Epoch 97/113] [Batch 100/445] [D loss: 0.348134] [G loss: 0.207004] [ema: 0.999840] 
[Epoch 97/113] [Batch 200/445] [D loss: 0.383760] [G loss: 0.205057] [ema: 0.999840] 
[Epoch 97/113] [Batch 300/445] [D loss: 0.329371] [G loss: 0.202144] [ema: 0.999841] 
[Epoch 97/113] [Batch 400/445] [D loss: 0.348057] [G loss: 0.199306] [ema: 0.999841] 
[Epoch 98/113] [Batch 0/445] [D loss: 0.331545] [G loss: 0.223037] [ema: 0.999841] 
[Epoch 98/113] [Batch 100/445] [D loss: 0.399118] [G loss: 0.201771] [ema: 0.999841] 
[Epoch 98/113] [Batch 200/445] [D loss: 0.372259] [G loss: 0.205933] [ema: 0.999842] 
[Epoch 98/113] [Batch 300/445] [D loss: 0.327370] [G loss: 0.191379] [ema: 0.999842] 
[Epoch 98/113] [Batch 400/445] [D loss: 0.414806] [G loss: 0.201833] [ema: 0.999843] 
[Epoch 99/113] [Batch 0/445] [D loss: 0.311235] [G loss: 0.228219] [ema: 0.999843] 
[Epoch 99/113] [Batch 100/445] [D loss: 0.374934] [G loss: 0.208776] [ema: 0.999843] 
[Epoch 99/113] [Batch 200/445] [D loss: 0.334585] [G loss: 0.169748] [ema: 0.999843] 
[Epoch 99/113] [Batch 300/445] [D loss: 0.351993] [G loss: 0.227148] [ema: 0.999844] 
[Epoch 99/113] [Batch 400/445] [D loss: 0.366153] [G loss: 0.202965] [ema: 0.999844] 
[Epoch 100/113] [Batch 0/445] [D loss: 0.329191] [G loss: 0.212331] [ema: 0.999844] 
[Epoch 100/113] [Batch 100/445] [D loss: 0.350719] [G loss: 0.171048] [ema: 0.999845] 
[Epoch 100/113] [Batch 200/445] [D loss: 0.355735] [G loss: 0.163398] [ema: 0.999845] 
[Epoch 100/113] [Batch 300/445] [D loss: 0.336907] [G loss: 0.197685] [ema: 0.999845] 
[Epoch 100/113] [Batch 400/445] [D loss: 0.392629] [G loss: 0.210753] [ema: 0.999846] 
[Epoch 101/113] [Batch 0/445] [D loss: 0.335482] [G loss: 0.194366] [ema: 0.999846] 
[Epoch 101/113] [Batch 100/445] [D loss: 0.337499] [G loss: 0.218176] [ema: 0.999846] 
[Epoch 101/113] [Batch 200/445] [D loss: 0.354854] [G loss: 0.222483] [ema: 0.999846] 
[Epoch 101/113] [Batch 300/445] [D loss: 0.368460] [G loss: 0.196050] [ema: 0.999847] 
[Epoch 101/113] [Batch 400/445] [D loss: 0.445206] [G loss: 0.211491] [ema: 0.999847] 
[Epoch 102/113] [Batch 0/445] [D loss: 0.398776] [G loss: 0.189652] [ema: 0.999847] 
[Epoch 102/113] [Batch 100/445] [D loss: 0.381353] [G loss: 0.201460] [ema: 0.999848] 
[Epoch 102/113] [Batch 200/445] [D loss: 0.351377] [G loss: 0.232792] [ema: 0.999848] 
[Epoch 102/113] [Batch 300/445] [D loss: 0.391166] [G loss: 0.201817] [ema: 0.999848] 
[Epoch 102/113] [Batch 400/445] [D loss: 0.337421] [G loss: 0.146932] [ema: 0.999849] 
[Epoch 103/113] [Batch 0/445] [D loss: 0.352835] [G loss: 0.204778] [ema: 0.999849] 
[Epoch 103/113] [Batch 100/445] [D loss: 0.404618] [G loss: 0.212289] [ema: 0.999849] 
[Epoch 103/113] [Batch 200/445] [D loss: 0.385181] [G loss: 0.209666] [ema: 0.999849] 
[Epoch 103/113] [Batch 300/445] [D loss: 0.320526] [G loss: 0.173843] [ema: 0.999850] 
[Epoch 103/113] [Batch 400/445] [D loss: 0.334021] [G loss: 0.214539] [ema: 0.999850] 
[Epoch 104/113] [Batch 0/445] [D loss: 0.367561] [G loss: 0.181487] [ema: 0.999850] 
[Epoch 104/113] [Batch 100/445] [D loss: 0.341613] [G loss: 0.199092] [ema: 0.999851] 
[Epoch 104/113] [Batch 200/445] [D loss: 0.340256] [G loss: 0.187942] [ema: 0.999851] 
[Epoch 104/113] [Batch 300/445] [D loss: 0.403671] [G loss: 0.208609] [ema: 0.999851] 
[Epoch 104/113] [Batch 400/445] [D loss: 0.321462] [G loss: 0.193886] [ema: 0.999852] 
[Epoch 105/113] [Batch 0/445] [D loss: 0.355389] [G loss: 0.182447] [ema: 0.999852] 
[Epoch 105/113] [Batch 100/445] [D loss: 0.431766] [G loss: 0.202048] [ema: 0.999852] 
[Epoch 105/113] [Batch 200/445] [D loss: 0.413022] [G loss: 0.218480] [ema: 0.999852] 
[Epoch 105/113] [Batch 300/445] [D loss: 0.392260] [G loss: 0.201005] [ema: 0.999853] 
[Epoch 105/113] [Batch 400/445] [D loss: 0.397794] [G loss: 0.229328] [ema: 0.999853] 
[Epoch 106/113] [Batch 0/445] [D loss: 0.362025] [G loss: 0.215469] [ema: 0.999853] 
[Epoch 106/113] [Batch 100/445] [D loss: 0.384332] [G loss: 0.211874] [ema: 0.999853] 
[Epoch 106/113] [Batch 200/445] [D loss: 0.350815] [G loss: 0.187011] [ema: 0.999854] 
[Epoch 106/113] [Batch 300/445] [D loss: 0.300294] [G loss: 0.194113] [ema: 0.999854] 
[Epoch 106/113] [Batch 400/445] [D loss: 0.321609] [G loss: 0.187579] [ema: 0.999854] 
[Epoch 107/113] [Batch 0/445] [D loss: 0.291232] [G loss: 0.233525] [ema: 0.999854] 
[Epoch 107/113] [Batch 100/445] [D loss: 0.355502] [G loss: 0.182326] [ema: 0.999855] 
[Epoch 107/113] [Batch 200/445] [D loss: 0.319608] [G loss: 0.199627] [ema: 0.999855] 
[Epoch 107/113] [Batch 300/445] [D loss: 0.357881] [G loss: 0.197062] [ema: 0.999855] 
[Epoch 107/113] [Batch 400/445] [D loss: 0.346996] [G loss: 0.210433] [ema: 0.999856] 
[Epoch 108/113] [Batch 0/445] [D loss: 0.334608] [G loss: 0.201776] [ema: 0.999856] 
[Epoch 108/113] [Batch 100/445] [D loss: 0.353158] [G loss: 0.197383] [ema: 0.999856] 
[Epoch 108/113] [Batch 200/445] [D loss: 0.366329] [G loss: 0.207231] [ema: 0.999856] 
[Epoch 108/113] [Batch 300/445] [D loss: 0.404506] [G loss: 0.211365] [ema: 0.999857] 
[Epoch 108/113] [Batch 400/445] [D loss: 0.393809] [G loss: 0.188119] [ema: 0.999857] 
[Epoch 109/113] [Batch 0/445] [D loss: 0.315910] [G loss: 0.179001] [ema: 0.999857] 
[Epoch 109/113] [Batch 100/445] [D loss: 0.367303] [G loss: 0.199115] [ema: 0.999857] 
[Epoch 109/113] [Batch 200/445] [D loss: 0.355490] [G loss: 0.195895] [ema: 0.999858] 
[Epoch 109/113] [Batch 300/445] [D loss: 0.315677] [G loss: 0.214092] [ema: 0.999858] 
[Epoch 109/113] [Batch 400/445] [D loss: 0.385313] [G loss: 0.201871] [ema: 0.999858] 
[Epoch 110/113] [Batch 0/445] [D loss: 0.358657] [G loss: 0.193237] [ema: 0.999858] 
[Epoch 110/113] [Batch 100/445] [D loss: 0.353604] [G loss: 0.212430] [ema: 0.999859] 
[Epoch 110/113] [Batch 200/445] [D loss: 0.329322] [G loss: 0.220654] [ema: 0.999859] 
[Epoch 110/113] [Batch 300/445] [D loss: 0.361158] [G loss: 0.196829] [ema: 0.999859] 
[Epoch 110/113] [Batch 400/445] [D loss: 0.388472] [G loss: 0.215824] [ema: 0.999860] 
[Epoch 111/113] [Batch 0/445] [D loss: 0.358849] [G loss: 0.205038] [ema: 0.999860] 
[Epoch 111/113] [Batch 100/445] [D loss: 0.419937] [G loss: 0.176940] [ema: 0.999860] 
[Epoch 111/113] [Batch 200/445] [D loss: 0.322921] [G loss: 0.205642] [ema: 0.999860] 
[Epoch 111/113] [Batch 300/445] [D loss: 0.344857] [G loss: 0.216659] [ema: 0.999861] 
[Epoch 111/113] [Batch 400/445] [D loss: 0.395279] [G loss: 0.184766] [ema: 0.999861] 
[Epoch 112/113] [Batch 0/445] [D loss: 0.316346] [G loss: 0.228216] [ema: 0.999861] 
[Epoch 112/113] [Batch 100/445] [D loss: 0.336338] [G loss: 0.204717] [ema: 0.999861] 
[Epoch 112/113] [Batch 200/445] [D loss: 0.305188] [G loss: 0.223859] [ema: 0.999861] 
[Epoch 112/113] [Batch 300/445] [D loss: 0.357569] [G loss: 0.198826] [ema: 0.999862] 
[Epoch 112/113] [Batch 400/445] [D loss: 0.346505] [G loss: 0.186761] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
RealWorld_thigh_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
RealWorld_thigh_DAGHAR_Multiclass
daghar
return single class data and labels, class is RealWorld_thigh_DAGHAR_Multiclass
data shape is (20676, 3, 1, 30)
label shape is (20676,)
1293
Epochs between checkpoint: 10



Saving checkpoint 1 in logs/daghar_split_dataset_50000_30_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_30_2024_10_25_00_33_27/Model



[Epoch 0/39] [Batch 0/1293] [D loss: 1.355401] [G loss: 0.901320] [ema: 0.000000] 
[Epoch 0/39] [Batch 100/1293] [D loss: 0.573301] [G loss: 0.143107] [ema: 0.933033] 
[Epoch 0/39] [Batch 200/1293] [D loss: 0.496216] [G loss: 0.150971] [ema: 0.965936] 
[Epoch 0/39] [Batch 300/1293] [D loss: 0.404030] [G loss: 0.209760] [ema: 0.977160] 
[Epoch 0/39] [Batch 400/1293] [D loss: 0.420239] [G loss: 0.154116] [ema: 0.982821] 
[Epoch 0/39] [Batch 500/1293] [D loss: 0.537465] [G loss: 0.150168] [ema: 0.986233] 
[Epoch 0/39] [Batch 600/1293] [D loss: 0.437491] [G loss: 0.217960] [ema: 0.988514] 
[Epoch 0/39] [Batch 700/1293] [D loss: 0.484602] [G loss: 0.173375] [ema: 0.990147] 
[Epoch 0/39] [Batch 800/1293] [D loss: 0.492225] [G loss: 0.153363] [ema: 0.991373] 
[Epoch 0/39] [Batch 900/1293] [D loss: 0.390152] [G loss: 0.180736] [ema: 0.992328] 
[Epoch 0/39] [Batch 1000/1293] [D loss: 0.513238] [G loss: 0.130172] [ema: 0.993092] 
[Epoch 0/39] [Batch 1100/1293] [D loss: 0.507125] [G loss: 0.176180] [ema: 0.993718] 
[Epoch 0/39] [Batch 1200/1293] [D loss: 0.517280] [G loss: 0.159927] [ema: 0.994240] 
[Epoch 1/39] [Batch 0/1293] [D loss: 0.486556] [G loss: 0.160459] [ema: 0.994654] 
[Epoch 1/39] [Batch 100/1293] [D loss: 0.418652] [G loss: 0.190051] [ema: 0.995036] 
[Epoch 1/39] [Batch 200/1293] [D loss: 0.437861] [G loss: 0.155092] [ema: 0.995368] 
[Epoch 1/39] [Batch 300/1293] [D loss: 0.352630] [G loss: 0.170220] [ema: 0.995658] 
[Epoch 1/39] [Batch 400/1293] [D loss: 0.446279] [G loss: 0.200040] [ema: 0.995914] 
[Epoch 1/39] [Batch 500/1293] [D loss: 0.379230] [G loss: 0.188377] [ema: 0.996142] 
[Epoch 1/39] [Batch 600/1293] [D loss: 0.410524] [G loss: 0.169254] [ema: 0.996345] 
[Epoch 1/39] [Batch 700/1293] [D loss: 0.464377] [G loss: 0.173175] [ema: 0.996528] 
[Epoch 1/39] [Batch 800/1293] [D loss: 0.400948] [G loss: 0.178411] [ema: 0.996694] 
[Epoch 1/39] [Batch 900/1293] [D loss: 0.422688] [G loss: 0.197149] [ema: 0.996844] 
[Epoch 1/39] [Batch 1000/1293] [D loss: 0.399621] [G loss: 0.181306] [ema: 0.996982] 
[Epoch 1/39] [Batch 1100/1293] [D loss: 0.358136] [G loss: 0.228442] [ema: 0.997108] 
[Epoch 1/39] [Batch 1200/1293] [D loss: 0.414807] [G loss: 0.172458] [ema: 0.997223] 
[Epoch 2/39] [Batch 0/1293] [D loss: 0.432234] [G loss: 0.220596] [ema: 0.997323] 
[Epoch 2/39] [Batch 100/1293] [D loss: 0.365753] [G loss: 0.171203] [ema: 0.997423] 
[Epoch 2/39] [Batch 200/1293] [D loss: 0.391598] [G loss: 0.189953] [ema: 0.997515] 
[Epoch 2/39] [Batch 300/1293] [D loss: 0.423481] [G loss: 0.195312] [ema: 0.997601] 
[Epoch 2/39] [Batch 400/1293] [D loss: 0.431001] [G loss: 0.178704] [ema: 0.997681] 
[Epoch 2/39] [Batch 500/1293] [D loss: 0.416047] [G loss: 0.167613] [ema: 0.997756] 
[Epoch 2/39] [Batch 600/1293] [D loss: 0.400325] [G loss: 0.162502] [ema: 0.997827] 
[Epoch 2/39] [Batch 700/1293] [D loss: 0.381314] [G loss: 0.189847] [ema: 0.997893] 
[Epoch 2/39] [Batch 800/1293] [D loss: 0.421942] [G loss: 0.179098] [ema: 0.997955] 
[Epoch 2/39] [Batch 900/1293] [D loss: 0.422089] [G loss: 0.150581] [ema: 0.998014] 
[Epoch 2/39] [Batch 1000/1293] [D loss: 0.425403] [G loss: 0.177953] [ema: 0.998069] 
[Epoch 2/39] [Batch 1100/1293] [D loss: 0.465598] [G loss: 0.184375] [ema: 0.998121] 
[Epoch 2/39] [Batch 1200/1293] [D loss: 0.369606] [G loss: 0.162629] [ema: 0.998171] 
[Epoch 3/39] [Batch 0/1293] [D loss: 0.416269] [G loss: 0.195496] [ema: 0.998215] 
[Epoch 3/39] [Batch 100/1293] [D loss: 0.390163] [G loss: 0.193377] [ema: 0.998260] 
[Epoch 3/39] [Batch 200/1293] [D loss: 0.455293] [G loss: 0.162170] [ema: 0.998302] 
[Epoch 3/39] [Batch 300/1293] [D loss: 0.396675] [G loss: 0.174281] [ema: 0.998343] 
[Epoch 3/39] [Batch 400/1293] [D loss: 0.442544] [G loss: 0.183432] [ema: 0.998381] 
[Epoch 3/39] [Batch 500/1293] [D loss: 0.414936] [G loss: 0.175724] [ema: 0.998418] 
[Epoch 3/39] [Batch 600/1293] [D loss: 0.445436] [G loss: 0.177885] [ema: 0.998454] 
[Epoch 3/39] [Batch 700/1293] [D loss: 0.406274] [G loss: 0.167037] [ema: 0.998487] 
[Epoch 3/39] [Batch 800/1293] [D loss: 0.415093] [G loss: 0.182472] [ema: 0.998520] 
[Epoch 3/39] [Batch 900/1293] [D loss: 0.424650] [G loss: 0.179343] [ema: 0.998551] 
[Epoch 3/39] [Batch 1000/1293] [D loss: 0.406027] [G loss: 0.172577] [ema: 0.998580] 
[Epoch 3/39] [Batch 1100/1293] [D loss: 0.391216] [G loss: 0.189719] [ema: 0.998609] 
[Epoch 3/39] [Batch 1200/1293] [D loss: 0.409124] [G loss: 0.190052] [ema: 0.998636] 
[Epoch 4/39] [Batch 0/1293] [D loss: 0.394360] [G loss: 0.187219] [ema: 0.998661] 
[Epoch 4/39] [Batch 100/1293] [D loss: 0.419004] [G loss: 0.164947] [ema: 0.998686] 
[Epoch 4/39] [Batch 200/1293] [D loss: 0.408283] [G loss: 0.176534] [ema: 0.998711] 
[Epoch 4/39] [Batch 300/1293] [D loss: 0.350123] [G loss: 0.185829] [ema: 0.998734] 
[Epoch 4/39] [Batch 400/1293] [D loss: 0.403693] [G loss: 0.167081] [ema: 0.998757] 
[Epoch 4/39] [Batch 500/1293] [D loss: 0.428442] [G loss: 0.185557] [ema: 0.998779] 
[Epoch 4/39] [Batch 600/1293] [D loss: 0.393518] [G loss: 0.178742] [ema: 0.998800] 
[Epoch 4/39] [Batch 700/1293] [D loss: 0.353085] [G loss: 0.178068] [ema: 0.998820] 
[Epoch 4/39] [Batch 800/1293] [D loss: 0.396967] [G loss: 0.184333] [ema: 0.998840] 
[Epoch 4/39] [Batch 900/1293] [D loss: 0.359421] [G loss: 0.181561] [ema: 0.998859] 
[Epoch 4/39] [Batch 1000/1293] [D loss: 0.396665] [G loss: 0.162381] [ema: 0.998878] 
[Epoch 4/39] [Batch 1100/1293] [D loss: 0.375815] [G loss: 0.177070] [ema: 0.998895] 
[Epoch 4/39] [Batch 1200/1293] [D loss: 0.405729] [G loss: 0.170983] [ema: 0.998913] 
[Epoch 5/39] [Batch 0/1293] [D loss: 0.388293] [G loss: 0.169339] [ema: 0.998928] 
[Epoch 5/39] [Batch 100/1293] [D loss: 0.416866] [G loss: 0.177555] [ema: 0.998945] 
[Epoch 5/39] [Batch 200/1293] [D loss: 0.427597] [G loss: 0.186627] [ema: 0.998961] 
[Epoch 5/39] [Batch 300/1293] [D loss: 0.382396] [G loss: 0.182345] [ema: 0.998976] 
[Epoch 5/39] [Batch 400/1293] [D loss: 0.390208] [G loss: 0.176174] [ema: 0.998991] 
[Epoch 5/39] [Batch 500/1293] [D loss: 0.396785] [G loss: 0.172604] [ema: 0.999005] 
[Epoch 5/39] [Batch 600/1293] [D loss: 0.433955] [G loss: 0.166979] [ema: 0.999019] 
[Epoch 5/39] [Batch 700/1293] [D loss: 0.399602] [G loss: 0.190295] [ema: 0.999033] 
[Epoch 5/39] [Batch 800/1293] [D loss: 0.404768] [G loss: 0.152433] [ema: 0.999046] 
[Epoch 5/39] [Batch 900/1293] [D loss: 0.374845] [G loss: 0.177124] [ema: 0.999059] 
[Epoch 5/39] [Batch 1000/1293] [D loss: 0.425876] [G loss: 0.178861] [ema: 0.999072] 
[Epoch 5/39] [Batch 1100/1293] [D loss: 0.396916] [G loss: 0.171299] [ema: 0.999084] 
[Epoch 5/39] [Batch 1200/1293] [D loss: 0.406144] [G loss: 0.172305] [ema: 0.999096] 
[Epoch 6/39] [Batch 0/1293] [D loss: 0.366414] [G loss: 0.207003] [ema: 0.999107] 
[Epoch 6/39] [Batch 100/1293] [D loss: 0.388924] [G loss: 0.181755] [ema: 0.999118] 
[Epoch 6/39] [Batch 200/1293] [D loss: 0.413429] [G loss: 0.193685] [ema: 0.999129] 
[Epoch 6/39] [Batch 300/1293] [D loss: 0.438711] [G loss: 0.155320] [ema: 0.999140] 
[Epoch 6/39] [Batch 400/1293] [D loss: 0.399898] [G loss: 0.189256] [ema: 0.999151] 
[Epoch 6/39] [Batch 500/1293] [D loss: 0.383746] [G loss: 0.185373] [ema: 0.999161] 
[Epoch 6/39] [Batch 600/1293] [D loss: 0.421275] [G loss: 0.168838] [ema: 0.999171] 
[Epoch 6/39] [Batch 700/1293] [D loss: 0.409322] [G loss: 0.185978] [ema: 0.999181] 
[Epoch 6/39] [Batch 800/1293] [D loss: 0.357590] [G loss: 0.177601] [ema: 0.999190] 
[Epoch 6/39] [Batch 900/1293] [D loss: 0.350973] [G loss: 0.203413] [ema: 0.999200] 
[Epoch 6/39] [Batch 1000/1293] [D loss: 0.456724] [G loss: 0.195130] [ema: 0.999209] 
[Epoch 6/39] [Batch 1100/1293] [D loss: 0.339700] [G loss: 0.191831] [ema: 0.999218] 
[Epoch 6/39] [Batch 1200/1293] [D loss: 0.351770] [G loss: 0.191766] [ema: 0.999227] 
[Epoch 7/39] [Batch 0/1293] [D loss: 0.405637] [G loss: 0.188949] [ema: 0.999234] 
[Epoch 7/39] [Batch 100/1293] [D loss: 0.366091] [G loss: 0.175953] [ema: 0.999243] 
[Epoch 7/39] [Batch 200/1293] [D loss: 0.395594] [G loss: 0.181342] [ema: 0.999251] 
[Epoch 7/39] [Batch 300/1293] [D loss: 0.436017] [G loss: 0.182739] [ema: 0.999259] 
[Epoch 7/39] [Batch 400/1293] [D loss: 0.401300] [G loss: 0.170198] [ema: 0.999267] 
[Epoch 7/39] [Batch 500/1293] [D loss: 0.378692] [G loss: 0.178106] [ema: 0.999275] 
[Epoch 7/39] [Batch 600/1293] [D loss: 0.413818] [G loss: 0.192189] [ema: 0.999282] 
[Epoch 7/39] [Batch 700/1293] [D loss: 0.370105] [G loss: 0.178535] [ema: 0.999289] 
[Epoch 7/39] [Batch 800/1293] [D loss: 0.415612] [G loss: 0.172796] [ema: 0.999297] 
[Epoch 7/39] [Batch 900/1293] [D loss: 0.398578] [G loss: 0.176451] [ema: 0.999304] 
[Epoch 7/39] [Batch 1000/1293] [D loss: 0.396078] [G loss: 0.169656] [ema: 0.999311] 
[Epoch 7/39] [Batch 1100/1293] [D loss: 0.371763] [G loss: 0.187584] [ema: 0.999317] 
[Epoch 7/39] [Batch 1200/1293] [D loss: 0.409216] [G loss: 0.195002] [ema: 0.999324] 
[Epoch 8/39] [Batch 0/1293] [D loss: 0.425776] [G loss: 0.176271] [ema: 0.999330] 
[Epoch 8/39] [Batch 100/1293] [D loss: 0.426347] [G loss: 0.188399] [ema: 0.999337] 
[Epoch 8/39] [Batch 200/1293] [D loss: 0.386400] [G loss: 0.187475] [ema: 0.999343] 
[Epoch 8/39] [Batch 300/1293] [D loss: 0.354398] [G loss: 0.194268] [ema: 0.999349] 
[Epoch 8/39] [Batch 400/1293] [D loss: 0.423175] [G loss: 0.178976] [ema: 0.999355] 
[Epoch 8/39] [Batch 500/1293] [D loss: 0.396293] [G loss: 0.183384] [ema: 0.999361] 
[Epoch 8/39] [Batch 600/1293] [D loss: 0.390635] [G loss: 0.172198] [ema: 0.999367] 
[Epoch 8/39] [Batch 700/1293] [D loss: 0.375712] [G loss: 0.191000] [ema: 0.999373] 
[Epoch 8/39] [Batch 800/1293] [D loss: 0.374921] [G loss: 0.188778] [ema: 0.999378] 
[Epoch 8/39] [Batch 900/1293] [D loss: 0.386291] [G loss: 0.174390] [ema: 0.999384] 
[Epoch 8/39] [Batch 1000/1293] [D loss: 0.415842] [G loss: 0.178578] [ema: 0.999389] 
[Epoch 8/39] [Batch 1100/1293] [D loss: 0.384758] [G loss: 0.175911] [ema: 0.999394] 
[Epoch 8/39] [Batch 1200/1293] [D loss: 0.365068] [G loss: 0.181695] [ema: 0.999400] 
[Epoch 9/39] [Batch 0/1293] [D loss: 0.394097] [G loss: 0.189231] [ema: 0.999405] 
[Epoch 9/39] [Batch 100/1293] [D loss: 0.396288] [G loss: 0.199079] [ema: 0.999410] 
[Epoch 9/39] [Batch 200/1293] [D loss: 0.377931] [G loss: 0.187450] [ema: 0.999415] 
[Epoch 9/39] [Batch 300/1293] [D loss: 0.367218] [G loss: 0.187817] [ema: 0.999419] 
[Epoch 9/39] [Batch 400/1293] [D loss: 0.348807] [G loss: 0.192526] [ema: 0.999424] 
[Epoch 9/39] [Batch 500/1293] [D loss: 0.351983] [G loss: 0.195536] [ema: 0.999429] 
[Epoch 9/39] [Batch 600/1293] [D loss: 0.407338] [G loss: 0.184849] [ema: 0.999434] 
[Epoch 9/39] [Batch 700/1293] [D loss: 0.380475] [G loss: 0.168425] [ema: 0.999438] 
[Epoch 9/39] [Batch 800/1293] [D loss: 0.410696] [G loss: 0.172050] [ema: 0.999443] 
[Epoch 9/39] [Batch 900/1293] [D loss: 0.400726] [G loss: 0.162078] [ema: 0.999447] 
[Epoch 9/39] [Batch 1000/1293] [D loss: 0.395827] [G loss: 0.184143] [ema: 0.999452] 
[Epoch 9/39] [Batch 1100/1293] [D loss: 0.349826] [G loss: 0.183817] [ema: 0.999456] 
[Epoch 9/39] [Batch 1200/1293] [D loss: 0.419379] [G loss: 0.187167] [ema: 0.999460] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_30_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_30_2024_10_25_00_33_27/Model



[Epoch 10/39] [Batch 0/1293] [D loss: 0.448585] [G loss: 0.179502] [ema: 0.999464] 
[Epoch 10/39] [Batch 100/1293] [D loss: 0.413365] [G loss: 0.179504] [ema: 0.999468] 
[Epoch 10/39] [Batch 200/1293] [D loss: 0.362254] [G loss: 0.192547] [ema: 0.999472] 
[Epoch 10/39] [Batch 300/1293] [D loss: 0.373011] [G loss: 0.187863] [ema: 0.999476] 
[Epoch 10/39] [Batch 400/1293] [D loss: 0.391002] [G loss: 0.185122] [ema: 0.999480] 
[Epoch 10/39] [Batch 500/1293] [D loss: 0.374968] [G loss: 0.183349] [ema: 0.999484] 
[Epoch 10/39] [Batch 600/1293] [D loss: 0.431260] [G loss: 0.193557] [ema: 0.999488] 
[Epoch 10/39] [Batch 700/1293] [D loss: 0.380891] [G loss: 0.188711] [ema: 0.999492] 
[Epoch 10/39] [Batch 800/1293] [D loss: 0.394832] [G loss: 0.185364] [ema: 0.999495] 
[Epoch 10/39] [Batch 900/1293] [D loss: 0.398092] [G loss: 0.182865] [ema: 0.999499] 
[Epoch 10/39] [Batch 1000/1293] [D loss: 0.377910] [G loss: 0.188347] [ema: 0.999503] 
[Epoch 10/39] [Batch 1100/1293] [D loss: 0.379345] [G loss: 0.188482] [ema: 0.999506] 
[Epoch 10/39] [Batch 1200/1293] [D loss: 0.367619] [G loss: 0.190993] [ema: 0.999510] 
[Epoch 11/39] [Batch 0/1293] [D loss: 0.373218] [G loss: 0.192172] [ema: 0.999513] 
[Epoch 11/39] [Batch 100/1293] [D loss: 0.393080] [G loss: 0.192896] [ema: 0.999516] 
[Epoch 11/39] [Batch 200/1293] [D loss: 0.401795] [G loss: 0.167615] [ema: 0.999520] 
[Epoch 11/39] [Batch 300/1293] [D loss: 0.391709] [G loss: 0.179266] [ema: 0.999523] 
[Epoch 11/39] [Batch 400/1293] [D loss: 0.384076] [G loss: 0.187099] [ema: 0.999526] 
[Epoch 11/39] [Batch 500/1293] [D loss: 0.417374] [G loss: 0.188095] [ema: 0.999529] 
[Epoch 11/39] [Batch 600/1293] [D loss: 0.383945] [G loss: 0.172717] [ema: 0.999532] 
[Epoch 11/39] [Batch 700/1293] [D loss: 0.397920] [G loss: 0.176714] [ema: 0.999536] 
[Epoch 11/39] [Batch 800/1293] [D loss: 0.421765] [G loss: 0.177993] [ema: 0.999539] 
[Epoch 11/39] [Batch 900/1293] [D loss: 0.386449] [G loss: 0.186447] [ema: 0.999542] 
[Epoch 11/39] [Batch 1000/1293] [D loss: 0.419905] [G loss: 0.182812] [ema: 0.999545] 
[Epoch 11/39] [Batch 1100/1293] [D loss: 0.401101] [G loss: 0.178497] [ema: 0.999548] 
[Epoch 11/39] [Batch 1200/1293] [D loss: 0.429381] [G loss: 0.175013] [ema: 0.999551] 
[Epoch 12/39] [Batch 0/1293] [D loss: 0.361141] [G loss: 0.190836] [ema: 0.999553] 
[Epoch 12/39] [Batch 100/1293] [D loss: 0.335205] [G loss: 0.189634] [ema: 0.999556] 
[Epoch 12/39] [Batch 200/1293] [D loss: 0.367855] [G loss: 0.182320] [ema: 0.999559] 
[Epoch 12/39] [Batch 300/1293] [D loss: 0.354423] [G loss: 0.188015] [ema: 0.999562] 
[Epoch 12/39] [Batch 400/1293] [D loss: 0.389744] [G loss: 0.185213] [ema: 0.999565] 
[Epoch 12/39] [Batch 500/1293] [D loss: 0.349654] [G loss: 0.169311] [ema: 0.999567] 
[Epoch 12/39] [Batch 600/1293] [D loss: 0.386741] [G loss: 0.174640] [ema: 0.999570] 
[Epoch 12/39] [Batch 700/1293] [D loss: 0.394344] [G loss: 0.186404] [ema: 0.999573] 
[Epoch 12/39] [Batch 800/1293] [D loss: 0.372624] [G loss: 0.193077] [ema: 0.999575] 
[Epoch 12/39] [Batch 900/1293] [D loss: 0.399139] [G loss: 0.186083] [ema: 0.999578] 
[Epoch 12/39] [Batch 1000/1293] [D loss: 0.414747] [G loss: 0.189177] [ema: 0.999580] 
[Epoch 12/39] [Batch 1100/1293] [D loss: 0.392718] [G loss: 0.184774] [ema: 0.999583] 
[Epoch 12/39] [Batch 1200/1293] [D loss: 0.379028] [G loss: 0.174680] [ema: 0.999585] 
[Epoch 13/39] [Batch 0/1293] [D loss: 0.363558] [G loss: 0.176020] [ema: 0.999588] 
[Epoch 13/39] [Batch 100/1293] [D loss: 0.373756] [G loss: 0.185172] [ema: 0.999590] 
[Epoch 13/39] [Batch 200/1293] [D loss: 0.365085] [G loss: 0.180417] [ema: 0.999593] 
[Epoch 13/39] [Batch 300/1293] [D loss: 0.397140] [G loss: 0.165576] [ema: 0.999595] 
[Epoch 13/39] [Batch 400/1293] [D loss: 0.369600] [G loss: 0.185940] [ema: 0.999597] 
[Epoch 13/39] [Batch 500/1293] [D loss: 0.410072] [G loss: 0.181867] [ema: 0.999600] 
[Epoch 13/39] [Batch 600/1293] [D loss: 0.425352] [G loss: 0.178480] [ema: 0.999602] 
[Epoch 13/39] [Batch 700/1293] [D loss: 0.393631] [G loss: 0.180739] [ema: 0.999604] 
[Epoch 13/39] [Batch 800/1293] [D loss: 0.398266] [G loss: 0.177271] [ema: 0.999606] 
[Epoch 13/39] [Batch 900/1293] [D loss: 0.385998] [G loss: 0.174564] [ema: 0.999609] 
[Epoch 13/39] [Batch 1000/1293] [D loss: 0.397405] [G loss: 0.186731] [ema: 0.999611] 
[Epoch 13/39] [Batch 1100/1293] [D loss: 0.390419] [G loss: 0.193422] [ema: 0.999613] 
[Epoch 13/39] [Batch 1200/1293] [D loss: 0.392221] [G loss: 0.188873] [ema: 0.999615] 
[Epoch 14/39] [Batch 0/1293] [D loss: 0.397980] [G loss: 0.193040] [ema: 0.999617] 
[Epoch 14/39] [Batch 100/1293] [D loss: 0.383030] [G loss: 0.177876] [ema: 0.999619] 
[Epoch 14/39] [Batch 200/1293] [D loss: 0.378125] [G loss: 0.198249] [ema: 0.999621] 
[Epoch 14/39] [Batch 300/1293] [D loss: 0.365257] [G loss: 0.175451] [ema: 0.999623] 
[Epoch 14/39] [Batch 400/1293] [D loss: 0.388368] [G loss: 0.193620] [ema: 0.999625] 
[Epoch 14/39] [Batch 500/1293] [D loss: 0.401481] [G loss: 0.175613] [ema: 0.999627] 
[Epoch 14/39] [Batch 600/1293] [D loss: 0.381311] [G loss: 0.187409] [ema: 0.999629] 
[Epoch 14/39] [Batch 700/1293] [D loss: 0.384724] [G loss: 0.181763] [ema: 0.999631] 
[Epoch 14/39] [Batch 800/1293] [D loss: 0.397515] [G loss: 0.177879] [ema: 0.999633] 
[Epoch 14/39] [Batch 900/1293] [D loss: 0.416957] [G loss: 0.158331] [ema: 0.999635] 
[Epoch 14/39] [Batch 1000/1293] [D loss: 0.429434] [G loss: 0.181324] [ema: 0.999637] 
[Epoch 14/39] [Batch 1100/1293] [D loss: 0.396202] [G loss: 0.183537] [ema: 0.999639] 
[Epoch 14/39] [Batch 1200/1293] [D loss: 0.373456] [G loss: 0.188710] [ema: 0.999641] 
[Epoch 15/39] [Batch 0/1293] [D loss: 0.430714] [G loss: 0.192940] [ema: 0.999643] 
[Epoch 15/39] [Batch 100/1293] [D loss: 0.362155] [G loss: 0.179724] [ema: 0.999645] 
[Epoch 15/39] [Batch 200/1293] [D loss: 0.402611] [G loss: 0.190239] [ema: 0.999646] 
[Epoch 15/39] [Batch 300/1293] [D loss: 0.392499] [G loss: 0.186948] [ema: 0.999648] 
[Epoch 15/39] [Batch 400/1293] [D loss: 0.329447] [G loss: 0.205158] [ema: 0.999650] 
[Epoch 15/39] [Batch 500/1293] [D loss: 0.370819] [G loss: 0.189690] [ema: 0.999652] 
[Epoch 15/39] [Batch 600/1293] [D loss: 0.416462] [G loss: 0.179286] [ema: 0.999653] 
[Epoch 15/39] [Batch 700/1293] [D loss: 0.383115] [G loss: 0.189540] [ema: 0.999655] 
[Epoch 15/39] [Batch 800/1293] [D loss: 0.434661] [G loss: 0.184241] [ema: 0.999657] 
[Epoch 15/39] [Batch 900/1293] [D loss: 0.379194] [G loss: 0.176272] [ema: 0.999659] 
[Epoch 15/39] [Batch 1000/1293] [D loss: 0.401710] [G loss: 0.190858] [ema: 0.999660] 
[Epoch 15/39] [Batch 1100/1293] [D loss: 0.369580] [G loss: 0.183674] [ema: 0.999662] 
[Epoch 15/39] [Batch 1200/1293] [D loss: 0.394687] [G loss: 0.197762] [ema: 0.999663] 
[Epoch 16/39] [Batch 0/1293] [D loss: 0.394474] [G loss: 0.197587] [ema: 0.999665] 
[Epoch 16/39] [Batch 100/1293] [D loss: 0.385944] [G loss: 0.197309] [ema: 0.999667] 
[Epoch 16/39] [Batch 200/1293] [D loss: 0.374364] [G loss: 0.191680] [ema: 0.999668] 
[Epoch 16/39] [Batch 300/1293] [D loss: 0.409061] [G loss: 0.190425] [ema: 0.999670] 
[Epoch 16/39] [Batch 400/1293] [D loss: 0.397115] [G loss: 0.185332] [ema: 0.999671] 
[Epoch 16/39] [Batch 500/1293] [D loss: 0.390234] [G loss: 0.192602] [ema: 0.999673] 
[Epoch 16/39] [Batch 600/1293] [D loss: 0.345859] [G loss: 0.188087] [ema: 0.999674] 
[Epoch 16/39] [Batch 700/1293] [D loss: 0.379398] [G loss: 0.197248] [ema: 0.999676] 
[Epoch 16/39] [Batch 800/1293] [D loss: 0.365699] [G loss: 0.186188] [ema: 0.999677] 
[Epoch 16/39] [Batch 900/1293] [D loss: 0.373277] [G loss: 0.211308] [ema: 0.999679] 
[Epoch 16/39] [Batch 1000/1293] [D loss: 0.407794] [G loss: 0.180193] [ema: 0.999680] 
[Epoch 16/39] [Batch 1100/1293] [D loss: 0.397823] [G loss: 0.165516] [ema: 0.999682] 
[Epoch 16/39] [Batch 1200/1293] [D loss: 0.361852] [G loss: 0.194190] [ema: 0.999683] 
[Epoch 17/39] [Batch 0/1293] [D loss: 0.395684] [G loss: 0.179732] [ema: 0.999685] 
[Epoch 17/39] [Batch 100/1293] [D loss: 0.419176] [G loss: 0.164056] [ema: 0.999686] 
[Epoch 17/39] [Batch 200/1293] [D loss: 0.391223] [G loss: 0.186296] [ema: 0.999688] 
[Epoch 17/39] [Batch 300/1293] [D loss: 0.382272] [G loss: 0.175956] [ema: 0.999689] 
[Epoch 17/39] [Batch 400/1293] [D loss: 0.439620] [G loss: 0.180770] [ema: 0.999690] 
[Epoch 17/39] [Batch 500/1293] [D loss: 0.429192] [G loss: 0.152095] [ema: 0.999692] 
[Epoch 17/39] [Batch 600/1293] [D loss: 0.393566] [G loss: 0.191388] [ema: 0.999693] 
[Epoch 17/39] [Batch 700/1293] [D loss: 0.413445] [G loss: 0.191823] [ema: 0.999694] 
[Epoch 17/39] [Batch 800/1293] [D loss: 0.362550] [G loss: 0.183033] [ema: 0.999696] 
[Epoch 17/39] [Batch 900/1293] [D loss: 0.338560] [G loss: 0.193008] [ema: 0.999697] 
[Epoch 17/39] [Batch 1000/1293] [D loss: 0.363552] [G loss: 0.187121] [ema: 0.999698] 
[Epoch 17/39] [Batch 1100/1293] [D loss: 0.390697] [G loss: 0.186264] [ema: 0.999700] 
[Epoch 17/39] [Batch 1200/1293] [D loss: 0.333206] [G loss: 0.196312] [ema: 0.999701] 
[Epoch 18/39] [Batch 0/1293] [D loss: 0.360404] [G loss: 0.199646] [ema: 0.999702] 
[Epoch 18/39] [Batch 100/1293] [D loss: 0.359763] [G loss: 0.193543] [ema: 0.999703] 
[Epoch 18/39] [Batch 200/1293] [D loss: 0.364948] [G loss: 0.197794] [ema: 0.999705] 
[Epoch 18/39] [Batch 300/1293] [D loss: 0.384011] [G loss: 0.200011] [ema: 0.999706] 
[Epoch 18/39] [Batch 400/1293] [D loss: 0.382584] [G loss: 0.177436] [ema: 0.999707] 
[Epoch 18/39] [Batch 500/1293] [D loss: 0.387864] [G loss: 0.187943] [ema: 0.999708] 
[Epoch 18/39] [Batch 600/1293] [D loss: 0.386308] [G loss: 0.176499] [ema: 0.999710] 
[Epoch 18/39] [Batch 700/1293] [D loss: 0.445888] [G loss: 0.200255] [ema: 0.999711] 
[Epoch 18/39] [Batch 800/1293] [D loss: 0.399962] [G loss: 0.182016] [ema: 0.999712] 
[Epoch 18/39] [Batch 900/1293] [D loss: 0.361563] [G loss: 0.179684] [ema: 0.999713] 
[Epoch 18/39] [Batch 1000/1293] [D loss: 0.372830] [G loss: 0.191045] [ema: 0.999714] 
[Epoch 18/39] [Batch 1100/1293] [D loss: 0.366863] [G loss: 0.182372] [ema: 0.999716] 
[Epoch 18/39] [Batch 1200/1293] [D loss: 0.388204] [G loss: 0.169466] [ema: 0.999717] 
[Epoch 19/39] [Batch 0/1293] [D loss: 0.405605] [G loss: 0.193818] [ema: 0.999718] 
[Epoch 19/39] [Batch 100/1293] [D loss: 0.389686] [G loss: 0.198753] [ema: 0.999719] 
[Epoch 19/39] [Batch 200/1293] [D loss: 0.400359] [G loss: 0.194027] [ema: 0.999720] 
[Epoch 19/39] [Batch 300/1293] [D loss: 0.376509] [G loss: 0.185623] [ema: 0.999721] 
[Epoch 19/39] [Batch 400/1293] [D loss: 0.328644] [G loss: 0.188088] [ema: 0.999722] 
[Epoch 19/39] [Batch 500/1293] [D loss: 0.409696] [G loss: 0.186356] [ema: 0.999724] 
[Epoch 19/39] [Batch 600/1293] [D loss: 0.403706] [G loss: 0.196111] [ema: 0.999725] 
[Epoch 19/39] [Batch 700/1293] [D loss: 0.343268] [G loss: 0.200262] [ema: 0.999726] 
[Epoch 19/39] [Batch 800/1293] [D loss: 0.341655] [G loss: 0.181460] [ema: 0.999727] 
[Epoch 19/39] [Batch 900/1293] [D loss: 0.369304] [G loss: 0.178658] [ema: 0.999728] 
[Epoch 19/39] [Batch 1000/1293] [D loss: 0.425755] [G loss: 0.189149] [ema: 0.999729] 
[Epoch 19/39] [Batch 1100/1293] [D loss: 0.338336] [G loss: 0.201136] [ema: 0.999730] 
[Epoch 19/39] [Batch 1200/1293] [D loss: 0.383140] [G loss: 0.201851] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_30_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_30_2024_10_25_00_33_27/Model



[Epoch 20/39] [Batch 0/1293] [D loss: 0.362023] [G loss: 0.196386] [ema: 0.999732] 
[Epoch 20/39] [Batch 100/1293] [D loss: 0.331995] [G loss: 0.199301] [ema: 0.999733] 
[Epoch 20/39] [Batch 200/1293] [D loss: 0.382675] [G loss: 0.187502] [ema: 0.999734] 
[Epoch 20/39] [Batch 300/1293] [D loss: 0.380541] [G loss: 0.183777] [ema: 0.999735] 
[Epoch 20/39] [Batch 400/1293] [D loss: 0.333622] [G loss: 0.188719] [ema: 0.999736] 
[Epoch 20/39] [Batch 500/1293] [D loss: 0.378699] [G loss: 0.196608] [ema: 0.999737] 
[Epoch 20/39] [Batch 600/1293] [D loss: 0.389474] [G loss: 0.187899] [ema: 0.999738] 
[Epoch 20/39] [Batch 700/1293] [D loss: 0.383459] [G loss: 0.215596] [ema: 0.999739] 
[Epoch 20/39] [Batch 800/1293] [D loss: 0.360229] [G loss: 0.216569] [ema: 0.999740] 
[Epoch 20/39] [Batch 900/1293] [D loss: 0.390537] [G loss: 0.197474] [ema: 0.999741] 
[Epoch 20/39] [Batch 1000/1293] [D loss: 0.453798] [G loss: 0.180872] [ema: 0.999742] 
[Epoch 20/39] [Batch 1100/1293] [D loss: 0.412368] [G loss: 0.187474] [ema: 0.999743] 
[Epoch 20/39] [Batch 1200/1293] [D loss: 0.386028] [G loss: 0.181052] [ema: 0.999744] 
[Epoch 21/39] [Batch 0/1293] [D loss: 0.358141] [G loss: 0.178452] [ema: 0.999745] 
[Epoch 21/39] [Batch 100/1293] [D loss: 0.421474] [G loss: 0.172038] [ema: 0.999746] 
[Epoch 21/39] [Batch 200/1293] [D loss: 0.408487] [G loss: 0.177225] [ema: 0.999747] 
[Epoch 21/39] [Batch 300/1293] [D loss: 0.337439] [G loss: 0.196621] [ema: 0.999748] 
[Epoch 21/39] [Batch 400/1293] [D loss: 0.367063] [G loss: 0.189924] [ema: 0.999748] 
[Epoch 21/39] [Batch 500/1293] [D loss: 0.323126] [G loss: 0.190569] [ema: 0.999749] 
[Epoch 21/39] [Batch 600/1293] [D loss: 0.385697] [G loss: 0.190387] [ema: 0.999750] 
[Epoch 21/39] [Batch 700/1293] [D loss: 0.467741] [G loss: 0.200102] [ema: 0.999751] 
[Epoch 21/39] [Batch 800/1293] [D loss: 0.336947] [G loss: 0.199245] [ema: 0.999752] 
[Epoch 21/39] [Batch 900/1293] [D loss: 0.368974] [G loss: 0.192010] [ema: 0.999753] 
[Epoch 21/39] [Batch 1000/1293] [D loss: 0.372426] [G loss: 0.190019] [ema: 0.999754] 
[Epoch 21/39] [Batch 1100/1293] [D loss: 0.377657] [G loss: 0.187930] [ema: 0.999755] 
[Epoch 21/39] [Batch 1200/1293] [D loss: 0.372701] [G loss: 0.182372] [ema: 0.999756] 
[Epoch 22/39] [Batch 0/1293] [D loss: 0.368565] [G loss: 0.178735] [ema: 0.999756] 
[Epoch 22/39] [Batch 100/1293] [D loss: 0.366021] [G loss: 0.205934] [ema: 0.999757] 
[Epoch 22/39] [Batch 200/1293] [D loss: 0.406105] [G loss: 0.201224] [ema: 0.999758] 
[Epoch 22/39] [Batch 300/1293] [D loss: 0.379288] [G loss: 0.188777] [ema: 0.999759] 
[Epoch 22/39] [Batch 400/1293] [D loss: 0.380825] [G loss: 0.192439] [ema: 0.999760] 
[Epoch 22/39] [Batch 500/1293] [D loss: 0.405185] [G loss: 0.178351] [ema: 0.999761] 
[Epoch 22/39] [Batch 600/1293] [D loss: 0.385049] [G loss: 0.176008] [ema: 0.999761] 
[Epoch 22/39] [Batch 700/1293] [D loss: 0.362252] [G loss: 0.196638] [ema: 0.999762] 
[Epoch 22/39] [Batch 800/1293] [D loss: 0.341312] [G loss: 0.190568] [ema: 0.999763] 
[Epoch 22/39] [Batch 900/1293] [D loss: 0.352365] [G loss: 0.204663] [ema: 0.999764] 
[Epoch 22/39] [Batch 1000/1293] [D loss: 0.327234] [G loss: 0.197890] [ema: 0.999765] 
[Epoch 22/39] [Batch 1100/1293] [D loss: 0.331894] [G loss: 0.189991] [ema: 0.999765] 
[Epoch 22/39] [Batch 1200/1293] [D loss: 0.362112] [G loss: 0.191075] [ema: 0.999766] 
[Epoch 23/39] [Batch 0/1293] [D loss: 0.408211] [G loss: 0.189184] [ema: 0.999767] 
[Epoch 23/39] [Batch 100/1293] [D loss: 0.340720] [G loss: 0.196715] [ema: 0.999768] 
[Epoch 23/39] [Batch 200/1293] [D loss: 0.337353] [G loss: 0.181766] [ema: 0.999769] 
[Epoch 23/39] [Batch 300/1293] [D loss: 0.390150] [G loss: 0.186504] [ema: 0.999769] 
[Epoch 23/39] [Batch 400/1293] [D loss: 0.390732] [G loss: 0.172253] [ema: 0.999770] 
[Epoch 23/39] [Batch 500/1293] [D loss: 0.424357] [G loss: 0.194472] [ema: 0.999771] 
[Epoch 23/39] [Batch 600/1293] [D loss: 0.427693] [G loss: 0.188593] [ema: 0.999772] 
[Epoch 23/39] [Batch 700/1293] [D loss: 0.374065] [G loss: 0.188750] [ema: 0.999772] 
[Epoch 23/39] [Batch 800/1293] [D loss: 0.384280] [G loss: 0.176763] [ema: 0.999773] 
[Epoch 23/39] [Batch 900/1293] [D loss: 0.403564] [G loss: 0.194868] [ema: 0.999774] 
[Epoch 23/39] [Batch 1000/1293] [D loss: 0.354483] [G loss: 0.196561] [ema: 0.999775] 
[Epoch 23/39] [Batch 1100/1293] [D loss: 0.385803] [G loss: 0.185585] [ema: 0.999775] 
[Epoch 23/39] [Batch 1200/1293] [D loss: 0.354701] [G loss: 0.167558] [ema: 0.999776] 
[Epoch 24/39] [Batch 0/1293] [D loss: 0.411727] [G loss: 0.155677] [ema: 0.999777] 
[Epoch 24/39] [Batch 100/1293] [D loss: 0.418465] [G loss: 0.177161] [ema: 0.999777] 
[Epoch 24/39] [Batch 200/1293] [D loss: 0.412902] [G loss: 0.188380] [ema: 0.999778] 
[Epoch 24/39] [Batch 300/1293] [D loss: 0.368394] [G loss: 0.199462] [ema: 0.999779] 
[Epoch 24/39] [Batch 400/1293] [D loss: 0.373141] [G loss: 0.188329] [ema: 0.999780] 
[Epoch 24/39] [Batch 500/1293] [D loss: 0.364991] [G loss: 0.180695] [ema: 0.999780] 
[Epoch 24/39] [Batch 600/1293] [D loss: 0.438105] [G loss: 0.198641] [ema: 0.999781] 
[Epoch 24/39] [Batch 700/1293] [D loss: 0.373272] [G loss: 0.198535] [ema: 0.999782] 
[Epoch 24/39] [Batch 800/1293] [D loss: 0.364475] [G loss: 0.182815] [ema: 0.999782] 
[Epoch 24/39] [Batch 900/1293] [D loss: 0.403994] [G loss: 0.179703] [ema: 0.999783] 
[Epoch 24/39] [Batch 1000/1293] [D loss: 0.364038] [G loss: 0.166841] [ema: 0.999784] 
[Epoch 24/39] [Batch 1100/1293] [D loss: 0.433559] [G loss: 0.158746] [ema: 0.999784] 
[Epoch 24/39] [Batch 1200/1293] [D loss: 0.387239] [G loss: 0.178134] [ema: 0.999785] 
[Epoch 25/39] [Batch 0/1293] [D loss: 0.377409] [G loss: 0.187731] [ema: 0.999786] 
[Epoch 25/39] [Batch 100/1293] [D loss: 0.386964] [G loss: 0.174522] [ema: 0.999786] 
[Epoch 25/39] [Batch 200/1293] [D loss: 0.422703] [G loss: 0.179932] [ema: 0.999787] 
[Epoch 25/39] [Batch 300/1293] [D loss: 0.377904] [G loss: 0.181975] [ema: 0.999788] 
[Epoch 25/39] [Batch 400/1293] [D loss: 0.383926] [G loss: 0.185980] [ema: 0.999788] 
[Epoch 25/39] [Batch 500/1293] [D loss: 0.400938] [G loss: 0.187704] [ema: 0.999789] 
[Epoch 25/39] [Batch 600/1293] [D loss: 0.330255] [G loss: 0.194031] [ema: 0.999789] 
[Epoch 25/39] [Batch 700/1293] [D loss: 0.372162] [G loss: 0.190399] [ema: 0.999790] 
[Epoch 25/39] [Batch 800/1293] [D loss: 0.420813] [G loss: 0.180016] [ema: 0.999791] 
[Epoch 25/39] [Batch 900/1293] [D loss: 0.365160] [G loss: 0.178758] [ema: 0.999791] 
[Epoch 25/39] [Batch 1000/1293] [D loss: 0.372581] [G loss: 0.176053] [ema: 0.999792] 
[Epoch 25/39] [Batch 1100/1293] [D loss: 0.439486] [G loss: 0.175241] [ema: 0.999793] 
[Epoch 25/39] [Batch 1200/1293] [D loss: 0.412788] [G loss: 0.191557] [ema: 0.999793] 
[Epoch 26/39] [Batch 0/1293] [D loss: 0.372713] [G loss: 0.189449] [ema: 0.999794] 
[Epoch 26/39] [Batch 100/1293] [D loss: 0.383850] [G loss: 0.196415] [ema: 0.999794] 
[Epoch 26/39] [Batch 200/1293] [D loss: 0.379265] [G loss: 0.182493] [ema: 0.999795] 
[Epoch 26/39] [Batch 300/1293] [D loss: 0.388784] [G loss: 0.185698] [ema: 0.999796] 
[Epoch 26/39] [Batch 400/1293] [D loss: 0.380345] [G loss: 0.189089] [ema: 0.999796] 
[Epoch 26/39] [Batch 500/1293] [D loss: 0.403610] [G loss: 0.177241] [ema: 0.999797] 
[Epoch 26/39] [Batch 600/1293] [D loss: 0.351802] [G loss: 0.185859] [ema: 0.999797] 
[Epoch 26/39] [Batch 700/1293] [D loss: 0.394169] [G loss: 0.199665] [ema: 0.999798] 
[Epoch 26/39] [Batch 800/1293] [D loss: 0.382093] [G loss: 0.181983] [ema: 0.999799] 
[Epoch 26/39] [Batch 900/1293] [D loss: 0.352348] [G loss: 0.188293] [ema: 0.999799] 
[Epoch 26/39] [Batch 1000/1293] [D loss: 0.397410] [G loss: 0.178693] [ema: 0.999800] 
[Epoch 26/39] [Batch 1100/1293] [D loss: 0.415821] [G loss: 0.181035] [ema: 0.999800] 
[Epoch 26/39] [Batch 1200/1293] [D loss: 0.369467] [G loss: 0.174589] [ema: 0.999801] 
[Epoch 27/39] [Batch 0/1293] [D loss: 0.364719] [G loss: 0.203974] [ema: 0.999801] 
[Epoch 27/39] [Batch 100/1293] [D loss: 0.393213] [G loss: 0.179876] [ema: 0.999802] 
[Epoch 27/39] [Batch 200/1293] [D loss: 0.400084] [G loss: 0.183039] [ema: 0.999803] 
[Epoch 27/39] [Batch 300/1293] [D loss: 0.407181] [G loss: 0.180349] [ema: 0.999803] 
[Epoch 27/39] [Batch 400/1293] [D loss: 0.387654] [G loss: 0.175801] [ema: 0.999804] 
[Epoch 27/39] [Batch 500/1293] [D loss: 0.373173] [G loss: 0.199573] [ema: 0.999804] 
[Epoch 27/39] [Batch 600/1293] [D loss: 0.390496] [G loss: 0.168373] [ema: 0.999805] 
[Epoch 27/39] [Batch 700/1293] [D loss: 0.365974] [G loss: 0.195531] [ema: 0.999805] 
[Epoch 27/39] [Batch 800/1293] [D loss: 0.414431] [G loss: 0.187527] [ema: 0.999806] 
[Epoch 27/39] [Batch 900/1293] [D loss: 0.379062] [G loss: 0.175818] [ema: 0.999806] 
[Epoch 27/39] [Batch 1000/1293] [D loss: 0.389459] [G loss: 0.174592] [ema: 0.999807] 
[Epoch 27/39] [Batch 1100/1293] [D loss: 0.390043] [G loss: 0.192215] [ema: 0.999808] 
[Epoch 27/39] [Batch 1200/1293] [D loss: 0.380310] [G loss: 0.187363] [ema: 0.999808] 
[Epoch 28/39] [Batch 0/1293] [D loss: 0.368789] [G loss: 0.182415] [ema: 0.999809] 
[Epoch 28/39] [Batch 100/1293] [D loss: 0.382089] [G loss: 0.174395] [ema: 0.999809] 
[Epoch 28/39] [Batch 200/1293] [D loss: 0.414657] [G loss: 0.188173] [ema: 0.999810] 
[Epoch 28/39] [Batch 300/1293] [D loss: 0.398071] [G loss: 0.170870] [ema: 0.999810] 
[Epoch 28/39] [Batch 400/1293] [D loss: 0.392406] [G loss: 0.188194] [ema: 0.999811] 
[Epoch 28/39] [Batch 500/1293] [D loss: 0.396203] [G loss: 0.168097] [ema: 0.999811] 
[Epoch 28/39] [Batch 600/1293] [D loss: 0.413768] [G loss: 0.187622] [ema: 0.999812] 
[Epoch 28/39] [Batch 700/1293] [D loss: 0.407382] [G loss: 0.187654] [ema: 0.999812] 
[Epoch 28/39] [Batch 800/1293] [D loss: 0.402450] [G loss: 0.185337] [ema: 0.999813] 
[Epoch 28/39] [Batch 900/1293] [D loss: 0.414168] [G loss: 0.166659] [ema: 0.999813] 
[Epoch 28/39] [Batch 1000/1293] [D loss: 0.394051] [G loss: 0.179875] [ema: 0.999814] 
[Epoch 28/39] [Batch 1100/1293] [D loss: 0.358456] [G loss: 0.166675] [ema: 0.999814] 
[Epoch 28/39] [Batch 1200/1293] [D loss: 0.382173] [G loss: 0.185029] [ema: 0.999815] 
[Epoch 29/39] [Batch 0/1293] [D loss: 0.392523] [G loss: 0.179645] [ema: 0.999815] 
[Epoch 29/39] [Batch 100/1293] [D loss: 0.437508] [G loss: 0.168279] [ema: 0.999816] 
[Epoch 29/39] [Batch 200/1293] [D loss: 0.377352] [G loss: 0.173059] [ema: 0.999816] 
[Epoch 29/39] [Batch 300/1293] [D loss: 0.410365] [G loss: 0.182913] [ema: 0.999817] 
[Epoch 29/39] [Batch 400/1293] [D loss: 0.439110] [G loss: 0.173915] [ema: 0.999817] 
[Epoch 29/39] [Batch 500/1293] [D loss: 0.448741] [G loss: 0.167543] [ema: 0.999818] 
[Epoch 29/39] [Batch 600/1293] [D loss: 0.423394] [G loss: 0.169956] [ema: 0.999818] 
[Epoch 29/39] [Batch 700/1293] [D loss: 0.426481] [G loss: 0.174294] [ema: 0.999819] 
[Epoch 29/39] [Batch 800/1293] [D loss: 0.417771] [G loss: 0.181707] [ema: 0.999819] 
[Epoch 29/39] [Batch 900/1293] [D loss: 0.403512] [G loss: 0.190153] [ema: 0.999819] 
[Epoch 29/39] [Batch 1000/1293] [D loss: 0.397436] [G loss: 0.178658] [ema: 0.999820] 
[Epoch 29/39] [Batch 1100/1293] [D loss: 0.382720] [G loss: 0.178503] [ema: 0.999820] 
[Epoch 29/39] [Batch 1200/1293] [D loss: 0.371428] [G loss: 0.191179] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_30_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_30_2024_10_25_00_33_27/Model



[Epoch 30/39] [Batch 0/1293] [D loss: 0.453393] [G loss: 0.153800] [ema: 0.999821] 
[Epoch 30/39] [Batch 100/1293] [D loss: 0.387482] [G loss: 0.179628] [ema: 0.999822] 
[Epoch 30/39] [Batch 200/1293] [D loss: 0.397654] [G loss: 0.182536] [ema: 0.999822] 
[Epoch 30/39] [Batch 300/1293] [D loss: 0.387696] [G loss: 0.186106] [ema: 0.999823] 
[Epoch 30/39] [Batch 400/1293] [D loss: 0.394947] [G loss: 0.177388] [ema: 0.999823] 
[Epoch 30/39] [Batch 500/1293] [D loss: 0.416560] [G loss: 0.183410] [ema: 0.999824] 
[Epoch 30/39] [Batch 600/1293] [D loss: 0.387520] [G loss: 0.183566] [ema: 0.999824] 
[Epoch 30/39] [Batch 700/1293] [D loss: 0.377451] [G loss: 0.184132] [ema: 0.999824] 
[Epoch 30/39] [Batch 800/1293] [D loss: 0.380618] [G loss: 0.186861] [ema: 0.999825] 
[Epoch 30/39] [Batch 900/1293] [D loss: 0.411450] [G loss: 0.180211] [ema: 0.999825] 
[Epoch 30/39] [Batch 1000/1293] [D loss: 0.370019] [G loss: 0.189126] [ema: 0.999826] 
[Epoch 30/39] [Batch 1100/1293] [D loss: 0.364361] [G loss: 0.179536] [ema: 0.999826] 
[Epoch 30/39] [Batch 1200/1293] [D loss: 0.389167] [G loss: 0.178482] [ema: 0.999827] 
[Epoch 31/39] [Batch 0/1293] [D loss: 0.405621] [G loss: 0.179763] [ema: 0.999827] 
[Epoch 31/39] [Batch 100/1293] [D loss: 0.424492] [G loss: 0.171719] [ema: 0.999828] 
[Epoch 31/39] [Batch 200/1293] [D loss: 0.444959] [G loss: 0.192557] [ema: 0.999828] 
[Epoch 31/39] [Batch 300/1293] [D loss: 0.397502] [G loss: 0.172867] [ema: 0.999828] 
[Epoch 31/39] [Batch 400/1293] [D loss: 0.377376] [G loss: 0.165233] [ema: 0.999829] 
[Epoch 31/39] [Batch 500/1293] [D loss: 0.374566] [G loss: 0.187486] [ema: 0.999829] 
[Epoch 31/39] [Batch 600/1293] [D loss: 0.375246] [G loss: 0.185436] [ema: 0.999830] 
[Epoch 31/39] [Batch 700/1293] [D loss: 0.387819] [G loss: 0.176292] [ema: 0.999830] 
[Epoch 31/39] [Batch 800/1293] [D loss: 0.373787] [G loss: 0.178162] [ema: 0.999830] 
[Epoch 31/39] [Batch 900/1293] [D loss: 0.460581] [G loss: 0.178527] [ema: 0.999831] 
[Epoch 31/39] [Batch 1000/1293] [D loss: 0.394477] [G loss: 0.174360] [ema: 0.999831] 
[Epoch 31/39] [Batch 1100/1293] [D loss: 0.388537] [G loss: 0.170823] [ema: 0.999832] 
[Epoch 31/39] [Batch 1200/1293] [D loss: 0.407341] [G loss: 0.178927] [ema: 0.999832] 
[Epoch 32/39] [Batch 0/1293] [D loss: 0.389785] [G loss: 0.181988] [ema: 0.999832] 
[Epoch 32/39] [Batch 100/1293] [D loss: 0.398135] [G loss: 0.185604] [ema: 0.999833] 
[Epoch 32/39] [Batch 200/1293] [D loss: 0.411139] [G loss: 0.180684] [ema: 0.999833] 
[Epoch 32/39] [Batch 300/1293] [D loss: 0.420220] [G loss: 0.176326] [ema: 0.999834] 
[Epoch 32/39] [Batch 400/1293] [D loss: 0.449600] [G loss: 0.178560] [ema: 0.999834] 
[Epoch 32/39] [Batch 500/1293] [D loss: 0.395475] [G loss: 0.181936] [ema: 0.999834] 
[Epoch 32/39] [Batch 600/1293] [D loss: 0.381669] [G loss: 0.183717] [ema: 0.999835] 
[Epoch 32/39] [Batch 700/1293] [D loss: 0.378608] [G loss: 0.174671] [ema: 0.999835] 
[Epoch 32/39] [Batch 800/1293] [D loss: 0.443849] [G loss: 0.169306] [ema: 0.999836] 
[Epoch 32/39] [Batch 900/1293] [D loss: 0.397483] [G loss: 0.186243] [ema: 0.999836] 
[Epoch 32/39] [Batch 1000/1293] [D loss: 0.412778] [G loss: 0.176147] [ema: 0.999836] 
[Epoch 32/39] [Batch 1100/1293] [D loss: 0.413213] [G loss: 0.171970] [ema: 0.999837] 
[Epoch 32/39] [Batch 1200/1293] [D loss: 0.388818] [G loss: 0.178134] [ema: 0.999837] 
[Epoch 33/39] [Batch 0/1293] [D loss: 0.424494] [G loss: 0.184295] [ema: 0.999838] 
[Epoch 33/39] [Batch 100/1293] [D loss: 0.406225] [G loss: 0.180990] [ema: 0.999838] 
[Epoch 33/39] [Batch 200/1293] [D loss: 0.415213] [G loss: 0.174098] [ema: 0.999838] 
[Epoch 33/39] [Batch 300/1293] [D loss: 0.381359] [G loss: 0.181862] [ema: 0.999839] 
[Epoch 33/39] [Batch 400/1293] [D loss: 0.362260] [G loss: 0.198381] [ema: 0.999839] 
[Epoch 33/39] [Batch 500/1293] [D loss: 0.386676] [G loss: 0.181027] [ema: 0.999839] 
[Epoch 33/39] [Batch 600/1293] [D loss: 0.406373] [G loss: 0.174661] [ema: 0.999840] 
[Epoch 33/39] [Batch 700/1293] [D loss: 0.412736] [G loss: 0.176176] [ema: 0.999840] 
[Epoch 33/39] [Batch 800/1293] [D loss: 0.433163] [G loss: 0.174495] [ema: 0.999841] 
[Epoch 33/39] [Batch 900/1293] [D loss: 0.417216] [G loss: 0.179143] [ema: 0.999841] 
[Epoch 33/39] [Batch 1000/1293] [D loss: 0.402355] [G loss: 0.171913] [ema: 0.999841] 
[Epoch 33/39] [Batch 1100/1293] [D loss: 0.394095] [G loss: 0.183885] [ema: 0.999842] 
[Epoch 33/39] [Batch 1200/1293] [D loss: 0.383702] [G loss: 0.170672] [ema: 0.999842] 
[Epoch 34/39] [Batch 0/1293] [D loss: 0.411629] [G loss: 0.183320] [ema: 0.999842] 
[Epoch 34/39] [Batch 100/1293] [D loss: 0.380780] [G loss: 0.189181] [ema: 0.999843] 
[Epoch 34/39] [Batch 200/1293] [D loss: 0.378902] [G loss: 0.174915] [ema: 0.999843] 
[Epoch 34/39] [Batch 300/1293] [D loss: 0.429095] [G loss: 0.178900] [ema: 0.999843] 
[Epoch 34/39] [Batch 400/1293] [D loss: 0.379113] [G loss: 0.183801] [ema: 0.999844] 
[Epoch 34/39] [Batch 500/1293] [D loss: 0.378005] [G loss: 0.181381] [ema: 0.999844] 
[Epoch 34/39] [Batch 600/1293] [D loss: 0.373975] [G loss: 0.178488] [ema: 0.999844] 
[Epoch 34/39] [Batch 700/1293] [D loss: 0.362389] [G loss: 0.177173] [ema: 0.999845] 
[Epoch 34/39] [Batch 800/1293] [D loss: 0.429578] [G loss: 0.159747] [ema: 0.999845] 
[Epoch 34/39] [Batch 900/1293] [D loss: 0.363653] [G loss: 0.173131] [ema: 0.999846] 
[Epoch 34/39] [Batch 1000/1293] [D loss: 0.386260] [G loss: 0.175862] [ema: 0.999846] 
[Epoch 34/39] [Batch 1100/1293] [D loss: 0.435692] [G loss: 0.176665] [ema: 0.999846] 
[Epoch 34/39] [Batch 1200/1293] [D loss: 0.425212] [G loss: 0.176448] [ema: 0.999847] 
[Epoch 35/39] [Batch 0/1293] [D loss: 0.405194] [G loss: 0.174190] [ema: 0.999847] 
[Epoch 35/39] [Batch 100/1293] [D loss: 0.397240] [G loss: 0.169029] [ema: 0.999847] 
[Epoch 35/39] [Batch 200/1293] [D loss: 0.412810] [G loss: 0.173840] [ema: 0.999848] 
[Epoch 35/39] [Batch 300/1293] [D loss: 0.373965] [G loss: 0.176548] [ema: 0.999848] 
[Epoch 35/39] [Batch 400/1293] [D loss: 0.384434] [G loss: 0.186875] [ema: 0.999848] 
[Epoch 35/39] [Batch 500/1293] [D loss: 0.401836] [G loss: 0.174325] [ema: 0.999849] 
[Epoch 35/39] [Batch 600/1293] [D loss: 0.439197] [G loss: 0.176995] [ema: 0.999849] 
[Epoch 35/39] [Batch 700/1293] [D loss: 0.389142] [G loss: 0.179425] [ema: 0.999849] 
[Epoch 35/39] [Batch 800/1293] [D loss: 0.390644] [G loss: 0.171188] [ema: 0.999850] 
[Epoch 35/39] [Batch 900/1293] [D loss: 0.371552] [G loss: 0.191430] [ema: 0.999850] 
[Epoch 35/39] [Batch 1000/1293] [D loss: 0.378855] [G loss: 0.177931] [ema: 0.999850] 
[Epoch 35/39] [Batch 1100/1293] [D loss: 0.421789] [G loss: 0.175542] [ema: 0.999850] 
[Epoch 35/39] [Batch 1200/1293] [D loss: 0.377146] [G loss: 0.185207] [ema: 0.999851] 
[Epoch 36/39] [Batch 0/1293] [D loss: 0.427484] [G loss: 0.161669] [ema: 0.999851] 
[Epoch 36/39] [Batch 100/1293] [D loss: 0.451417] [G loss: 0.176563] [ema: 0.999851] 
[Epoch 36/39] [Batch 200/1293] [D loss: 0.401949] [G loss: 0.182448] [ema: 0.999852] 
[Epoch 36/39] [Batch 300/1293] [D loss: 0.411210] [G loss: 0.162829] [ema: 0.999852] 
[Epoch 36/39] [Batch 400/1293] [D loss: 0.406278] [G loss: 0.165330] [ema: 0.999852] 
[Epoch 36/39] [Batch 500/1293] [D loss: 0.437183] [G loss: 0.169516] [ema: 0.999853] 
[Epoch 36/39] [Batch 600/1293] [D loss: 0.412754] [G loss: 0.170477] [ema: 0.999853] 
[Epoch 36/39] [Batch 700/1293] [D loss: 0.439177] [G loss: 0.180717] [ema: 0.999853] 
[Epoch 36/39] [Batch 800/1293] [D loss: 0.402436] [G loss: 0.172250] [ema: 0.999854] 
[Epoch 36/39] [Batch 900/1293] [D loss: 0.422107] [G loss: 0.176671] [ema: 0.999854] 
[Epoch 36/39] [Batch 1000/1293] [D loss: 0.398884] [G loss: 0.186551] [ema: 0.999854] 
[Epoch 36/39] [Batch 1100/1293] [D loss: 0.381012] [G loss: 0.186780] [ema: 0.999855] 
[Epoch 36/39] [Batch 1200/1293] [D loss: 0.391627] [G loss: 0.180652] [ema: 0.999855] 
[Epoch 37/39] [Batch 0/1293] [D loss: 0.406969] [G loss: 0.184876] [ema: 0.999855] 
[Epoch 37/39] [Batch 100/1293] [D loss: 0.510480] [G loss: 0.169620] [ema: 0.999855] 
[Epoch 37/39] [Batch 200/1293] [D loss: 0.408346] [G loss: 0.175265] [ema: 0.999856] 
[Epoch 37/39] [Batch 300/1293] [D loss: 0.443142] [G loss: 0.173200] [ema: 0.999856] 
[Epoch 37/39] [Batch 400/1293] [D loss: 0.414840] [G loss: 0.181453] [ema: 0.999856] 
[Epoch 37/39] [Batch 500/1293] [D loss: 0.466653] [G loss: 0.178116] [ema: 0.999857] 
[Epoch 37/39] [Batch 600/1293] [D loss: 0.406287] [G loss: 0.173749] [ema: 0.999857] 
[Epoch 37/39] [Batch 700/1293] [D loss: 0.419269] [G loss: 0.165519] [ema: 0.999857] 
[Epoch 37/39] [Batch 800/1293] [D loss: 0.386068] [G loss: 0.170719] [ema: 0.999858] 
[Epoch 37/39] [Batch 900/1293] [D loss: 0.431722] [G loss: 0.177629] [ema: 0.999858] 
[Epoch 37/39] [Batch 1000/1293] [D loss: 0.369376] [G loss: 0.161010] [ema: 0.999858] 
[Epoch 37/39] [Batch 1100/1293] [D loss: 0.452656] [G loss: 0.163732] [ema: 0.999858] 
[Epoch 37/39] [Batch 1200/1293] [D loss: 0.426258] [G loss: 0.161431] [ema: 0.999859] 
[Epoch 38/39] [Batch 0/1293] [D loss: 0.430496] [G loss: 0.149244] [ema: 0.999859] 
[Epoch 38/39] [Batch 100/1293] [D loss: 0.431056] [G loss: 0.169783] [ema: 0.999859] 
[Epoch 38/39] [Batch 200/1293] [D loss: 0.482583] [G loss: 0.167478] [ema: 0.999860] 
[Epoch 38/39] [Batch 300/1293] [D loss: 0.401837] [G loss: 0.157094] [ema: 0.999860] 
[Epoch 38/39] [Batch 400/1293] [D loss: 0.477509] [G loss: 0.180501] [ema: 0.999860] 
[Epoch 38/39] [Batch 500/1293] [D loss: 0.451457] [G loss: 0.191116] [ema: 0.999860] 
[Epoch 38/39] [Batch 600/1293] [D loss: 0.408108] [G loss: 0.154100] [ema: 0.999861] 
[Epoch 38/39] [Batch 700/1293] [D loss: 0.429838] [G loss: 0.184302] [ema: 0.999861] 
[Epoch 38/39] [Batch 800/1293] [D loss: 0.403553] [G loss: 0.164577] [ema: 0.999861] 
[Epoch 38/39] [Batch 900/1293] [D loss: 0.451574] [G loss: 0.182080] [ema: 0.999861] 
[Epoch 38/39] [Batch 1000/1293] [D loss: 0.428179] [G loss: 0.162209] [ema: 0.999862] 
[Epoch 38/39] [Batch 1100/1293] [D loss: 0.382735] [G loss: 0.174413] [ema: 0.999862] 
[Epoch 38/39] [Batch 1200/1293] [D loss: 0.391041] [G loss: 0.175417] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
WISDM_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
WISDM_DAGHAR_Multiclass
daghar
return single class data and labels, class is WISDM_DAGHAR_Multiclass
data shape is (17496, 3, 1, 30)
label shape is (17496,)
1094
Epochs between checkpoint: 12



Saving checkpoint 1 in logs/daghar_split_dataset_50000_30_100/WISDM_DAGHAR_Multiclass_50000_D_30_2024_10_25_01_05_31/Model



[Epoch 0/46] [Batch 0/1094] [D loss: 1.332026] [G loss: 0.927365] [ema: 0.000000] 
[Epoch 0/46] [Batch 100/1094] [D loss: 0.590168] [G loss: 0.141005] [ema: 0.933033] 
[Epoch 0/46] [Batch 200/1094] [D loss: 0.551645] [G loss: 0.156523] [ema: 0.965936] 
[Epoch 0/46] [Batch 300/1094] [D loss: 0.439181] [G loss: 0.160978] [ema: 0.977160] 
[Epoch 0/46] [Batch 400/1094] [D loss: 0.400785] [G loss: 0.162370] [ema: 0.982821] 
[Epoch 0/46] [Batch 500/1094] [D loss: 0.504114] [G loss: 0.159511] [ema: 0.986233] 
[Epoch 0/46] [Batch 600/1094] [D loss: 0.430702] [G loss: 0.191744] [ema: 0.988514] 
[Epoch 0/46] [Batch 700/1094] [D loss: 0.499923] [G loss: 0.143675] [ema: 0.990147] 
[Epoch 0/46] [Batch 800/1094] [D loss: 0.537731] [G loss: 0.153345] [ema: 0.991373] 
[Epoch 0/46] [Batch 900/1094] [D loss: 0.444324] [G loss: 0.162296] [ema: 0.992328] 
[Epoch 0/46] [Batch 1000/1094] [D loss: 0.567964] [G loss: 0.106521] [ema: 0.993092] 
[Epoch 1/46] [Batch 0/1094] [D loss: 0.425748] [G loss: 0.152594] [ema: 0.993684] 
[Epoch 1/46] [Batch 100/1094] [D loss: 0.456973] [G loss: 0.166443] [ema: 0.994212] 
[Epoch 1/46] [Batch 200/1094] [D loss: 0.487872] [G loss: 0.144473] [ema: 0.994658] 
[Epoch 1/46] [Batch 300/1094] [D loss: 0.463610] [G loss: 0.166191] [ema: 0.995040] 
[Epoch 1/46] [Batch 400/1094] [D loss: 0.435919] [G loss: 0.128522] [ema: 0.995371] 
[Epoch 1/46] [Batch 500/1094] [D loss: 0.474048] [G loss: 0.139746] [ema: 0.995661] 
[Epoch 1/46] [Batch 600/1094] [D loss: 0.449404] [G loss: 0.179996] [ema: 0.995917] 
[Epoch 1/46] [Batch 700/1094] [D loss: 0.517597] [G loss: 0.138898] [ema: 0.996144] 
[Epoch 1/46] [Batch 800/1094] [D loss: 0.440305] [G loss: 0.180932] [ema: 0.996347] 
[Epoch 1/46] [Batch 900/1094] [D loss: 0.459109] [G loss: 0.139738] [ema: 0.996530] 
[Epoch 1/46] [Batch 1000/1094] [D loss: 0.461455] [G loss: 0.161732] [ema: 0.996695] 
[Epoch 2/46] [Batch 0/1094] [D loss: 0.386213] [G loss: 0.186449] [ema: 0.996837] 
[Epoch 2/46] [Batch 100/1094] [D loss: 0.385393] [G loss: 0.137289] [ema: 0.996975] 
[Epoch 2/46] [Batch 200/1094] [D loss: 0.360260] [G loss: 0.175313] [ema: 0.997102] 
[Epoch 2/46] [Batch 300/1094] [D loss: 0.449110] [G loss: 0.174560] [ema: 0.997218] 
[Epoch 2/46] [Batch 400/1094] [D loss: 0.456584] [G loss: 0.164218] [ema: 0.997325] 
[Epoch 2/46] [Batch 500/1094] [D loss: 0.431635] [G loss: 0.150105] [ema: 0.997425] 
[Epoch 2/46] [Batch 600/1094] [D loss: 0.460584] [G loss: 0.179976] [ema: 0.997517] 
[Epoch 2/46] [Batch 700/1094] [D loss: 0.466333] [G loss: 0.148006] [ema: 0.997603] 
[Epoch 2/46] [Batch 800/1094] [D loss: 0.484213] [G loss: 0.174898] [ema: 0.997683] 
[Epoch 2/46] [Batch 900/1094] [D loss: 0.454031] [G loss: 0.159974] [ema: 0.997758] 
[Epoch 2/46] [Batch 1000/1094] [D loss: 0.441397] [G loss: 0.184477] [ema: 0.997828] 
[Epoch 3/46] [Batch 0/1094] [D loss: 0.429698] [G loss: 0.163017] [ema: 0.997890] 
[Epoch 3/46] [Batch 100/1094] [D loss: 0.482912] [G loss: 0.149168] [ema: 0.997953] 
[Epoch 3/46] [Batch 200/1094] [D loss: 0.477917] [G loss: 0.161544] [ema: 0.998011] 
[Epoch 3/46] [Batch 300/1094] [D loss: 0.458207] [G loss: 0.173966] [ema: 0.998067] 
[Epoch 3/46] [Batch 400/1094] [D loss: 0.455195] [G loss: 0.167561] [ema: 0.998119] 
[Epoch 3/46] [Batch 500/1094] [D loss: 0.437563] [G loss: 0.165325] [ema: 0.998169] 
[Epoch 3/46] [Batch 600/1094] [D loss: 0.437752] [G loss: 0.175645] [ema: 0.998216] 
[Epoch 3/46] [Batch 700/1094] [D loss: 0.445112] [G loss: 0.154642] [ema: 0.998261] 
[Epoch 3/46] [Batch 800/1094] [D loss: 0.419784] [G loss: 0.158931] [ema: 0.998303] 
[Epoch 3/46] [Batch 900/1094] [D loss: 0.460049] [G loss: 0.153608] [ema: 0.998344] 
[Epoch 3/46] [Batch 1000/1094] [D loss: 0.459784] [G loss: 0.149093] [ema: 0.998383] 
[Epoch 4/46] [Batch 0/1094] [D loss: 0.454841] [G loss: 0.171379] [ema: 0.998417] 
[Epoch 4/46] [Batch 100/1094] [D loss: 0.472340] [G loss: 0.159244] [ema: 0.998453] 
[Epoch 4/46] [Batch 200/1094] [D loss: 0.394685] [G loss: 0.173355] [ema: 0.998486] 
[Epoch 4/46] [Batch 300/1094] [D loss: 0.462275] [G loss: 0.155044] [ema: 0.998519] 
[Epoch 4/46] [Batch 400/1094] [D loss: 0.452822] [G loss: 0.168614] [ema: 0.998550] 
[Epoch 4/46] [Batch 500/1094] [D loss: 0.454365] [G loss: 0.147424] [ema: 0.998579] 
[Epoch 4/46] [Batch 600/1094] [D loss: 0.432357] [G loss: 0.176450] [ema: 0.998608] 
[Epoch 4/46] [Batch 700/1094] [D loss: 0.448136] [G loss: 0.160624] [ema: 0.998635] 
[Epoch 4/46] [Batch 800/1094] [D loss: 0.400470] [G loss: 0.164408] [ema: 0.998662] 
[Epoch 4/46] [Batch 900/1094] [D loss: 0.440706] [G loss: 0.146670] [ema: 0.998687] 
[Epoch 4/46] [Batch 1000/1094] [D loss: 0.450892] [G loss: 0.163518] [ema: 0.998711] 
[Epoch 5/46] [Batch 0/1094] [D loss: 0.459105] [G loss: 0.155544] [ema: 0.998734] 
[Epoch 5/46] [Batch 100/1094] [D loss: 0.394988] [G loss: 0.165253] [ema: 0.998756] 
[Epoch 5/46] [Batch 200/1094] [D loss: 0.434167] [G loss: 0.161576] [ema: 0.998778] 
[Epoch 5/46] [Batch 300/1094] [D loss: 0.437446] [G loss: 0.167159] [ema: 0.998799] 
[Epoch 5/46] [Batch 400/1094] [D loss: 0.430248] [G loss: 0.176391] [ema: 0.998820] 
[Epoch 5/46] [Batch 500/1094] [D loss: 0.421306] [G loss: 0.175069] [ema: 0.998840] 
[Epoch 5/46] [Batch 600/1094] [D loss: 0.466722] [G loss: 0.163890] [ema: 0.998859] 
[Epoch 5/46] [Batch 700/1094] [D loss: 0.440274] [G loss: 0.157558] [ema: 0.998877] 
[Epoch 5/46] [Batch 800/1094] [D loss: 0.444363] [G loss: 0.186071] [ema: 0.998895] 
[Epoch 5/46] [Batch 900/1094] [D loss: 0.485465] [G loss: 0.165520] [ema: 0.998912] 
[Epoch 5/46] [Batch 1000/1094] [D loss: 0.460839] [G loss: 0.153299] [ema: 0.998929] 
[Epoch 6/46] [Batch 0/1094] [D loss: 0.458552] [G loss: 0.167079] [ema: 0.998945] 
[Epoch 6/46] [Batch 100/1094] [D loss: 0.417657] [G loss: 0.169000] [ema: 0.998960] 
[Epoch 6/46] [Batch 200/1094] [D loss: 0.437687] [G loss: 0.162462] [ema: 0.998976] 
[Epoch 6/46] [Batch 300/1094] [D loss: 0.434291] [G loss: 0.165031] [ema: 0.998991] 
[Epoch 6/46] [Batch 400/1094] [D loss: 0.428774] [G loss: 0.158803] [ema: 0.999005] 
[Epoch 6/46] [Batch 500/1094] [D loss: 0.444334] [G loss: 0.151681] [ema: 0.999019] 
[Epoch 6/46] [Batch 600/1094] [D loss: 0.432954] [G loss: 0.175027] [ema: 0.999033] 
[Epoch 6/46] [Batch 700/1094] [D loss: 0.446051] [G loss: 0.155488] [ema: 0.999046] 
[Epoch 6/46] [Batch 800/1094] [D loss: 0.481448] [G loss: 0.156788] [ema: 0.999059] 
[Epoch 6/46] [Batch 900/1094] [D loss: 0.432503] [G loss: 0.178499] [ema: 0.999072] 
[Epoch 6/46] [Batch 1000/1094] [D loss: 0.430421] [G loss: 0.175869] [ema: 0.999084] 
[Epoch 7/46] [Batch 0/1094] [D loss: 0.406740] [G loss: 0.173881] [ema: 0.999095] 
[Epoch 7/46] [Batch 100/1094] [D loss: 0.410926] [G loss: 0.178175] [ema: 0.999107] 
[Epoch 7/46] [Batch 200/1094] [D loss: 0.433463] [G loss: 0.169770] [ema: 0.999118] 
[Epoch 7/46] [Batch 300/1094] [D loss: 0.442954] [G loss: 0.176988] [ema: 0.999129] 
[Epoch 7/46] [Batch 400/1094] [D loss: 0.408029] [G loss: 0.158905] [ema: 0.999140] 
[Epoch 7/46] [Batch 500/1094] [D loss: 0.457042] [G loss: 0.174821] [ema: 0.999151] 
[Epoch 7/46] [Batch 600/1094] [D loss: 0.437503] [G loss: 0.168192] [ema: 0.999161] 
[Epoch 7/46] [Batch 700/1094] [D loss: 0.353912] [G loss: 0.179887] [ema: 0.999171] 
[Epoch 7/46] [Batch 800/1094] [D loss: 0.389418] [G loss: 0.178984] [ema: 0.999181] 
[Epoch 7/46] [Batch 900/1094] [D loss: 0.450431] [G loss: 0.176298] [ema: 0.999190] 
[Epoch 7/46] [Batch 1000/1094] [D loss: 0.425315] [G loss: 0.145695] [ema: 0.999200] 
[Epoch 8/46] [Batch 0/1094] [D loss: 0.436206] [G loss: 0.184336] [ema: 0.999208] 
[Epoch 8/46] [Batch 100/1094] [D loss: 0.409319] [G loss: 0.177871] [ema: 0.999217] 
[Epoch 8/46] [Batch 200/1094] [D loss: 0.414299] [G loss: 0.153622] [ema: 0.999226] 
[Epoch 8/46] [Batch 300/1094] [D loss: 0.419136] [G loss: 0.177602] [ema: 0.999235] 
[Epoch 8/46] [Batch 400/1094] [D loss: 0.449177] [G loss: 0.162143] [ema: 0.999243] 
[Epoch 8/46] [Batch 500/1094] [D loss: 0.437046] [G loss: 0.181781] [ema: 0.999251] 
[Epoch 8/46] [Batch 600/1094] [D loss: 0.466103] [G loss: 0.156942] [ema: 0.999259] 
[Epoch 8/46] [Batch 700/1094] [D loss: 0.443435] [G loss: 0.158946] [ema: 0.999267] 
[Epoch 8/46] [Batch 800/1094] [D loss: 0.451116] [G loss: 0.156153] [ema: 0.999275] 
[Epoch 8/46] [Batch 900/1094] [D loss: 0.426789] [G loss: 0.154367] [ema: 0.999282] 
[Epoch 8/46] [Batch 1000/1094] [D loss: 0.417463] [G loss: 0.177651] [ema: 0.999289] 
[Epoch 9/46] [Batch 0/1094] [D loss: 0.414839] [G loss: 0.169457] [ema: 0.999296] 
[Epoch 9/46] [Batch 100/1094] [D loss: 0.451718] [G loss: 0.171008] [ema: 0.999303] 
[Epoch 9/46] [Batch 200/1094] [D loss: 0.362237] [G loss: 0.158767] [ema: 0.999310] 
[Epoch 9/46] [Batch 300/1094] [D loss: 0.417143] [G loss: 0.170690] [ema: 0.999317] 
[Epoch 9/46] [Batch 400/1094] [D loss: 0.451001] [G loss: 0.156288] [ema: 0.999324] 
[Epoch 9/46] [Batch 500/1094] [D loss: 0.392170] [G loss: 0.173807] [ema: 0.999330] 
[Epoch 9/46] [Batch 600/1094] [D loss: 0.442910] [G loss: 0.154103] [ema: 0.999337] 
[Epoch 9/46] [Batch 700/1094] [D loss: 0.446529] [G loss: 0.168009] [ema: 0.999343] 
[Epoch 9/46] [Batch 800/1094] [D loss: 0.453807] [G loss: 0.162559] [ema: 0.999349] 
[Epoch 9/46] [Batch 900/1094] [D loss: 0.411246] [G loss: 0.163471] [ema: 0.999355] 
[Epoch 9/46] [Batch 1000/1094] [D loss: 0.424103] [G loss: 0.174732] [ema: 0.999361] 
[Epoch 10/46] [Batch 0/1094] [D loss: 0.423795] [G loss: 0.160069] [ema: 0.999367] 
[Epoch 10/46] [Batch 100/1094] [D loss: 0.447760] [G loss: 0.177683] [ema: 0.999372] 
[Epoch 10/46] [Batch 200/1094] [D loss: 0.464389] [G loss: 0.170507] [ema: 0.999378] 
[Epoch 10/46] [Batch 300/1094] [D loss: 0.411211] [G loss: 0.175140] [ema: 0.999384] 
[Epoch 10/46] [Batch 400/1094] [D loss: 0.414185] [G loss: 0.167096] [ema: 0.999389] 
[Epoch 10/46] [Batch 500/1094] [D loss: 0.414555] [G loss: 0.168361] [ema: 0.999394] 
[Epoch 10/46] [Batch 600/1094] [D loss: 0.433573] [G loss: 0.161518] [ema: 0.999400] 
[Epoch 10/46] [Batch 700/1094] [D loss: 0.416401] [G loss: 0.176791] [ema: 0.999405] 
[Epoch 10/46] [Batch 800/1094] [D loss: 0.428878] [G loss: 0.174875] [ema: 0.999410] 
[Epoch 10/46] [Batch 900/1094] [D loss: 0.441836] [G loss: 0.175752] [ema: 0.999415] 
[Epoch 10/46] [Batch 1000/1094] [D loss: 0.413087] [G loss: 0.180266] [ema: 0.999420] 
[Epoch 11/46] [Batch 0/1094] [D loss: 0.436802] [G loss: 0.163030] [ema: 0.999424] 
[Epoch 11/46] [Batch 100/1094] [D loss: 0.413578] [G loss: 0.185849] [ema: 0.999429] 
[Epoch 11/46] [Batch 200/1094] [D loss: 0.416385] [G loss: 0.173997] [ema: 0.999434] 
[Epoch 11/46] [Batch 300/1094] [D loss: 0.416465] [G loss: 0.153161] [ema: 0.999438] 
[Epoch 11/46] [Batch 400/1094] [D loss: 0.410124] [G loss: 0.167304] [ema: 0.999443] 
[Epoch 11/46] [Batch 500/1094] [D loss: 0.427065] [G loss: 0.163780] [ema: 0.999447] 
[Epoch 11/46] [Batch 600/1094] [D loss: 0.410049] [G loss: 0.156700] [ema: 0.999452] 
[Epoch 11/46] [Batch 700/1094] [D loss: 0.399068] [G loss: 0.167792] [ema: 0.999456] 
[Epoch 11/46] [Batch 800/1094] [D loss: 0.426038] [G loss: 0.158300] [ema: 0.999460] 
[Epoch 11/46] [Batch 900/1094] [D loss: 0.405923] [G loss: 0.191672] [ema: 0.999464] 
[Epoch 11/46] [Batch 1000/1094] [D loss: 0.411183] [G loss: 0.174620] [ema: 0.999468] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_30_100/WISDM_DAGHAR_Multiclass_50000_D_30_2024_10_25_01_05_31/Model



[Epoch 12/46] [Batch 0/1094] [D loss: 0.445674] [G loss: 0.191560] [ema: 0.999472] 
[Epoch 12/46] [Batch 100/1094] [D loss: 0.451913] [G loss: 0.183150] [ema: 0.999476] 
[Epoch 12/46] [Batch 200/1094] [D loss: 0.433828] [G loss: 0.162735] [ema: 0.999480] 
[Epoch 12/46] [Batch 300/1094] [D loss: 0.427387] [G loss: 0.159124] [ema: 0.999484] 
[Epoch 12/46] [Batch 400/1094] [D loss: 0.399727] [G loss: 0.180134] [ema: 0.999488] 
[Epoch 12/46] [Batch 500/1094] [D loss: 0.400395] [G loss: 0.181340] [ema: 0.999492] 
[Epoch 12/46] [Batch 600/1094] [D loss: 0.410175] [G loss: 0.159208] [ema: 0.999495] 
[Epoch 12/46] [Batch 700/1094] [D loss: 0.427874] [G loss: 0.158585] [ema: 0.999499] 
[Epoch 12/46] [Batch 800/1094] [D loss: 0.412251] [G loss: 0.173030] [ema: 0.999502] 
[Epoch 12/46] [Batch 900/1094] [D loss: 0.394047] [G loss: 0.171438] [ema: 0.999506] 
[Epoch 12/46] [Batch 1000/1094] [D loss: 0.448966] [G loss: 0.173053] [ema: 0.999510] 
[Epoch 13/46] [Batch 0/1094] [D loss: 0.360948] [G loss: 0.187076] [ema: 0.999513] 
[Epoch 13/46] [Batch 100/1094] [D loss: 0.368834] [G loss: 0.188447] [ema: 0.999516] 
[Epoch 13/46] [Batch 200/1094] [D loss: 0.426117] [G loss: 0.164812] [ema: 0.999519] 
[Epoch 13/46] [Batch 300/1094] [D loss: 0.430066] [G loss: 0.164872] [ema: 0.999523] 
[Epoch 13/46] [Batch 400/1094] [D loss: 0.419809] [G loss: 0.166456] [ema: 0.999526] 
[Epoch 13/46] [Batch 500/1094] [D loss: 0.449295] [G loss: 0.162505] [ema: 0.999529] 
[Epoch 13/46] [Batch 600/1094] [D loss: 0.358963] [G loss: 0.179491] [ema: 0.999532] 
[Epoch 13/46] [Batch 700/1094] [D loss: 0.406078] [G loss: 0.153032] [ema: 0.999536] 
[Epoch 13/46] [Batch 800/1094] [D loss: 0.443165] [G loss: 0.173107] [ema: 0.999539] 
[Epoch 13/46] [Batch 900/1094] [D loss: 0.400275] [G loss: 0.178524] [ema: 0.999542] 
[Epoch 13/46] [Batch 1000/1094] [D loss: 0.405946] [G loss: 0.170076] [ema: 0.999545] 
[Epoch 14/46] [Batch 0/1094] [D loss: 0.411174] [G loss: 0.166457] [ema: 0.999548] 
[Epoch 14/46] [Batch 100/1094] [D loss: 0.423602] [G loss: 0.176466] [ema: 0.999550] 
[Epoch 14/46] [Batch 200/1094] [D loss: 0.430619] [G loss: 0.172025] [ema: 0.999553] 
[Epoch 14/46] [Batch 300/1094] [D loss: 0.406909] [G loss: 0.155270] [ema: 0.999556] 
[Epoch 14/46] [Batch 400/1094] [D loss: 0.439983] [G loss: 0.182737] [ema: 0.999559] 
[Epoch 14/46] [Batch 500/1094] [D loss: 0.393705] [G loss: 0.190909] [ema: 0.999562] 
[Epoch 14/46] [Batch 600/1094] [D loss: 0.405371] [G loss: 0.167141] [ema: 0.999565] 
[Epoch 14/46] [Batch 700/1094] [D loss: 0.450128] [G loss: 0.167267] [ema: 0.999567] 
[Epoch 14/46] [Batch 800/1094] [D loss: 0.398324] [G loss: 0.165599] [ema: 0.999570] 
[Epoch 14/46] [Batch 900/1094] [D loss: 0.415127] [G loss: 0.175519] [ema: 0.999573] 
[Epoch 14/46] [Batch 1000/1094] [D loss: 0.445307] [G loss: 0.170014] [ema: 0.999575] 
[Epoch 15/46] [Batch 0/1094] [D loss: 0.420566] [G loss: 0.154947] [ema: 0.999578] 
[Epoch 15/46] [Batch 100/1094] [D loss: 0.419478] [G loss: 0.164117] [ema: 0.999580] 
[Epoch 15/46] [Batch 200/1094] [D loss: 0.390350] [G loss: 0.178866] [ema: 0.999583] 
[Epoch 15/46] [Batch 300/1094] [D loss: 0.409339] [G loss: 0.171419] [ema: 0.999585] 
[Epoch 15/46] [Batch 400/1094] [D loss: 0.421842] [G loss: 0.179177] [ema: 0.999588] 
[Epoch 15/46] [Batch 500/1094] [D loss: 0.393929] [G loss: 0.161989] [ema: 0.999590] 
[Epoch 15/46] [Batch 600/1094] [D loss: 0.415825] [G loss: 0.160392] [ema: 0.999593] 
[Epoch 15/46] [Batch 700/1094] [D loss: 0.388587] [G loss: 0.183875] [ema: 0.999595] 
[Epoch 15/46] [Batch 800/1094] [D loss: 0.390634] [G loss: 0.185837] [ema: 0.999597] 
[Epoch 15/46] [Batch 900/1094] [D loss: 0.413495] [G loss: 0.146521] [ema: 0.999600] 
[Epoch 15/46] [Batch 1000/1094] [D loss: 0.432692] [G loss: 0.165256] [ema: 0.999602] 
[Epoch 16/46] [Batch 0/1094] [D loss: 0.380155] [G loss: 0.171498] [ema: 0.999604] 
[Epoch 16/46] [Batch 100/1094] [D loss: 0.415867] [G loss: 0.175398] [ema: 0.999606] 
[Epoch 16/46] [Batch 200/1094] [D loss: 0.356209] [G loss: 0.195351] [ema: 0.999609] 
[Epoch 16/46] [Batch 300/1094] [D loss: 0.373371] [G loss: 0.198607] [ema: 0.999611] 
[Epoch 16/46] [Batch 400/1094] [D loss: 0.406329] [G loss: 0.177859] [ema: 0.999613] 
[Epoch 16/46] [Batch 500/1094] [D loss: 0.418465] [G loss: 0.169671] [ema: 0.999615] 
[Epoch 16/46] [Batch 600/1094] [D loss: 0.497211] [G loss: 0.156521] [ema: 0.999617] 
[Epoch 16/46] [Batch 700/1094] [D loss: 0.390881] [G loss: 0.164363] [ema: 0.999619] 
[Epoch 16/46] [Batch 800/1094] [D loss: 0.441138] [G loss: 0.186440] [ema: 0.999621] 
[Epoch 16/46] [Batch 900/1094] [D loss: 0.396155] [G loss: 0.185068] [ema: 0.999623] 
[Epoch 16/46] [Batch 1000/1094] [D loss: 0.431586] [G loss: 0.178380] [ema: 0.999625] 
[Epoch 17/46] [Batch 0/1094] [D loss: 0.413940] [G loss: 0.184952] [ema: 0.999627] 
[Epoch 17/46] [Batch 100/1094] [D loss: 0.416892] [G loss: 0.177100] [ema: 0.999629] 
[Epoch 17/46] [Batch 200/1094] [D loss: 0.463574] [G loss: 0.170579] [ema: 0.999631] 
[Epoch 17/46] [Batch 300/1094] [D loss: 0.431298] [G loss: 0.167435] [ema: 0.999633] 
[Epoch 17/46] [Batch 400/1094] [D loss: 0.441809] [G loss: 0.174191] [ema: 0.999635] 
[Epoch 17/46] [Batch 500/1094] [D loss: 0.400867] [G loss: 0.176236] [ema: 0.999637] 
[Epoch 17/46] [Batch 600/1094] [D loss: 0.417484] [G loss: 0.166078] [ema: 0.999639] 
[Epoch 17/46] [Batch 700/1094] [D loss: 0.388711] [G loss: 0.180722] [ema: 0.999641] 
[Epoch 17/46] [Batch 800/1094] [D loss: 0.429441] [G loss: 0.166309] [ema: 0.999643] 
[Epoch 17/46] [Batch 900/1094] [D loss: 0.411788] [G loss: 0.183957] [ema: 0.999645] 
[Epoch 17/46] [Batch 1000/1094] [D loss: 0.399464] [G loss: 0.172095] [ema: 0.999646] 
[Epoch 18/46] [Batch 0/1094] [D loss: 0.378045] [G loss: 0.194649] [ema: 0.999648] 
[Epoch 18/46] [Batch 100/1094] [D loss: 0.350645] [G loss: 0.198844] [ema: 0.999650] 
[Epoch 18/46] [Batch 200/1094] [D loss: 0.457745] [G loss: 0.185708] [ema: 0.999652] 
[Epoch 18/46] [Batch 300/1094] [D loss: 0.426524] [G loss: 0.177231] [ema: 0.999653] 
[Epoch 18/46] [Batch 400/1094] [D loss: 0.395482] [G loss: 0.182711] [ema: 0.999655] 
[Epoch 18/46] [Batch 500/1094] [D loss: 0.383039] [G loss: 0.187213] [ema: 0.999657] 
[Epoch 18/46] [Batch 600/1094] [D loss: 0.378868] [G loss: 0.182472] [ema: 0.999658] 
[Epoch 18/46] [Batch 700/1094] [D loss: 0.387491] [G loss: 0.191683] [ema: 0.999660] 
[Epoch 18/46] [Batch 800/1094] [D loss: 0.425667] [G loss: 0.174004] [ema: 0.999662] 
[Epoch 18/46] [Batch 900/1094] [D loss: 0.409281] [G loss: 0.184999] [ema: 0.999663] 
[Epoch 18/46] [Batch 1000/1094] [D loss: 0.391346] [G loss: 0.177934] [ema: 0.999665] 
[Epoch 19/46] [Batch 0/1094] [D loss: 0.412301] [G loss: 0.199294] [ema: 0.999667] 
[Epoch 19/46] [Batch 100/1094] [D loss: 0.391985] [G loss: 0.176949] [ema: 0.999668] 
[Epoch 19/46] [Batch 200/1094] [D loss: 0.393838] [G loss: 0.190141] [ema: 0.999670] 
[Epoch 19/46] [Batch 300/1094] [D loss: 0.401338] [G loss: 0.184247] [ema: 0.999671] 
[Epoch 19/46] [Batch 400/1094] [D loss: 0.371474] [G loss: 0.186035] [ema: 0.999673] 
[Epoch 19/46] [Batch 500/1094] [D loss: 0.367576] [G loss: 0.189302] [ema: 0.999674] 
[Epoch 19/46] [Batch 600/1094] [D loss: 0.399681] [G loss: 0.166949] [ema: 0.999676] 
[Epoch 19/46] [Batch 700/1094] [D loss: 0.429285] [G loss: 0.157068] [ema: 0.999677] 
[Epoch 19/46] [Batch 800/1094] [D loss: 0.433716] [G loss: 0.175197] [ema: 0.999679] 
[Epoch 19/46] [Batch 900/1094] [D loss: 0.439029] [G loss: 0.169107] [ema: 0.999680] 
[Epoch 19/46] [Batch 1000/1094] [D loss: 0.397897] [G loss: 0.182590] [ema: 0.999682] 
[Epoch 20/46] [Batch 0/1094] [D loss: 0.404570] [G loss: 0.190116] [ema: 0.999683] 
[Epoch 20/46] [Batch 100/1094] [D loss: 0.446511] [G loss: 0.172146] [ema: 0.999685] 
[Epoch 20/46] [Batch 200/1094] [D loss: 0.369853] [G loss: 0.191543] [ema: 0.999686] 
[Epoch 20/46] [Batch 300/1094] [D loss: 0.390694] [G loss: 0.161842] [ema: 0.999688] 
[Epoch 20/46] [Batch 400/1094] [D loss: 0.415557] [G loss: 0.180386] [ema: 0.999689] 
[Epoch 20/46] [Batch 500/1094] [D loss: 0.405338] [G loss: 0.159978] [ema: 0.999690] 
[Epoch 20/46] [Batch 600/1094] [D loss: 0.408219] [G loss: 0.167498] [ema: 0.999692] 
[Epoch 20/46] [Batch 700/1094] [D loss: 0.395646] [G loss: 0.168858] [ema: 0.999693] 
[Epoch 20/46] [Batch 800/1094] [D loss: 0.416142] [G loss: 0.164924] [ema: 0.999694] 
[Epoch 20/46] [Batch 900/1094] [D loss: 0.415892] [G loss: 0.179875] [ema: 0.999696] 
[Epoch 20/46] [Batch 1000/1094] [D loss: 0.430073] [G loss: 0.171737] [ema: 0.999697] 
[Epoch 21/46] [Batch 0/1094] [D loss: 0.385504] [G loss: 0.185478] [ema: 0.999698] 
[Epoch 21/46] [Batch 100/1094] [D loss: 0.370299] [G loss: 0.159172] [ema: 0.999700] 
[Epoch 21/46] [Batch 200/1094] [D loss: 0.386896] [G loss: 0.184355] [ema: 0.999701] 
[Epoch 21/46] [Batch 300/1094] [D loss: 0.368820] [G loss: 0.157512] [ema: 0.999702] 
[Epoch 21/46] [Batch 400/1094] [D loss: 0.391031] [G loss: 0.163685] [ema: 0.999703] 
[Epoch 21/46] [Batch 500/1094] [D loss: 0.410334] [G loss: 0.188818] [ema: 0.999705] 
[Epoch 21/46] [Batch 600/1094] [D loss: 0.364467] [G loss: 0.196531] [ema: 0.999706] 
[Epoch 21/46] [Batch 700/1094] [D loss: 0.398662] [G loss: 0.162788] [ema: 0.999707] 
[Epoch 21/46] [Batch 800/1094] [D loss: 0.378579] [G loss: 0.189639] [ema: 0.999708] 
[Epoch 21/46] [Batch 900/1094] [D loss: 0.410584] [G loss: 0.172724] [ema: 0.999710] 
[Epoch 21/46] [Batch 1000/1094] [D loss: 0.361943] [G loss: 0.194879] [ema: 0.999711] 
[Epoch 22/46] [Batch 0/1094] [D loss: 0.378588] [G loss: 0.177703] [ema: 0.999712] 
[Epoch 22/46] [Batch 100/1094] [D loss: 0.439682] [G loss: 0.168092] [ema: 0.999713] 
[Epoch 22/46] [Batch 200/1094] [D loss: 0.405676] [G loss: 0.162626] [ema: 0.999714] 
[Epoch 22/46] [Batch 300/1094] [D loss: 0.406002] [G loss: 0.152119] [ema: 0.999716] 
[Epoch 22/46] [Batch 400/1094] [D loss: 0.371867] [G loss: 0.178602] [ema: 0.999717] 
[Epoch 22/46] [Batch 500/1094] [D loss: 0.370294] [G loss: 0.168221] [ema: 0.999718] 
[Epoch 22/46] [Batch 600/1094] [D loss: 0.374915] [G loss: 0.178654] [ema: 0.999719] 
[Epoch 22/46] [Batch 700/1094] [D loss: 0.444872] [G loss: 0.188151] [ema: 0.999720] 
[Epoch 22/46] [Batch 800/1094] [D loss: 0.386794] [G loss: 0.187229] [ema: 0.999721] 
[Epoch 22/46] [Batch 900/1094] [D loss: 0.372197] [G loss: 0.178693] [ema: 0.999722] 
[Epoch 22/46] [Batch 1000/1094] [D loss: 0.405947] [G loss: 0.182630] [ema: 0.999724] 
[Epoch 23/46] [Batch 0/1094] [D loss: 0.376527] [G loss: 0.196528] [ema: 0.999725] 
[Epoch 23/46] [Batch 100/1094] [D loss: 0.426149] [G loss: 0.173441] [ema: 0.999726] 
[Epoch 23/46] [Batch 200/1094] [D loss: 0.415146] [G loss: 0.157607] [ema: 0.999727] 
[Epoch 23/46] [Batch 300/1094] [D loss: 0.377387] [G loss: 0.188644] [ema: 0.999728] 
[Epoch 23/46] [Batch 400/1094] [D loss: 0.432124] [G loss: 0.160107] [ema: 0.999729] 
[Epoch 23/46] [Batch 500/1094] [D loss: 0.401576] [G loss: 0.169376] [ema: 0.999730] 
[Epoch 23/46] [Batch 600/1094] [D loss: 0.414872] [G loss: 0.163072] [ema: 0.999731] 
[Epoch 23/46] [Batch 700/1094] [D loss: 0.397972] [G loss: 0.155290] [ema: 0.999732] 
[Epoch 23/46] [Batch 800/1094] [D loss: 0.435767] [G loss: 0.161549] [ema: 0.999733] 
[Epoch 23/46] [Batch 900/1094] [D loss: 0.430353] [G loss: 0.164229] [ema: 0.999734] 
[Epoch 23/46] [Batch 1000/1094] [D loss: 0.449227] [G loss: 0.173653] [ema: 0.999735] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_30_100/WISDM_DAGHAR_Multiclass_50000_D_30_2024_10_25_01_05_31/Model



[Epoch 24/46] [Batch 0/1094] [D loss: 0.433369] [G loss: 0.174281] [ema: 0.999736] 
[Epoch 24/46] [Batch 100/1094] [D loss: 0.398128] [G loss: 0.167575] [ema: 0.999737] 
[Epoch 24/46] [Batch 200/1094] [D loss: 0.390703] [G loss: 0.181369] [ema: 0.999738] 
[Epoch 24/46] [Batch 300/1094] [D loss: 0.420436] [G loss: 0.175258] [ema: 0.999739] 
[Epoch 24/46] [Batch 400/1094] [D loss: 0.391610] [G loss: 0.176771] [ema: 0.999740] 
[Epoch 24/46] [Batch 500/1094] [D loss: 0.417672] [G loss: 0.168306] [ema: 0.999741] 
[Epoch 24/46] [Batch 600/1094] [D loss: 0.422295] [G loss: 0.174809] [ema: 0.999742] 
[Epoch 24/46] [Batch 700/1094] [D loss: 0.443055] [G loss: 0.179842] [ema: 0.999743] 
[Epoch 24/46] [Batch 800/1094] [D loss: 0.392948] [G loss: 0.175053] [ema: 0.999744] 
[Epoch 24/46] [Batch 900/1094] [D loss: 0.445621] [G loss: 0.158165] [ema: 0.999745] 
[Epoch 24/46] [Batch 1000/1094] [D loss: 0.445115] [G loss: 0.162284] [ema: 0.999746] 
[Epoch 25/46] [Batch 0/1094] [D loss: 0.417077] [G loss: 0.184171] [ema: 0.999747] 
[Epoch 25/46] [Batch 100/1094] [D loss: 0.424526] [G loss: 0.169577] [ema: 0.999748] 
[Epoch 25/46] [Batch 200/1094] [D loss: 0.418350] [G loss: 0.187795] [ema: 0.999748] 
[Epoch 25/46] [Batch 300/1094] [D loss: 0.377124] [G loss: 0.184467] [ema: 0.999749] 
[Epoch 25/46] [Batch 400/1094] [D loss: 0.421333] [G loss: 0.192039] [ema: 0.999750] 
[Epoch 25/46] [Batch 500/1094] [D loss: 0.433515] [G loss: 0.176303] [ema: 0.999751] 
[Epoch 25/46] [Batch 600/1094] [D loss: 0.428636] [G loss: 0.178090] [ema: 0.999752] 
[Epoch 25/46] [Batch 700/1094] [D loss: 0.431548] [G loss: 0.165649] [ema: 0.999753] 
[Epoch 25/46] [Batch 800/1094] [D loss: 0.397453] [G loss: 0.185924] [ema: 0.999754] 
[Epoch 25/46] [Batch 900/1094] [D loss: 0.391127] [G loss: 0.187271] [ema: 0.999755] 
[Epoch 25/46] [Batch 1000/1094] [D loss: 0.372745] [G loss: 0.149179] [ema: 0.999756] 
[Epoch 26/46] [Batch 0/1094] [D loss: 0.396555] [G loss: 0.179368] [ema: 0.999756] 
[Epoch 26/46] [Batch 100/1094] [D loss: 0.382364] [G loss: 0.173211] [ema: 0.999757] 
[Epoch 26/46] [Batch 200/1094] [D loss: 0.424011] [G loss: 0.176722] [ema: 0.999758] 
[Epoch 26/46] [Batch 300/1094] [D loss: 0.388934] [G loss: 0.197511] [ema: 0.999759] 
[Epoch 26/46] [Batch 400/1094] [D loss: 0.402559] [G loss: 0.163497] [ema: 0.999760] 
[Epoch 26/46] [Batch 500/1094] [D loss: 0.455652] [G loss: 0.170916] [ema: 0.999761] 
[Epoch 26/46] [Batch 600/1094] [D loss: 0.377216] [G loss: 0.166819] [ema: 0.999761] 
[Epoch 26/46] [Batch 700/1094] [D loss: 0.405857] [G loss: 0.158223] [ema: 0.999762] 
[Epoch 26/46] [Batch 800/1094] [D loss: 0.488597] [G loss: 0.163889] [ema: 0.999763] 
[Epoch 26/46] [Batch 900/1094] [D loss: 0.431553] [G loss: 0.154555] [ema: 0.999764] 
[Epoch 26/46] [Batch 1000/1094] [D loss: 0.422645] [G loss: 0.163380] [ema: 0.999765] 
[Epoch 27/46] [Batch 0/1094] [D loss: 0.463740] [G loss: 0.181470] [ema: 0.999765] 
[Epoch 27/46] [Batch 100/1094] [D loss: 0.450561] [G loss: 0.163327] [ema: 0.999766] 
[Epoch 27/46] [Batch 200/1094] [D loss: 0.393470] [G loss: 0.171887] [ema: 0.999767] 
[Epoch 27/46] [Batch 300/1094] [D loss: 0.453022] [G loss: 0.169462] [ema: 0.999768] 
[Epoch 27/46] [Batch 400/1094] [D loss: 0.463125] [G loss: 0.161917] [ema: 0.999768] 
[Epoch 27/46] [Batch 500/1094] [D loss: 0.418416] [G loss: 0.165339] [ema: 0.999769] 
[Epoch 27/46] [Batch 600/1094] [D loss: 0.397587] [G loss: 0.173451] [ema: 0.999770] 
[Epoch 27/46] [Batch 700/1094] [D loss: 0.416945] [G loss: 0.169395] [ema: 0.999771] 
[Epoch 27/46] [Batch 800/1094] [D loss: 0.442569] [G loss: 0.173641] [ema: 0.999772] 
[Epoch 27/46] [Batch 900/1094] [D loss: 0.426638] [G loss: 0.155856] [ema: 0.999772] 
[Epoch 27/46] [Batch 1000/1094] [D loss: 0.425340] [G loss: 0.171319] [ema: 0.999773] 
[Epoch 28/46] [Batch 0/1094] [D loss: 0.473263] [G loss: 0.177948] [ema: 0.999774] 
[Epoch 28/46] [Batch 100/1094] [D loss: 0.456611] [G loss: 0.166351] [ema: 0.999774] 
[Epoch 28/46] [Batch 200/1094] [D loss: 0.419569] [G loss: 0.166921] [ema: 0.999775] 
[Epoch 28/46] [Batch 300/1094] [D loss: 0.420043] [G loss: 0.171667] [ema: 0.999776] 
[Epoch 28/46] [Batch 400/1094] [D loss: 0.403918] [G loss: 0.152106] [ema: 0.999777] 
[Epoch 28/46] [Batch 500/1094] [D loss: 0.439416] [G loss: 0.162642] [ema: 0.999777] 
[Epoch 28/46] [Batch 600/1094] [D loss: 0.458149] [G loss: 0.154074] [ema: 0.999778] 
[Epoch 28/46] [Batch 700/1094] [D loss: 0.446823] [G loss: 0.166786] [ema: 0.999779] 
[Epoch 28/46] [Batch 800/1094] [D loss: 0.450946] [G loss: 0.164190] [ema: 0.999780] 
[Epoch 28/46] [Batch 900/1094] [D loss: 0.432709] [G loss: 0.165097] [ema: 0.999780] 
[Epoch 28/46] [Batch 1000/1094] [D loss: 0.443387] [G loss: 0.144201] [ema: 0.999781] 
[Epoch 29/46] [Batch 0/1094] [D loss: 0.460681] [G loss: 0.158789] [ema: 0.999782] 
[Epoch 29/46] [Batch 100/1094] [D loss: 0.444934] [G loss: 0.160630] [ema: 0.999782] 
[Epoch 29/46] [Batch 200/1094] [D loss: 0.442299] [G loss: 0.165019] [ema: 0.999783] 
[Epoch 29/46] [Batch 300/1094] [D loss: 0.449135] [G loss: 0.160748] [ema: 0.999784] 
[Epoch 29/46] [Batch 400/1094] [D loss: 0.440913] [G loss: 0.164459] [ema: 0.999784] 
[Epoch 29/46] [Batch 500/1094] [D loss: 0.418789] [G loss: 0.165275] [ema: 0.999785] 
[Epoch 29/46] [Batch 600/1094] [D loss: 0.429101] [G loss: 0.159017] [ema: 0.999786] 
[Epoch 29/46] [Batch 700/1094] [D loss: 0.432188] [G loss: 0.168988] [ema: 0.999786] 
[Epoch 29/46] [Batch 800/1094] [D loss: 0.457596] [G loss: 0.152709] [ema: 0.999787] 
[Epoch 29/46] [Batch 900/1094] [D loss: 0.446715] [G loss: 0.177909] [ema: 0.999788] 
[Epoch 29/46] [Batch 1000/1094] [D loss: 0.433245] [G loss: 0.156319] [ema: 0.999788] 
[Epoch 30/46] [Batch 0/1094] [D loss: 0.450889] [G loss: 0.171802] [ema: 0.999789] 
[Epoch 30/46] [Batch 100/1094] [D loss: 0.452552] [G loss: 0.159361] [ema: 0.999789] 
[Epoch 30/46] [Batch 200/1094] [D loss: 0.439253] [G loss: 0.168971] [ema: 0.999790] 
[Epoch 30/46] [Batch 300/1094] [D loss: 0.420551] [G loss: 0.170260] [ema: 0.999791] 
[Epoch 30/46] [Batch 400/1094] [D loss: 0.450292] [G loss: 0.165065] [ema: 0.999791] 
[Epoch 30/46] [Batch 500/1094] [D loss: 0.384697] [G loss: 0.164452] [ema: 0.999792] 
[Epoch 30/46] [Batch 600/1094] [D loss: 0.434698] [G loss: 0.167864] [ema: 0.999793] 
[Epoch 30/46] [Batch 700/1094] [D loss: 0.412563] [G loss: 0.167587] [ema: 0.999793] 
[Epoch 30/46] [Batch 800/1094] [D loss: 0.419917] [G loss: 0.159781] [ema: 0.999794] 
[Epoch 30/46] [Batch 900/1094] [D loss: 0.392412] [G loss: 0.174060] [ema: 0.999794] 
[Epoch 30/46] [Batch 1000/1094] [D loss: 0.438134] [G loss: 0.154336] [ema: 0.999795] 
[Epoch 31/46] [Batch 0/1094] [D loss: 0.430035] [G loss: 0.154668] [ema: 0.999796] 
[Epoch 31/46] [Batch 100/1094] [D loss: 0.416590] [G loss: 0.164497] [ema: 0.999796] 
[Epoch 31/46] [Batch 200/1094] [D loss: 0.412918] [G loss: 0.159666] [ema: 0.999797] 
[Epoch 31/46] [Batch 300/1094] [D loss: 0.472122] [G loss: 0.156071] [ema: 0.999797] 
[Epoch 31/46] [Batch 400/1094] [D loss: 0.452891] [G loss: 0.155951] [ema: 0.999798] 
[Epoch 31/46] [Batch 500/1094] [D loss: 0.410347] [G loss: 0.167520] [ema: 0.999799] 
[Epoch 31/46] [Batch 600/1094] [D loss: 0.457102] [G loss: 0.171706] [ema: 0.999799] 
[Epoch 31/46] [Batch 700/1094] [D loss: 0.481981] [G loss: 0.150975] [ema: 0.999800] 
[Epoch 31/46] [Batch 800/1094] [D loss: 0.412414] [G loss: 0.162936] [ema: 0.999800] 
[Epoch 31/46] [Batch 900/1094] [D loss: 0.451886] [G loss: 0.165791] [ema: 0.999801] 
[Epoch 31/46] [Batch 1000/1094] [D loss: 0.442143] [G loss: 0.159894] [ema: 0.999801] 
[Epoch 32/46] [Batch 0/1094] [D loss: 0.446484] [G loss: 0.173126] [ema: 0.999802] 
[Epoch 32/46] [Batch 100/1094] [D loss: 0.450310] [G loss: 0.162017] [ema: 0.999803] 
[Epoch 32/46] [Batch 200/1094] [D loss: 0.442526] [G loss: 0.166024] [ema: 0.999803] 
[Epoch 32/46] [Batch 300/1094] [D loss: 0.443108] [G loss: 0.165498] [ema: 0.999804] 
[Epoch 32/46] [Batch 400/1094] [D loss: 0.458963] [G loss: 0.172419] [ema: 0.999804] 
[Epoch 32/46] [Batch 500/1094] [D loss: 0.432138] [G loss: 0.158520] [ema: 0.999805] 
[Epoch 32/46] [Batch 600/1094] [D loss: 0.440541] [G loss: 0.155650] [ema: 0.999805] 
[Epoch 32/46] [Batch 700/1094] [D loss: 0.426344] [G loss: 0.158494] [ema: 0.999806] 
[Epoch 32/46] [Batch 800/1094] [D loss: 0.420077] [G loss: 0.165951] [ema: 0.999806] 
[Epoch 32/46] [Batch 900/1094] [D loss: 0.446533] [G loss: 0.161271] [ema: 0.999807] 
[Epoch 32/46] [Batch 1000/1094] [D loss: 0.439950] [G loss: 0.146924] [ema: 0.999808] 
[Epoch 33/46] [Batch 0/1094] [D loss: 0.449546] [G loss: 0.158007] [ema: 0.999808] 
[Epoch 33/46] [Batch 100/1094] [D loss: 0.456322] [G loss: 0.163744] [ema: 0.999809] 
[Epoch 33/46] [Batch 200/1094] [D loss: 0.449126] [G loss: 0.156049] [ema: 0.999809] 
[Epoch 33/46] [Batch 300/1094] [D loss: 0.439597] [G loss: 0.164571] [ema: 0.999810] 
[Epoch 33/46] [Batch 400/1094] [D loss: 0.470918] [G loss: 0.165344] [ema: 0.999810] 
[Epoch 33/46] [Batch 500/1094] [D loss: 0.427397] [G loss: 0.178580] [ema: 0.999811] 
[Epoch 33/46] [Batch 600/1094] [D loss: 0.416457] [G loss: 0.158534] [ema: 0.999811] 
[Epoch 33/46] [Batch 700/1094] [D loss: 0.423928] [G loss: 0.163332] [ema: 0.999812] 
[Epoch 33/46] [Batch 800/1094] [D loss: 0.454977] [G loss: 0.148698] [ema: 0.999812] 
[Epoch 33/46] [Batch 900/1094] [D loss: 0.453328] [G loss: 0.159843] [ema: 0.999813] 
[Epoch 33/46] [Batch 1000/1094] [D loss: 0.440130] [G loss: 0.161538] [ema: 0.999813] 
[Epoch 34/46] [Batch 0/1094] [D loss: 0.466154] [G loss: 0.157287] [ema: 0.999814] 
[Epoch 34/46] [Batch 100/1094] [D loss: 0.448794] [G loss: 0.172592] [ema: 0.999814] 
[Epoch 34/46] [Batch 200/1094] [D loss: 0.427670] [G loss: 0.162503] [ema: 0.999815] 
[Epoch 34/46] [Batch 300/1094] [D loss: 0.512935] [G loss: 0.145854] [ema: 0.999815] 
[Epoch 34/46] [Batch 400/1094] [D loss: 0.476503] [G loss: 0.158506] [ema: 0.999816] 
[Epoch 34/46] [Batch 500/1094] [D loss: 0.472990] [G loss: 0.162546] [ema: 0.999816] 
[Epoch 34/46] [Batch 600/1094] [D loss: 0.481728] [G loss: 0.159479] [ema: 0.999817] 
[Epoch 34/46] [Batch 700/1094] [D loss: 0.473721] [G loss: 0.151854] [ema: 0.999817] 
[Epoch 34/46] [Batch 800/1094] [D loss: 0.453394] [G loss: 0.156628] [ema: 0.999818] 
[Epoch 34/46] [Batch 900/1094] [D loss: 0.447413] [G loss: 0.156824] [ema: 0.999818] 
[Epoch 34/46] [Batch 1000/1094] [D loss: 0.462185] [G loss: 0.149719] [ema: 0.999819] 
[Epoch 35/46] [Batch 0/1094] [D loss: 0.483088] [G loss: 0.160313] [ema: 0.999819] 
[Epoch 35/46] [Batch 100/1094] [D loss: 0.507941] [G loss: 0.132218] [ema: 0.999819] 
[Epoch 35/46] [Batch 200/1094] [D loss: 0.483573] [G loss: 0.154317] [ema: 0.999820] 
[Epoch 35/46] [Batch 300/1094] [D loss: 0.498802] [G loss: 0.130324] [ema: 0.999820] 
[Epoch 35/46] [Batch 400/1094] [D loss: 0.461804] [G loss: 0.138277] [ema: 0.999821] 
[Epoch 35/46] [Batch 500/1094] [D loss: 0.481051] [G loss: 0.139705] [ema: 0.999821] 
[Epoch 35/46] [Batch 600/1094] [D loss: 0.488330] [G loss: 0.134730] [ema: 0.999822] 
[Epoch 35/46] [Batch 700/1094] [D loss: 0.515121] [G loss: 0.125763] [ema: 0.999822] 
[Epoch 35/46] [Batch 800/1094] [D loss: 0.532129] [G loss: 0.142368] [ema: 0.999823] 
[Epoch 35/46] [Batch 900/1094] [D loss: 0.496795] [G loss: 0.129518] [ema: 0.999823] 
[Epoch 35/46] [Batch 1000/1094] [D loss: 0.495869] [G loss: 0.121597] [ema: 0.999824] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_30_100/WISDM_DAGHAR_Multiclass_50000_D_30_2024_10_25_01_05_31/Model



[Epoch 36/46] [Batch 0/1094] [D loss: 0.505396] [G loss: 0.130241] [ema: 0.999824] 
[Epoch 36/46] [Batch 100/1094] [D loss: 0.487873] [G loss: 0.146664] [ema: 0.999824] 
[Epoch 36/46] [Batch 200/1094] [D loss: 0.547692] [G loss: 0.118403] [ema: 0.999825] 
[Epoch 36/46] [Batch 300/1094] [D loss: 0.520704] [G loss: 0.119886] [ema: 0.999825] 
[Epoch 36/46] [Batch 400/1094] [D loss: 0.485278] [G loss: 0.132871] [ema: 0.999826] 
[Epoch 36/46] [Batch 500/1094] [D loss: 0.497190] [G loss: 0.139167] [ema: 0.999826] 
[Epoch 36/46] [Batch 600/1094] [D loss: 0.534382] [G loss: 0.115180] [ema: 0.999827] 
[Epoch 36/46] [Batch 700/1094] [D loss: 0.544582] [G loss: 0.122229] [ema: 0.999827] 
[Epoch 36/46] [Batch 800/1094] [D loss: 0.561334] [G loss: 0.108176] [ema: 0.999828] 
[Epoch 36/46] [Batch 900/1094] [D loss: 0.506369] [G loss: 0.125490] [ema: 0.999828] 
[Epoch 36/46] [Batch 1000/1094] [D loss: 0.523411] [G loss: 0.135382] [ema: 0.999828] 
[Epoch 37/46] [Batch 0/1094] [D loss: 0.499622] [G loss: 0.128330] [ema: 0.999829] 
[Epoch 37/46] [Batch 100/1094] [D loss: 0.562351] [G loss: 0.117199] [ema: 0.999829] 
[Epoch 37/46] [Batch 200/1094] [D loss: 0.507992] [G loss: 0.129371] [ema: 0.999830] 
[Epoch 37/46] [Batch 300/1094] [D loss: 0.512809] [G loss: 0.141086] [ema: 0.999830] 
[Epoch 37/46] [Batch 400/1094] [D loss: 0.561571] [G loss: 0.128439] [ema: 0.999830] 
[Epoch 37/46] [Batch 500/1094] [D loss: 0.469573] [G loss: 0.136153] [ema: 0.999831] 
[Epoch 37/46] [Batch 600/1094] [D loss: 0.505537] [G loss: 0.144735] [ema: 0.999831] 
[Epoch 37/46] [Batch 700/1094] [D loss: 0.493361] [G loss: 0.140614] [ema: 0.999832] 
[Epoch 37/46] [Batch 800/1094] [D loss: 0.460063] [G loss: 0.139343] [ema: 0.999832] 
[Epoch 37/46] [Batch 900/1094] [D loss: 0.474640] [G loss: 0.112041] [ema: 0.999832] 
[Epoch 37/46] [Batch 1000/1094] [D loss: 0.481675] [G loss: 0.113944] [ema: 0.999833] 
[Epoch 38/46] [Batch 0/1094] [D loss: 0.465951] [G loss: 0.154544] [ema: 0.999833] 
[Epoch 38/46] [Batch 100/1094] [D loss: 0.397810] [G loss: 0.158889] [ema: 0.999834] 
[Epoch 38/46] [Batch 200/1094] [D loss: 0.453659] [G loss: 0.152574] [ema: 0.999834] 
[Epoch 38/46] [Batch 300/1094] [D loss: 0.465857] [G loss: 0.132490] [ema: 0.999834] 
[Epoch 38/46] [Batch 400/1094] [D loss: 0.467685] [G loss: 0.137845] [ema: 0.999835] 
[Epoch 38/46] [Batch 500/1094] [D loss: 0.502016] [G loss: 0.124175] [ema: 0.999835] 
[Epoch 38/46] [Batch 600/1094] [D loss: 0.479960] [G loss: 0.170310] [ema: 0.999836] 
[Epoch 38/46] [Batch 700/1094] [D loss: 0.486292] [G loss: 0.170231] [ema: 0.999836] 
[Epoch 38/46] [Batch 800/1094] [D loss: 0.442581] [G loss: 0.133028] [ema: 0.999836] 
[Epoch 38/46] [Batch 900/1094] [D loss: 0.462578] [G loss: 0.135454] [ema: 0.999837] 
[Epoch 38/46] [Batch 1000/1094] [D loss: 0.544452] [G loss: 0.138404] [ema: 0.999837] 
[Epoch 39/46] [Batch 0/1094] [D loss: 0.491562] [G loss: 0.133357] [ema: 0.999838] 
[Epoch 39/46] [Batch 100/1094] [D loss: 0.455294] [G loss: 0.182990] [ema: 0.999838] 
[Epoch 39/46] [Batch 200/1094] [D loss: 0.409938] [G loss: 0.177266] [ema: 0.999838] 
[Epoch 39/46] [Batch 300/1094] [D loss: 0.445422] [G loss: 0.143913] [ema: 0.999839] 
[Epoch 39/46] [Batch 400/1094] [D loss: 0.501262] [G loss: 0.127076] [ema: 0.999839] 
[Epoch 39/46] [Batch 500/1094] [D loss: 0.497454] [G loss: 0.150220] [ema: 0.999839] 
[Epoch 39/46] [Batch 600/1094] [D loss: 0.506526] [G loss: 0.151142] [ema: 0.999840] 
[Epoch 39/46] [Batch 700/1094] [D loss: 0.467168] [G loss: 0.139822] [ema: 0.999840] 
[Epoch 39/46] [Batch 800/1094] [D loss: 0.549535] [G loss: 0.156473] [ema: 0.999841] 
[Epoch 39/46] [Batch 900/1094] [D loss: 0.480023] [G loss: 0.142643] [ema: 0.999841] 
[Epoch 39/46] [Batch 1000/1094] [D loss: 0.490101] [G loss: 0.162626] [ema: 0.999841] 
[Epoch 40/46] [Batch 0/1094] [D loss: 0.444394] [G loss: 0.170299] [ema: 0.999842] 
[Epoch 40/46] [Batch 100/1094] [D loss: 0.449203] [G loss: 0.151644] [ema: 0.999842] 
[Epoch 40/46] [Batch 200/1094] [D loss: 0.464847] [G loss: 0.165028] [ema: 0.999842] 
[Epoch 40/46] [Batch 300/1094] [D loss: 0.439735] [G loss: 0.146307] [ema: 0.999843] 
[Epoch 40/46] [Batch 400/1094] [D loss: 0.493946] [G loss: 0.129604] [ema: 0.999843] 
[Epoch 40/46] [Batch 500/1094] [D loss: 0.436478] [G loss: 0.158206] [ema: 0.999843] 
[Epoch 40/46] [Batch 600/1094] [D loss: 0.457654] [G loss: 0.138483] [ema: 0.999844] 
[Epoch 40/46] [Batch 700/1094] [D loss: 0.474273] [G loss: 0.155850] [ema: 0.999844] 
[Epoch 40/46] [Batch 800/1094] [D loss: 0.510360] [G loss: 0.175960] [ema: 0.999844] 
[Epoch 40/46] [Batch 900/1094] [D loss: 0.471894] [G loss: 0.144782] [ema: 0.999845] 
[Epoch 40/46] [Batch 1000/1094] [D loss: 0.505005] [G loss: 0.162930] [ema: 0.999845] 
[Epoch 41/46] [Batch 0/1094] [D loss: 0.467377] [G loss: 0.162398] [ema: 0.999845] 
[Epoch 41/46] [Batch 100/1094] [D loss: 0.443472] [G loss: 0.141406] [ema: 0.999846] 
[Epoch 41/46] [Batch 200/1094] [D loss: 0.435179] [G loss: 0.163029] [ema: 0.999846] 
[Epoch 41/46] [Batch 300/1094] [D loss: 0.420193] [G loss: 0.164254] [ema: 0.999847] 
[Epoch 41/46] [Batch 400/1094] [D loss: 0.520756] [G loss: 0.160774] [ema: 0.999847] 
[Epoch 41/46] [Batch 500/1094] [D loss: 0.461171] [G loss: 0.142572] [ema: 0.999847] 
[Epoch 41/46] [Batch 600/1094] [D loss: 0.455437] [G loss: 0.158253] [ema: 0.999848] 
[Epoch 41/46] [Batch 700/1094] [D loss: 0.469811] [G loss: 0.155636] [ema: 0.999848] 
[Epoch 41/46] [Batch 800/1094] [D loss: 0.424306] [G loss: 0.154885] [ema: 0.999848] 
[Epoch 41/46] [Batch 900/1094] [D loss: 0.467775] [G loss: 0.137805] [ema: 0.999849] 
[Epoch 41/46] [Batch 1000/1094] [D loss: 0.453265] [G loss: 0.162326] [ema: 0.999849] 
[Epoch 42/46] [Batch 0/1094] [D loss: 0.492588] [G loss: 0.147240] [ema: 0.999849] 
[Epoch 42/46] [Batch 100/1094] [D loss: 0.456358] [G loss: 0.146326] [ema: 0.999849] 
[Epoch 42/46] [Batch 200/1094] [D loss: 0.463072] [G loss: 0.154956] [ema: 0.999850] 
[Epoch 42/46] [Batch 300/1094] [D loss: 0.474416] [G loss: 0.158156] [ema: 0.999850] 
[Epoch 42/46] [Batch 400/1094] [D loss: 0.429704] [G loss: 0.174289] [ema: 0.999850] 
[Epoch 42/46] [Batch 500/1094] [D loss: 0.456378] [G loss: 0.142171] [ema: 0.999851] 
[Epoch 42/46] [Batch 600/1094] [D loss: 0.486931] [G loss: 0.155678] [ema: 0.999851] 
[Epoch 42/46] [Batch 700/1094] [D loss: 0.492836] [G loss: 0.144936] [ema: 0.999851] 
[Epoch 42/46] [Batch 800/1094] [D loss: 0.463484] [G loss: 0.148261] [ema: 0.999852] 
[Epoch 42/46] [Batch 900/1094] [D loss: 0.492033] [G loss: 0.140635] [ema: 0.999852] 
[Epoch 42/46] [Batch 1000/1094] [D loss: 0.505266] [G loss: 0.149498] [ema: 0.999852] 
[Epoch 43/46] [Batch 0/1094] [D loss: 0.449651] [G loss: 0.146826] [ema: 0.999853] 
[Epoch 43/46] [Batch 100/1094] [D loss: 0.439162] [G loss: 0.160235] [ema: 0.999853] 
[Epoch 43/46] [Batch 200/1094] [D loss: 0.462428] [G loss: 0.145291] [ema: 0.999853] 
[Epoch 43/46] [Batch 300/1094] [D loss: 0.450812] [G loss: 0.147651] [ema: 0.999854] 
[Epoch 43/46] [Batch 400/1094] [D loss: 0.457323] [G loss: 0.149208] [ema: 0.999854] 
[Epoch 43/46] [Batch 500/1094] [D loss: 0.480360] [G loss: 0.136889] [ema: 0.999854] 
[Epoch 43/46] [Batch 600/1094] [D loss: 0.481610] [G loss: 0.154539] [ema: 0.999855] 
[Epoch 43/46] [Batch 700/1094] [D loss: 0.476859] [G loss: 0.148520] [ema: 0.999855] 
[Epoch 43/46] [Batch 800/1094] [D loss: 0.494436] [G loss: 0.139508] [ema: 0.999855] 
[Epoch 43/46] [Batch 900/1094] [D loss: 0.473068] [G loss: 0.152713] [ema: 0.999855] 
[Epoch 43/46] [Batch 1000/1094] [D loss: 0.447417] [G loss: 0.152077] [ema: 0.999856] 
[Epoch 44/46] [Batch 0/1094] [D loss: 0.420046] [G loss: 0.150939] [ema: 0.999856] 
[Epoch 44/46] [Batch 100/1094] [D loss: 0.488206] [G loss: 0.147669] [ema: 0.999856] 
[Epoch 44/46] [Batch 200/1094] [D loss: 0.478342] [G loss: 0.147821] [ema: 0.999857] 
[Epoch 44/46] [Batch 300/1094] [D loss: 0.459789] [G loss: 0.154540] [ema: 0.999857] 
[Epoch 44/46] [Batch 400/1094] [D loss: 0.482334] [G loss: 0.140306] [ema: 0.999857] 
[Epoch 44/46] [Batch 500/1094] [D loss: 0.416644] [G loss: 0.160395] [ema: 0.999857] 
[Epoch 44/46] [Batch 600/1094] [D loss: 0.439384] [G loss: 0.160493] [ema: 0.999858] 
[Epoch 44/46] [Batch 700/1094] [D loss: 0.488712] [G loss: 0.138644] [ema: 0.999858] 
[Epoch 44/46] [Batch 800/1094] [D loss: 0.402898] [G loss: 0.159481] [ema: 0.999858] 
[Epoch 44/46] [Batch 900/1094] [D loss: 0.456188] [G loss: 0.155161] [ema: 0.999859] 
[Epoch 44/46] [Batch 1000/1094] [D loss: 0.476661] [G loss: 0.149848] [ema: 0.999859] 
[Epoch 45/46] [Batch 0/1094] [D loss: 0.491183] [G loss: 0.164055] [ema: 0.999859] 
[Epoch 45/46] [Batch 100/1094] [D loss: 0.426198] [G loss: 0.164934] [ema: 0.999859] 
[Epoch 45/46] [Batch 200/1094] [D loss: 0.476599] [G loss: 0.141852] [ema: 0.999860] 
[Epoch 45/46] [Batch 300/1094] [D loss: 0.449268] [G loss: 0.156126] [ema: 0.999860] 
[Epoch 45/46] [Batch 400/1094] [D loss: 0.456812] [G loss: 0.147987] [ema: 0.999860] 
[Epoch 45/46] [Batch 500/1094] [D loss: 0.462988] [G loss: 0.146228] [ema: 0.999861] 
[Epoch 45/46] [Batch 600/1094] [D loss: 0.487988] [G loss: 0.148360] [ema: 0.999861] 
[Epoch 45/46] [Batch 700/1094] [D loss: 0.459863] [G loss: 0.158221] [ema: 0.999861] 
[Epoch 45/46] [Batch 800/1094] [D loss: 0.441275] [G loss: 0.158808] [ema: 0.999861] 
[Epoch 45/46] [Batch 900/1094] [D loss: 0.465057] [G loss: 0.155389] [ema: 0.999862] 
[Epoch 45/46] [Batch 1000/1094] [D loss: 0.523207] [G loss: 0.157768] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
UCI_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
UCI_DAGHAR_Multiclass
daghar
return single class data and labels, class is UCI_DAGHAR_Multiclass
data shape is (4840, 3, 1, 30)
label shape is (4840,)
303
Epochs between checkpoint: 42



Saving checkpoint 1 in logs/daghar_split_dataset_50000_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_25_01_37_21/Model



[Epoch 0/166] [Batch 0/303] [D loss: 1.130715] [G loss: 0.810689] [ema: 0.000000] 
[Epoch 0/166] [Batch 100/303] [D loss: 0.532435] [G loss: 0.133022] [ema: 0.933033] 
[Epoch 0/166] [Batch 200/303] [D loss: 0.578963] [G loss: 0.118706] [ema: 0.965936] 
[Epoch 0/166] [Batch 300/303] [D loss: 0.484714] [G loss: 0.156255] [ema: 0.977160] 
[Epoch 1/166] [Batch 0/303] [D loss: 0.512400] [G loss: 0.136216] [ema: 0.977384] 
[Epoch 1/166] [Batch 100/303] [D loss: 0.439875] [G loss: 0.170365] [ema: 0.982947] 
[Epoch 1/166] [Batch 200/303] [D loss: 0.454257] [G loss: 0.123642] [ema: 0.986314] 
[Epoch 1/166] [Batch 300/303] [D loss: 0.523910] [G loss: 0.141695] [ema: 0.988571] 
[Epoch 2/166] [Batch 0/303] [D loss: 0.545423] [G loss: 0.145404] [ema: 0.988627] 
[Epoch 2/166] [Batch 100/303] [D loss: 0.427842] [G loss: 0.168006] [ema: 0.990230] 
[Epoch 2/166] [Batch 200/303] [D loss: 0.464897] [G loss: 0.126429] [ema: 0.991437] 
[Epoch 2/166] [Batch 300/303] [D loss: 0.424788] [G loss: 0.157248] [ema: 0.992379] 
[Epoch 3/166] [Batch 0/303] [D loss: 0.443368] [G loss: 0.170262] [ema: 0.992404] 
[Epoch 3/166] [Batch 100/303] [D loss: 0.547571] [G loss: 0.132978] [ema: 0.993154] 
[Epoch 3/166] [Batch 200/303] [D loss: 0.469578] [G loss: 0.140982] [ema: 0.993769] 
[Epoch 3/166] [Batch 300/303] [D loss: 0.509014] [G loss: 0.140952] [ema: 0.994283] 
[Epoch 4/166] [Batch 0/303] [D loss: 0.483431] [G loss: 0.130229] [ema: 0.994297] 
[Epoch 4/166] [Batch 100/303] [D loss: 0.540929] [G loss: 0.136349] [ema: 0.994731] 
[Epoch 4/166] [Batch 200/303] [D loss: 0.479675] [G loss: 0.143122] [ema: 0.995103] 
[Epoch 4/166] [Batch 300/303] [D loss: 0.467767] [G loss: 0.144516] [ema: 0.995426] 
[Epoch 5/166] [Batch 0/303] [D loss: 0.448502] [G loss: 0.162446] [ema: 0.995435] 
[Epoch 5/166] [Batch 100/303] [D loss: 0.500383] [G loss: 0.128584] [ema: 0.995717] 
[Epoch 5/166] [Batch 200/303] [D loss: 0.572985] [G loss: 0.114082] [ema: 0.995966] 
[Epoch 5/166] [Batch 300/303] [D loss: 0.404225] [G loss: 0.164806] [ema: 0.996188] 
[Epoch 6/166] [Batch 0/303] [D loss: 0.467253] [G loss: 0.146842] [ema: 0.996195] 
[Epoch 6/166] [Batch 100/303] [D loss: 0.498929] [G loss: 0.152020] [ema: 0.996393] 
[Epoch 6/166] [Batch 200/303] [D loss: 0.479950] [G loss: 0.142151] [ema: 0.996571] 
[Epoch 6/166] [Batch 300/303] [D loss: 0.426560] [G loss: 0.151095] [ema: 0.996733] 
[Epoch 7/166] [Batch 0/303] [D loss: 0.467400] [G loss: 0.157922] [ema: 0.996737] 
[Epoch 7/166] [Batch 100/303] [D loss: 0.418541] [G loss: 0.166347] [ema: 0.996884] 
[Epoch 7/166] [Batch 200/303] [D loss: 0.395439] [G loss: 0.177145] [ema: 0.997018] 
[Epoch 7/166] [Batch 300/303] [D loss: 0.425578] [G loss: 0.164748] [ema: 0.997141] 
[Epoch 8/166] [Batch 0/303] [D loss: 0.394947] [G loss: 0.163131] [ema: 0.997145] 
[Epoch 8/166] [Batch 100/303] [D loss: 0.392473] [G loss: 0.173732] [ema: 0.997258] 
[Epoch 8/166] [Batch 200/303] [D loss: 0.373814] [G loss: 0.182912] [ema: 0.997362] 
[Epoch 8/166] [Batch 300/303] [D loss: 0.433022] [G loss: 0.168702] [ema: 0.997459] 
[Epoch 9/166] [Batch 0/303] [D loss: 0.424496] [G loss: 0.184318] [ema: 0.997461] 
[Epoch 9/166] [Batch 100/303] [D loss: 0.402661] [G loss: 0.193529] [ema: 0.997551] 
[Epoch 9/166] [Batch 200/303] [D loss: 0.371689] [G loss: 0.195592] [ema: 0.997635] 
[Epoch 9/166] [Batch 300/303] [D loss: 0.375336] [G loss: 0.181943] [ema: 0.997713] 
[Epoch 10/166] [Batch 0/303] [D loss: 0.342419] [G loss: 0.231511] [ema: 0.997715] 
[Epoch 10/166] [Batch 100/303] [D loss: 0.440546] [G loss: 0.192095] [ema: 0.997788] 
[Epoch 10/166] [Batch 200/303] [D loss: 0.326802] [G loss: 0.205109] [ema: 0.997856] 
[Epoch 10/166] [Batch 300/303] [D loss: 0.363951] [G loss: 0.218382] [ema: 0.997921] 
[Epoch 11/166] [Batch 0/303] [D loss: 0.406935] [G loss: 0.196256] [ema: 0.997923] 
[Epoch 11/166] [Batch 100/303] [D loss: 0.393758] [G loss: 0.158095] [ema: 0.997983] 
[Epoch 11/166] [Batch 200/303] [D loss: 0.394396] [G loss: 0.164092] [ema: 0.998040] 
[Epoch 11/166] [Batch 300/303] [D loss: 0.392760] [G loss: 0.184052] [ema: 0.998094] 
[Epoch 12/166] [Batch 0/303] [D loss: 0.408816] [G loss: 0.202107] [ema: 0.998095] 
[Epoch 12/166] [Batch 100/303] [D loss: 0.397515] [G loss: 0.155074] [ema: 0.998146] 
[Epoch 12/166] [Batch 200/303] [D loss: 0.395856] [G loss: 0.180458] [ema: 0.998195] 
[Epoch 12/166] [Batch 300/303] [D loss: 0.402695] [G loss: 0.194377] [ema: 0.998241] 
[Epoch 13/166] [Batch 0/303] [D loss: 0.398109] [G loss: 0.196470] [ema: 0.998242] 
[Epoch 13/166] [Batch 100/303] [D loss: 0.396932] [G loss: 0.161098] [ema: 0.998285] 
[Epoch 13/166] [Batch 200/303] [D loss: 0.376937] [G loss: 0.170158] [ema: 0.998327] 
[Epoch 13/166] [Batch 300/303] [D loss: 0.409538] [G loss: 0.180462] [ema: 0.998366] 
[Epoch 14/166] [Batch 0/303] [D loss: 0.400158] [G loss: 0.203487] [ema: 0.998367] 
[Epoch 14/166] [Batch 100/303] [D loss: 0.398262] [G loss: 0.178245] [ema: 0.998405] 
[Epoch 14/166] [Batch 200/303] [D loss: 0.416358] [G loss: 0.181275] [ema: 0.998441] 
[Epoch 14/166] [Batch 300/303] [D loss: 0.414044] [G loss: 0.177858] [ema: 0.998475] 
[Epoch 15/166] [Batch 0/303] [D loss: 0.413320] [G loss: 0.183239] [ema: 0.998476] 
[Epoch 15/166] [Batch 100/303] [D loss: 0.387230] [G loss: 0.194106] [ema: 0.998509] 
[Epoch 15/166] [Batch 200/303] [D loss: 0.395048] [G loss: 0.178596] [ema: 0.998540] 
[Epoch 15/166] [Batch 300/303] [D loss: 0.412004] [G loss: 0.180067] [ema: 0.998570] 
[Epoch 16/166] [Batch 0/303] [D loss: 0.420066] [G loss: 0.184597] [ema: 0.998571] 
[Epoch 16/166] [Batch 100/303] [D loss: 0.483178] [G loss: 0.183818] [ema: 0.998600] 
[Epoch 16/166] [Batch 200/303] [D loss: 0.383125] [G loss: 0.175160] [ema: 0.998628] 
[Epoch 16/166] [Batch 300/303] [D loss: 0.375374] [G loss: 0.191515] [ema: 0.998654] 
[Epoch 17/166] [Batch 0/303] [D loss: 0.422575] [G loss: 0.197302] [ema: 0.998655] 
[Epoch 17/166] [Batch 100/303] [D loss: 0.383203] [G loss: 0.190829] [ema: 0.998681] 
[Epoch 17/166] [Batch 200/303] [D loss: 0.440436] [G loss: 0.188180] [ema: 0.998705] 
[Epoch 17/166] [Batch 300/303] [D loss: 0.386211] [G loss: 0.178264] [ema: 0.998729] 
[Epoch 18/166] [Batch 0/303] [D loss: 0.438226] [G loss: 0.181332] [ema: 0.998730] 
[Epoch 18/166] [Batch 100/303] [D loss: 0.386120] [G loss: 0.190293] [ema: 0.998753] 
[Epoch 18/166] [Batch 200/303] [D loss: 0.426845] [G loss: 0.174519] [ema: 0.998775] 
[Epoch 18/166] [Batch 300/303] [D loss: 0.378294] [G loss: 0.219198] [ema: 0.998796] 
[Epoch 19/166] [Batch 0/303] [D loss: 0.366182] [G loss: 0.210876] [ema: 0.998797] 
[Epoch 19/166] [Batch 100/303] [D loss: 0.417602] [G loss: 0.186768] [ema: 0.998817] 
[Epoch 19/166] [Batch 200/303] [D loss: 0.368570] [G loss: 0.187898] [ema: 0.998837] 
[Epoch 19/166] [Batch 300/303] [D loss: 0.404899] [G loss: 0.172573] [ema: 0.998856] 
[Epoch 20/166] [Batch 0/303] [D loss: 0.365244] [G loss: 0.190487] [ema: 0.998857] 
[Epoch 20/166] [Batch 100/303] [D loss: 0.413543] [G loss: 0.157651] [ema: 0.998875] 
[Epoch 20/166] [Batch 200/303] [D loss: 0.400614] [G loss: 0.199446] [ema: 0.998893] 
[Epoch 20/166] [Batch 300/303] [D loss: 0.403328] [G loss: 0.175348] [ema: 0.998911] 
[Epoch 21/166] [Batch 0/303] [D loss: 0.379446] [G loss: 0.186978] [ema: 0.998911] 
[Epoch 21/166] [Batch 100/303] [D loss: 0.405412] [G loss: 0.185342] [ema: 0.998928] 
[Epoch 21/166] [Batch 200/303] [D loss: 0.401033] [G loss: 0.166811] [ema: 0.998944] 
[Epoch 21/166] [Batch 300/303] [D loss: 0.346790] [G loss: 0.188610] [ema: 0.998960] 
[Epoch 22/166] [Batch 0/303] [D loss: 0.374224] [G loss: 0.195285] [ema: 0.998961] 
[Epoch 22/166] [Batch 100/303] [D loss: 0.394152] [G loss: 0.179572] [ema: 0.998976] 
[Epoch 22/166] [Batch 200/303] [D loss: 0.402309] [G loss: 0.189492] [ema: 0.998991] 
[Epoch 22/166] [Batch 300/303] [D loss: 0.386190] [G loss: 0.180181] [ema: 0.999005] 
[Epoch 23/166] [Batch 0/303] [D loss: 0.467186] [G loss: 0.194389] [ema: 0.999006] 
[Epoch 23/166] [Batch 100/303] [D loss: 0.412650] [G loss: 0.186059] [ema: 0.999020] 
[Epoch 23/166] [Batch 200/303] [D loss: 0.365149] [G loss: 0.179491] [ema: 0.999034] 
[Epoch 23/166] [Batch 300/303] [D loss: 0.428871] [G loss: 0.158834] [ema: 0.999047] 
[Epoch 24/166] [Batch 0/303] [D loss: 0.388363] [G loss: 0.165612] [ema: 0.999047] 
[Epoch 24/166] [Batch 100/303] [D loss: 0.357427] [G loss: 0.206750] [ema: 0.999060] 
[Epoch 24/166] [Batch 200/303] [D loss: 0.392910] [G loss: 0.184031] [ema: 0.999073] 
[Epoch 24/166] [Batch 300/303] [D loss: 0.383189] [G loss: 0.184789] [ema: 0.999085] 
[Epoch 25/166] [Batch 0/303] [D loss: 0.381872] [G loss: 0.190645] [ema: 0.999085] 
[Epoch 25/166] [Batch 100/303] [D loss: 0.412692] [G loss: 0.168295] [ema: 0.999097] 
[Epoch 25/166] [Batch 200/303] [D loss: 0.406069] [G loss: 0.178706] [ema: 0.999109] 
[Epoch 25/166] [Batch 300/303] [D loss: 0.411315] [G loss: 0.166051] [ema: 0.999120] 
[Epoch 26/166] [Batch 0/303] [D loss: 0.392720] [G loss: 0.184789] [ema: 0.999121] 
[Epoch 26/166] [Batch 100/303] [D loss: 0.380444] [G loss: 0.183015] [ema: 0.999132] 
[Epoch 26/166] [Batch 200/303] [D loss: 0.423946] [G loss: 0.198288] [ema: 0.999142] 
[Epoch 26/166] [Batch 300/303] [D loss: 0.398355] [G loss: 0.191567] [ema: 0.999153] 
[Epoch 27/166] [Batch 0/303] [D loss: 0.423405] [G loss: 0.177916] [ema: 0.999153] 
[Epoch 27/166] [Batch 100/303] [D loss: 0.371013] [G loss: 0.185461] [ema: 0.999163] 
[Epoch 27/166] [Batch 200/303] [D loss: 0.432644] [G loss: 0.152177] [ema: 0.999173] 
[Epoch 27/166] [Batch 300/303] [D loss: 0.447424] [G loss: 0.168002] [ema: 0.999183] 
[Epoch 28/166] [Batch 0/303] [D loss: 0.417032] [G loss: 0.181301] [ema: 0.999183] 
[Epoch 28/166] [Batch 100/303] [D loss: 0.406989] [G loss: 0.190566] [ema: 0.999193] 
[Epoch 28/166] [Batch 200/303] [D loss: 0.411372] [G loss: 0.168685] [ema: 0.999202] 
[Epoch 28/166] [Batch 300/303] [D loss: 0.369382] [G loss: 0.193981] [ema: 0.999211] 
[Epoch 29/166] [Batch 0/303] [D loss: 0.418800] [G loss: 0.196883] [ema: 0.999211] 
[Epoch 29/166] [Batch 100/303] [D loss: 0.394543] [G loss: 0.189668] [ema: 0.999220] 
[Epoch 29/166] [Batch 200/303] [D loss: 0.395479] [G loss: 0.198983] [ema: 0.999229] 
[Epoch 29/166] [Batch 300/303] [D loss: 0.393924] [G loss: 0.189605] [ema: 0.999238] 
[Epoch 30/166] [Batch 0/303] [D loss: 0.399193] [G loss: 0.186647] [ema: 0.999238] 
[Epoch 30/166] [Batch 100/303] [D loss: 0.422335] [G loss: 0.174244] [ema: 0.999246] 
[Epoch 30/166] [Batch 200/303] [D loss: 0.385772] [G loss: 0.170361] [ema: 0.999254] 
[Epoch 30/166] [Batch 300/303] [D loss: 0.402060] [G loss: 0.185602] [ema: 0.999262] 
[Epoch 31/166] [Batch 0/303] [D loss: 0.395294] [G loss: 0.184624] [ema: 0.999262] 
[Epoch 31/166] [Batch 100/303] [D loss: 0.429693] [G loss: 0.175891] [ema: 0.999270] 
[Epoch 31/166] [Batch 200/303] [D loss: 0.361470] [G loss: 0.197586] [ema: 0.999278] 
[Epoch 31/166] [Batch 300/303] [D loss: 0.397423] [G loss: 0.172776] [ema: 0.999285] 
[Epoch 32/166] [Batch 0/303] [D loss: 0.425828] [G loss: 0.186621] [ema: 0.999285] 
[Epoch 32/166] [Batch 100/303] [D loss: 0.395057] [G loss: 0.182530] [ema: 0.999293] 
[Epoch 32/166] [Batch 200/303] [D loss: 0.402440] [G loss: 0.193637] [ema: 0.999300] 
[Epoch 32/166] [Batch 300/303] [D loss: 0.346616] [G loss: 0.183423] [ema: 0.999307] 
[Epoch 33/166] [Batch 0/303] [D loss: 0.352087] [G loss: 0.197474] [ema: 0.999307] 
[Epoch 33/166] [Batch 100/303] [D loss: 0.395408] [G loss: 0.190261] [ema: 0.999314] 
[Epoch 33/166] [Batch 200/303] [D loss: 0.390132] [G loss: 0.184528] [ema: 0.999321] 
[Epoch 33/166] [Batch 300/303] [D loss: 0.400928] [G loss: 0.186322] [ema: 0.999327] 
[Epoch 34/166] [Batch 0/303] [D loss: 0.401168] [G loss: 0.184111] [ema: 0.999327] 
[Epoch 34/166] [Batch 100/303] [D loss: 0.368843] [G loss: 0.182197] [ema: 0.999334] 
[Epoch 34/166] [Batch 200/303] [D loss: 0.428081] [G loss: 0.177759] [ema: 0.999340] 
[Epoch 34/166] [Batch 300/303] [D loss: 0.392224] [G loss: 0.195766] [ema: 0.999346] 
[Epoch 35/166] [Batch 0/303] [D loss: 0.374737] [G loss: 0.199204] [ema: 0.999347] 
[Epoch 35/166] [Batch 100/303] [D loss: 0.406148] [G loss: 0.186331] [ema: 0.999353] 
[Epoch 35/166] [Batch 200/303] [D loss: 0.399857] [G loss: 0.179754] [ema: 0.999359] 
[Epoch 35/166] [Batch 300/303] [D loss: 0.381496] [G loss: 0.196394] [ema: 0.999365] 
[Epoch 36/166] [Batch 0/303] [D loss: 0.377363] [G loss: 0.192786] [ema: 0.999365] 
[Epoch 36/166] [Batch 100/303] [D loss: 0.408109] [G loss: 0.166403] [ema: 0.999371] 
[Epoch 36/166] [Batch 200/303] [D loss: 0.433025] [G loss: 0.184977] [ema: 0.999376] 
[Epoch 36/166] [Batch 300/303] [D loss: 0.399889] [G loss: 0.186786] [ema: 0.999382] 
[Epoch 37/166] [Batch 0/303] [D loss: 0.384870] [G loss: 0.196114] [ema: 0.999382] 
[Epoch 37/166] [Batch 100/303] [D loss: 0.411524] [G loss: 0.200711] [ema: 0.999387] 
[Epoch 37/166] [Batch 200/303] [D loss: 0.392984] [G loss: 0.203748] [ema: 0.999393] 
[Epoch 37/166] [Batch 300/303] [D loss: 0.403675] [G loss: 0.184168] [ema: 0.999398] 
[Epoch 38/166] [Batch 0/303] [D loss: 0.416237] [G loss: 0.174194] [ema: 0.999398] 
[Epoch 38/166] [Batch 100/303] [D loss: 0.344384] [G loss: 0.197890] [ema: 0.999403] 
[Epoch 38/166] [Batch 200/303] [D loss: 0.377089] [G loss: 0.181420] [ema: 0.999408] 
[Epoch 38/166] [Batch 300/303] [D loss: 0.362361] [G loss: 0.195884] [ema: 0.999413] 
[Epoch 39/166] [Batch 0/303] [D loss: 0.392140] [G loss: 0.192400] [ema: 0.999414] 
[Epoch 39/166] [Batch 100/303] [D loss: 0.401204] [G loss: 0.184882] [ema: 0.999419] 
[Epoch 39/166] [Batch 200/303] [D loss: 0.377843] [G loss: 0.189241] [ema: 0.999423] 
[Epoch 39/166] [Batch 300/303] [D loss: 0.380408] [G loss: 0.185740] [ema: 0.999428] 
[Epoch 40/166] [Batch 0/303] [D loss: 0.397073] [G loss: 0.189722] [ema: 0.999428] 
[Epoch 40/166] [Batch 100/303] [D loss: 0.376051] [G loss: 0.180059] [ema: 0.999433] 
[Epoch 40/166] [Batch 200/303] [D loss: 0.359204] [G loss: 0.197185] [ema: 0.999438] 
[Epoch 40/166] [Batch 300/303] [D loss: 0.387692] [G loss: 0.189668] [ema: 0.999442] 
[Epoch 41/166] [Batch 0/303] [D loss: 0.386204] [G loss: 0.181270] [ema: 0.999442] 
[Epoch 41/166] [Batch 100/303] [D loss: 0.331695] [G loss: 0.192128] [ema: 0.999447] 
[Epoch 41/166] [Batch 200/303] [D loss: 0.336696] [G loss: 0.197600] [ema: 0.999451] 
[Epoch 41/166] [Batch 300/303] [D loss: 0.389639] [G loss: 0.168551] [ema: 0.999455] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_25_01_37_21/Model



[Epoch 42/166] [Batch 0/303] [D loss: 0.406098] [G loss: 0.171332] [ema: 0.999455] 
[Epoch 42/166] [Batch 100/303] [D loss: 0.433818] [G loss: 0.170545] [ema: 0.999460] 
[Epoch 42/166] [Batch 200/303] [D loss: 0.376270] [G loss: 0.187398] [ema: 0.999464] 
[Epoch 42/166] [Batch 300/303] [D loss: 0.390882] [G loss: 0.159050] [ema: 0.999468] 
[Epoch 43/166] [Batch 0/303] [D loss: 0.351092] [G loss: 0.185449] [ema: 0.999468] 
[Epoch 43/166] [Batch 100/303] [D loss: 0.384214] [G loss: 0.185835] [ema: 0.999472] 
[Epoch 43/166] [Batch 200/303] [D loss: 0.405195] [G loss: 0.177742] [ema: 0.999476] 
[Epoch 43/166] [Batch 300/303] [D loss: 0.412766] [G loss: 0.175545] [ema: 0.999480] 
[Epoch 44/166] [Batch 0/303] [D loss: 0.386522] [G loss: 0.174397] [ema: 0.999480] 
[Epoch 44/166] [Batch 100/303] [D loss: 0.385056] [G loss: 0.161901] [ema: 0.999484] 
[Epoch 44/166] [Batch 200/303] [D loss: 0.420969] [G loss: 0.183514] [ema: 0.999488] 
[Epoch 44/166] [Batch 300/303] [D loss: 0.379291] [G loss: 0.175312] [ema: 0.999492] 
[Epoch 45/166] [Batch 0/303] [D loss: 0.397911] [G loss: 0.192216] [ema: 0.999492] 
[Epoch 45/166] [Batch 100/303] [D loss: 0.389976] [G loss: 0.174101] [ema: 0.999495] 
[Epoch 45/166] [Batch 200/303] [D loss: 0.393410] [G loss: 0.195930] [ema: 0.999499] 
[Epoch 45/166] [Batch 300/303] [D loss: 0.359841] [G loss: 0.191021] [ema: 0.999503] 
[Epoch 46/166] [Batch 0/303] [D loss: 0.374768] [G loss: 0.192056] [ema: 0.999503] 
[Epoch 46/166] [Batch 100/303] [D loss: 0.366260] [G loss: 0.176997] [ema: 0.999506] 
[Epoch 46/166] [Batch 200/303] [D loss: 0.384434] [G loss: 0.190693] [ema: 0.999510] 
[Epoch 46/166] [Batch 300/303] [D loss: 0.355005] [G loss: 0.171070] [ema: 0.999513] 
[Epoch 47/166] [Batch 0/303] [D loss: 0.430069] [G loss: 0.184960] [ema: 0.999513] 
[Epoch 47/166] [Batch 100/303] [D loss: 0.408390] [G loss: 0.170838] [ema: 0.999517] 
[Epoch 47/166] [Batch 200/303] [D loss: 0.414431] [G loss: 0.174629] [ema: 0.999520] 
[Epoch 47/166] [Batch 300/303] [D loss: 0.390472] [G loss: 0.178370] [ema: 0.999523] 
[Epoch 48/166] [Batch 0/303] [D loss: 0.421294] [G loss: 0.200416] [ema: 0.999524] 
[Epoch 48/166] [Batch 100/303] [D loss: 0.401481] [G loss: 0.163254] [ema: 0.999527] 
[Epoch 48/166] [Batch 200/303] [D loss: 0.390410] [G loss: 0.173196] [ema: 0.999530] 
[Epoch 48/166] [Batch 300/303] [D loss: 0.366925] [G loss: 0.192519] [ema: 0.999533] 
[Epoch 49/166] [Batch 0/303] [D loss: 0.368663] [G loss: 0.194769] [ema: 0.999533] 
[Epoch 49/166] [Batch 100/303] [D loss: 0.382475] [G loss: 0.195980] [ema: 0.999536] 
[Epoch 49/166] [Batch 200/303] [D loss: 0.420615] [G loss: 0.191825] [ema: 0.999539] 
[Epoch 49/166] [Batch 300/303] [D loss: 0.425477] [G loss: 0.179640] [ema: 0.999542] 
[Epoch 50/166] [Batch 0/303] [D loss: 0.408728] [G loss: 0.185088] [ema: 0.999543] 
[Epoch 50/166] [Batch 100/303] [D loss: 0.394009] [G loss: 0.171319] [ema: 0.999546] 
[Epoch 50/166] [Batch 200/303] [D loss: 0.414412] [G loss: 0.161660] [ema: 0.999549] 
[Epoch 50/166] [Batch 300/303] [D loss: 0.351437] [G loss: 0.196734] [ema: 0.999551] 
[Epoch 51/166] [Batch 0/303] [D loss: 0.392596] [G loss: 0.206417] [ema: 0.999552] 
[Epoch 51/166] [Batch 100/303] [D loss: 0.370358] [G loss: 0.186464] [ema: 0.999554] 
[Epoch 51/166] [Batch 200/303] [D loss: 0.391572] [G loss: 0.168010] [ema: 0.999557] 
[Epoch 51/166] [Batch 300/303] [D loss: 0.345094] [G loss: 0.189126] [ema: 0.999560] 
[Epoch 52/166] [Batch 0/303] [D loss: 0.362504] [G loss: 0.194597] [ema: 0.999560] 
[Epoch 52/166] [Batch 100/303] [D loss: 0.383020] [G loss: 0.182545] [ema: 0.999563] 
[Epoch 52/166] [Batch 200/303] [D loss: 0.365940] [G loss: 0.187473] [ema: 0.999566] 
[Epoch 52/166] [Batch 300/303] [D loss: 0.387609] [G loss: 0.181827] [ema: 0.999568] 
[Epoch 53/166] [Batch 0/303] [D loss: 0.385722] [G loss: 0.179593] [ema: 0.999568] 
[Epoch 53/166] [Batch 100/303] [D loss: 0.382217] [G loss: 0.176505] [ema: 0.999571] 
[Epoch 53/166] [Batch 200/303] [D loss: 0.382841] [G loss: 0.174052] [ema: 0.999574] 
[Epoch 53/166] [Batch 300/303] [D loss: 0.455365] [G loss: 0.174070] [ema: 0.999576] 
[Epoch 54/166] [Batch 0/303] [D loss: 0.400906] [G loss: 0.181787] [ema: 0.999576] 
[Epoch 54/166] [Batch 100/303] [D loss: 0.359195] [G loss: 0.197274] [ema: 0.999579] 
[Epoch 54/166] [Batch 200/303] [D loss: 0.447528] [G loss: 0.183798] [ema: 0.999582] 
[Epoch 54/166] [Batch 300/303] [D loss: 0.391161] [G loss: 0.176562] [ema: 0.999584] 
[Epoch 55/166] [Batch 0/303] [D loss: 0.385291] [G loss: 0.188354] [ema: 0.999584] 
[Epoch 55/166] [Batch 100/303] [D loss: 0.394851] [G loss: 0.175209] [ema: 0.999587] 
[Epoch 55/166] [Batch 200/303] [D loss: 0.412839] [G loss: 0.189043] [ema: 0.999589] 
[Epoch 55/166] [Batch 300/303] [D loss: 0.383848] [G loss: 0.193133] [ema: 0.999592] 
[Epoch 56/166] [Batch 0/303] [D loss: 0.423245] [G loss: 0.184420] [ema: 0.999592] 
[Epoch 56/166] [Batch 100/303] [D loss: 0.486483] [G loss: 0.183517] [ema: 0.999594] 
[Epoch 56/166] [Batch 200/303] [D loss: 0.376317] [G loss: 0.172175] [ema: 0.999596] 
[Epoch 56/166] [Batch 300/303] [D loss: 0.413610] [G loss: 0.173751] [ema: 0.999599] 
[Epoch 57/166] [Batch 0/303] [D loss: 0.378735] [G loss: 0.177352] [ema: 0.999599] 
[Epoch 57/166] [Batch 100/303] [D loss: 0.354670] [G loss: 0.196039] [ema: 0.999601] 
[Epoch 57/166] [Batch 200/303] [D loss: 0.415830] [G loss: 0.178946] [ema: 0.999603] 
[Epoch 57/166] [Batch 300/303] [D loss: 0.378908] [G loss: 0.195212] [ema: 0.999606] 
[Epoch 58/166] [Batch 0/303] [D loss: 0.389786] [G loss: 0.186146] [ema: 0.999606] 
[Epoch 58/166] [Batch 100/303] [D loss: 0.373624] [G loss: 0.170724] [ema: 0.999608] 
[Epoch 58/166] [Batch 200/303] [D loss: 0.397339] [G loss: 0.189085] [ema: 0.999610] 
[Epoch 58/166] [Batch 300/303] [D loss: 0.410776] [G loss: 0.194115] [ema: 0.999612] 
[Epoch 59/166] [Batch 0/303] [D loss: 0.413020] [G loss: 0.183185] [ema: 0.999612] 
[Epoch 59/166] [Batch 100/303] [D loss: 0.409977] [G loss: 0.195534] [ema: 0.999614] 
[Epoch 59/166] [Batch 200/303] [D loss: 0.393431] [G loss: 0.185305] [ema: 0.999617] 
[Epoch 59/166] [Batch 300/303] [D loss: 0.366739] [G loss: 0.193880] [ema: 0.999619] 
[Epoch 60/166] [Batch 0/303] [D loss: 0.384787] [G loss: 0.192367] [ema: 0.999619] 
[Epoch 60/166] [Batch 100/303] [D loss: 0.372068] [G loss: 0.172382] [ema: 0.999621] 
[Epoch 60/166] [Batch 200/303] [D loss: 0.358799] [G loss: 0.178105] [ema: 0.999623] 
[Epoch 60/166] [Batch 300/303] [D loss: 0.418821] [G loss: 0.185724] [ema: 0.999625] 
[Epoch 61/166] [Batch 0/303] [D loss: 0.399290] [G loss: 0.181741] [ema: 0.999625] 
[Epoch 61/166] [Batch 100/303] [D loss: 0.365815] [G loss: 0.189962] [ema: 0.999627] 
[Epoch 61/166] [Batch 200/303] [D loss: 0.427996] [G loss: 0.173122] [ema: 0.999629] 
[Epoch 61/166] [Batch 300/303] [D loss: 0.381643] [G loss: 0.194747] [ema: 0.999631] 
[Epoch 62/166] [Batch 0/303] [D loss: 0.388798] [G loss: 0.185997] [ema: 0.999631] 
[Epoch 62/166] [Batch 100/303] [D loss: 0.355958] [G loss: 0.196866] [ema: 0.999633] 
[Epoch 62/166] [Batch 200/303] [D loss: 0.374521] [G loss: 0.180244] [ema: 0.999635] 
[Epoch 62/166] [Batch 300/303] [D loss: 0.402615] [G loss: 0.205770] [ema: 0.999637] 
[Epoch 63/166] [Batch 0/303] [D loss: 0.367486] [G loss: 0.210027] [ema: 0.999637] 
[Epoch 63/166] [Batch 100/303] [D loss: 0.413432] [G loss: 0.184740] [ema: 0.999639] 
[Epoch 63/166] [Batch 200/303] [D loss: 0.353543] [G loss: 0.183013] [ema: 0.999641] 
[Epoch 63/166] [Batch 300/303] [D loss: 0.390622] [G loss: 0.178016] [ema: 0.999643] 
[Epoch 64/166] [Batch 0/303] [D loss: 0.367160] [G loss: 0.186340] [ema: 0.999643] 
[Epoch 64/166] [Batch 100/303] [D loss: 0.381297] [G loss: 0.193346] [ema: 0.999644] 
[Epoch 64/166] [Batch 200/303] [D loss: 0.360042] [G loss: 0.182816] [ema: 0.999646] 
[Epoch 64/166] [Batch 300/303] [D loss: 0.396041] [G loss: 0.176382] [ema: 0.999648] 
[Epoch 65/166] [Batch 0/303] [D loss: 0.431061] [G loss: 0.174718] [ema: 0.999648] 
[Epoch 65/166] [Batch 100/303] [D loss: 0.411482] [G loss: 0.170617] [ema: 0.999650] 
[Epoch 65/166] [Batch 200/303] [D loss: 0.422237] [G loss: 0.179178] [ema: 0.999652] 
[Epoch 65/166] [Batch 300/303] [D loss: 0.369494] [G loss: 0.186199] [ema: 0.999653] 
[Epoch 66/166] [Batch 0/303] [D loss: 0.385732] [G loss: 0.195097] [ema: 0.999653] 
[Epoch 66/166] [Batch 100/303] [D loss: 0.392714] [G loss: 0.197524] [ema: 0.999655] 
[Epoch 66/166] [Batch 200/303] [D loss: 0.389982] [G loss: 0.182566] [ema: 0.999657] 
[Epoch 66/166] [Batch 300/303] [D loss: 0.426198] [G loss: 0.195406] [ema: 0.999659] 
[Epoch 67/166] [Batch 0/303] [D loss: 0.374954] [G loss: 0.196005] [ema: 0.999659] 
[Epoch 67/166] [Batch 100/303] [D loss: 0.395523] [G loss: 0.190707] [ema: 0.999660] 
[Epoch 67/166] [Batch 200/303] [D loss: 0.401663] [G loss: 0.182158] [ema: 0.999662] 
[Epoch 67/166] [Batch 300/303] [D loss: 0.401054] [G loss: 0.179315] [ema: 0.999664] 
[Epoch 68/166] [Batch 0/303] [D loss: 0.357349] [G loss: 0.193738] [ema: 0.999664] 
[Epoch 68/166] [Batch 100/303] [D loss: 0.408993] [G loss: 0.173191] [ema: 0.999665] 
[Epoch 68/166] [Batch 200/303] [D loss: 0.375287] [G loss: 0.184577] [ema: 0.999667] 
[Epoch 68/166] [Batch 300/303] [D loss: 0.331588] [G loss: 0.194201] [ema: 0.999668] 
[Epoch 69/166] [Batch 0/303] [D loss: 0.380425] [G loss: 0.192866] [ema: 0.999669] 
[Epoch 69/166] [Batch 100/303] [D loss: 0.413771] [G loss: 0.184554] [ema: 0.999670] 
[Epoch 69/166] [Batch 200/303] [D loss: 0.382903] [G loss: 0.186348] [ema: 0.999672] 
[Epoch 69/166] [Batch 300/303] [D loss: 0.373711] [G loss: 0.191733] [ema: 0.999673] 
[Epoch 70/166] [Batch 0/303] [D loss: 0.388656] [G loss: 0.192841] [ema: 0.999673] 
[Epoch 70/166] [Batch 100/303] [D loss: 0.424229] [G loss: 0.195715] [ema: 0.999675] 
[Epoch 70/166] [Batch 200/303] [D loss: 0.321772] [G loss: 0.199916] [ema: 0.999676] 
[Epoch 70/166] [Batch 300/303] [D loss: 0.364684] [G loss: 0.187693] [ema: 0.999678] 
[Epoch 71/166] [Batch 0/303] [D loss: 0.343507] [G loss: 0.197224] [ema: 0.999678] 
[Epoch 71/166] [Batch 100/303] [D loss: 0.374544] [G loss: 0.212389] [ema: 0.999679] 
[Epoch 71/166] [Batch 200/303] [D loss: 0.345637] [G loss: 0.187962] [ema: 0.999681] 
[Epoch 71/166] [Batch 300/303] [D loss: 0.421429] [G loss: 0.190834] [ema: 0.999682] 
[Epoch 72/166] [Batch 0/303] [D loss: 0.411090] [G loss: 0.190908] [ema: 0.999682] 
[Epoch 72/166] [Batch 100/303] [D loss: 0.409877] [G loss: 0.174286] [ema: 0.999684] 
[Epoch 72/166] [Batch 200/303] [D loss: 0.395638] [G loss: 0.193200] [ema: 0.999685] 
[Epoch 72/166] [Batch 300/303] [D loss: 0.369490] [G loss: 0.177579] [ema: 0.999687] 
[Epoch 73/166] [Batch 0/303] [D loss: 0.355249] [G loss: 0.203526] [ema: 0.999687] 
[Epoch 73/166] [Batch 100/303] [D loss: 0.402837] [G loss: 0.171937] [ema: 0.999688] 
[Epoch 73/166] [Batch 200/303] [D loss: 0.342942] [G loss: 0.194150] [ema: 0.999689] 
[Epoch 73/166] [Batch 300/303] [D loss: 0.379217] [G loss: 0.191776] [ema: 0.999691] 
[Epoch 74/166] [Batch 0/303] [D loss: 0.392323] [G loss: 0.191278] [ema: 0.999691] 
[Epoch 74/166] [Batch 100/303] [D loss: 0.445119] [G loss: 0.181650] [ema: 0.999692] 
[Epoch 74/166] [Batch 200/303] [D loss: 0.416729] [G loss: 0.177305] [ema: 0.999694] 
[Epoch 74/166] [Batch 300/303] [D loss: 0.428639] [G loss: 0.172799] [ema: 0.999695] 
[Epoch 75/166] [Batch 0/303] [D loss: 0.415072] [G loss: 0.192804] [ema: 0.999695] 
[Epoch 75/166] [Batch 100/303] [D loss: 0.395948] [G loss: 0.177267] [ema: 0.999696] 
[Epoch 75/166] [Batch 200/303] [D loss: 0.378877] [G loss: 0.176092] [ema: 0.999698] 
[Epoch 75/166] [Batch 300/303] [D loss: 0.352613] [G loss: 0.187884] [ema: 0.999699] 
[Epoch 76/166] [Batch 0/303] [D loss: 0.394562] [G loss: 0.197545] [ema: 0.999699] 
[Epoch 76/166] [Batch 100/303] [D loss: 0.409495] [G loss: 0.182062] [ema: 0.999700] 
[Epoch 76/166] [Batch 200/303] [D loss: 0.411741] [G loss: 0.185178] [ema: 0.999702] 
[Epoch 76/166] [Batch 300/303] [D loss: 0.432612] [G loss: 0.177370] [ema: 0.999703] 
[Epoch 77/166] [Batch 0/303] [D loss: 0.399159] [G loss: 0.189436] [ema: 0.999703] 
[Epoch 77/166] [Batch 100/303] [D loss: 0.400881] [G loss: 0.169806] [ema: 0.999704] 
[Epoch 77/166] [Batch 200/303] [D loss: 0.437641] [G loss: 0.173302] [ema: 0.999705] 
[Epoch 77/166] [Batch 300/303] [D loss: 0.366589] [G loss: 0.187035] [ema: 0.999707] 
[Epoch 78/166] [Batch 0/303] [D loss: 0.359006] [G loss: 0.192140] [ema: 0.999707] 
[Epoch 78/166] [Batch 100/303] [D loss: 0.401702] [G loss: 0.169703] [ema: 0.999708] 
[Epoch 78/166] [Batch 200/303] [D loss: 0.385009] [G loss: 0.173277] [ema: 0.999709] 
[Epoch 78/166] [Batch 300/303] [D loss: 0.409186] [G loss: 0.179812] [ema: 0.999710] 
[Epoch 79/166] [Batch 0/303] [D loss: 0.429719] [G loss: 0.174975] [ema: 0.999710] 
[Epoch 79/166] [Batch 100/303] [D loss: 0.374766] [G loss: 0.188991] [ema: 0.999712] 
[Epoch 79/166] [Batch 200/303] [D loss: 0.446333] [G loss: 0.180499] [ema: 0.999713] 
[Epoch 79/166] [Batch 300/303] [D loss: 0.397592] [G loss: 0.186826] [ema: 0.999714] 
[Epoch 80/166] [Batch 0/303] [D loss: 0.412582] [G loss: 0.173936] [ema: 0.999714] 
[Epoch 80/166] [Batch 100/303] [D loss: 0.402012] [G loss: 0.178679] [ema: 0.999715] 
[Epoch 80/166] [Batch 200/303] [D loss: 0.397449] [G loss: 0.178863] [ema: 0.999716] 
[Epoch 80/166] [Batch 300/303] [D loss: 0.390001] [G loss: 0.172297] [ema: 0.999718] 
[Epoch 81/166] [Batch 0/303] [D loss: 0.419731] [G loss: 0.182162] [ema: 0.999718] 
[Epoch 81/166] [Batch 100/303] [D loss: 0.354626] [G loss: 0.173533] [ema: 0.999719] 
[Epoch 81/166] [Batch 200/303] [D loss: 0.404902] [G loss: 0.189284] [ema: 0.999720] 
[Epoch 81/166] [Batch 300/303] [D loss: 0.396493] [G loss: 0.174905] [ema: 0.999721] 
[Epoch 82/166] [Batch 0/303] [D loss: 0.381924] [G loss: 0.183597] [ema: 0.999721] 
[Epoch 82/166] [Batch 100/303] [D loss: 0.418913] [G loss: 0.173400] [ema: 0.999722] 
[Epoch 82/166] [Batch 200/303] [D loss: 0.400737] [G loss: 0.192199] [ema: 0.999723] 
[Epoch 82/166] [Batch 300/303] [D loss: 0.397540] [G loss: 0.198299] [ema: 0.999724] 
[Epoch 83/166] [Batch 0/303] [D loss: 0.363146] [G loss: 0.209872] [ema: 0.999724] 
[Epoch 83/166] [Batch 100/303] [D loss: 0.382551] [G loss: 0.179634] [ema: 0.999726] 
[Epoch 83/166] [Batch 200/303] [D loss: 0.378613] [G loss: 0.168707] [ema: 0.999727] 
[Epoch 83/166] [Batch 300/303] [D loss: 0.450164] [G loss: 0.184503] [ema: 0.999728] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_25_01_37_21/Model



[Epoch 84/166] [Batch 0/303] [D loss: 0.371661] [G loss: 0.196799] [ema: 0.999728] 
[Epoch 84/166] [Batch 100/303] [D loss: 0.418294] [G loss: 0.182317] [ema: 0.999729] 
[Epoch 84/166] [Batch 200/303] [D loss: 0.401016] [G loss: 0.182936] [ema: 0.999730] 
[Epoch 84/166] [Batch 300/303] [D loss: 0.390019] [G loss: 0.185911] [ema: 0.999731] 
[Epoch 85/166] [Batch 0/303] [D loss: 0.365387] [G loss: 0.190731] [ema: 0.999731] 
[Epoch 85/166] [Batch 100/303] [D loss: 0.391372] [G loss: 0.164091] [ema: 0.999732] 
[Epoch 85/166] [Batch 200/303] [D loss: 0.367380] [G loss: 0.186598] [ema: 0.999733] 
[Epoch 85/166] [Batch 300/303] [D loss: 0.417936] [G loss: 0.179868] [ema: 0.999734] 
[Epoch 86/166] [Batch 0/303] [D loss: 0.421729] [G loss: 0.179901] [ema: 0.999734] 
[Epoch 86/166] [Batch 100/303] [D loss: 0.382194] [G loss: 0.190951] [ema: 0.999735] 
[Epoch 86/166] [Batch 200/303] [D loss: 0.411254] [G loss: 0.179043] [ema: 0.999736] 
[Epoch 86/166] [Batch 300/303] [D loss: 0.436917] [G loss: 0.168360] [ema: 0.999737] 
[Epoch 87/166] [Batch 0/303] [D loss: 0.383515] [G loss: 0.184403] [ema: 0.999737] 
[Epoch 87/166] [Batch 100/303] [D loss: 0.425287] [G loss: 0.196854] [ema: 0.999738] 
[Epoch 87/166] [Batch 200/303] [D loss: 0.443842] [G loss: 0.171519] [ema: 0.999739] 
[Epoch 87/166] [Batch 300/303] [D loss: 0.359944] [G loss: 0.179267] [ema: 0.999740] 
[Epoch 88/166] [Batch 0/303] [D loss: 0.384630] [G loss: 0.188993] [ema: 0.999740] 
[Epoch 88/166] [Batch 100/303] [D loss: 0.418089] [G loss: 0.162765] [ema: 0.999741] 
[Epoch 88/166] [Batch 200/303] [D loss: 0.431586] [G loss: 0.176343] [ema: 0.999742] 
[Epoch 88/166] [Batch 300/303] [D loss: 0.421836] [G loss: 0.194034] [ema: 0.999743] 
[Epoch 89/166] [Batch 0/303] [D loss: 0.374293] [G loss: 0.189214] [ema: 0.999743] 
[Epoch 89/166] [Batch 100/303] [D loss: 0.412198] [G loss: 0.176602] [ema: 0.999744] 
[Epoch 89/166] [Batch 200/303] [D loss: 0.415700] [G loss: 0.172641] [ema: 0.999745] 
[Epoch 89/166] [Batch 300/303] [D loss: 0.424159] [G loss: 0.161207] [ema: 0.999746] 
[Epoch 90/166] [Batch 0/303] [D loss: 0.392419] [G loss: 0.189507] [ema: 0.999746] 
[Epoch 90/166] [Batch 100/303] [D loss: 0.406011] [G loss: 0.180153] [ema: 0.999747] 
[Epoch 90/166] [Batch 200/303] [D loss: 0.408045] [G loss: 0.159391] [ema: 0.999748] 
[Epoch 90/166] [Batch 300/303] [D loss: 0.390539] [G loss: 0.172005] [ema: 0.999749] 
[Epoch 91/166] [Batch 0/303] [D loss: 0.374751] [G loss: 0.185322] [ema: 0.999749] 
[Epoch 91/166] [Batch 100/303] [D loss: 0.377894] [G loss: 0.175155] [ema: 0.999750] 
[Epoch 91/166] [Batch 200/303] [D loss: 0.410288] [G loss: 0.181149] [ema: 0.999750] 
[Epoch 91/166] [Batch 300/303] [D loss: 0.400167] [G loss: 0.190149] [ema: 0.999751] 
[Epoch 92/166] [Batch 0/303] [D loss: 0.370315] [G loss: 0.184243] [ema: 0.999751] 
[Epoch 92/166] [Batch 100/303] [D loss: 0.429265] [G loss: 0.180100] [ema: 0.999752] 
[Epoch 92/166] [Batch 200/303] [D loss: 0.399201] [G loss: 0.179449] [ema: 0.999753] 
[Epoch 92/166] [Batch 300/303] [D loss: 0.414122] [G loss: 0.166841] [ema: 0.999754] 
[Epoch 93/166] [Batch 0/303] [D loss: 0.432415] [G loss: 0.192137] [ema: 0.999754] 
[Epoch 93/166] [Batch 100/303] [D loss: 0.376979] [G loss: 0.183051] [ema: 0.999755] 
[Epoch 93/166] [Batch 200/303] [D loss: 0.379615] [G loss: 0.196307] [ema: 0.999756] 
[Epoch 93/166] [Batch 300/303] [D loss: 0.364317] [G loss: 0.186127] [ema: 0.999757] 
[Epoch 94/166] [Batch 0/303] [D loss: 0.411523] [G loss: 0.179359] [ema: 0.999757] 
[Epoch 94/166] [Batch 100/303] [D loss: 0.434891] [G loss: 0.186817] [ema: 0.999758] 
[Epoch 94/166] [Batch 200/303] [D loss: 0.408375] [G loss: 0.163153] [ema: 0.999758] 
[Epoch 94/166] [Batch 300/303] [D loss: 0.399704] [G loss: 0.182492] [ema: 0.999759] 
[Epoch 95/166] [Batch 0/303] [D loss: 0.405593] [G loss: 0.179206] [ema: 0.999759] 
[Epoch 95/166] [Batch 100/303] [D loss: 0.388927] [G loss: 0.176284] [ema: 0.999760] 
[Epoch 95/166] [Batch 200/303] [D loss: 0.346651] [G loss: 0.189384] [ema: 0.999761] 
[Epoch 95/166] [Batch 300/303] [D loss: 0.434940] [G loss: 0.180411] [ema: 0.999762] 
[Epoch 96/166] [Batch 0/303] [D loss: 0.430313] [G loss: 0.181789] [ema: 0.999762] 
[Epoch 96/166] [Batch 100/303] [D loss: 0.368280] [G loss: 0.184312] [ema: 0.999763] 
[Epoch 96/166] [Batch 200/303] [D loss: 0.391310] [G loss: 0.166939] [ema: 0.999763] 
[Epoch 96/166] [Batch 300/303] [D loss: 0.434134] [G loss: 0.187559] [ema: 0.999764] 
[Epoch 97/166] [Batch 0/303] [D loss: 0.360698] [G loss: 0.165782] [ema: 0.999764] 
[Epoch 97/166] [Batch 100/303] [D loss: 0.389144] [G loss: 0.178272] [ema: 0.999765] 
[Epoch 97/166] [Batch 200/303] [D loss: 0.421888] [G loss: 0.173484] [ema: 0.999766] 
[Epoch 97/166] [Batch 300/303] [D loss: 0.397837] [G loss: 0.169945] [ema: 0.999767] 
[Epoch 98/166] [Batch 0/303] [D loss: 0.431964] [G loss: 0.174660] [ema: 0.999767] 
[Epoch 98/166] [Batch 100/303] [D loss: 0.391229] [G loss: 0.183432] [ema: 0.999767] 
[Epoch 98/166] [Batch 200/303] [D loss: 0.407922] [G loss: 0.170756] [ema: 0.999768] 
[Epoch 98/166] [Batch 300/303] [D loss: 0.381697] [G loss: 0.177360] [ema: 0.999769] 
[Epoch 99/166] [Batch 0/303] [D loss: 0.395606] [G loss: 0.187848] [ema: 0.999769] 
[Epoch 99/166] [Batch 100/303] [D loss: 0.376764] [G loss: 0.184289] [ema: 0.999770] 
[Epoch 99/166] [Batch 200/303] [D loss: 0.414816] [G loss: 0.177898] [ema: 0.999770] 
[Epoch 99/166] [Batch 300/303] [D loss: 0.389986] [G loss: 0.190801] [ema: 0.999771] 
[Epoch 100/166] [Batch 0/303] [D loss: 0.383419] [G loss: 0.197533] [ema: 0.999771] 
[Epoch 100/166] [Batch 100/303] [D loss: 0.431781] [G loss: 0.172242] [ema: 0.999772] 
[Epoch 100/166] [Batch 200/303] [D loss: 0.400583] [G loss: 0.178804] [ema: 0.999773] 
[Epoch 100/166] [Batch 300/303] [D loss: 0.377717] [G loss: 0.175278] [ema: 0.999774] 
[Epoch 101/166] [Batch 0/303] [D loss: 0.360580] [G loss: 0.177125] [ema: 0.999774] 
[Epoch 101/166] [Batch 100/303] [D loss: 0.378926] [G loss: 0.179424] [ema: 0.999774] 
[Epoch 101/166] [Batch 200/303] [D loss: 0.403031] [G loss: 0.188315] [ema: 0.999775] 
[Epoch 101/166] [Batch 300/303] [D loss: 0.414077] [G loss: 0.159682] [ema: 0.999776] 
[Epoch 102/166] [Batch 0/303] [D loss: 0.429194] [G loss: 0.173101] [ema: 0.999776] 
[Epoch 102/166] [Batch 100/303] [D loss: 0.420933] [G loss: 0.187930] [ema: 0.999776] 
[Epoch 102/166] [Batch 200/303] [D loss: 0.436518] [G loss: 0.164375] [ema: 0.999777] 
[Epoch 102/166] [Batch 300/303] [D loss: 0.410398] [G loss: 0.151924] [ema: 0.999778] 
[Epoch 103/166] [Batch 0/303] [D loss: 0.428861] [G loss: 0.171087] [ema: 0.999778] 
[Epoch 103/166] [Batch 100/303] [D loss: 0.409948] [G loss: 0.178115] [ema: 0.999779] 
[Epoch 103/166] [Batch 200/303] [D loss: 0.428544] [G loss: 0.183653] [ema: 0.999779] 
[Epoch 103/166] [Batch 300/303] [D loss: 0.421467] [G loss: 0.177964] [ema: 0.999780] 
[Epoch 104/166] [Batch 0/303] [D loss: 0.397839] [G loss: 0.192900] [ema: 0.999780] 
[Epoch 104/166] [Batch 100/303] [D loss: 0.422393] [G loss: 0.165956] [ema: 0.999781] 
[Epoch 104/166] [Batch 200/303] [D loss: 0.400547] [G loss: 0.189269] [ema: 0.999781] 
[Epoch 104/166] [Batch 300/303] [D loss: 0.452182] [G loss: 0.169965] [ema: 0.999782] 
[Epoch 105/166] [Batch 0/303] [D loss: 0.448769] [G loss: 0.167241] [ema: 0.999782] 
[Epoch 105/166] [Batch 100/303] [D loss: 0.423681] [G loss: 0.165706] [ema: 0.999783] 
[Epoch 105/166] [Batch 200/303] [D loss: 0.382501] [G loss: 0.174232] [ema: 0.999784] 
[Epoch 105/166] [Batch 300/303] [D loss: 0.357531] [G loss: 0.174017] [ema: 0.999784] 
[Epoch 106/166] [Batch 0/303] [D loss: 0.385524] [G loss: 0.173421] [ema: 0.999784] 
[Epoch 106/166] [Batch 100/303] [D loss: 0.403918] [G loss: 0.167280] [ema: 0.999785] 
[Epoch 106/166] [Batch 200/303] [D loss: 0.417534] [G loss: 0.169347] [ema: 0.999786] 
[Epoch 106/166] [Batch 300/303] [D loss: 0.360528] [G loss: 0.171034] [ema: 0.999786] 
[Epoch 107/166] [Batch 0/303] [D loss: 0.373589] [G loss: 0.186716] [ema: 0.999786] 
[Epoch 107/166] [Batch 100/303] [D loss: 0.413233] [G loss: 0.178826] [ema: 0.999787] 
[Epoch 107/166] [Batch 200/303] [D loss: 0.450733] [G loss: 0.170932] [ema: 0.999788] 
[Epoch 107/166] [Batch 300/303] [D loss: 0.394830] [G loss: 0.162426] [ema: 0.999788] 
[Epoch 108/166] [Batch 0/303] [D loss: 0.405148] [G loss: 0.182369] [ema: 0.999788] 
[Epoch 108/166] [Batch 100/303] [D loss: 0.409240] [G loss: 0.171062] [ema: 0.999789] 
[Epoch 108/166] [Batch 200/303] [D loss: 0.409930] [G loss: 0.184391] [ema: 0.999789] 
[Epoch 108/166] [Batch 300/303] [D loss: 0.420778] [G loss: 0.171022] [ema: 0.999790] 
[Epoch 109/166] [Batch 0/303] [D loss: 0.391889] [G loss: 0.177768] [ema: 0.999790] 
[Epoch 109/166] [Batch 100/303] [D loss: 0.384248] [G loss: 0.179145] [ema: 0.999791] 
[Epoch 109/166] [Batch 200/303] [D loss: 0.385683] [G loss: 0.178041] [ema: 0.999791] 
[Epoch 109/166] [Batch 300/303] [D loss: 0.447753] [G loss: 0.177681] [ema: 0.999792] 
[Epoch 110/166] [Batch 0/303] [D loss: 0.395453] [G loss: 0.177462] [ema: 0.999792] 
[Epoch 110/166] [Batch 100/303] [D loss: 0.403620] [G loss: 0.167149] [ema: 0.999793] 
[Epoch 110/166] [Batch 200/303] [D loss: 0.421939] [G loss: 0.165266] [ema: 0.999793] 
[Epoch 110/166] [Batch 300/303] [D loss: 0.375366] [G loss: 0.176052] [ema: 0.999794] 
[Epoch 111/166] [Batch 0/303] [D loss: 0.425411] [G loss: 0.186486] [ema: 0.999794] 
[Epoch 111/166] [Batch 100/303] [D loss: 0.417579] [G loss: 0.178933] [ema: 0.999795] 
[Epoch 111/166] [Batch 200/303] [D loss: 0.390527] [G loss: 0.173339] [ema: 0.999795] 
[Epoch 111/166] [Batch 300/303] [D loss: 0.392177] [G loss: 0.178068] [ema: 0.999796] 
[Epoch 112/166] [Batch 0/303] [D loss: 0.389431] [G loss: 0.182997] [ema: 0.999796] 
[Epoch 112/166] [Batch 100/303] [D loss: 0.371354] [G loss: 0.167305] [ema: 0.999796] 
[Epoch 112/166] [Batch 200/303] [D loss: 0.399954] [G loss: 0.189536] [ema: 0.999797] 
[Epoch 112/166] [Batch 300/303] [D loss: 0.376981] [G loss: 0.181214] [ema: 0.999798] 
[Epoch 113/166] [Batch 0/303] [D loss: 0.385307] [G loss: 0.185818] [ema: 0.999798] 
[Epoch 113/166] [Batch 100/303] [D loss: 0.390578] [G loss: 0.181782] [ema: 0.999798] 
[Epoch 113/166] [Batch 200/303] [D loss: 0.457381] [G loss: 0.182754] [ema: 0.999799] 
[Epoch 113/166] [Batch 300/303] [D loss: 0.435174] [G loss: 0.182434] [ema: 0.999799] 
[Epoch 114/166] [Batch 0/303] [D loss: 0.377248] [G loss: 0.180856] [ema: 0.999799] 
[Epoch 114/166] [Batch 100/303] [D loss: 0.424745] [G loss: 0.193140] [ema: 0.999800] 
[Epoch 114/166] [Batch 200/303] [D loss: 0.426703] [G loss: 0.179442] [ema: 0.999801] 
[Epoch 114/166] [Batch 300/303] [D loss: 0.382819] [G loss: 0.170591] [ema: 0.999801] 
[Epoch 115/166] [Batch 0/303] [D loss: 0.355460] [G loss: 0.183494] [ema: 0.999801] 
[Epoch 115/166] [Batch 100/303] [D loss: 0.399448] [G loss: 0.176416] [ema: 0.999802] 
[Epoch 115/166] [Batch 200/303] [D loss: 0.386700] [G loss: 0.166014] [ema: 0.999802] 
[Epoch 115/166] [Batch 300/303] [D loss: 0.377070] [G loss: 0.163411] [ema: 0.999803] 
[Epoch 116/166] [Batch 0/303] [D loss: 0.422997] [G loss: 0.184341] [ema: 0.999803] 
[Epoch 116/166] [Batch 100/303] [D loss: 0.412608] [G loss: 0.176065] [ema: 0.999803] 
[Epoch 116/166] [Batch 200/303] [D loss: 0.413749] [G loss: 0.203324] [ema: 0.999804] 
[Epoch 116/166] [Batch 300/303] [D loss: 0.430673] [G loss: 0.174083] [ema: 0.999804] 
[Epoch 117/166] [Batch 0/303] [D loss: 0.369077] [G loss: 0.176186] [ema: 0.999804] 
[Epoch 117/166] [Batch 100/303] [D loss: 0.421767] [G loss: 0.174937] [ema: 0.999805] 
[Epoch 117/166] [Batch 200/303] [D loss: 0.392529] [G loss: 0.175430] [ema: 0.999806] 
[Epoch 117/166] [Batch 300/303] [D loss: 0.398395] [G loss: 0.181456] [ema: 0.999806] 
[Epoch 118/166] [Batch 0/303] [D loss: 0.392914] [G loss: 0.182984] [ema: 0.999806] 
[Epoch 118/166] [Batch 100/303] [D loss: 0.401104] [G loss: 0.181538] [ema: 0.999807] 
[Epoch 118/166] [Batch 200/303] [D loss: 0.397283] [G loss: 0.161340] [ema: 0.999807] 
[Epoch 118/166] [Batch 300/303] [D loss: 0.407319] [G loss: 0.177822] [ema: 0.999808] 
[Epoch 119/166] [Batch 0/303] [D loss: 0.417691] [G loss: 0.179165] [ema: 0.999808] 
[Epoch 119/166] [Batch 100/303] [D loss: 0.407055] [G loss: 0.184888] [ema: 0.999808] 
[Epoch 119/166] [Batch 200/303] [D loss: 0.431674] [G loss: 0.164735] [ema: 0.999809] 
[Epoch 119/166] [Batch 300/303] [D loss: 0.409310] [G loss: 0.166808] [ema: 0.999809] 
[Epoch 120/166] [Batch 0/303] [D loss: 0.403801] [G loss: 0.178489] [ema: 0.999809] 
[Epoch 120/166] [Batch 100/303] [D loss: 0.423943] [G loss: 0.178002] [ema: 0.999810] 
[Epoch 120/166] [Batch 200/303] [D loss: 0.407076] [G loss: 0.174872] [ema: 0.999810] 
[Epoch 120/166] [Batch 300/303] [D loss: 0.429435] [G loss: 0.178381] [ema: 0.999811] 
[Epoch 121/166] [Batch 0/303] [D loss: 0.377422] [G loss: 0.186744] [ema: 0.999811] 
[Epoch 121/166] [Batch 100/303] [D loss: 0.422685] [G loss: 0.172231] [ema: 0.999811] 
[Epoch 121/166] [Batch 200/303] [D loss: 0.407402] [G loss: 0.162248] [ema: 0.999812] 
[Epoch 121/166] [Batch 300/303] [D loss: 0.419585] [G loss: 0.161253] [ema: 0.999812] 
[Epoch 122/166] [Batch 0/303] [D loss: 0.426139] [G loss: 0.170167] [ema: 0.999813] 
[Epoch 122/166] [Batch 100/303] [D loss: 0.457103] [G loss: 0.159337] [ema: 0.999813] 
[Epoch 122/166] [Batch 200/303] [D loss: 0.427276] [G loss: 0.159119] [ema: 0.999814] 
[Epoch 122/166] [Batch 300/303] [D loss: 0.437256] [G loss: 0.170663] [ema: 0.999814] 
[Epoch 123/166] [Batch 0/303] [D loss: 0.396547] [G loss: 0.178527] [ema: 0.999814] 
[Epoch 123/166] [Batch 100/303] [D loss: 0.412062] [G loss: 0.174436] [ema: 0.999815] 
[Epoch 123/166] [Batch 200/303] [D loss: 0.453671] [G loss: 0.184880] [ema: 0.999815] 
[Epoch 123/166] [Batch 300/303] [D loss: 0.407125] [G loss: 0.183045] [ema: 0.999816] 
[Epoch 124/166] [Batch 0/303] [D loss: 0.400517] [G loss: 0.185230] [ema: 0.999816] 
[Epoch 124/166] [Batch 100/303] [D loss: 0.454278] [G loss: 0.164502] [ema: 0.999816] 
[Epoch 124/166] [Batch 200/303] [D loss: 0.407470] [G loss: 0.163434] [ema: 0.999817] 
[Epoch 124/166] [Batch 300/303] [D loss: 0.422033] [G loss: 0.184123] [ema: 0.999817] 
[Epoch 125/166] [Batch 0/303] [D loss: 0.414957] [G loss: 0.188311] [ema: 0.999817] 
[Epoch 125/166] [Batch 100/303] [D loss: 0.385932] [G loss: 0.177582] [ema: 0.999817] 
[Epoch 125/166] [Batch 200/303] [D loss: 0.396555] [G loss: 0.171214] [ema: 0.999818] 
[Epoch 125/166] [Batch 300/303] [D loss: 0.426363] [G loss: 0.180688] [ema: 0.999818] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_25_01_37_21/Model



[Epoch 126/166] [Batch 0/303] [D loss: 0.407165] [G loss: 0.184373] [ema: 0.999818] 
[Epoch 126/166] [Batch 100/303] [D loss: 0.431313] [G loss: 0.185256] [ema: 0.999819] 
[Epoch 126/166] [Batch 200/303] [D loss: 0.393996] [G loss: 0.170660] [ema: 0.999819] 
[Epoch 126/166] [Batch 300/303] [D loss: 0.379947] [G loss: 0.168170] [ema: 0.999820] 
[Epoch 127/166] [Batch 0/303] [D loss: 0.416255] [G loss: 0.173766] [ema: 0.999820] 
[Epoch 127/166] [Batch 100/303] [D loss: 0.416557] [G loss: 0.178338] [ema: 0.999820] 
[Epoch 127/166] [Batch 200/303] [D loss: 0.401404] [G loss: 0.176993] [ema: 0.999821] 
[Epoch 127/166] [Batch 300/303] [D loss: 0.379107] [G loss: 0.178378] [ema: 0.999821] 
[Epoch 128/166] [Batch 0/303] [D loss: 0.388470] [G loss: 0.183144] [ema: 0.999821] 
[Epoch 128/166] [Batch 100/303] [D loss: 0.372823] [G loss: 0.184495] [ema: 0.999822] 
[Epoch 128/166] [Batch 200/303] [D loss: 0.410699] [G loss: 0.188006] [ema: 0.999822] 
[Epoch 128/166] [Batch 300/303] [D loss: 0.387952] [G loss: 0.174555] [ema: 0.999823] 
[Epoch 129/166] [Batch 0/303] [D loss: 0.401464] [G loss: 0.169394] [ema: 0.999823] 
[Epoch 129/166] [Batch 100/303] [D loss: 0.370801] [G loss: 0.181298] [ema: 0.999823] 
[Epoch 129/166] [Batch 200/303] [D loss: 0.452335] [G loss: 0.170345] [ema: 0.999824] 
[Epoch 129/166] [Batch 300/303] [D loss: 0.393682] [G loss: 0.176430] [ema: 0.999824] 
[Epoch 130/166] [Batch 0/303] [D loss: 0.407389] [G loss: 0.175944] [ema: 0.999824] 
[Epoch 130/166] [Batch 100/303] [D loss: 0.372286] [G loss: 0.184107] [ema: 0.999824] 
[Epoch 130/166] [Batch 200/303] [D loss: 0.443846] [G loss: 0.166890] [ema: 0.999825] 
[Epoch 130/166] [Batch 300/303] [D loss: 0.404638] [G loss: 0.167035] [ema: 0.999825] 
[Epoch 131/166] [Batch 0/303] [D loss: 0.443628] [G loss: 0.172958] [ema: 0.999825] 
[Epoch 131/166] [Batch 100/303] [D loss: 0.404959] [G loss: 0.173778] [ema: 0.999826] 
[Epoch 131/166] [Batch 200/303] [D loss: 0.400460] [G loss: 0.180540] [ema: 0.999826] 
[Epoch 131/166] [Batch 300/303] [D loss: 0.432092] [G loss: 0.174743] [ema: 0.999827] 
[Epoch 132/166] [Batch 0/303] [D loss: 0.377158] [G loss: 0.187289] [ema: 0.999827] 
[Epoch 132/166] [Batch 100/303] [D loss: 0.408174] [G loss: 0.187042] [ema: 0.999827] 
[Epoch 132/166] [Batch 200/303] [D loss: 0.392867] [G loss: 0.181866] [ema: 0.999828] 
[Epoch 132/166] [Batch 300/303] [D loss: 0.399636] [G loss: 0.174101] [ema: 0.999828] 
[Epoch 133/166] [Batch 0/303] [D loss: 0.438422] [G loss: 0.176417] [ema: 0.999828] 
[Epoch 133/166] [Batch 100/303] [D loss: 0.431100] [G loss: 0.173560] [ema: 0.999828] 
[Epoch 133/166] [Batch 200/303] [D loss: 0.414207] [G loss: 0.183462] [ema: 0.999829] 
[Epoch 133/166] [Batch 300/303] [D loss: 0.397218] [G loss: 0.177869] [ema: 0.999829] 
[Epoch 134/166] [Batch 0/303] [D loss: 0.477912] [G loss: 0.188347] [ema: 0.999829] 
[Epoch 134/166] [Batch 100/303] [D loss: 0.408539] [G loss: 0.166418] [ema: 0.999830] 
[Epoch 134/166] [Batch 200/303] [D loss: 0.396582] [G loss: 0.167824] [ema: 0.999830] 
[Epoch 134/166] [Batch 300/303] [D loss: 0.375597] [G loss: 0.173236] [ema: 0.999831] 
[Epoch 135/166] [Batch 0/303] [D loss: 0.430248] [G loss: 0.182282] [ema: 0.999831] 
[Epoch 135/166] [Batch 100/303] [D loss: 0.389735] [G loss: 0.181906] [ema: 0.999831] 
[Epoch 135/166] [Batch 200/303] [D loss: 0.426363] [G loss: 0.172365] [ema: 0.999831] 
[Epoch 135/166] [Batch 300/303] [D loss: 0.429717] [G loss: 0.169750] [ema: 0.999832] 
[Epoch 136/166] [Batch 0/303] [D loss: 0.438901] [G loss: 0.169264] [ema: 0.999832] 
[Epoch 136/166] [Batch 100/303] [D loss: 0.363868] [G loss: 0.175933] [ema: 0.999832] 
[Epoch 136/166] [Batch 200/303] [D loss: 0.427775] [G loss: 0.170022] [ema: 0.999833] 
[Epoch 136/166] [Batch 300/303] [D loss: 0.428471] [G loss: 0.182622] [ema: 0.999833] 
[Epoch 137/166] [Batch 0/303] [D loss: 0.424524] [G loss: 0.174991] [ema: 0.999833] 
[Epoch 137/166] [Batch 100/303] [D loss: 0.413311] [G loss: 0.179904] [ema: 0.999833] 
[Epoch 137/166] [Batch 200/303] [D loss: 0.430618] [G loss: 0.172423] [ema: 0.999834] 
[Epoch 137/166] [Batch 300/303] [D loss: 0.424028] [G loss: 0.163946] [ema: 0.999834] 
[Epoch 138/166] [Batch 0/303] [D loss: 0.404920] [G loss: 0.174804] [ema: 0.999834] 
[Epoch 138/166] [Batch 100/303] [D loss: 0.410128] [G loss: 0.172122] [ema: 0.999835] 
[Epoch 138/166] [Batch 200/303] [D loss: 0.409176] [G loss: 0.168241] [ema: 0.999835] 
[Epoch 138/166] [Batch 300/303] [D loss: 0.365314] [G loss: 0.174324] [ema: 0.999835] 
[Epoch 139/166] [Batch 0/303] [D loss: 0.369565] [G loss: 0.182481] [ema: 0.999835] 
[Epoch 139/166] [Batch 100/303] [D loss: 0.412071] [G loss: 0.188873] [ema: 0.999836] 
[Epoch 139/166] [Batch 200/303] [D loss: 0.429877] [G loss: 0.175071] [ema: 0.999836] 
[Epoch 139/166] [Batch 300/303] [D loss: 0.420691] [G loss: 0.186780] [ema: 0.999837] 
[Epoch 140/166] [Batch 0/303] [D loss: 0.393336] [G loss: 0.181275] [ema: 0.999837] 
[Epoch 140/166] [Batch 100/303] [D loss: 0.373803] [G loss: 0.181953] [ema: 0.999837] 
[Epoch 140/166] [Batch 200/303] [D loss: 0.403084] [G loss: 0.166540] [ema: 0.999837] 
[Epoch 140/166] [Batch 300/303] [D loss: 0.402556] [G loss: 0.173278] [ema: 0.999838] 
[Epoch 141/166] [Batch 0/303] [D loss: 0.386919] [G loss: 0.182483] [ema: 0.999838] 
[Epoch 141/166] [Batch 100/303] [D loss: 0.409132] [G loss: 0.166670] [ema: 0.999838] 
[Epoch 141/166] [Batch 200/303] [D loss: 0.401984] [G loss: 0.184072] [ema: 0.999839] 
[Epoch 141/166] [Batch 300/303] [D loss: 0.441010] [G loss: 0.182783] [ema: 0.999839] 
[Epoch 142/166] [Batch 0/303] [D loss: 0.426839] [G loss: 0.180674] [ema: 0.999839] 
[Epoch 142/166] [Batch 100/303] [D loss: 0.432747] [G loss: 0.171049] [ema: 0.999839] 
[Epoch 142/166] [Batch 200/303] [D loss: 0.401814] [G loss: 0.175264] [ema: 0.999840] 
[Epoch 142/166] [Batch 300/303] [D loss: 0.422515] [G loss: 0.162051] [ema: 0.999840] 
[Epoch 143/166] [Batch 0/303] [D loss: 0.431263] [G loss: 0.170031] [ema: 0.999840] 
[Epoch 143/166] [Batch 100/303] [D loss: 0.415605] [G loss: 0.177252] [ema: 0.999840] 
[Epoch 143/166] [Batch 200/303] [D loss: 0.397377] [G loss: 0.171791] [ema: 0.999841] 
[Epoch 143/166] [Batch 300/303] [D loss: 0.422144] [G loss: 0.167315] [ema: 0.999841] 
[Epoch 144/166] [Batch 0/303] [D loss: 0.434358] [G loss: 0.158054] [ema: 0.999841] 
[Epoch 144/166] [Batch 100/303] [D loss: 0.397374] [G loss: 0.168439] [ema: 0.999842] 
[Epoch 144/166] [Batch 200/303] [D loss: 0.526556] [G loss: 0.167658] [ema: 0.999842] 
[Epoch 144/166] [Batch 300/303] [D loss: 0.467688] [G loss: 0.173620] [ema: 0.999842] 
[Epoch 145/166] [Batch 0/303] [D loss: 0.380655] [G loss: 0.175451] [ema: 0.999842] 
[Epoch 145/166] [Batch 100/303] [D loss: 0.425502] [G loss: 0.165842] [ema: 0.999843] 
[Epoch 145/166] [Batch 200/303] [D loss: 0.367729] [G loss: 0.172416] [ema: 0.999843] 
[Epoch 145/166] [Batch 300/303] [D loss: 0.422392] [G loss: 0.170522] [ema: 0.999843] 
[Epoch 146/166] [Batch 0/303] [D loss: 0.404532] [G loss: 0.168774] [ema: 0.999843] 
[Epoch 146/166] [Batch 100/303] [D loss: 0.400210] [G loss: 0.183033] [ema: 0.999844] 
[Epoch 146/166] [Batch 200/303] [D loss: 0.374347] [G loss: 0.176611] [ema: 0.999844] 
[Epoch 146/166] [Batch 300/303] [D loss: 0.398795] [G loss: 0.169933] [ema: 0.999844] 
[Epoch 147/166] [Batch 0/303] [D loss: 0.384057] [G loss: 0.167473] [ema: 0.999844] 
[Epoch 147/166] [Batch 100/303] [D loss: 0.406687] [G loss: 0.165785] [ema: 0.999845] 
[Epoch 147/166] [Batch 200/303] [D loss: 0.390588] [G loss: 0.172891] [ema: 0.999845] 
[Epoch 147/166] [Batch 300/303] [D loss: 0.427490] [G loss: 0.173354] [ema: 0.999845] 
[Epoch 148/166] [Batch 0/303] [D loss: 0.370067] [G loss: 0.172882] [ema: 0.999845] 
[Epoch 148/166] [Batch 100/303] [D loss: 0.388601] [G loss: 0.177931] [ema: 0.999846] 
[Epoch 148/166] [Batch 200/303] [D loss: 0.397642] [G loss: 0.177834] [ema: 0.999846] 
[Epoch 148/166] [Batch 300/303] [D loss: 0.430453] [G loss: 0.166602] [ema: 0.999846] 
[Epoch 149/166] [Batch 0/303] [D loss: 0.432042] [G loss: 0.169732] [ema: 0.999846] 
[Epoch 149/166] [Batch 100/303] [D loss: 0.397119] [G loss: 0.188636] [ema: 0.999847] 
[Epoch 149/166] [Batch 200/303] [D loss: 0.396724] [G loss: 0.179420] [ema: 0.999847] 
[Epoch 149/166] [Batch 300/303] [D loss: 0.423137] [G loss: 0.149082] [ema: 0.999847] 
[Epoch 150/166] [Batch 0/303] [D loss: 0.433675] [G loss: 0.160087] [ema: 0.999848] 
[Epoch 150/166] [Batch 100/303] [D loss: 0.431049] [G loss: 0.168070] [ema: 0.999848] 
[Epoch 150/166] [Batch 200/303] [D loss: 0.389158] [G loss: 0.165514] [ema: 0.999848] 
[Epoch 150/166] [Batch 300/303] [D loss: 0.406227] [G loss: 0.178052] [ema: 0.999849] 
[Epoch 151/166] [Batch 0/303] [D loss: 0.431139] [G loss: 0.181146] [ema: 0.999849] 
[Epoch 151/166] [Batch 100/303] [D loss: 0.383638] [G loss: 0.172851] [ema: 0.999849] 
[Epoch 151/166] [Batch 200/303] [D loss: 0.400612] [G loss: 0.174721] [ema: 0.999849] 
[Epoch 151/166] [Batch 300/303] [D loss: 0.393854] [G loss: 0.178733] [ema: 0.999850] 
[Epoch 152/166] [Batch 0/303] [D loss: 0.391418] [G loss: 0.186479] [ema: 0.999850] 
[Epoch 152/166] [Batch 100/303] [D loss: 0.440296] [G loss: 0.168287] [ema: 0.999850] 
[Epoch 152/166] [Batch 200/303] [D loss: 0.402341] [G loss: 0.184962] [ema: 0.999850] 
[Epoch 152/166] [Batch 300/303] [D loss: 0.392929] [G loss: 0.183861] [ema: 0.999850] 
[Epoch 153/166] [Batch 0/303] [D loss: 0.405402] [G loss: 0.187119] [ema: 0.999850] 
[Epoch 153/166] [Batch 100/303] [D loss: 0.430579] [G loss: 0.165394] [ema: 0.999851] 
[Epoch 153/166] [Batch 200/303] [D loss: 0.438094] [G loss: 0.168087] [ema: 0.999851] 
[Epoch 153/166] [Batch 300/303] [D loss: 0.440385] [G loss: 0.172624] [ema: 0.999851] 
[Epoch 154/166] [Batch 0/303] [D loss: 0.380630] [G loss: 0.178601] [ema: 0.999851] 
[Epoch 154/166] [Batch 100/303] [D loss: 0.401883] [G loss: 0.172372] [ema: 0.999852] 
[Epoch 154/166] [Batch 200/303] [D loss: 0.390956] [G loss: 0.178264] [ema: 0.999852] 
[Epoch 154/166] [Batch 300/303] [D loss: 0.449112] [G loss: 0.158547] [ema: 0.999852] 
[Epoch 155/166] [Batch 0/303] [D loss: 0.435423] [G loss: 0.171778] [ema: 0.999852] 
[Epoch 155/166] [Batch 100/303] [D loss: 0.399731] [G loss: 0.172813] [ema: 0.999853] 
[Epoch 155/166] [Batch 200/303] [D loss: 0.397569] [G loss: 0.181106] [ema: 0.999853] 
[Epoch 155/166] [Batch 300/303] [D loss: 0.479085] [G loss: 0.170312] [ema: 0.999853] 
[Epoch 156/166] [Batch 0/303] [D loss: 0.396845] [G loss: 0.182058] [ema: 0.999853] 
[Epoch 156/166] [Batch 100/303] [D loss: 0.401038] [G loss: 0.174796] [ema: 0.999854] 
[Epoch 156/166] [Batch 200/303] [D loss: 0.431258] [G loss: 0.167912] [ema: 0.999854] 
[Epoch 156/166] [Batch 300/303] [D loss: 0.411033] [G loss: 0.172921] [ema: 0.999854] 
[Epoch 157/166] [Batch 0/303] [D loss: 0.424383] [G loss: 0.180953] [ema: 0.999854] 
[Epoch 157/166] [Batch 100/303] [D loss: 0.411554] [G loss: 0.178838] [ema: 0.999855] 
[Epoch 157/166] [Batch 200/303] [D loss: 0.443451] [G loss: 0.163709] [ema: 0.999855] 
[Epoch 157/166] [Batch 300/303] [D loss: 0.432112] [G loss: 0.158994] [ema: 0.999855] 
[Epoch 158/166] [Batch 0/303] [D loss: 0.438577] [G loss: 0.164389] [ema: 0.999855] 
[Epoch 158/166] [Batch 100/303] [D loss: 0.406160] [G loss: 0.158015] [ema: 0.999856] 
[Epoch 158/166] [Batch 200/303] [D loss: 0.444384] [G loss: 0.161383] [ema: 0.999856] 
[Epoch 158/166] [Batch 300/303] [D loss: 0.444127] [G loss: 0.165443] [ema: 0.999856] 
[Epoch 159/166] [Batch 0/303] [D loss: 0.444973] [G loss: 0.162066] [ema: 0.999856] 
[Epoch 159/166] [Batch 100/303] [D loss: 0.451885] [G loss: 0.166731] [ema: 0.999856] 
[Epoch 159/166] [Batch 200/303] [D loss: 0.423819] [G loss: 0.148027] [ema: 0.999857] 
[Epoch 159/166] [Batch 300/303] [D loss: 0.443646] [G loss: 0.161496] [ema: 0.999857] 
[Epoch 160/166] [Batch 0/303] [D loss: 0.440560] [G loss: 0.167160] [ema: 0.999857] 
[Epoch 160/166] [Batch 100/303] [D loss: 0.509038] [G loss: 0.167844] [ema: 0.999857] 
[Epoch 160/166] [Batch 200/303] [D loss: 0.444461] [G loss: 0.155802] [ema: 0.999858] 
[Epoch 160/166] [Batch 300/303] [D loss: 0.436240] [G loss: 0.167822] [ema: 0.999858] 
[Epoch 161/166] [Batch 0/303] [D loss: 0.435460] [G loss: 0.171927] [ema: 0.999858] 
[Epoch 161/166] [Batch 100/303] [D loss: 0.458100] [G loss: 0.159857] [ema: 0.999858] 
[Epoch 161/166] [Batch 200/303] [D loss: 0.439008] [G loss: 0.166740] [ema: 0.999859] 
[Epoch 161/166] [Batch 300/303] [D loss: 0.508697] [G loss: 0.170604] [ema: 0.999859] 
[Epoch 162/166] [Batch 0/303] [D loss: 0.417395] [G loss: 0.171921] [ema: 0.999859] 
[Epoch 162/166] [Batch 100/303] [D loss: 0.431659] [G loss: 0.178515] [ema: 0.999859] 
[Epoch 162/166] [Batch 200/303] [D loss: 0.452478] [G loss: 0.164706] [ema: 0.999859] 
[Epoch 162/166] [Batch 300/303] [D loss: 0.426222] [G loss: 0.184363] [ema: 0.999860] 
[Epoch 163/166] [Batch 0/303] [D loss: 0.438740] [G loss: 0.163777] [ema: 0.999860] 
[Epoch 163/166] [Batch 100/303] [D loss: 0.424854] [G loss: 0.162735] [ema: 0.999860] 
[Epoch 163/166] [Batch 200/303] [D loss: 0.429151] [G loss: 0.135626] [ema: 0.999860] 
[Epoch 163/166] [Batch 300/303] [D loss: 0.381699] [G loss: 0.169389] [ema: 0.999861] 
[Epoch 164/166] [Batch 0/303] [D loss: 0.392957] [G loss: 0.198156] [ema: 0.999861] 
[Epoch 164/166] [Batch 100/303] [D loss: 0.424444] [G loss: 0.163445] [ema: 0.999861] 
[Epoch 164/166] [Batch 200/303] [D loss: 0.416163] [G loss: 0.174302] [ema: 0.999861] 
[Epoch 164/166] [Batch 300/303] [D loss: 0.409360] [G loss: 0.161132] [ema: 0.999861] 
[Epoch 165/166] [Batch 0/303] [D loss: 0.450352] [G loss: 0.172406] [ema: 0.999861] 
[Epoch 165/166] [Batch 100/303] [D loss: 0.408898] [G loss: 0.180901] [ema: 0.999862] 
[Epoch 165/166] [Batch 200/303] [D loss: 0.425531] [G loss: 0.176438] [ema: 0.999862] 
[Epoch 165/166] [Batch 300/303] [D loss: 0.411828] [G loss: 0.166504] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
RealWorld_waist_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
RealWorld_waist_DAGHAR_Multiclass
daghar
return single class data and labels, class is RealWorld_waist_DAGHAR_Multiclass
data shape is (20664, 3, 1, 30)
label shape is (20664,)
1292
Epochs between checkpoint: 10



Saving checkpoint 1 in logs/daghar_split_dataset_50000_30_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_30_2024_10_25_02_11_06/Model



[Epoch 0/39] [Batch 0/1292] [D loss: 0.970593] [G loss: 0.898036] [ema: 0.000000] 
[Epoch 0/39] [Batch 100/1292] [D loss: 0.529367] [G loss: 0.159822] [ema: 0.933033] 
[Epoch 0/39] [Batch 200/1292] [D loss: 0.547210] [G loss: 0.139867] [ema: 0.965936] 
[Epoch 0/39] [Batch 300/1292] [D loss: 0.387203] [G loss: 0.216739] [ema: 0.977160] 
[Epoch 0/39] [Batch 400/1292] [D loss: 0.398110] [G loss: 0.154983] [ema: 0.982821] 
[Epoch 0/39] [Batch 500/1292] [D loss: 0.522781] [G loss: 0.166973] [ema: 0.986233] 
[Epoch 0/39] [Batch 600/1292] [D loss: 0.394468] [G loss: 0.215678] [ema: 0.988514] 
[Epoch 0/39] [Batch 700/1292] [D loss: 0.471743] [G loss: 0.178938] [ema: 0.990147] 
[Epoch 0/39] [Batch 800/1292] [D loss: 0.430513] [G loss: 0.177857] [ema: 0.991373] 
[Epoch 0/39] [Batch 900/1292] [D loss: 0.429856] [G loss: 0.182838] [ema: 0.992328] 
[Epoch 0/39] [Batch 1000/1292] [D loss: 0.482082] [G loss: 0.135101] [ema: 0.993092] 
[Epoch 0/39] [Batch 1100/1292] [D loss: 0.475886] [G loss: 0.156672] [ema: 0.993718] 
[Epoch 0/39] [Batch 1200/1292] [D loss: 0.501191] [G loss: 0.142367] [ema: 0.994240] 
[Epoch 1/39] [Batch 0/1292] [D loss: 0.440726] [G loss: 0.171877] [ema: 0.994649] 
[Epoch 1/39] [Batch 100/1292] [D loss: 0.471203] [G loss: 0.178094] [ema: 0.995033] 
[Epoch 1/39] [Batch 200/1292] [D loss: 0.429695] [G loss: 0.149926] [ema: 0.995365] 
[Epoch 1/39] [Batch 300/1292] [D loss: 0.416087] [G loss: 0.184503] [ema: 0.995656] 
[Epoch 1/39] [Batch 400/1292] [D loss: 0.386599] [G loss: 0.143350] [ema: 0.995912] 
[Epoch 1/39] [Batch 500/1292] [D loss: 0.448779] [G loss: 0.122724] [ema: 0.996139] 
[Epoch 1/39] [Batch 600/1292] [D loss: 0.466371] [G loss: 0.174738] [ema: 0.996343] 
[Epoch 1/39] [Batch 700/1292] [D loss: 0.461260] [G loss: 0.139764] [ema: 0.996526] 
[Epoch 1/39] [Batch 800/1292] [D loss: 0.441884] [G loss: 0.168507] [ema: 0.996692] 
[Epoch 1/39] [Batch 900/1292] [D loss: 0.383221] [G loss: 0.142885] [ema: 0.996843] 
[Epoch 1/39] [Batch 1000/1292] [D loss: 0.449609] [G loss: 0.182565] [ema: 0.996980] 
[Epoch 1/39] [Batch 1100/1292] [D loss: 0.425553] [G loss: 0.215224] [ema: 0.997106] 
[Epoch 1/39] [Batch 1200/1292] [D loss: 0.338745] [G loss: 0.191419] [ema: 0.997222] 
[Epoch 2/39] [Batch 0/1292] [D loss: 0.340140] [G loss: 0.188486] [ema: 0.997321] 
[Epoch 2/39] [Batch 100/1292] [D loss: 0.412417] [G loss: 0.185071] [ema: 0.997421] 
[Epoch 2/39] [Batch 200/1292] [D loss: 0.369701] [G loss: 0.202433] [ema: 0.997513] 
[Epoch 2/39] [Batch 300/1292] [D loss: 0.360012] [G loss: 0.193689] [ema: 0.997599] 
[Epoch 2/39] [Batch 400/1292] [D loss: 0.420985] [G loss: 0.193978] [ema: 0.997680] 
[Epoch 2/39] [Batch 500/1292] [D loss: 0.380387] [G loss: 0.212681] [ema: 0.997755] 
[Epoch 2/39] [Batch 600/1292] [D loss: 0.402192] [G loss: 0.182010] [ema: 0.997825] 
[Epoch 2/39] [Batch 700/1292] [D loss: 0.391888] [G loss: 0.187179] [ema: 0.997892] 
[Epoch 2/39] [Batch 800/1292] [D loss: 0.395971] [G loss: 0.210427] [ema: 0.997954] 
[Epoch 2/39] [Batch 900/1292] [D loss: 0.340570] [G loss: 0.185498] [ema: 0.998012] 
[Epoch 2/39] [Batch 1000/1292] [D loss: 0.419829] [G loss: 0.165922] [ema: 0.998068] 
[Epoch 2/39] [Batch 1100/1292] [D loss: 0.380839] [G loss: 0.173955] [ema: 0.998120] 
[Epoch 2/39] [Batch 1200/1292] [D loss: 0.409983] [G loss: 0.190530] [ema: 0.998170] 
[Epoch 3/39] [Batch 0/1292] [D loss: 0.422632] [G loss: 0.184799] [ema: 0.998213] 
[Epoch 3/39] [Batch 100/1292] [D loss: 0.386890] [G loss: 0.184238] [ema: 0.998258] 
[Epoch 3/39] [Batch 200/1292] [D loss: 0.355350] [G loss: 0.176044] [ema: 0.998301] 
[Epoch 3/39] [Batch 300/1292] [D loss: 0.374094] [G loss: 0.182499] [ema: 0.998342] 
[Epoch 3/39] [Batch 400/1292] [D loss: 0.413335] [G loss: 0.177121] [ema: 0.998380] 
[Epoch 3/39] [Batch 500/1292] [D loss: 0.424818] [G loss: 0.194488] [ema: 0.998417] 
[Epoch 3/39] [Batch 600/1292] [D loss: 0.396081] [G loss: 0.178390] [ema: 0.998453] 
[Epoch 3/39] [Batch 700/1292] [D loss: 0.414801] [G loss: 0.181595] [ema: 0.998486] 
[Epoch 3/39] [Batch 800/1292] [D loss: 0.418895] [G loss: 0.173111] [ema: 0.998519] 
[Epoch 3/39] [Batch 900/1292] [D loss: 0.372008] [G loss: 0.181000] [ema: 0.998550] 
[Epoch 3/39] [Batch 1000/1292] [D loss: 0.401214] [G loss: 0.169381] [ema: 0.998579] 
[Epoch 3/39] [Batch 1100/1292] [D loss: 0.361954] [G loss: 0.178303] [ema: 0.998608] 
[Epoch 3/39] [Batch 1200/1292] [D loss: 0.426511] [G loss: 0.173120] [ema: 0.998635] 
[Epoch 4/39] [Batch 0/1292] [D loss: 0.388302] [G loss: 0.198322] [ema: 0.998660] 
[Epoch 4/39] [Batch 100/1292] [D loss: 0.383720] [G loss: 0.205864] [ema: 0.998685] 
[Epoch 4/39] [Batch 200/1292] [D loss: 0.361346] [G loss: 0.188885] [ema: 0.998710] 
[Epoch 4/39] [Batch 300/1292] [D loss: 0.346548] [G loss: 0.187763] [ema: 0.998733] 
[Epoch 4/39] [Batch 400/1292] [D loss: 0.418023] [G loss: 0.184778] [ema: 0.998756] 
[Epoch 4/39] [Batch 500/1292] [D loss: 0.377036] [G loss: 0.180882] [ema: 0.998778] 
[Epoch 4/39] [Batch 600/1292] [D loss: 0.436245] [G loss: 0.188896] [ema: 0.998799] 
[Epoch 4/39] [Batch 700/1292] [D loss: 0.451441] [G loss: 0.167666] [ema: 0.998819] 
[Epoch 4/39] [Batch 800/1292] [D loss: 0.409867] [G loss: 0.174107] [ema: 0.998839] 
[Epoch 4/39] [Batch 900/1292] [D loss: 0.386114] [G loss: 0.175661] [ema: 0.998858] 
[Epoch 4/39] [Batch 1000/1292] [D loss: 0.395019] [G loss: 0.179059] [ema: 0.998877] 
[Epoch 4/39] [Batch 1100/1292] [D loss: 0.350927] [G loss: 0.187378] [ema: 0.998895] 
[Epoch 4/39] [Batch 1200/1292] [D loss: 0.376103] [G loss: 0.185972] [ema: 0.998912] 
[Epoch 5/39] [Batch 0/1292] [D loss: 0.367963] [G loss: 0.179909] [ema: 0.998928] 
[Epoch 5/39] [Batch 100/1292] [D loss: 0.396294] [G loss: 0.176218] [ema: 0.998944] 
[Epoch 5/39] [Batch 200/1292] [D loss: 0.401362] [G loss: 0.191263] [ema: 0.998960] 
[Epoch 5/39] [Batch 300/1292] [D loss: 0.369190] [G loss: 0.185526] [ema: 0.998975] 
[Epoch 5/39] [Batch 400/1292] [D loss: 0.441468] [G loss: 0.180811] [ema: 0.998990] 
[Epoch 5/39] [Batch 500/1292] [D loss: 0.391295] [G loss: 0.180407] [ema: 0.999005] 
[Epoch 5/39] [Batch 600/1292] [D loss: 0.400868] [G loss: 0.194779] [ema: 0.999019] 
[Epoch 5/39] [Batch 700/1292] [D loss: 0.398875] [G loss: 0.173617] [ema: 0.999032] 
[Epoch 5/39] [Batch 800/1292] [D loss: 0.414121] [G loss: 0.160259] [ema: 0.999046] 
[Epoch 5/39] [Batch 900/1292] [D loss: 0.391989] [G loss: 0.190895] [ema: 0.999059] 
[Epoch 5/39] [Batch 1000/1292] [D loss: 0.408368] [G loss: 0.181801] [ema: 0.999071] 
[Epoch 5/39] [Batch 1100/1292] [D loss: 0.395337] [G loss: 0.171599] [ema: 0.999084] 
[Epoch 5/39] [Batch 1200/1292] [D loss: 0.399102] [G loss: 0.175696] [ema: 0.999096] 
[Epoch 6/39] [Batch 0/1292] [D loss: 0.384109] [G loss: 0.192731] [ema: 0.999106] 
[Epoch 6/39] [Batch 100/1292] [D loss: 0.434575] [G loss: 0.175431] [ema: 0.999118] 
[Epoch 6/39] [Batch 200/1292] [D loss: 0.408621] [G loss: 0.159475] [ema: 0.999129] 
[Epoch 6/39] [Batch 300/1292] [D loss: 0.409161] [G loss: 0.153729] [ema: 0.999140] 
[Epoch 6/39] [Batch 400/1292] [D loss: 0.358195] [G loss: 0.175515] [ema: 0.999150] 
[Epoch 6/39] [Batch 500/1292] [D loss: 0.367904] [G loss: 0.186694] [ema: 0.999160] 
[Epoch 6/39] [Batch 600/1292] [D loss: 0.417417] [G loss: 0.171773] [ema: 0.999170] 
[Epoch 6/39] [Batch 700/1292] [D loss: 0.422908] [G loss: 0.179766] [ema: 0.999180] 
[Epoch 6/39] [Batch 800/1292] [D loss: 0.387824] [G loss: 0.183236] [ema: 0.999190] 
[Epoch 6/39] [Batch 900/1292] [D loss: 0.386641] [G loss: 0.180265] [ema: 0.999199] 
[Epoch 6/39] [Batch 1000/1292] [D loss: 0.414107] [G loss: 0.195139] [ema: 0.999208] 
[Epoch 6/39] [Batch 1100/1292] [D loss: 0.393271] [G loss: 0.170583] [ema: 0.999217] 
[Epoch 6/39] [Batch 1200/1292] [D loss: 0.397354] [G loss: 0.189559] [ema: 0.999226] 
[Epoch 7/39] [Batch 0/1292] [D loss: 0.395365] [G loss: 0.201213] [ema: 0.999234] 
[Epoch 7/39] [Batch 100/1292] [D loss: 0.412308] [G loss: 0.177872] [ema: 0.999242] 
[Epoch 7/39] [Batch 200/1292] [D loss: 0.387212] [G loss: 0.180749] [ema: 0.999250] 
[Epoch 7/39] [Batch 300/1292] [D loss: 0.393480] [G loss: 0.180447] [ema: 0.999258] 
[Epoch 7/39] [Batch 400/1292] [D loss: 0.389082] [G loss: 0.176237] [ema: 0.999266] 
[Epoch 7/39] [Batch 500/1292] [D loss: 0.412804] [G loss: 0.182856] [ema: 0.999274] 
[Epoch 7/39] [Batch 600/1292] [D loss: 0.383775] [G loss: 0.201061] [ema: 0.999282] 
[Epoch 7/39] [Batch 700/1292] [D loss: 0.426741] [G loss: 0.186596] [ema: 0.999289] 
[Epoch 7/39] [Batch 800/1292] [D loss: 0.378303] [G loss: 0.180213] [ema: 0.999296] 
[Epoch 7/39] [Batch 900/1292] [D loss: 0.364663] [G loss: 0.185094] [ema: 0.999303] 
[Epoch 7/39] [Batch 1000/1292] [D loss: 0.394311] [G loss: 0.192152] [ema: 0.999310] 
[Epoch 7/39] [Batch 1100/1292] [D loss: 0.361995] [G loss: 0.171541] [ema: 0.999317] 
[Epoch 7/39] [Batch 1200/1292] [D loss: 0.366188] [G loss: 0.194131] [ema: 0.999324] 
[Epoch 8/39] [Batch 0/1292] [D loss: 0.416263] [G loss: 0.180324] [ema: 0.999330] 
[Epoch 8/39] [Batch 100/1292] [D loss: 0.393253] [G loss: 0.183305] [ema: 0.999336] 
[Epoch 8/39] [Batch 200/1292] [D loss: 0.411480] [G loss: 0.169001] [ema: 0.999342] 
[Epoch 8/39] [Batch 300/1292] [D loss: 0.387291] [G loss: 0.171397] [ema: 0.999349] 
[Epoch 8/39] [Batch 400/1292] [D loss: 0.345412] [G loss: 0.180148] [ema: 0.999355] 
[Epoch 8/39] [Batch 500/1292] [D loss: 0.393310] [G loss: 0.195521] [ema: 0.999361] 
[Epoch 8/39] [Batch 600/1292] [D loss: 0.399445] [G loss: 0.174680] [ema: 0.999366] 
[Epoch 8/39] [Batch 700/1292] [D loss: 0.373984] [G loss: 0.186027] [ema: 0.999372] 
[Epoch 8/39] [Batch 800/1292] [D loss: 0.389371] [G loss: 0.171280] [ema: 0.999378] 
[Epoch 8/39] [Batch 900/1292] [D loss: 0.366274] [G loss: 0.187319] [ema: 0.999383] 
[Epoch 8/39] [Batch 1000/1292] [D loss: 0.379458] [G loss: 0.182847] [ema: 0.999389] 
[Epoch 8/39] [Batch 1100/1292] [D loss: 0.387117] [G loss: 0.186180] [ema: 0.999394] 
[Epoch 8/39] [Batch 1200/1292] [D loss: 0.392076] [G loss: 0.176994] [ema: 0.999399] 
[Epoch 9/39] [Batch 0/1292] [D loss: 0.419231] [G loss: 0.182635] [ema: 0.999404] 
[Epoch 9/39] [Batch 100/1292] [D loss: 0.375820] [G loss: 0.184432] [ema: 0.999409] 
[Epoch 9/39] [Batch 200/1292] [D loss: 0.398673] [G loss: 0.184127] [ema: 0.999414] 
[Epoch 9/39] [Batch 300/1292] [D loss: 0.374823] [G loss: 0.182270] [ema: 0.999419] 
[Epoch 9/39] [Batch 400/1292] [D loss: 0.376550] [G loss: 0.197794] [ema: 0.999424] 
[Epoch 9/39] [Batch 500/1292] [D loss: 0.401191] [G loss: 0.170612] [ema: 0.999429] 
[Epoch 9/39] [Batch 600/1292] [D loss: 0.400486] [G loss: 0.193370] [ema: 0.999433] 
[Epoch 9/39] [Batch 700/1292] [D loss: 0.399232] [G loss: 0.197623] [ema: 0.999438] 
[Epoch 9/39] [Batch 800/1292] [D loss: 0.397580] [G loss: 0.171833] [ema: 0.999442] 
[Epoch 9/39] [Batch 900/1292] [D loss: 0.384071] [G loss: 0.178640] [ema: 0.999447] 
[Epoch 9/39] [Batch 1000/1292] [D loss: 0.396414] [G loss: 0.185869] [ema: 0.999451] 
[Epoch 9/39] [Batch 1100/1292] [D loss: 0.388472] [G loss: 0.179126] [ema: 0.999456] 
[Epoch 9/39] [Batch 1200/1292] [D loss: 0.388065] [G loss: 0.181580] [ema: 0.999460] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_30_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_30_2024_10_25_02_11_06/Model



[Epoch 10/39] [Batch 0/1292] [D loss: 0.378709] [G loss: 0.185561] [ema: 0.999464] 
[Epoch 10/39] [Batch 100/1292] [D loss: 0.407676] [G loss: 0.176008] [ema: 0.999468] 
[Epoch 10/39] [Batch 200/1292] [D loss: 0.394522] [G loss: 0.198824] [ema: 0.999472] 
[Epoch 10/39] [Batch 300/1292] [D loss: 0.406360] [G loss: 0.188648] [ema: 0.999476] 
[Epoch 10/39] [Batch 400/1292] [D loss: 0.408175] [G loss: 0.178299] [ema: 0.999480] 
[Epoch 10/39] [Batch 500/1292] [D loss: 0.385921] [G loss: 0.162781] [ema: 0.999484] 
[Epoch 10/39] [Batch 600/1292] [D loss: 0.370907] [G loss: 0.191274] [ema: 0.999487] 
[Epoch 10/39] [Batch 700/1292] [D loss: 0.388016] [G loss: 0.180533] [ema: 0.999491] 
[Epoch 10/39] [Batch 800/1292] [D loss: 0.340148] [G loss: 0.193903] [ema: 0.999495] 
[Epoch 10/39] [Batch 900/1292] [D loss: 0.384854] [G loss: 0.189318] [ema: 0.999499] 
[Epoch 10/39] [Batch 1000/1292] [D loss: 0.374952] [G loss: 0.194091] [ema: 0.999502] 
[Epoch 10/39] [Batch 1100/1292] [D loss: 0.408805] [G loss: 0.178772] [ema: 0.999506] 
[Epoch 10/39] [Batch 1200/1292] [D loss: 0.371702] [G loss: 0.186077] [ema: 0.999509] 
[Epoch 11/39] [Batch 0/1292] [D loss: 0.354546] [G loss: 0.205672] [ema: 0.999512] 
[Epoch 11/39] [Batch 100/1292] [D loss: 0.347133] [G loss: 0.178895] [ema: 0.999516] 
[Epoch 11/39] [Batch 200/1292] [D loss: 0.381900] [G loss: 0.192473] [ema: 0.999519] 
[Epoch 11/39] [Batch 300/1292] [D loss: 0.418835] [G loss: 0.184063] [ema: 0.999522] 
[Epoch 11/39] [Batch 400/1292] [D loss: 0.417771] [G loss: 0.172497] [ema: 0.999526] 
[Epoch 11/39] [Batch 500/1292] [D loss: 0.422151] [G loss: 0.162701] [ema: 0.999529] 
[Epoch 11/39] [Batch 600/1292] [D loss: 0.409566] [G loss: 0.169573] [ema: 0.999532] 
[Epoch 11/39] [Batch 700/1292] [D loss: 0.383618] [G loss: 0.180830] [ema: 0.999535] 
[Epoch 11/39] [Batch 800/1292] [D loss: 0.353284] [G loss: 0.202031] [ema: 0.999538] 
[Epoch 11/39] [Batch 900/1292] [D loss: 0.411208] [G loss: 0.187137] [ema: 0.999541] 
[Epoch 11/39] [Batch 1000/1292] [D loss: 0.442803] [G loss: 0.176904] [ema: 0.999544] 
[Epoch 11/39] [Batch 1100/1292] [D loss: 0.402138] [G loss: 0.183028] [ema: 0.999547] 
[Epoch 11/39] [Batch 1200/1292] [D loss: 0.405350] [G loss: 0.180504] [ema: 0.999550] 
[Epoch 12/39] [Batch 0/1292] [D loss: 0.359689] [G loss: 0.189312] [ema: 0.999553] 
[Epoch 12/39] [Batch 100/1292] [D loss: 0.400762] [G loss: 0.180997] [ema: 0.999556] 
[Epoch 12/39] [Batch 200/1292] [D loss: 0.353522] [G loss: 0.173848] [ema: 0.999559] 
[Epoch 12/39] [Batch 300/1292] [D loss: 0.394888] [G loss: 0.177233] [ema: 0.999562] 
[Epoch 12/39] [Batch 400/1292] [D loss: 0.366177] [G loss: 0.190944] [ema: 0.999564] 
[Epoch 12/39] [Batch 500/1292] [D loss: 0.425867] [G loss: 0.175302] [ema: 0.999567] 
[Epoch 12/39] [Batch 600/1292] [D loss: 0.397674] [G loss: 0.187010] [ema: 0.999570] 
[Epoch 12/39] [Batch 700/1292] [D loss: 0.406637] [G loss: 0.190684] [ema: 0.999572] 
[Epoch 12/39] [Batch 800/1292] [D loss: 0.368119] [G loss: 0.188295] [ema: 0.999575] 
[Epoch 12/39] [Batch 900/1292] [D loss: 0.412977] [G loss: 0.170067] [ema: 0.999578] 
[Epoch 12/39] [Batch 1000/1292] [D loss: 0.408735] [G loss: 0.187052] [ema: 0.999580] 
[Epoch 12/39] [Batch 1100/1292] [D loss: 0.349107] [G loss: 0.191709] [ema: 0.999583] 
[Epoch 12/39] [Batch 1200/1292] [D loss: 0.371130] [G loss: 0.185122] [ema: 0.999585] 
[Epoch 13/39] [Batch 0/1292] [D loss: 0.370696] [G loss: 0.200941] [ema: 0.999587] 
[Epoch 13/39] [Batch 100/1292] [D loss: 0.395750] [G loss: 0.181348] [ema: 0.999590] 
[Epoch 13/39] [Batch 200/1292] [D loss: 0.380029] [G loss: 0.192858] [ema: 0.999592] 
[Epoch 13/39] [Batch 300/1292] [D loss: 0.390138] [G loss: 0.192360] [ema: 0.999595] 
[Epoch 13/39] [Batch 400/1292] [D loss: 0.370546] [G loss: 0.207023] [ema: 0.999597] 
[Epoch 13/39] [Batch 500/1292] [D loss: 0.371947] [G loss: 0.185551] [ema: 0.999599] 
[Epoch 13/39] [Batch 600/1292] [D loss: 0.425568] [G loss: 0.188354] [ema: 0.999602] 
[Epoch 13/39] [Batch 700/1292] [D loss: 0.382974] [G loss: 0.197713] [ema: 0.999604] 
[Epoch 13/39] [Batch 800/1292] [D loss: 0.402046] [G loss: 0.180885] [ema: 0.999606] 
[Epoch 13/39] [Batch 900/1292] [D loss: 0.388042] [G loss: 0.172165] [ema: 0.999608] 
[Epoch 13/39] [Batch 1000/1292] [D loss: 0.370562] [G loss: 0.187705] [ema: 0.999611] 
[Epoch 13/39] [Batch 1100/1292] [D loss: 0.402777] [G loss: 0.182200] [ema: 0.999613] 
[Epoch 13/39] [Batch 1200/1292] [D loss: 0.379250] [G loss: 0.177438] [ema: 0.999615] 
[Epoch 14/39] [Batch 0/1292] [D loss: 0.399132] [G loss: 0.172131] [ema: 0.999617] 
[Epoch 14/39] [Batch 100/1292] [D loss: 0.357460] [G loss: 0.182136] [ema: 0.999619] 
[Epoch 14/39] [Batch 200/1292] [D loss: 0.349681] [G loss: 0.183512] [ema: 0.999621] 
[Epoch 14/39] [Batch 300/1292] [D loss: 0.380833] [G loss: 0.175899] [ema: 0.999623] 
[Epoch 14/39] [Batch 400/1292] [D loss: 0.409957] [G loss: 0.181580] [ema: 0.999625] 
[Epoch 14/39] [Batch 500/1292] [D loss: 0.426726] [G loss: 0.190261] [ema: 0.999627] 
[Epoch 14/39] [Batch 600/1292] [D loss: 0.386746] [G loss: 0.180502] [ema: 0.999629] 
[Epoch 14/39] [Batch 700/1292] [D loss: 0.419006] [G loss: 0.162050] [ema: 0.999631] 
[Epoch 14/39] [Batch 800/1292] [D loss: 0.362421] [G loss: 0.186146] [ema: 0.999633] 
[Epoch 14/39] [Batch 900/1292] [D loss: 0.416756] [G loss: 0.190061] [ema: 0.999635] 
[Epoch 14/39] [Batch 1000/1292] [D loss: 0.393413] [G loss: 0.177921] [ema: 0.999637] 
[Epoch 14/39] [Batch 1100/1292] [D loss: 0.364946] [G loss: 0.191836] [ema: 0.999639] 
[Epoch 14/39] [Batch 1200/1292] [D loss: 0.367236] [G loss: 0.185714] [ema: 0.999641] 
[Epoch 15/39] [Batch 0/1292] [D loss: 0.419627] [G loss: 0.195934] [ema: 0.999642] 
[Epoch 15/39] [Batch 100/1292] [D loss: 0.361183] [G loss: 0.184327] [ema: 0.999644] 
[Epoch 15/39] [Batch 200/1292] [D loss: 0.385039] [G loss: 0.175797] [ema: 0.999646] 
[Epoch 15/39] [Batch 300/1292] [D loss: 0.407993] [G loss: 0.187262] [ema: 0.999648] 
[Epoch 15/39] [Batch 400/1292] [D loss: 0.433510] [G loss: 0.175199] [ema: 0.999650] 
[Epoch 15/39] [Batch 500/1292] [D loss: 0.417504] [G loss: 0.186733] [ema: 0.999651] 
[Epoch 15/39] [Batch 600/1292] [D loss: 0.415578] [G loss: 0.175108] [ema: 0.999653] 
[Epoch 15/39] [Batch 700/1292] [D loss: 0.407106] [G loss: 0.190157] [ema: 0.999655] 
[Epoch 15/39] [Batch 800/1292] [D loss: 0.381900] [G loss: 0.190430] [ema: 0.999657] 
[Epoch 15/39] [Batch 900/1292] [D loss: 0.347640] [G loss: 0.185983] [ema: 0.999658] 
[Epoch 15/39] [Batch 1000/1292] [D loss: 0.419570] [G loss: 0.179076] [ema: 0.999660] 
[Epoch 15/39] [Batch 1100/1292] [D loss: 0.361252] [G loss: 0.176343] [ema: 0.999662] 
[Epoch 15/39] [Batch 1200/1292] [D loss: 0.382989] [G loss: 0.208107] [ema: 0.999663] 
[Epoch 16/39] [Batch 0/1292] [D loss: 0.361525] [G loss: 0.193481] [ema: 0.999665] 
[Epoch 16/39] [Batch 100/1292] [D loss: 0.373458] [G loss: 0.182092] [ema: 0.999666] 
[Epoch 16/39] [Batch 200/1292] [D loss: 0.365685] [G loss: 0.186766] [ema: 0.999668] 
[Epoch 16/39] [Batch 300/1292] [D loss: 0.393741] [G loss: 0.187878] [ema: 0.999670] 
[Epoch 16/39] [Batch 400/1292] [D loss: 0.370490] [G loss: 0.187817] [ema: 0.999671] 
[Epoch 16/39] [Batch 500/1292] [D loss: 0.454694] [G loss: 0.191357] [ema: 0.999673] 
[Epoch 16/39] [Batch 600/1292] [D loss: 0.427528] [G loss: 0.181583] [ema: 0.999674] 
[Epoch 16/39] [Batch 700/1292] [D loss: 0.449113] [G loss: 0.179179] [ema: 0.999676] 
[Epoch 16/39] [Batch 800/1292] [D loss: 0.376739] [G loss: 0.175474] [ema: 0.999677] 
[Epoch 16/39] [Batch 900/1292] [D loss: 0.412240] [G loss: 0.174870] [ema: 0.999679] 
[Epoch 16/39] [Batch 1000/1292] [D loss: 0.417948] [G loss: 0.167044] [ema: 0.999680] 
[Epoch 16/39] [Batch 1100/1292] [D loss: 0.379902] [G loss: 0.183066] [ema: 0.999682] 
[Epoch 16/39] [Batch 1200/1292] [D loss: 0.424299] [G loss: 0.181408] [ema: 0.999683] 
[Epoch 17/39] [Batch 0/1292] [D loss: 0.358742] [G loss: 0.189672] [ema: 0.999684] 
[Epoch 17/39] [Batch 100/1292] [D loss: 0.412195] [G loss: 0.181976] [ema: 0.999686] 
[Epoch 17/39] [Batch 200/1292] [D loss: 0.378918] [G loss: 0.179518] [ema: 0.999687] 
[Epoch 17/39] [Batch 300/1292] [D loss: 0.355573] [G loss: 0.192261] [ema: 0.999689] 
[Epoch 17/39] [Batch 400/1292] [D loss: 0.418675] [G loss: 0.187794] [ema: 0.999690] 
[Epoch 17/39] [Batch 500/1292] [D loss: 0.387296] [G loss: 0.176968] [ema: 0.999691] 
[Epoch 17/39] [Batch 600/1292] [D loss: 0.404198] [G loss: 0.164167] [ema: 0.999693] 
[Epoch 17/39] [Batch 700/1292] [D loss: 0.397318] [G loss: 0.187402] [ema: 0.999694] 
[Epoch 17/39] [Batch 800/1292] [D loss: 0.414931] [G loss: 0.187793] [ema: 0.999696] 
[Epoch 17/39] [Batch 900/1292] [D loss: 0.372772] [G loss: 0.186606] [ema: 0.999697] 
[Epoch 17/39] [Batch 1000/1292] [D loss: 0.392877] [G loss: 0.189411] [ema: 0.999698] 
[Epoch 17/39] [Batch 1100/1292] [D loss: 0.378676] [G loss: 0.184099] [ema: 0.999700] 
[Epoch 17/39] [Batch 1200/1292] [D loss: 0.405282] [G loss: 0.184021] [ema: 0.999701] 
[Epoch 18/39] [Batch 0/1292] [D loss: 0.409360] [G loss: 0.182312] [ema: 0.999702] 
[Epoch 18/39] [Batch 100/1292] [D loss: 0.427570] [G loss: 0.190938] [ema: 0.999703] 
[Epoch 18/39] [Batch 200/1292] [D loss: 0.416581] [G loss: 0.177044] [ema: 0.999705] 
[Epoch 18/39] [Batch 300/1292] [D loss: 0.391523] [G loss: 0.202652] [ema: 0.999706] 
[Epoch 18/39] [Batch 400/1292] [D loss: 0.400948] [G loss: 0.198667] [ema: 0.999707] 
[Epoch 18/39] [Batch 500/1292] [D loss: 0.406246] [G loss: 0.183660] [ema: 0.999708] 
[Epoch 18/39] [Batch 600/1292] [D loss: 0.364839] [G loss: 0.180155] [ema: 0.999709] 
[Epoch 18/39] [Batch 700/1292] [D loss: 0.405647] [G loss: 0.189336] [ema: 0.999711] 
[Epoch 18/39] [Batch 800/1292] [D loss: 0.454962] [G loss: 0.169224] [ema: 0.999712] 
[Epoch 18/39] [Batch 900/1292] [D loss: 0.433462] [G loss: 0.176953] [ema: 0.999713] 
[Epoch 18/39] [Batch 1000/1292] [D loss: 0.355163] [G loss: 0.173923] [ema: 0.999714] 
[Epoch 18/39] [Batch 1100/1292] [D loss: 0.436899] [G loss: 0.194042] [ema: 0.999715] 
[Epoch 18/39] [Batch 1200/1292] [D loss: 0.410434] [G loss: 0.173767] [ema: 0.999717] 
[Epoch 19/39] [Batch 0/1292] [D loss: 0.398916] [G loss: 0.196996] [ema: 0.999718] 
[Epoch 19/39] [Batch 100/1292] [D loss: 0.387037] [G loss: 0.183281] [ema: 0.999719] 
[Epoch 19/39] [Batch 200/1292] [D loss: 0.377052] [G loss: 0.173576] [ema: 0.999720] 
[Epoch 19/39] [Batch 300/1292] [D loss: 0.376368] [G loss: 0.190682] [ema: 0.999721] 
[Epoch 19/39] [Batch 400/1292] [D loss: 0.352374] [G loss: 0.201359] [ema: 0.999722] 
[Epoch 19/39] [Batch 500/1292] [D loss: 0.357948] [G loss: 0.173865] [ema: 0.999723] 
[Epoch 19/39] [Batch 600/1292] [D loss: 0.403653] [G loss: 0.197371] [ema: 0.999724] 
[Epoch 19/39] [Batch 700/1292] [D loss: 0.394718] [G loss: 0.174986] [ema: 0.999726] 
[Epoch 19/39] [Batch 800/1292] [D loss: 0.380474] [G loss: 0.178796] [ema: 0.999727] 
[Epoch 19/39] [Batch 900/1292] [D loss: 0.404291] [G loss: 0.178555] [ema: 0.999728] 
[Epoch 19/39] [Batch 1000/1292] [D loss: 0.385527] [G loss: 0.182228] [ema: 0.999729] 
[Epoch 19/39] [Batch 1100/1292] [D loss: 0.370568] [G loss: 0.185935] [ema: 0.999730] 
[Epoch 19/39] [Batch 1200/1292] [D loss: 0.380107] [G loss: 0.186997] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_30_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_30_2024_10_25_02_11_06/Model



[Epoch 20/39] [Batch 0/1292] [D loss: 0.355223] [G loss: 0.194440] [ema: 0.999732] 
[Epoch 20/39] [Batch 100/1292] [D loss: 0.382740] [G loss: 0.177557] [ema: 0.999733] 
[Epoch 20/39] [Batch 200/1292] [D loss: 0.393936] [G loss: 0.172118] [ema: 0.999734] 
[Epoch 20/39] [Batch 300/1292] [D loss: 0.420722] [G loss: 0.182581] [ema: 0.999735] 
[Epoch 20/39] [Batch 400/1292] [D loss: 0.437385] [G loss: 0.179423] [ema: 0.999736] 
[Epoch 20/39] [Batch 500/1292] [D loss: 0.406648] [G loss: 0.179366] [ema: 0.999737] 
[Epoch 20/39] [Batch 600/1292] [D loss: 0.392834] [G loss: 0.172016] [ema: 0.999738] 
[Epoch 20/39] [Batch 700/1292] [D loss: 0.414561] [G loss: 0.168332] [ema: 0.999739] 
[Epoch 20/39] [Batch 800/1292] [D loss: 0.394956] [G loss: 0.188600] [ema: 0.999740] 
[Epoch 20/39] [Batch 900/1292] [D loss: 0.385136] [G loss: 0.188909] [ema: 0.999741] 
[Epoch 20/39] [Batch 1000/1292] [D loss: 0.412445] [G loss: 0.168831] [ema: 0.999742] 
[Epoch 20/39] [Batch 1100/1292] [D loss: 0.392050] [G loss: 0.188954] [ema: 0.999743] 
[Epoch 20/39] [Batch 1200/1292] [D loss: 0.347231] [G loss: 0.186737] [ema: 0.999744] 
[Epoch 21/39] [Batch 0/1292] [D loss: 0.385803] [G loss: 0.182879] [ema: 0.999745] 
[Epoch 21/39] [Batch 100/1292] [D loss: 0.403335] [G loss: 0.188561] [ema: 0.999745] 
[Epoch 21/39] [Batch 200/1292] [D loss: 0.370942] [G loss: 0.173969] [ema: 0.999746] 
[Epoch 21/39] [Batch 300/1292] [D loss: 0.357663] [G loss: 0.175013] [ema: 0.999747] 
[Epoch 21/39] [Batch 400/1292] [D loss: 0.407508] [G loss: 0.186892] [ema: 0.999748] 
[Epoch 21/39] [Batch 500/1292] [D loss: 0.403288] [G loss: 0.192300] [ema: 0.999749] 
[Epoch 21/39] [Batch 600/1292] [D loss: 0.416799] [G loss: 0.178306] [ema: 0.999750] 
[Epoch 21/39] [Batch 700/1292] [D loss: 0.376621] [G loss: 0.185961] [ema: 0.999751] 
[Epoch 21/39] [Batch 800/1292] [D loss: 0.420926] [G loss: 0.170985] [ema: 0.999752] 
[Epoch 21/39] [Batch 900/1292] [D loss: 0.461295] [G loss: 0.178157] [ema: 0.999753] 
[Epoch 21/39] [Batch 1000/1292] [D loss: 0.375200] [G loss: 0.182738] [ema: 0.999754] 
[Epoch 21/39] [Batch 1100/1292] [D loss: 0.376081] [G loss: 0.167210] [ema: 0.999755] 
[Epoch 21/39] [Batch 1200/1292] [D loss: 0.435464] [G loss: 0.176964] [ema: 0.999755] 
[Epoch 22/39] [Batch 0/1292] [D loss: 0.404547] [G loss: 0.173606] [ema: 0.999756] 
[Epoch 22/39] [Batch 100/1292] [D loss: 0.376195] [G loss: 0.178182] [ema: 0.999757] 
[Epoch 22/39] [Batch 200/1292] [D loss: 0.400950] [G loss: 0.168634] [ema: 0.999758] 
[Epoch 22/39] [Batch 300/1292] [D loss: 0.416517] [G loss: 0.171814] [ema: 0.999759] 
[Epoch 22/39] [Batch 400/1292] [D loss: 0.406733] [G loss: 0.170437] [ema: 0.999760] 
[Epoch 22/39] [Batch 500/1292] [D loss: 0.385865] [G loss: 0.184377] [ema: 0.999760] 
[Epoch 22/39] [Batch 600/1292] [D loss: 0.409115] [G loss: 0.179989] [ema: 0.999761] 
[Epoch 22/39] [Batch 700/1292] [D loss: 0.390080] [G loss: 0.183903] [ema: 0.999762] 
[Epoch 22/39] [Batch 800/1292] [D loss: 0.398687] [G loss: 0.184429] [ema: 0.999763] 
[Epoch 22/39] [Batch 900/1292] [D loss: 0.380396] [G loss: 0.179494] [ema: 0.999764] 
[Epoch 22/39] [Batch 1000/1292] [D loss: 0.406562] [G loss: 0.164251] [ema: 0.999764] 
[Epoch 22/39] [Batch 1100/1292] [D loss: 0.387991] [G loss: 0.165352] [ema: 0.999765] 
[Epoch 22/39] [Batch 1200/1292] [D loss: 0.399588] [G loss: 0.171917] [ema: 0.999766] 
[Epoch 23/39] [Batch 0/1292] [D loss: 0.416986] [G loss: 0.196423] [ema: 0.999767] 
[Epoch 23/39] [Batch 100/1292] [D loss: 0.389681] [G loss: 0.181714] [ema: 0.999768] 
[Epoch 23/39] [Batch 200/1292] [D loss: 0.400492] [G loss: 0.176628] [ema: 0.999768] 
[Epoch 23/39] [Batch 300/1292] [D loss: 0.393091] [G loss: 0.168860] [ema: 0.999769] 
[Epoch 23/39] [Batch 400/1292] [D loss: 0.375220] [G loss: 0.193094] [ema: 0.999770] 
[Epoch 23/39] [Batch 500/1292] [D loss: 0.430549] [G loss: 0.176100] [ema: 0.999771] 
[Epoch 23/39] [Batch 600/1292] [D loss: 0.355968] [G loss: 0.173364] [ema: 0.999771] 
[Epoch 23/39] [Batch 700/1292] [D loss: 0.403719] [G loss: 0.184949] [ema: 0.999772] 
[Epoch 23/39] [Batch 800/1292] [D loss: 0.401081] [G loss: 0.180612] [ema: 0.999773] 
[Epoch 23/39] [Batch 900/1292] [D loss: 0.418592] [G loss: 0.157175] [ema: 0.999774] 
[Epoch 23/39] [Batch 1000/1292] [D loss: 0.398066] [G loss: 0.167358] [ema: 0.999774] 
[Epoch 23/39] [Batch 1100/1292] [D loss: 0.415416] [G loss: 0.174303] [ema: 0.999775] 
[Epoch 23/39] [Batch 1200/1292] [D loss: 0.376119] [G loss: 0.186770] [ema: 0.999776] 
[Epoch 24/39] [Batch 0/1292] [D loss: 0.414555] [G loss: 0.175541] [ema: 0.999776] 
[Epoch 24/39] [Batch 100/1292] [D loss: 0.334078] [G loss: 0.192458] [ema: 0.999777] 
[Epoch 24/39] [Batch 200/1292] [D loss: 0.351965] [G loss: 0.185649] [ema: 0.999778] 
[Epoch 24/39] [Batch 300/1292] [D loss: 0.371913] [G loss: 0.191101] [ema: 0.999779] 
[Epoch 24/39] [Batch 400/1292] [D loss: 0.372737] [G loss: 0.187495] [ema: 0.999779] 
[Epoch 24/39] [Batch 500/1292] [D loss: 0.385436] [G loss: 0.163886] [ema: 0.999780] 
[Epoch 24/39] [Batch 600/1292] [D loss: 0.443569] [G loss: 0.167439] [ema: 0.999781] 
[Epoch 24/39] [Batch 700/1292] [D loss: 0.358802] [G loss: 0.168220] [ema: 0.999781] 
[Epoch 24/39] [Batch 800/1292] [D loss: 0.552618] [G loss: 0.174454] [ema: 0.999782] 
[Epoch 24/39] [Batch 900/1292] [D loss: 0.460187] [G loss: 0.182097] [ema: 0.999783] 
[Epoch 24/39] [Batch 1000/1292] [D loss: 0.475190] [G loss: 0.145043] [ema: 0.999783] 
[Epoch 24/39] [Batch 1100/1292] [D loss: 0.495313] [G loss: 0.170538] [ema: 0.999784] 
[Epoch 24/39] [Batch 1200/1292] [D loss: 0.394677] [G loss: 0.180116] [ema: 0.999785] 
[Epoch 25/39] [Batch 0/1292] [D loss: 0.439617] [G loss: 0.164114] [ema: 0.999785] 
[Epoch 25/39] [Batch 100/1292] [D loss: 0.421828] [G loss: 0.172734] [ema: 0.999786] 
[Epoch 25/39] [Batch 200/1292] [D loss: 0.456717] [G loss: 0.158667] [ema: 0.999787] 
[Epoch 25/39] [Batch 300/1292] [D loss: 0.384453] [G loss: 0.198670] [ema: 0.999787] 
[Epoch 25/39] [Batch 400/1292] [D loss: 0.364766] [G loss: 0.203567] [ema: 0.999788] 
[Epoch 25/39] [Batch 500/1292] [D loss: 0.384237] [G loss: 0.166295] [ema: 0.999789] 
[Epoch 25/39] [Batch 600/1292] [D loss: 0.384719] [G loss: 0.190932] [ema: 0.999789] 
[Epoch 25/39] [Batch 700/1292] [D loss: 0.471344] [G loss: 0.169460] [ema: 0.999790] 
[Epoch 25/39] [Batch 800/1292] [D loss: 0.362326] [G loss: 0.161636] [ema: 0.999791] 
[Epoch 25/39] [Batch 900/1292] [D loss: 0.394358] [G loss: 0.162208] [ema: 0.999791] 
[Epoch 25/39] [Batch 1000/1292] [D loss: 0.383124] [G loss: 0.173328] [ema: 0.999792] 
[Epoch 25/39] [Batch 1100/1292] [D loss: 0.385951] [G loss: 0.173515] [ema: 0.999792] 
[Epoch 25/39] [Batch 1200/1292] [D loss: 0.444834] [G loss: 0.178984] [ema: 0.999793] 
[Epoch 26/39] [Batch 0/1292] [D loss: 0.431750] [G loss: 0.181453] [ema: 0.999794] 
[Epoch 26/39] [Batch 100/1292] [D loss: 0.361394] [G loss: 0.206387] [ema: 0.999794] 
[Epoch 26/39] [Batch 200/1292] [D loss: 0.421933] [G loss: 0.177509] [ema: 0.999795] 
[Epoch 26/39] [Batch 300/1292] [D loss: 0.381804] [G loss: 0.174384] [ema: 0.999796] 
[Epoch 26/39] [Batch 400/1292] [D loss: 0.385033] [G loss: 0.177733] [ema: 0.999796] 
[Epoch 26/39] [Batch 500/1292] [D loss: 0.390779] [G loss: 0.184032] [ema: 0.999797] 
[Epoch 26/39] [Batch 600/1292] [D loss: 0.396665] [G loss: 0.178492] [ema: 0.999797] 
[Epoch 26/39] [Batch 700/1292] [D loss: 0.422149] [G loss: 0.178839] [ema: 0.999798] 
[Epoch 26/39] [Batch 800/1292] [D loss: 0.385484] [G loss: 0.166135] [ema: 0.999798] 
[Epoch 26/39] [Batch 900/1292] [D loss: 0.373427] [G loss: 0.170112] [ema: 0.999799] 
[Epoch 26/39] [Batch 1000/1292] [D loss: 0.425930] [G loss: 0.176981] [ema: 0.999800] 
[Epoch 26/39] [Batch 1100/1292] [D loss: 0.394190] [G loss: 0.178890] [ema: 0.999800] 
[Epoch 26/39] [Batch 1200/1292] [D loss: 0.372018] [G loss: 0.179286] [ema: 0.999801] 
[Epoch 27/39] [Batch 0/1292] [D loss: 0.450139] [G loss: 0.183572] [ema: 0.999801] 
[Epoch 27/39] [Batch 100/1292] [D loss: 0.427603] [G loss: 0.164258] [ema: 0.999802] 
[Epoch 27/39] [Batch 200/1292] [D loss: 0.360096] [G loss: 0.187565] [ema: 0.999802] 
[Epoch 27/39] [Batch 300/1292] [D loss: 0.399550] [G loss: 0.175781] [ema: 0.999803] 
[Epoch 27/39] [Batch 400/1292] [D loss: 0.379896] [G loss: 0.174643] [ema: 0.999804] 
[Epoch 27/39] [Batch 500/1292] [D loss: 0.426434] [G loss: 0.168772] [ema: 0.999804] 
[Epoch 27/39] [Batch 600/1292] [D loss: 0.389945] [G loss: 0.152906] [ema: 0.999805] 
[Epoch 27/39] [Batch 700/1292] [D loss: 0.417551] [G loss: 0.173109] [ema: 0.999805] 
[Epoch 27/39] [Batch 800/1292] [D loss: 0.366052] [G loss: 0.180379] [ema: 0.999806] 
[Epoch 27/39] [Batch 900/1292] [D loss: 0.391652] [G loss: 0.166150] [ema: 0.999806] 
[Epoch 27/39] [Batch 1000/1292] [D loss: 0.386670] [G loss: 0.180903] [ema: 0.999807] 
[Epoch 27/39] [Batch 1100/1292] [D loss: 0.407849] [G loss: 0.168940] [ema: 0.999807] 
[Epoch 27/39] [Batch 1200/1292] [D loss: 0.408413] [G loss: 0.173556] [ema: 0.999808] 
[Epoch 28/39] [Batch 0/1292] [D loss: 0.393427] [G loss: 0.175962] [ema: 0.999808] 
[Epoch 28/39] [Batch 100/1292] [D loss: 0.431624] [G loss: 0.165128] [ema: 0.999809] 
[Epoch 28/39] [Batch 200/1292] [D loss: 0.439742] [G loss: 0.179478] [ema: 0.999809] 
[Epoch 28/39] [Batch 300/1292] [D loss: 0.396464] [G loss: 0.174542] [ema: 0.999810] 
[Epoch 28/39] [Batch 400/1292] [D loss: 0.389463] [G loss: 0.157491] [ema: 0.999811] 
[Epoch 28/39] [Batch 500/1292] [D loss: 0.443821] [G loss: 0.172344] [ema: 0.999811] 
[Epoch 28/39] [Batch 600/1292] [D loss: 0.430664] [G loss: 0.191465] [ema: 0.999812] 
[Epoch 28/39] [Batch 700/1292] [D loss: 0.418645] [G loss: 0.174062] [ema: 0.999812] 
[Epoch 28/39] [Batch 800/1292] [D loss: 0.376959] [G loss: 0.184202] [ema: 0.999813] 
[Epoch 28/39] [Batch 900/1292] [D loss: 0.407344] [G loss: 0.172851] [ema: 0.999813] 
[Epoch 28/39] [Batch 1000/1292] [D loss: 0.437722] [G loss: 0.167028] [ema: 0.999814] 
[Epoch 28/39] [Batch 1100/1292] [D loss: 0.421386] [G loss: 0.180583] [ema: 0.999814] 
[Epoch 28/39] [Batch 1200/1292] [D loss: 0.399315] [G loss: 0.180681] [ema: 0.999815] 
[Epoch 29/39] [Batch 0/1292] [D loss: 0.388492] [G loss: 0.170845] [ema: 0.999815] 
[Epoch 29/39] [Batch 100/1292] [D loss: 0.429120] [G loss: 0.182518] [ema: 0.999816] 
[Epoch 29/39] [Batch 200/1292] [D loss: 0.346047] [G loss: 0.180031] [ema: 0.999816] 
[Epoch 29/39] [Batch 300/1292] [D loss: 0.382747] [G loss: 0.162709] [ema: 0.999816] 
[Epoch 29/39] [Batch 400/1292] [D loss: 0.380950] [G loss: 0.188564] [ema: 0.999817] 
[Epoch 29/39] [Batch 500/1292] [D loss: 0.393390] [G loss: 0.168914] [ema: 0.999817] 
[Epoch 29/39] [Batch 600/1292] [D loss: 0.378649] [G loss: 0.168138] [ema: 0.999818] 
[Epoch 29/39] [Batch 700/1292] [D loss: 0.417580] [G loss: 0.181209] [ema: 0.999818] 
[Epoch 29/39] [Batch 800/1292] [D loss: 0.397874] [G loss: 0.181765] [ema: 0.999819] 
[Epoch 29/39] [Batch 900/1292] [D loss: 0.402282] [G loss: 0.189465] [ema: 0.999819] 
[Epoch 29/39] [Batch 1000/1292] [D loss: 0.392805] [G loss: 0.179482] [ema: 0.999820] 
[Epoch 29/39] [Batch 1100/1292] [D loss: 0.429284] [G loss: 0.172086] [ema: 0.999820] 
[Epoch 29/39] [Batch 1200/1292] [D loss: 0.408694] [G loss: 0.189429] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_30_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_30_2024_10_25_02_11_06/Model



[Epoch 30/39] [Batch 0/1292] [D loss: 0.405876] [G loss: 0.183301] [ema: 0.999821] 
[Epoch 30/39] [Batch 100/1292] [D loss: 0.406964] [G loss: 0.177820] [ema: 0.999822] 
[Epoch 30/39] [Batch 200/1292] [D loss: 0.413197] [G loss: 0.174475] [ema: 0.999822] 
[Epoch 30/39] [Batch 300/1292] [D loss: 0.413448] [G loss: 0.167310] [ema: 0.999823] 
[Epoch 30/39] [Batch 400/1292] [D loss: 0.403013] [G loss: 0.178763] [ema: 0.999823] 
[Epoch 30/39] [Batch 500/1292] [D loss: 0.389528] [G loss: 0.181675] [ema: 0.999823] 
[Epoch 30/39] [Batch 600/1292] [D loss: 0.383122] [G loss: 0.189871] [ema: 0.999824] 
[Epoch 30/39] [Batch 700/1292] [D loss: 0.441405] [G loss: 0.166367] [ema: 0.999824] 
[Epoch 30/39] [Batch 800/1292] [D loss: 0.395502] [G loss: 0.177673] [ema: 0.999825] 
[Epoch 30/39] [Batch 900/1292] [D loss: 0.372420] [G loss: 0.182129] [ema: 0.999825] 
[Epoch 30/39] [Batch 1000/1292] [D loss: 0.395886] [G loss: 0.168515] [ema: 0.999826] 
[Epoch 30/39] [Batch 1100/1292] [D loss: 0.381631] [G loss: 0.176394] [ema: 0.999826] 
[Epoch 30/39] [Batch 1200/1292] [D loss: 0.369821] [G loss: 0.172647] [ema: 0.999827] 
[Epoch 31/39] [Batch 0/1292] [D loss: 0.392645] [G loss: 0.180338] [ema: 0.999827] 
[Epoch 31/39] [Batch 100/1292] [D loss: 0.355594] [G loss: 0.191709] [ema: 0.999827] 
[Epoch 31/39] [Batch 200/1292] [D loss: 0.428597] [G loss: 0.164084] [ema: 0.999828] 
[Epoch 31/39] [Batch 300/1292] [D loss: 0.385089] [G loss: 0.171831] [ema: 0.999828] 
[Epoch 31/39] [Batch 400/1292] [D loss: 0.387875] [G loss: 0.189288] [ema: 0.999829] 
[Epoch 31/39] [Batch 500/1292] [D loss: 0.415247] [G loss: 0.165477] [ema: 0.999829] 
[Epoch 31/39] [Batch 600/1292] [D loss: 0.410975] [G loss: 0.153313] [ema: 0.999830] 
[Epoch 31/39] [Batch 700/1292] [D loss: 0.401043] [G loss: 0.147944] [ema: 0.999830] 
[Epoch 31/39] [Batch 800/1292] [D loss: 0.445544] [G loss: 0.167317] [ema: 0.999830] 
[Epoch 31/39] [Batch 900/1292] [D loss: 0.456014] [G loss: 0.169229] [ema: 0.999831] 
[Epoch 31/39] [Batch 1000/1292] [D loss: 0.468857] [G loss: 0.175927] [ema: 0.999831] 
[Epoch 31/39] [Batch 1100/1292] [D loss: 0.442814] [G loss: 0.180090] [ema: 0.999832] 
[Epoch 31/39] [Batch 1200/1292] [D loss: 0.440776] [G loss: 0.139894] [ema: 0.999832] 
[Epoch 32/39] [Batch 0/1292] [D loss: 0.517642] [G loss: 0.110109] [ema: 0.999832] 
[Epoch 32/39] [Batch 100/1292] [D loss: 0.508191] [G loss: 0.146994] [ema: 0.999833] 
[Epoch 32/39] [Batch 200/1292] [D loss: 0.515612] [G loss: 0.129015] [ema: 0.999833] 
[Epoch 32/39] [Batch 300/1292] [D loss: 0.485244] [G loss: 0.138884] [ema: 0.999834] 
[Epoch 32/39] [Batch 400/1292] [D loss: 0.501372] [G loss: 0.145829] [ema: 0.999834] 
[Epoch 32/39] [Batch 500/1292] [D loss: 0.421500] [G loss: 0.129371] [ema: 0.999834] 
[Epoch 32/39] [Batch 600/1292] [D loss: 0.479561] [G loss: 0.156697] [ema: 0.999835] 
[Epoch 32/39] [Batch 700/1292] [D loss: 0.403860] [G loss: 0.151091] [ema: 0.999835] 
[Epoch 32/39] [Batch 800/1292] [D loss: 0.499374] [G loss: 0.163244] [ema: 0.999836] 
[Epoch 32/39] [Batch 900/1292] [D loss: 0.508269] [G loss: 0.159434] [ema: 0.999836] 
[Epoch 32/39] [Batch 1000/1292] [D loss: 0.449897] [G loss: 0.169595] [ema: 0.999836] 
[Epoch 32/39] [Batch 1100/1292] [D loss: 0.407071] [G loss: 0.163383] [ema: 0.999837] 
[Epoch 32/39] [Batch 1200/1292] [D loss: 0.446014] [G loss: 0.165858] [ema: 0.999837] 
[Epoch 33/39] [Batch 0/1292] [D loss: 0.462604] [G loss: 0.178619] [ema: 0.999837] 
[Epoch 33/39] [Batch 100/1292] [D loss: 0.436092] [G loss: 0.172421] [ema: 0.999838] 
[Epoch 33/39] [Batch 200/1292] [D loss: 0.384600] [G loss: 0.171311] [ema: 0.999838] 
[Epoch 33/39] [Batch 300/1292] [D loss: 0.450495] [G loss: 0.156956] [ema: 0.999839] 
[Epoch 33/39] [Batch 400/1292] [D loss: 0.438685] [G loss: 0.163576] [ema: 0.999839] 
[Epoch 33/39] [Batch 500/1292] [D loss: 0.477173] [G loss: 0.177859] [ema: 0.999839] 
[Epoch 33/39] [Batch 600/1292] [D loss: 0.480072] [G loss: 0.142100] [ema: 0.999840] 
[Epoch 33/39] [Batch 700/1292] [D loss: 0.444685] [G loss: 0.116980] [ema: 0.999840] 
[Epoch 33/39] [Batch 800/1292] [D loss: 0.393394] [G loss: 0.152328] [ema: 0.999840] 
[Epoch 33/39] [Batch 900/1292] [D loss: 0.417882] [G loss: 0.150790] [ema: 0.999841] 
[Epoch 33/39] [Batch 1000/1292] [D loss: 0.438971] [G loss: 0.160551] [ema: 0.999841] 
[Epoch 33/39] [Batch 1100/1292] [D loss: 0.415914] [G loss: 0.159977] [ema: 0.999842] 
[Epoch 33/39] [Batch 1200/1292] [D loss: 0.373820] [G loss: 0.156603] [ema: 0.999842] 
[Epoch 34/39] [Batch 0/1292] [D loss: 0.369430] [G loss: 0.176324] [ema: 0.999842] 
[Epoch 34/39] [Batch 100/1292] [D loss: 0.427502] [G loss: 0.175118] [ema: 0.999843] 
[Epoch 34/39] [Batch 200/1292] [D loss: 0.454107] [G loss: 0.172175] [ema: 0.999843] 
[Epoch 34/39] [Batch 300/1292] [D loss: 0.399657] [G loss: 0.181415] [ema: 0.999843] 
[Epoch 34/39] [Batch 400/1292] [D loss: 0.382496] [G loss: 0.189617] [ema: 0.999844] 
[Epoch 34/39] [Batch 500/1292] [D loss: 0.422599] [G loss: 0.179269] [ema: 0.999844] 
[Epoch 34/39] [Batch 600/1292] [D loss: 0.415471] [G loss: 0.172465] [ema: 0.999844] 
[Epoch 34/39] [Batch 700/1292] [D loss: 0.432487] [G loss: 0.172311] [ema: 0.999845] 
[Epoch 34/39] [Batch 800/1292] [D loss: 0.402043] [G loss: 0.169583] [ema: 0.999845] 
[Epoch 34/39] [Batch 900/1292] [D loss: 0.437803] [G loss: 0.169236] [ema: 0.999845] 
[Epoch 34/39] [Batch 1000/1292] [D loss: 0.425557] [G loss: 0.175567] [ema: 0.999846] 
[Epoch 34/39] [Batch 1100/1292] [D loss: 0.367658] [G loss: 0.186533] [ema: 0.999846] 
[Epoch 34/39] [Batch 1200/1292] [D loss: 0.383130] [G loss: 0.173719] [ema: 0.999846] 
[Epoch 35/39] [Batch 0/1292] [D loss: 0.418355] [G loss: 0.165122] [ema: 0.999847] 
[Epoch 35/39] [Batch 100/1292] [D loss: 0.443823] [G loss: 0.182815] [ema: 0.999847] 
[Epoch 35/39] [Batch 200/1292] [D loss: 0.412983] [G loss: 0.159562] [ema: 0.999847] 
[Epoch 35/39] [Batch 300/1292] [D loss: 0.410399] [G loss: 0.173357] [ema: 0.999848] 
[Epoch 35/39] [Batch 400/1292] [D loss: 0.457825] [G loss: 0.177030] [ema: 0.999848] 
[Epoch 35/39] [Batch 500/1292] [D loss: 0.409892] [G loss: 0.183018] [ema: 0.999848] 
[Epoch 35/39] [Batch 600/1292] [D loss: 0.410359] [G loss: 0.168424] [ema: 0.999849] 
[Epoch 35/39] [Batch 700/1292] [D loss: 0.398250] [G loss: 0.173935] [ema: 0.999849] 
[Epoch 35/39] [Batch 800/1292] [D loss: 0.372646] [G loss: 0.177095] [ema: 0.999849] 
[Epoch 35/39] [Batch 900/1292] [D loss: 0.392619] [G loss: 0.188142] [ema: 0.999850] 
[Epoch 35/39] [Batch 1000/1292] [D loss: 0.430362] [G loss: 0.156606] [ema: 0.999850] 
[Epoch 35/39] [Batch 1100/1292] [D loss: 0.507148] [G loss: 0.160156] [ema: 0.999850] 
[Epoch 35/39] [Batch 1200/1292] [D loss: 0.478079] [G loss: 0.152736] [ema: 0.999851] 
[Epoch 36/39] [Batch 0/1292] [D loss: 0.480672] [G loss: 0.117042] [ema: 0.999851] 
[Epoch 36/39] [Batch 100/1292] [D loss: 0.470970] [G loss: 0.164721] [ema: 0.999851] 
[Epoch 36/39] [Batch 200/1292] [D loss: 0.506051] [G loss: 0.148516] [ema: 0.999852] 
[Epoch 36/39] [Batch 300/1292] [D loss: 0.430091] [G loss: 0.167632] [ema: 0.999852] 
[Epoch 36/39] [Batch 400/1292] [D loss: 0.506461] [G loss: 0.164720] [ema: 0.999852] 
[Epoch 36/39] [Batch 500/1292] [D loss: 0.400942] [G loss: 0.143402] [ema: 0.999853] 
[Epoch 36/39] [Batch 600/1292] [D loss: 0.446447] [G loss: 0.152471] [ema: 0.999853] 
[Epoch 36/39] [Batch 700/1292] [D loss: 0.451366] [G loss: 0.160953] [ema: 0.999853] 
[Epoch 36/39] [Batch 800/1292] [D loss: 0.457393] [G loss: 0.146009] [ema: 0.999854] 
[Epoch 36/39] [Batch 900/1292] [D loss: 0.444127] [G loss: 0.173993] [ema: 0.999854] 
[Epoch 36/39] [Batch 1000/1292] [D loss: 0.461127] [G loss: 0.148814] [ema: 0.999854] 
[Epoch 36/39] [Batch 1100/1292] [D loss: 0.396321] [G loss: 0.160510] [ema: 0.999854] 
[Epoch 36/39] [Batch 1200/1292] [D loss: 0.449385] [G loss: 0.176025] [ema: 0.999855] 
[Epoch 37/39] [Batch 0/1292] [D loss: 0.457653] [G loss: 0.169568] [ema: 0.999855] 
[Epoch 37/39] [Batch 100/1292] [D loss: 0.445976] [G loss: 0.178108] [ema: 0.999855] 
[Epoch 37/39] [Batch 200/1292] [D loss: 0.448375] [G loss: 0.154701] [ema: 0.999856] 
[Epoch 37/39] [Batch 300/1292] [D loss: 0.436388] [G loss: 0.137399] [ema: 0.999856] 
[Epoch 37/39] [Batch 400/1292] [D loss: 0.394717] [G loss: 0.163197] [ema: 0.999856] 
[Epoch 37/39] [Batch 500/1292] [D loss: 0.457359] [G loss: 0.145867] [ema: 0.999857] 
[Epoch 37/39] [Batch 600/1292] [D loss: 0.423264] [G loss: 0.157133] [ema: 0.999857] 
[Epoch 37/39] [Batch 700/1292] [D loss: 0.408361] [G loss: 0.185486] [ema: 0.999857] 
[Epoch 37/39] [Batch 800/1292] [D loss: 0.465273] [G loss: 0.144073] [ema: 0.999857] 
[Epoch 37/39] [Batch 900/1292] [D loss: 0.452460] [G loss: 0.146715] [ema: 0.999858] 
[Epoch 37/39] [Batch 1000/1292] [D loss: 0.431260] [G loss: 0.181547] [ema: 0.999858] 
[Epoch 37/39] [Batch 1100/1292] [D loss: 0.474388] [G loss: 0.152968] [ema: 0.999858] 
[Epoch 37/39] [Batch 1200/1292] [D loss: 0.460453] [G loss: 0.163770] [ema: 0.999859] 
[Epoch 38/39] [Batch 0/1292] [D loss: 0.449147] [G loss: 0.164583] [ema: 0.999859] 
[Epoch 38/39] [Batch 100/1292] [D loss: 0.414354] [G loss: 0.159823] [ema: 0.999859] 
[Epoch 38/39] [Batch 200/1292] [D loss: 0.415744] [G loss: 0.152362] [ema: 0.999859] 
[Epoch 38/39] [Batch 300/1292] [D loss: 0.450094] [G loss: 0.177174] [ema: 0.999860] 
[Epoch 38/39] [Batch 400/1292] [D loss: 0.461528] [G loss: 0.144210] [ema: 0.999860] 
[Epoch 38/39] [Batch 500/1292] [D loss: 0.462686] [G loss: 0.152564] [ema: 0.999860] 
[Epoch 38/39] [Batch 600/1292] [D loss: 0.451010] [G loss: 0.157681] [ema: 0.999861] 
[Epoch 38/39] [Batch 700/1292] [D loss: 0.469680] [G loss: 0.158868] [ema: 0.999861] 
[Epoch 38/39] [Batch 800/1292] [D loss: 0.456942] [G loss: 0.151685] [ema: 0.999861] 
[Epoch 38/39] [Batch 900/1292] [D loss: 0.478121] [G loss: 0.156433] [ema: 0.999861] 
[Epoch 38/39] [Batch 1000/1292] [D loss: 0.441379] [G loss: 0.157075] [ema: 0.999862] 
[Epoch 38/39] [Batch 1100/1292] [D loss: 0.399871] [G loss: 0.169932] [ema: 0.999862] 
[Epoch 38/39] [Batch 1200/1292] [D loss: 0.477637] [G loss: 0.148804] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
KuHar_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
KuHar_DAGHAR_Multiclass
daghar
return single class data and labels, class is KuHar_DAGHAR_Multiclass
data shape is (2784, 3, 1, 30)
label shape is (2784,)
174
Epochs between checkpoint: 72



Saving checkpoint 1 in logs/daghar_split_dataset_50000_30_100/KuHar_DAGHAR_Multiclass_50000_D_30_2024_10_25_02_42_43/Model



[Epoch 0/288] [Batch 0/174] [D loss: 1.126713] [G loss: 0.901499] [ema: 0.000000] 
[Epoch 0/288] [Batch 100/174] [D loss: 0.539584] [G loss: 0.139663] [ema: 0.933033] 
[Epoch 1/288] [Batch 0/174] [D loss: 0.438694] [G loss: 0.187270] [ema: 0.960947] 
[Epoch 1/288] [Batch 100/174] [D loss: 0.420905] [G loss: 0.181580] [ema: 0.975020] 
[Epoch 2/288] [Batch 0/174] [D loss: 0.398003] [G loss: 0.184761] [ema: 0.980279] 
[Epoch 2/288] [Batch 100/174] [D loss: 0.368982] [G loss: 0.210800] [ema: 0.984647] 
[Epoch 3/288] [Batch 0/174] [D loss: 0.486908] [G loss: 0.152111] [ema: 0.986809] 
[Epoch 3/288] [Batch 100/174] [D loss: 0.367216] [G loss: 0.202572] [ema: 0.988918] 
[Epoch 4/288] [Batch 0/174] [D loss: 0.481477] [G loss: 0.164426] [ema: 0.990090] 
[Epoch 4/288] [Batch 100/174] [D loss: 0.420868] [G loss: 0.184376] [ema: 0.991330] 
[Epoch 5/288] [Batch 0/174] [D loss: 0.458174] [G loss: 0.176601] [ema: 0.992064] 
[Epoch 5/288] [Batch 100/174] [D loss: 0.438494] [G loss: 0.163897] [ema: 0.992880] 
[Epoch 6/288] [Batch 0/174] [D loss: 0.483339] [G loss: 0.150361] [ema: 0.993383] 
[Epoch 6/288] [Batch 100/174] [D loss: 0.485047] [G loss: 0.170330] [ema: 0.993959] 
[Epoch 7/288] [Batch 0/174] [D loss: 0.445368] [G loss: 0.152672] [ema: 0.994325] 
[Epoch 7/288] [Batch 100/174] [D loss: 0.445893] [G loss: 0.182368] [ema: 0.994755] 
[Epoch 8/288] [Batch 0/174] [D loss: 0.433802] [G loss: 0.158909] [ema: 0.995033] 
[Epoch 8/288] [Batch 100/174] [D loss: 0.391605] [G loss: 0.157390] [ema: 0.995365] 
[Epoch 9/288] [Batch 0/174] [D loss: 0.429911] [G loss: 0.220682] [ema: 0.995584] 
[Epoch 9/288] [Batch 100/174] [D loss: 0.411793] [G loss: 0.171899] [ema: 0.995848] 
[Epoch 10/288] [Batch 0/174] [D loss: 0.418169] [G loss: 0.156995] [ema: 0.996024] 
[Epoch 10/288] [Batch 100/174] [D loss: 0.375210] [G loss: 0.195344] [ema: 0.996240] 
[Epoch 11/288] [Batch 0/174] [D loss: 0.398316] [G loss: 0.169073] [ema: 0.996385] 
[Epoch 11/288] [Batch 100/174] [D loss: 0.486238] [G loss: 0.167513] [ema: 0.996564] 
[Epoch 12/288] [Batch 0/174] [D loss: 0.381262] [G loss: 0.190377] [ema: 0.996686] 
[Epoch 12/288] [Batch 100/174] [D loss: 0.446467] [G loss: 0.170200] [ema: 0.996837] 
[Epoch 13/288] [Batch 0/174] [D loss: 0.386820] [G loss: 0.179353] [ema: 0.996940] 
[Epoch 13/288] [Batch 100/174] [D loss: 0.381450] [G loss: 0.199665] [ema: 0.997070] 
[Epoch 14/288] [Batch 0/174] [D loss: 0.370328] [G loss: 0.187055] [ema: 0.997159] 
[Epoch 14/288] [Batch 100/174] [D loss: 0.386205] [G loss: 0.202115] [ema: 0.997271] 
[Epoch 15/288] [Batch 0/174] [D loss: 0.381677] [G loss: 0.223062] [ema: 0.997348] 
[Epoch 15/288] [Batch 100/174] [D loss: 0.353868] [G loss: 0.221505] [ema: 0.997446] 
[Epoch 16/288] [Batch 0/174] [D loss: 0.396063] [G loss: 0.184976] [ema: 0.997513] 
[Epoch 16/288] [Batch 100/174] [D loss: 0.409344] [G loss: 0.205485] [ema: 0.997599] 
[Epoch 17/288] [Batch 0/174] [D loss: 0.412103] [G loss: 0.225501] [ema: 0.997659] 
[Epoch 17/288] [Batch 100/174] [D loss: 0.326122] [G loss: 0.198190] [ema: 0.997736] 
[Epoch 18/288] [Batch 0/174] [D loss: 0.349923] [G loss: 0.205489] [ema: 0.997789] 
[Epoch 18/288] [Batch 100/174] [D loss: 0.329944] [G loss: 0.205039] [ema: 0.997858] 
[Epoch 19/288] [Batch 0/174] [D loss: 0.328759] [G loss: 0.223272] [ema: 0.997906] 
[Epoch 19/288] [Batch 100/174] [D loss: 0.330760] [G loss: 0.181850] [ema: 0.997967] 
[Epoch 20/288] [Batch 0/174] [D loss: 0.314358] [G loss: 0.231935] [ema: 0.998010] 
[Epoch 20/288] [Batch 100/174] [D loss: 0.371358] [G loss: 0.187780] [ema: 0.998066] 
[Epoch 21/288] [Batch 0/174] [D loss: 0.417500] [G loss: 0.180967] [ema: 0.998105] 
[Epoch 21/288] [Batch 100/174] [D loss: 0.383445] [G loss: 0.212398] [ema: 0.998155] 
[Epoch 22/288] [Batch 0/174] [D loss: 0.398734] [G loss: 0.179776] [ema: 0.998191] 
[Epoch 22/288] [Batch 100/174] [D loss: 0.435290] [G loss: 0.160350] [ema: 0.998237] 
[Epoch 23/288] [Batch 0/174] [D loss: 0.422760] [G loss: 0.182828] [ema: 0.998269] 
[Epoch 23/288] [Batch 100/174] [D loss: 0.414936] [G loss: 0.185510] [ema: 0.998312] 
[Epoch 24/288] [Batch 0/174] [D loss: 0.353329] [G loss: 0.195827] [ema: 0.998342] 
[Epoch 24/288] [Batch 100/174] [D loss: 0.354072] [G loss: 0.209958] [ema: 0.998380] 
[Epoch 25/288] [Batch 0/174] [D loss: 0.373646] [G loss: 0.191699] [ema: 0.998408] 
[Epoch 25/288] [Batch 100/174] [D loss: 0.432376] [G loss: 0.176509] [ema: 0.998444] 
[Epoch 26/288] [Batch 0/174] [D loss: 0.440160] [G loss: 0.186518] [ema: 0.998469] 
[Epoch 26/288] [Batch 100/174] [D loss: 0.364639] [G loss: 0.191418] [ema: 0.998502] 
[Epoch 27/288] [Batch 0/174] [D loss: 0.369786] [G loss: 0.209828] [ema: 0.998526] 
[Epoch 27/288] [Batch 100/174] [D loss: 0.403936] [G loss: 0.174187] [ema: 0.998556] 
[Epoch 28/288] [Batch 0/174] [D loss: 0.366375] [G loss: 0.185692] [ema: 0.998578] 
[Epoch 28/288] [Batch 100/174] [D loss: 0.381586] [G loss: 0.189307] [ema: 0.998607] 
[Epoch 29/288] [Batch 0/174] [D loss: 0.341845] [G loss: 0.220942] [ema: 0.998627] 
[Epoch 29/288] [Batch 100/174] [D loss: 0.354405] [G loss: 0.189415] [ema: 0.998654] 
[Epoch 30/288] [Batch 0/174] [D loss: 0.370570] [G loss: 0.172404] [ema: 0.998673] 
[Epoch 30/288] [Batch 100/174] [D loss: 0.381740] [G loss: 0.179586] [ema: 0.998698] 
[Epoch 31/288] [Batch 0/174] [D loss: 0.393342] [G loss: 0.193935] [ema: 0.998716] 
[Epoch 31/288] [Batch 100/174] [D loss: 0.362711] [G loss: 0.193610] [ema: 0.998739] 
[Epoch 32/288] [Batch 0/174] [D loss: 0.410115] [G loss: 0.174118] [ema: 0.998756] 
[Epoch 32/288] [Batch 100/174] [D loss: 0.394801] [G loss: 0.171112] [ema: 0.998778] 
[Epoch 33/288] [Batch 0/174] [D loss: 0.366167] [G loss: 0.186090] [ema: 0.998794] 
[Epoch 33/288] [Batch 100/174] [D loss: 0.383082] [G loss: 0.183432] [ema: 0.998814] 
[Epoch 34/288] [Batch 0/174] [D loss: 0.385652] [G loss: 0.195685] [ema: 0.998829] 
[Epoch 34/288] [Batch 100/174] [D loss: 0.411596] [G loss: 0.176304] [ema: 0.998848] 
[Epoch 35/288] [Batch 0/174] [D loss: 0.388916] [G loss: 0.189507] [ema: 0.998862] 
[Epoch 35/288] [Batch 100/174] [D loss: 0.390194] [G loss: 0.194049] [ema: 0.998881] 
[Epoch 36/288] [Batch 0/174] [D loss: 0.373755] [G loss: 0.177522] [ema: 0.998894] 
[Epoch 36/288] [Batch 100/174] [D loss: 0.419231] [G loss: 0.197602] [ema: 0.998911] 
[Epoch 37/288] [Batch 0/174] [D loss: 0.420949] [G loss: 0.192361] [ema: 0.998924] 
[Epoch 37/288] [Batch 100/174] [D loss: 0.370631] [G loss: 0.184449] [ema: 0.998940] 
[Epoch 38/288] [Batch 0/174] [D loss: 0.405318] [G loss: 0.192258] [ema: 0.998952] 
[Epoch 38/288] [Batch 100/174] [D loss: 0.397785] [G loss: 0.191238] [ema: 0.998968] 
[Epoch 39/288] [Batch 0/174] [D loss: 0.359874] [G loss: 0.196028] [ema: 0.998979] 
[Epoch 39/288] [Batch 100/174] [D loss: 0.392422] [G loss: 0.191497] [ema: 0.998994] 
[Epoch 40/288] [Batch 0/174] [D loss: 0.368822] [G loss: 0.195559] [ema: 0.999005] 
[Epoch 40/288] [Batch 100/174] [D loss: 0.400903] [G loss: 0.180045] [ema: 0.999019] 
[Epoch 41/288] [Batch 0/174] [D loss: 0.405433] [G loss: 0.176937] [ema: 0.999029] 
[Epoch 41/288] [Batch 100/174] [D loss: 0.390844] [G loss: 0.169018] [ema: 0.999042] 
[Epoch 42/288] [Batch 0/174] [D loss: 0.401319] [G loss: 0.190355] [ema: 0.999052] 
[Epoch 42/288] [Batch 100/174] [D loss: 0.375115] [G loss: 0.187950] [ema: 0.999065] 
[Epoch 43/288] [Batch 0/174] [D loss: 0.411465] [G loss: 0.189470] [ema: 0.999074] 
[Epoch 43/288] [Batch 100/174] [D loss: 0.349252] [G loss: 0.174988] [ema: 0.999086] 
[Epoch 44/288] [Batch 0/174] [D loss: 0.381740] [G loss: 0.192183] [ema: 0.999095] 
[Epoch 44/288] [Batch 100/174] [D loss: 0.401557] [G loss: 0.190373] [ema: 0.999107] 
[Epoch 45/288] [Batch 0/174] [D loss: 0.407083] [G loss: 0.199735] [ema: 0.999115] 
[Epoch 45/288] [Batch 100/174] [D loss: 0.392295] [G loss: 0.181043] [ema: 0.999126] 
[Epoch 46/288] [Batch 0/174] [D loss: 0.364378] [G loss: 0.191046] [ema: 0.999134] 
[Epoch 46/288] [Batch 100/174] [D loss: 0.407953] [G loss: 0.182469] [ema: 0.999145] 
[Epoch 47/288] [Batch 0/174] [D loss: 0.381543] [G loss: 0.206760] [ema: 0.999153] 
[Epoch 47/288] [Batch 100/174] [D loss: 0.347637] [G loss: 0.198182] [ema: 0.999163] 
[Epoch 48/288] [Batch 0/174] [D loss: 0.350531] [G loss: 0.195162] [ema: 0.999170] 
[Epoch 48/288] [Batch 100/174] [D loss: 0.398424] [G loss: 0.186135] [ema: 0.999180] 
[Epoch 49/288] [Batch 0/174] [D loss: 0.397910] [G loss: 0.188087] [ema: 0.999187] 
[Epoch 49/288] [Batch 100/174] [D loss: 0.372430] [G loss: 0.185653] [ema: 0.999197] 
[Epoch 50/288] [Batch 0/174] [D loss: 0.412453] [G loss: 0.196054] [ema: 0.999204] 
[Epoch 50/288] [Batch 100/174] [D loss: 0.371263] [G loss: 0.184237] [ema: 0.999213] 
[Epoch 51/288] [Batch 0/174] [D loss: 0.406210] [G loss: 0.182629] [ema: 0.999219] 
[Epoch 51/288] [Batch 100/174] [D loss: 0.395521] [G loss: 0.172291] [ema: 0.999228] 
[Epoch 52/288] [Batch 0/174] [D loss: 0.369623] [G loss: 0.192660] [ema: 0.999234] 
[Epoch 52/288] [Batch 100/174] [D loss: 0.429561] [G loss: 0.170915] [ema: 0.999243] 
[Epoch 53/288] [Batch 0/174] [D loss: 0.376513] [G loss: 0.194815] [ema: 0.999249] 
[Epoch 53/288] [Batch 100/174] [D loss: 0.376790] [G loss: 0.190448] [ema: 0.999257] 
[Epoch 54/288] [Batch 0/174] [D loss: 0.377701] [G loss: 0.197019] [ema: 0.999263] 
[Epoch 54/288] [Batch 100/174] [D loss: 0.393786] [G loss: 0.185928] [ema: 0.999270] 
[Epoch 55/288] [Batch 0/174] [D loss: 0.406420] [G loss: 0.191807] [ema: 0.999276] 
[Epoch 55/288] [Batch 100/174] [D loss: 0.373139] [G loss: 0.181025] [ema: 0.999283] 
[Epoch 56/288] [Batch 0/174] [D loss: 0.368162] [G loss: 0.181265] [ema: 0.999289] 
[Epoch 56/288] [Batch 100/174] [D loss: 0.374076] [G loss: 0.185562] [ema: 0.999296] 
[Epoch 57/288] [Batch 0/174] [D loss: 0.380739] [G loss: 0.196169] [ema: 0.999301] 
[Epoch 57/288] [Batch 100/174] [D loss: 0.418679] [G loss: 0.175653] [ema: 0.999308] 
[Epoch 58/288] [Batch 0/174] [D loss: 0.367968] [G loss: 0.188772] [ema: 0.999313] 
[Epoch 58/288] [Batch 100/174] [D loss: 0.405494] [G loss: 0.179624] [ema: 0.999320] 
[Epoch 59/288] [Batch 0/174] [D loss: 0.353762] [G loss: 0.188748] [ema: 0.999325] 
[Epoch 59/288] [Batch 100/174] [D loss: 0.376357] [G loss: 0.181321] [ema: 0.999332] 
[Epoch 60/288] [Batch 0/174] [D loss: 0.396136] [G loss: 0.184223] [ema: 0.999336] 
[Epoch 60/288] [Batch 100/174] [D loss: 0.393749] [G loss: 0.164805] [ema: 0.999343] 
[Epoch 61/288] [Batch 0/174] [D loss: 0.389416] [G loss: 0.176543] [ema: 0.999347] 
[Epoch 61/288] [Batch 100/174] [D loss: 0.381090] [G loss: 0.177799] [ema: 0.999353] 
[Epoch 62/288] [Batch 0/174] [D loss: 0.372912] [G loss: 0.184538] [ema: 0.999358] 
[Epoch 62/288] [Batch 100/174] [D loss: 0.399292] [G loss: 0.181800] [ema: 0.999364] 
[Epoch 63/288] [Batch 0/174] [D loss: 0.350252] [G loss: 0.208081] [ema: 0.999368] 
[Epoch 63/288] [Batch 100/174] [D loss: 0.400644] [G loss: 0.174664] [ema: 0.999374] 
[Epoch 64/288] [Batch 0/174] [D loss: 0.362831] [G loss: 0.191682] [ema: 0.999378] 
[Epoch 64/288] [Batch 100/174] [D loss: 0.404605] [G loss: 0.177356] [ema: 0.999383] 
[Epoch 65/288] [Batch 0/174] [D loss: 0.384113] [G loss: 0.183103] [ema: 0.999387] 
[Epoch 65/288] [Batch 100/174] [D loss: 0.428411] [G loss: 0.182872] [ema: 0.999393] 
[Epoch 66/288] [Batch 0/174] [D loss: 0.436726] [G loss: 0.200159] [ema: 0.999397] 
[Epoch 66/288] [Batch 100/174] [D loss: 0.377782] [G loss: 0.191718] [ema: 0.999402] 
[Epoch 67/288] [Batch 0/174] [D loss: 0.395123] [G loss: 0.178438] [ema: 0.999406] 
[Epoch 67/288] [Batch 100/174] [D loss: 0.396466] [G loss: 0.176077] [ema: 0.999411] 
[Epoch 68/288] [Batch 0/174] [D loss: 0.382922] [G loss: 0.193569] [ema: 0.999414] 
[Epoch 68/288] [Batch 100/174] [D loss: 0.384399] [G loss: 0.177481] [ema: 0.999419] 
[Epoch 69/288] [Batch 0/174] [D loss: 0.381338] [G loss: 0.193428] [ema: 0.999423] 
[Epoch 69/288] [Batch 100/174] [D loss: 0.341185] [G loss: 0.191890] [ema: 0.999428] 
[Epoch 70/288] [Batch 0/174] [D loss: 0.402644] [G loss: 0.196202] [ema: 0.999431] 
[Epoch 70/288] [Batch 100/174] [D loss: 0.384421] [G loss: 0.191784] [ema: 0.999436] 
[Epoch 71/288] [Batch 0/174] [D loss: 0.366926] [G loss: 0.195068] [ema: 0.999439] 
[Epoch 71/288] [Batch 100/174] [D loss: 0.392049] [G loss: 0.192330] [ema: 0.999444] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_30_100/KuHar_DAGHAR_Multiclass_50000_D_30_2024_10_25_02_42_43/Model



[Epoch 72/288] [Batch 0/174] [D loss: 0.368048] [G loss: 0.186347] [ema: 0.999447] 
[Epoch 72/288] [Batch 100/174] [D loss: 0.385136] [G loss: 0.181295] [ema: 0.999451] 
[Epoch 73/288] [Batch 0/174] [D loss: 0.364812] [G loss: 0.183103] [ema: 0.999454] 
[Epoch 73/288] [Batch 100/174] [D loss: 0.405509] [G loss: 0.182716] [ema: 0.999459] 
[Epoch 74/288] [Batch 0/174] [D loss: 0.399315] [G loss: 0.175042] [ema: 0.999462] 
[Epoch 74/288] [Batch 100/174] [D loss: 0.387447] [G loss: 0.195966] [ema: 0.999466] 
[Epoch 75/288] [Batch 0/174] [D loss: 0.403658] [G loss: 0.180099] [ema: 0.999469] 
[Epoch 75/288] [Batch 100/174] [D loss: 0.372154] [G loss: 0.182373] [ema: 0.999473] 
[Epoch 76/288] [Batch 0/174] [D loss: 0.372753] [G loss: 0.181071] [ema: 0.999476] 
[Epoch 76/288] [Batch 100/174] [D loss: 0.418825] [G loss: 0.195150] [ema: 0.999480] 
[Epoch 77/288] [Batch 0/174] [D loss: 0.424995] [G loss: 0.187702] [ema: 0.999483] 
[Epoch 77/288] [Batch 100/174] [D loss: 0.369566] [G loss: 0.186628] [ema: 0.999487] 
[Epoch 78/288] [Batch 0/174] [D loss: 0.401921] [G loss: 0.182204] [ema: 0.999489] 
[Epoch 78/288] [Batch 100/174] [D loss: 0.351680] [G loss: 0.192957] [ema: 0.999493] 
[Epoch 79/288] [Batch 0/174] [D loss: 0.397500] [G loss: 0.187878] [ema: 0.999496] 
[Epoch 79/288] [Batch 100/174] [D loss: 0.381549] [G loss: 0.186690] [ema: 0.999500] 
[Epoch 80/288] [Batch 0/174] [D loss: 0.384358] [G loss: 0.191309] [ema: 0.999502] 
[Epoch 80/288] [Batch 100/174] [D loss: 0.359201] [G loss: 0.192431] [ema: 0.999506] 
[Epoch 81/288] [Batch 0/174] [D loss: 0.372418] [G loss: 0.196431] [ema: 0.999508] 
[Epoch 81/288] [Batch 100/174] [D loss: 0.391417] [G loss: 0.183654] [ema: 0.999512] 
[Epoch 82/288] [Batch 0/174] [D loss: 0.369350] [G loss: 0.193896] [ema: 0.999514] 
[Epoch 82/288] [Batch 100/174] [D loss: 0.368236] [G loss: 0.190084] [ema: 0.999518] 
[Epoch 83/288] [Batch 0/174] [D loss: 0.424947] [G loss: 0.187607] [ema: 0.999520] 
[Epoch 83/288] [Batch 100/174] [D loss: 0.388730] [G loss: 0.188778] [ema: 0.999523] 
[Epoch 84/288] [Batch 0/174] [D loss: 0.395376] [G loss: 0.190309] [ema: 0.999526] 
[Epoch 84/288] [Batch 100/174] [D loss: 0.387244] [G loss: 0.180172] [ema: 0.999529] 
[Epoch 85/288] [Batch 0/174] [D loss: 0.371599] [G loss: 0.201293] [ema: 0.999531] 
[Epoch 85/288] [Batch 100/174] [D loss: 0.400469] [G loss: 0.182858] [ema: 0.999535] 
[Epoch 86/288] [Batch 0/174] [D loss: 0.414678] [G loss: 0.180511] [ema: 0.999537] 
[Epoch 86/288] [Batch 100/174] [D loss: 0.386158] [G loss: 0.179190] [ema: 0.999540] 
[Epoch 87/288] [Batch 0/174] [D loss: 0.378724] [G loss: 0.189951] [ema: 0.999542] 
[Epoch 87/288] [Batch 100/174] [D loss: 0.394433] [G loss: 0.183666] [ema: 0.999545] 
[Epoch 88/288] [Batch 0/174] [D loss: 0.386894] [G loss: 0.184968] [ema: 0.999547] 
[Epoch 88/288] [Batch 100/174] [D loss: 0.356146] [G loss: 0.194195] [ema: 0.999550] 
[Epoch 89/288] [Batch 0/174] [D loss: 0.390470] [G loss: 0.187235] [ema: 0.999553] 
[Epoch 89/288] [Batch 100/174] [D loss: 0.396799] [G loss: 0.171730] [ema: 0.999555] 
[Epoch 90/288] [Batch 0/174] [D loss: 0.443458] [G loss: 0.189130] [ema: 0.999557] 
[Epoch 90/288] [Batch 100/174] [D loss: 0.385518] [G loss: 0.188624] [ema: 0.999560] 
[Epoch 91/288] [Batch 0/174] [D loss: 0.370886] [G loss: 0.178942] [ema: 0.999562] 
[Epoch 91/288] [Batch 100/174] [D loss: 0.381087] [G loss: 0.191218] [ema: 0.999565] 
[Epoch 92/288] [Batch 0/174] [D loss: 0.409972] [G loss: 0.172875] [ema: 0.999567] 
[Epoch 92/288] [Batch 100/174] [D loss: 0.403160] [G loss: 0.188459] [ema: 0.999570] 
[Epoch 93/288] [Batch 0/174] [D loss: 0.409290] [G loss: 0.181916] [ema: 0.999572] 
[Epoch 93/288] [Batch 100/174] [D loss: 0.398027] [G loss: 0.183828] [ema: 0.999574] 
[Epoch 94/288] [Batch 0/174] [D loss: 0.420584] [G loss: 0.173232] [ema: 0.999576] 
[Epoch 94/288] [Batch 100/174] [D loss: 0.348224] [G loss: 0.182411] [ema: 0.999579] 
[Epoch 95/288] [Batch 0/174] [D loss: 0.393445] [G loss: 0.183418] [ema: 0.999581] 
[Epoch 95/288] [Batch 100/174] [D loss: 0.351749] [G loss: 0.191204] [ema: 0.999583] 
[Epoch 96/288] [Batch 0/174] [D loss: 0.384059] [G loss: 0.187177] [ema: 0.999585] 
[Epoch 96/288] [Batch 100/174] [D loss: 0.335547] [G loss: 0.195550] [ema: 0.999588] 
[Epoch 97/288] [Batch 0/174] [D loss: 0.360065] [G loss: 0.190426] [ema: 0.999589] 
[Epoch 97/288] [Batch 100/174] [D loss: 0.394964] [G loss: 0.180413] [ema: 0.999592] 
[Epoch 98/288] [Batch 0/174] [D loss: 0.388763] [G loss: 0.179982] [ema: 0.999594] 
[Epoch 98/288] [Batch 100/174] [D loss: 0.385411] [G loss: 0.183719] [ema: 0.999596] 
[Epoch 99/288] [Batch 0/174] [D loss: 0.357353] [G loss: 0.204203] [ema: 0.999598] 
[Epoch 99/288] [Batch 100/174] [D loss: 0.378356] [G loss: 0.183597] [ema: 0.999600] 
[Epoch 100/288] [Batch 0/174] [D loss: 0.440378] [G loss: 0.185069] [ema: 0.999602] 
[Epoch 100/288] [Batch 100/174] [D loss: 0.395143] [G loss: 0.180634] [ema: 0.999604] 
[Epoch 101/288] [Batch 0/174] [D loss: 0.351750] [G loss: 0.191165] [ema: 0.999606] 
[Epoch 101/288] [Batch 100/174] [D loss: 0.393496] [G loss: 0.172577] [ema: 0.999608] 
[Epoch 102/288] [Batch 0/174] [D loss: 0.372838] [G loss: 0.198246] [ema: 0.999610] 
[Epoch 102/288] [Batch 100/174] [D loss: 0.397618] [G loss: 0.183214] [ema: 0.999612] 
[Epoch 103/288] [Batch 0/174] [D loss: 0.395872] [G loss: 0.197856] [ema: 0.999613] 
[Epoch 103/288] [Batch 100/174] [D loss: 0.396595] [G loss: 0.191195] [ema: 0.999615] 
[Epoch 104/288] [Batch 0/174] [D loss: 0.412314] [G loss: 0.187197] [ema: 0.999617] 
[Epoch 104/288] [Batch 100/174] [D loss: 0.339475] [G loss: 0.189463] [ema: 0.999619] 
[Epoch 105/288] [Batch 0/174] [D loss: 0.388806] [G loss: 0.192355] [ema: 0.999621] 
[Epoch 105/288] [Batch 100/174] [D loss: 0.381035] [G loss: 0.193809] [ema: 0.999623] 
[Epoch 106/288] [Batch 0/174] [D loss: 0.374264] [G loss: 0.191878] [ema: 0.999624] 
[Epoch 106/288] [Batch 100/174] [D loss: 0.412198] [G loss: 0.182747] [ema: 0.999626] 
[Epoch 107/288] [Batch 0/174] [D loss: 0.401710] [G loss: 0.187812] [ema: 0.999628] 
[Epoch 107/288] [Batch 100/174] [D loss: 0.410065] [G loss: 0.178746] [ema: 0.999630] 
[Epoch 108/288] [Batch 0/174] [D loss: 0.361639] [G loss: 0.196347] [ema: 0.999631] 
[Epoch 108/288] [Batch 100/174] [D loss: 0.414160] [G loss: 0.189828] [ema: 0.999633] 
[Epoch 109/288] [Batch 0/174] [D loss: 0.368029] [G loss: 0.184154] [ema: 0.999635] 
[Epoch 109/288] [Batch 100/174] [D loss: 0.375251] [G loss: 0.186069] [ema: 0.999637] 
[Epoch 110/288] [Batch 0/174] [D loss: 0.389020] [G loss: 0.185423] [ema: 0.999638] 
[Epoch 110/288] [Batch 100/174] [D loss: 0.344320] [G loss: 0.192384] [ema: 0.999640] 
[Epoch 111/288] [Batch 0/174] [D loss: 0.402098] [G loss: 0.187875] [ema: 0.999641] 
[Epoch 111/288] [Batch 100/174] [D loss: 0.378214] [G loss: 0.189590] [ema: 0.999643] 
[Epoch 112/288] [Batch 0/174] [D loss: 0.388593] [G loss: 0.164197] [ema: 0.999644] 
[Epoch 112/288] [Batch 100/174] [D loss: 0.367820] [G loss: 0.190892] [ema: 0.999646] 
[Epoch 113/288] [Batch 0/174] [D loss: 0.387849] [G loss: 0.186844] [ema: 0.999648] 
[Epoch 113/288] [Batch 100/174] [D loss: 0.395343] [G loss: 0.184886] [ema: 0.999649] 
[Epoch 114/288] [Batch 0/174] [D loss: 0.361125] [G loss: 0.190067] [ema: 0.999651] 
[Epoch 114/288] [Batch 100/174] [D loss: 0.420731] [G loss: 0.181247] [ema: 0.999652] 
[Epoch 115/288] [Batch 0/174] [D loss: 0.379989] [G loss: 0.181967] [ema: 0.999654] 
[Epoch 115/288] [Batch 100/174] [D loss: 0.355204] [G loss: 0.187210] [ema: 0.999655] 
[Epoch 116/288] [Batch 0/174] [D loss: 0.408037] [G loss: 0.183890] [ema: 0.999657] 
[Epoch 116/288] [Batch 100/174] [D loss: 0.400977] [G loss: 0.192938] [ema: 0.999658] 
[Epoch 117/288] [Batch 0/174] [D loss: 0.357423] [G loss: 0.180465] [ema: 0.999660] 
[Epoch 117/288] [Batch 100/174] [D loss: 0.403814] [G loss: 0.178822] [ema: 0.999661] 
[Epoch 118/288] [Batch 0/174] [D loss: 0.402527] [G loss: 0.182625] [ema: 0.999662] 
[Epoch 118/288] [Batch 100/174] [D loss: 0.417459] [G loss: 0.187759] [ema: 0.999664] 
[Epoch 119/288] [Batch 0/174] [D loss: 0.388802] [G loss: 0.182726] [ema: 0.999665] 
[Epoch 119/288] [Batch 100/174] [D loss: 0.414748] [G loss: 0.179625] [ema: 0.999667] 
[Epoch 120/288] [Batch 0/174] [D loss: 0.357577] [G loss: 0.194477] [ema: 0.999668] 
[Epoch 120/288] [Batch 100/174] [D loss: 0.417066] [G loss: 0.187517] [ema: 0.999670] 
[Epoch 121/288] [Batch 0/174] [D loss: 0.368023] [G loss: 0.188029] [ema: 0.999671] 
[Epoch 121/288] [Batch 100/174] [D loss: 0.369262] [G loss: 0.187822] [ema: 0.999672] 
[Epoch 122/288] [Batch 0/174] [D loss: 0.389040] [G loss: 0.194311] [ema: 0.999674] 
[Epoch 122/288] [Batch 100/174] [D loss: 0.395744] [G loss: 0.180914] [ema: 0.999675] 
[Epoch 123/288] [Batch 0/174] [D loss: 0.402646] [G loss: 0.189179] [ema: 0.999676] 
[Epoch 123/288] [Batch 100/174] [D loss: 0.360939] [G loss: 0.187012] [ema: 0.999678] 
[Epoch 124/288] [Batch 0/174] [D loss: 0.432807] [G loss: 0.181024] [ema: 0.999679] 
[Epoch 124/288] [Batch 100/174] [D loss: 0.377636] [G loss: 0.182653] [ema: 0.999680] 
[Epoch 125/288] [Batch 0/174] [D loss: 0.358495] [G loss: 0.194118] [ema: 0.999681] 
[Epoch 125/288] [Batch 100/174] [D loss: 0.360423] [G loss: 0.190713] [ema: 0.999683] 
[Epoch 126/288] [Batch 0/174] [D loss: 0.397502] [G loss: 0.186138] [ema: 0.999684] 
[Epoch 126/288] [Batch 100/174] [D loss: 0.365263] [G loss: 0.196451] [ema: 0.999685] 
[Epoch 127/288] [Batch 0/174] [D loss: 0.382606] [G loss: 0.190905] [ema: 0.999686] 
[Epoch 127/288] [Batch 100/174] [D loss: 0.398188] [G loss: 0.178923] [ema: 0.999688] 
[Epoch 128/288] [Batch 0/174] [D loss: 0.378096] [G loss: 0.185175] [ema: 0.999689] 
[Epoch 128/288] [Batch 100/174] [D loss: 0.401502] [G loss: 0.179872] [ema: 0.999690] 
[Epoch 129/288] [Batch 0/174] [D loss: 0.431933] [G loss: 0.195529] [ema: 0.999691] 
[Epoch 129/288] [Batch 100/174] [D loss: 0.374116] [G loss: 0.190749] [ema: 0.999693] 
[Epoch 130/288] [Batch 0/174] [D loss: 0.382829] [G loss: 0.191932] [ema: 0.999694] 
[Epoch 130/288] [Batch 100/174] [D loss: 0.368340] [G loss: 0.188432] [ema: 0.999695] 
[Epoch 131/288] [Batch 0/174] [D loss: 0.372516] [G loss: 0.194721] [ema: 0.999696] 
[Epoch 131/288] [Batch 100/174] [D loss: 0.449628] [G loss: 0.172869] [ema: 0.999697] 
[Epoch 132/288] [Batch 0/174] [D loss: 0.397733] [G loss: 0.192157] [ema: 0.999698] 
[Epoch 132/288] [Batch 100/174] [D loss: 0.382840] [G loss: 0.192992] [ema: 0.999700] 
[Epoch 133/288] [Batch 0/174] [D loss: 0.424892] [G loss: 0.185742] [ema: 0.999701] 
[Epoch 133/288] [Batch 100/174] [D loss: 0.377623] [G loss: 0.192750] [ema: 0.999702] 
[Epoch 134/288] [Batch 0/174] [D loss: 0.415551] [G loss: 0.188649] [ema: 0.999703] 
[Epoch 134/288] [Batch 100/174] [D loss: 0.384762] [G loss: 0.172296] [ema: 0.999704] 
[Epoch 135/288] [Batch 0/174] [D loss: 0.414382] [G loss: 0.193040] [ema: 0.999705] 
[Epoch 135/288] [Batch 100/174] [D loss: 0.335094] [G loss: 0.190289] [ema: 0.999706] 
[Epoch 136/288] [Batch 0/174] [D loss: 0.378104] [G loss: 0.192574] [ema: 0.999707] 
[Epoch 136/288] [Batch 100/174] [D loss: 0.424782] [G loss: 0.172551] [ema: 0.999708] 
[Epoch 137/288] [Batch 0/174] [D loss: 0.367876] [G loss: 0.184310] [ema: 0.999709] 
[Epoch 137/288] [Batch 100/174] [D loss: 0.384627] [G loss: 0.173114] [ema: 0.999710] 
[Epoch 138/288] [Batch 0/174] [D loss: 0.358823] [G loss: 0.190799] [ema: 0.999711] 
[Epoch 138/288] [Batch 100/174] [D loss: 0.347502] [G loss: 0.185044] [ema: 0.999713] 
[Epoch 139/288] [Batch 0/174] [D loss: 0.362156] [G loss: 0.188839] [ema: 0.999713] 
[Epoch 139/288] [Batch 100/174] [D loss: 0.414240] [G loss: 0.178423] [ema: 0.999715] 
[Epoch 140/288] [Batch 0/174] [D loss: 0.393176] [G loss: 0.194891] [ema: 0.999715] 
[Epoch 140/288] [Batch 100/174] [D loss: 0.407235] [G loss: 0.180204] [ema: 0.999717] 
[Epoch 141/288] [Batch 0/174] [D loss: 0.406018] [G loss: 0.177851] [ema: 0.999718] 
[Epoch 141/288] [Batch 100/174] [D loss: 0.392331] [G loss: 0.178688] [ema: 0.999719] 
[Epoch 142/288] [Batch 0/174] [D loss: 0.409441] [G loss: 0.182092] [ema: 0.999720] 
[Epoch 142/288] [Batch 100/174] [D loss: 0.389819] [G loss: 0.188831] [ema: 0.999721] 
[Epoch 143/288] [Batch 0/174] [D loss: 0.399129] [G loss: 0.184192] [ema: 0.999721] 
[Epoch 143/288] [Batch 100/174] [D loss: 0.392227] [G loss: 0.191221] [ema: 0.999723] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_30_100/KuHar_DAGHAR_Multiclass_50000_D_30_2024_10_25_02_42_43/Model



[Epoch 144/288] [Batch 0/174] [D loss: 0.413588] [G loss: 0.189601] [ema: 0.999723] 
[Epoch 144/288] [Batch 100/174] [D loss: 0.410731] [G loss: 0.176718] [ema: 0.999724] 
[Epoch 145/288] [Batch 0/174] [D loss: 0.374166] [G loss: 0.182522] [ema: 0.999725] 
[Epoch 145/288] [Batch 100/174] [D loss: 0.376528] [G loss: 0.185775] [ema: 0.999726] 
[Epoch 146/288] [Batch 0/174] [D loss: 0.382162] [G loss: 0.185996] [ema: 0.999727] 
[Epoch 146/288] [Batch 100/174] [D loss: 0.387601] [G loss: 0.186687] [ema: 0.999728] 
[Epoch 147/288] [Batch 0/174] [D loss: 0.425642] [G loss: 0.184206] [ema: 0.999729] 
[Epoch 147/288] [Batch 100/174] [D loss: 0.403206] [G loss: 0.185870] [ema: 0.999730] 
[Epoch 148/288] [Batch 0/174] [D loss: 0.353729] [G loss: 0.187539] [ema: 0.999731] 
[Epoch 148/288] [Batch 100/174] [D loss: 0.362980] [G loss: 0.185137] [ema: 0.999732] 
[Epoch 149/288] [Batch 0/174] [D loss: 0.374878] [G loss: 0.184053] [ema: 0.999733] 
[Epoch 149/288] [Batch 100/174] [D loss: 0.380139] [G loss: 0.182239] [ema: 0.999734] 
[Epoch 150/288] [Batch 0/174] [D loss: 0.424586] [G loss: 0.199565] [ema: 0.999734] 
[Epoch 150/288] [Batch 100/174] [D loss: 0.359402] [G loss: 0.185600] [ema: 0.999735] 
[Epoch 151/288] [Batch 0/174] [D loss: 0.382340] [G loss: 0.180631] [ema: 0.999736] 
[Epoch 151/288] [Batch 100/174] [D loss: 0.390729] [G loss: 0.180736] [ema: 0.999737] 
[Epoch 152/288] [Batch 0/174] [D loss: 0.373205] [G loss: 0.175427] [ema: 0.999738] 
[Epoch 152/288] [Batch 100/174] [D loss: 0.388045] [G loss: 0.182192] [ema: 0.999739] 
[Epoch 153/288] [Batch 0/174] [D loss: 0.372553] [G loss: 0.181907] [ema: 0.999740] 
[Epoch 153/288] [Batch 100/174] [D loss: 0.400411] [G loss: 0.188697] [ema: 0.999741] 
[Epoch 154/288] [Batch 0/174] [D loss: 0.391317] [G loss: 0.182700] [ema: 0.999741] 
[Epoch 154/288] [Batch 100/174] [D loss: 0.389972] [G loss: 0.187914] [ema: 0.999742] 
[Epoch 155/288] [Batch 0/174] [D loss: 0.392588] [G loss: 0.188579] [ema: 0.999743] 
[Epoch 155/288] [Batch 100/174] [D loss: 0.381005] [G loss: 0.191597] [ema: 0.999744] 
[Epoch 156/288] [Batch 0/174] [D loss: 0.392707] [G loss: 0.192863] [ema: 0.999745] 
[Epoch 156/288] [Batch 100/174] [D loss: 0.358368] [G loss: 0.187922] [ema: 0.999746] 
[Epoch 157/288] [Batch 0/174] [D loss: 0.394778] [G loss: 0.188765] [ema: 0.999746] 
[Epoch 157/288] [Batch 100/174] [D loss: 0.397046] [G loss: 0.185424] [ema: 0.999747] 
[Epoch 158/288] [Batch 0/174] [D loss: 0.393638] [G loss: 0.181860] [ema: 0.999748] 
[Epoch 158/288] [Batch 100/174] [D loss: 0.386176] [G loss: 0.176448] [ema: 0.999749] 
[Epoch 159/288] [Batch 0/174] [D loss: 0.394450] [G loss: 0.199348] [ema: 0.999749] 
[Epoch 159/288] [Batch 100/174] [D loss: 0.439873] [G loss: 0.172493] [ema: 0.999750] 
[Epoch 160/288] [Batch 0/174] [D loss: 0.362203] [G loss: 0.190424] [ema: 0.999751] 
[Epoch 160/288] [Batch 100/174] [D loss: 0.402350] [G loss: 0.180369] [ema: 0.999752] 
[Epoch 161/288] [Batch 0/174] [D loss: 0.384111] [G loss: 0.190459] [ema: 0.999753] 
[Epoch 161/288] [Batch 100/174] [D loss: 0.362703] [G loss: 0.187853] [ema: 0.999753] 
[Epoch 162/288] [Batch 0/174] [D loss: 0.394794] [G loss: 0.195146] [ema: 0.999754] 
[Epoch 162/288] [Batch 100/174] [D loss: 0.345307] [G loss: 0.189370] [ema: 0.999755] 
[Epoch 163/288] [Batch 0/174] [D loss: 0.405090] [G loss: 0.189820] [ema: 0.999756] 
[Epoch 163/288] [Batch 100/174] [D loss: 0.349259] [G loss: 0.184717] [ema: 0.999756] 
[Epoch 164/288] [Batch 0/174] [D loss: 0.412058] [G loss: 0.183778] [ema: 0.999757] 
[Epoch 164/288] [Batch 100/174] [D loss: 0.409731] [G loss: 0.174942] [ema: 0.999758] 
[Epoch 165/288] [Batch 0/174] [D loss: 0.410421] [G loss: 0.187454] [ema: 0.999759] 
[Epoch 165/288] [Batch 100/174] [D loss: 0.376159] [G loss: 0.178892] [ema: 0.999759] 
[Epoch 166/288] [Batch 0/174] [D loss: 0.382209] [G loss: 0.191781] [ema: 0.999760] 
[Epoch 166/288] [Batch 100/174] [D loss: 0.335526] [G loss: 0.190189] [ema: 0.999761] 
[Epoch 167/288] [Batch 0/174] [D loss: 0.370297] [G loss: 0.189016] [ema: 0.999761] 
[Epoch 167/288] [Batch 100/174] [D loss: 0.386013] [G loss: 0.188859] [ema: 0.999762] 
[Epoch 168/288] [Batch 0/174] [D loss: 0.433518] [G loss: 0.175790] [ema: 0.999763] 
[Epoch 168/288] [Batch 100/174] [D loss: 0.378654] [G loss: 0.179200] [ema: 0.999764] 
[Epoch 169/288] [Batch 0/174] [D loss: 0.348972] [G loss: 0.186962] [ema: 0.999764] 
[Epoch 169/288] [Batch 100/174] [D loss: 0.405225] [G loss: 0.187194] [ema: 0.999765] 
[Epoch 170/288] [Batch 0/174] [D loss: 0.354789] [G loss: 0.193008] [ema: 0.999766] 
[Epoch 170/288] [Batch 100/174] [D loss: 0.403162] [G loss: 0.173181] [ema: 0.999766] 
[Epoch 171/288] [Batch 0/174] [D loss: 0.452852] [G loss: 0.193502] [ema: 0.999767] 
[Epoch 171/288] [Batch 100/174] [D loss: 0.402261] [G loss: 0.178266] [ema: 0.999768] 
[Epoch 172/288] [Batch 0/174] [D loss: 0.395159] [G loss: 0.189667] [ema: 0.999768] 
[Epoch 172/288] [Batch 100/174] [D loss: 0.354417] [G loss: 0.183641] [ema: 0.999769] 
[Epoch 173/288] [Batch 0/174] [D loss: 0.360632] [G loss: 0.186454] [ema: 0.999770] 
[Epoch 173/288] [Batch 100/174] [D loss: 0.393793] [G loss: 0.183643] [ema: 0.999771] 
[Epoch 174/288] [Batch 0/174] [D loss: 0.409634] [G loss: 0.177661] [ema: 0.999771] 
[Epoch 174/288] [Batch 100/174] [D loss: 0.365515] [G loss: 0.191455] [ema: 0.999772] 
[Epoch 175/288] [Batch 0/174] [D loss: 0.369249] [G loss: 0.189988] [ema: 0.999772] 
[Epoch 175/288] [Batch 100/174] [D loss: 0.407958] [G loss: 0.183361] [ema: 0.999773] 
[Epoch 176/288] [Batch 0/174] [D loss: 0.404138] [G loss: 0.188544] [ema: 0.999774] 
[Epoch 176/288] [Batch 100/174] [D loss: 0.383312] [G loss: 0.192864] [ema: 0.999774] 
[Epoch 177/288] [Batch 0/174] [D loss: 0.403594] [G loss: 0.182325] [ema: 0.999775] 
[Epoch 177/288] [Batch 100/174] [D loss: 0.350854] [G loss: 0.183933] [ema: 0.999776] 
[Epoch 178/288] [Batch 0/174] [D loss: 0.366746] [G loss: 0.181073] [ema: 0.999776] 
[Epoch 178/288] [Batch 100/174] [D loss: 0.389012] [G loss: 0.179655] [ema: 0.999777] 
[Epoch 179/288] [Batch 0/174] [D loss: 0.354495] [G loss: 0.196383] [ema: 0.999777] 
[Epoch 179/288] [Batch 100/174] [D loss: 0.395149] [G loss: 0.187122] [ema: 0.999778] 
[Epoch 180/288] [Batch 0/174] [D loss: 0.396466] [G loss: 0.182687] [ema: 0.999779] 
[Epoch 180/288] [Batch 100/174] [D loss: 0.357211] [G loss: 0.188814] [ema: 0.999779] 
[Epoch 181/288] [Batch 0/174] [D loss: 0.384885] [G loss: 0.191546] [ema: 0.999780] 
[Epoch 181/288] [Batch 100/174] [D loss: 0.369100] [G loss: 0.178379] [ema: 0.999781] 
[Epoch 182/288] [Batch 0/174] [D loss: 0.373996] [G loss: 0.188152] [ema: 0.999781] 
[Epoch 182/288] [Batch 100/174] [D loss: 0.384277] [G loss: 0.178588] [ema: 0.999782] 
[Epoch 183/288] [Batch 0/174] [D loss: 0.399192] [G loss: 0.205703] [ema: 0.999782] 
[Epoch 183/288] [Batch 100/174] [D loss: 0.404883] [G loss: 0.185671] [ema: 0.999783] 
[Epoch 184/288] [Batch 0/174] [D loss: 0.395790] [G loss: 0.183403] [ema: 0.999784] 
[Epoch 184/288] [Batch 100/174] [D loss: 0.409493] [G loss: 0.187123] [ema: 0.999784] 
[Epoch 185/288] [Batch 0/174] [D loss: 0.389310] [G loss: 0.184862] [ema: 0.999785] 
[Epoch 185/288] [Batch 100/174] [D loss: 0.415651] [G loss: 0.192315] [ema: 0.999785] 
[Epoch 186/288] [Batch 0/174] [D loss: 0.347921] [G loss: 0.183766] [ema: 0.999786] 
[Epoch 186/288] [Batch 100/174] [D loss: 0.374513] [G loss: 0.188900] [ema: 0.999787] 
[Epoch 187/288] [Batch 0/174] [D loss: 0.363808] [G loss: 0.184429] [ema: 0.999787] 
[Epoch 187/288] [Batch 100/174] [D loss: 0.353144] [G loss: 0.191499] [ema: 0.999788] 
[Epoch 188/288] [Batch 0/174] [D loss: 0.346256] [G loss: 0.187672] [ema: 0.999788] 
[Epoch 188/288] [Batch 100/174] [D loss: 0.391773] [G loss: 0.199961] [ema: 0.999789] 
[Epoch 189/288] [Batch 0/174] [D loss: 0.380888] [G loss: 0.179203] [ema: 0.999789] 
[Epoch 189/288] [Batch 100/174] [D loss: 0.414611] [G loss: 0.173053] [ema: 0.999790] 
[Epoch 190/288] [Batch 0/174] [D loss: 0.406004] [G loss: 0.174104] [ema: 0.999790] 
[Epoch 190/288] [Batch 100/174] [D loss: 0.376847] [G loss: 0.188052] [ema: 0.999791] 
[Epoch 191/288] [Batch 0/174] [D loss: 0.401540] [G loss: 0.181132] [ema: 0.999791] 
[Epoch 191/288] [Batch 100/174] [D loss: 0.394653] [G loss: 0.175473] [ema: 0.999792] 
[Epoch 192/288] [Batch 0/174] [D loss: 0.383545] [G loss: 0.188769] [ema: 0.999793] 
[Epoch 192/288] [Batch 100/174] [D loss: 0.346643] [G loss: 0.188648] [ema: 0.999793] 
[Epoch 193/288] [Batch 0/174] [D loss: 0.411370] [G loss: 0.185262] [ema: 0.999794] 
[Epoch 193/288] [Batch 100/174] [D loss: 0.380118] [G loss: 0.189607] [ema: 0.999794] 
[Epoch 194/288] [Batch 0/174] [D loss: 0.411924] [G loss: 0.190974] [ema: 0.999795] 
[Epoch 194/288] [Batch 100/174] [D loss: 0.325651] [G loss: 0.194835] [ema: 0.999795] 
[Epoch 195/288] [Batch 0/174] [D loss: 0.415248] [G loss: 0.187733] [ema: 0.999796] 
[Epoch 195/288] [Batch 100/174] [D loss: 0.372086] [G loss: 0.187441] [ema: 0.999796] 
[Epoch 196/288] [Batch 0/174] [D loss: 0.393942] [G loss: 0.193565] [ema: 0.999797] 
[Epoch 196/288] [Batch 100/174] [D loss: 0.365389] [G loss: 0.174276] [ema: 0.999797] 
[Epoch 197/288] [Batch 0/174] [D loss: 0.400826] [G loss: 0.194421] [ema: 0.999798] 
[Epoch 197/288] [Batch 100/174] [D loss: 0.389266] [G loss: 0.194090] [ema: 0.999798] 
[Epoch 198/288] [Batch 0/174] [D loss: 0.407937] [G loss: 0.185171] [ema: 0.999799] 
[Epoch 198/288] [Batch 100/174] [D loss: 0.382443] [G loss: 0.178428] [ema: 0.999799] 
[Epoch 199/288] [Batch 0/174] [D loss: 0.397606] [G loss: 0.185130] [ema: 0.999800] 
[Epoch 199/288] [Batch 100/174] [D loss: 0.405671] [G loss: 0.181741] [ema: 0.999800] 
[Epoch 200/288] [Batch 0/174] [D loss: 0.405349] [G loss: 0.188525] [ema: 0.999801] 
[Epoch 200/288] [Batch 100/174] [D loss: 0.434692] [G loss: 0.177509] [ema: 0.999801] 
[Epoch 201/288] [Batch 0/174] [D loss: 0.361560] [G loss: 0.188373] [ema: 0.999802] 
[Epoch 201/288] [Batch 100/174] [D loss: 0.382587] [G loss: 0.183396] [ema: 0.999802] 
[Epoch 202/288] [Batch 0/174] [D loss: 0.423853] [G loss: 0.181803] [ema: 0.999803] 
[Epoch 202/288] [Batch 100/174] [D loss: 0.361522] [G loss: 0.194927] [ema: 0.999803] 
[Epoch 203/288] [Batch 0/174] [D loss: 0.428296] [G loss: 0.181858] [ema: 0.999804] 
[Epoch 203/288] [Batch 100/174] [D loss: 0.411467] [G loss: 0.186258] [ema: 0.999804] 
[Epoch 204/288] [Batch 0/174] [D loss: 0.399241] [G loss: 0.195960] [ema: 0.999805] 
[Epoch 204/288] [Batch 100/174] [D loss: 0.413010] [G loss: 0.168926] [ema: 0.999805] 
[Epoch 205/288] [Batch 0/174] [D loss: 0.372308] [G loss: 0.181977] [ema: 0.999806] 
[Epoch 205/288] [Batch 100/174] [D loss: 0.385957] [G loss: 0.187623] [ema: 0.999806] 
[Epoch 206/288] [Batch 0/174] [D loss: 0.388815] [G loss: 0.192194] [ema: 0.999807] 
[Epoch 206/288] [Batch 100/174] [D loss: 0.392428] [G loss: 0.185933] [ema: 0.999807] 
[Epoch 207/288] [Batch 0/174] [D loss: 0.373257] [G loss: 0.203160] [ema: 0.999808] 
[Epoch 207/288] [Batch 100/174] [D loss: 0.401753] [G loss: 0.178498] [ema: 0.999808] 
[Epoch 208/288] [Batch 0/174] [D loss: 0.403872] [G loss: 0.185553] [ema: 0.999808] 
[Epoch 208/288] [Batch 100/174] [D loss: 0.415768] [G loss: 0.187178] [ema: 0.999809] 
[Epoch 209/288] [Batch 0/174] [D loss: 0.378216] [G loss: 0.178476] [ema: 0.999809] 
[Epoch 209/288] [Batch 100/174] [D loss: 0.407361] [G loss: 0.185350] [ema: 0.999810] 
[Epoch 210/288] [Batch 0/174] [D loss: 0.402472] [G loss: 0.195233] [ema: 0.999810] 
[Epoch 210/288] [Batch 100/174] [D loss: 0.361621] [G loss: 0.192001] [ema: 0.999811] 
[Epoch 211/288] [Batch 0/174] [D loss: 0.413931] [G loss: 0.188341] [ema: 0.999811] 
[Epoch 211/288] [Batch 100/174] [D loss: 0.407895] [G loss: 0.185228] [ema: 0.999812] 
[Epoch 212/288] [Batch 0/174] [D loss: 0.384898] [G loss: 0.188127] [ema: 0.999812] 
[Epoch 212/288] [Batch 100/174] [D loss: 0.417511] [G loss: 0.178665] [ema: 0.999813] 
[Epoch 213/288] [Batch 0/174] [D loss: 0.419764] [G loss: 0.179352] [ema: 0.999813] 
[Epoch 213/288] [Batch 100/174] [D loss: 0.396342] [G loss: 0.181232] [ema: 0.999813] 
[Epoch 214/288] [Batch 0/174] [D loss: 0.365972] [G loss: 0.188833] [ema: 0.999814] 
[Epoch 214/288] [Batch 100/174] [D loss: 0.371577] [G loss: 0.179041] [ema: 0.999814] 
[Epoch 215/288] [Batch 0/174] [D loss: 0.390138] [G loss: 0.175677] [ema: 0.999815] 
[Epoch 215/288] [Batch 100/174] [D loss: 0.411944] [G loss: 0.176757] [ema: 0.999815] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_30_100/KuHar_DAGHAR_Multiclass_50000_D_30_2024_10_25_02_42_43/Model



[Epoch 216/288] [Batch 0/174] [D loss: 0.364892] [G loss: 0.192314] [ema: 0.999816] 
[Epoch 216/288] [Batch 100/174] [D loss: 0.403497] [G loss: 0.179802] [ema: 0.999816] 
[Epoch 217/288] [Batch 0/174] [D loss: 0.384990] [G loss: 0.185654] [ema: 0.999816] 
[Epoch 217/288] [Batch 100/174] [D loss: 0.386534] [G loss: 0.182673] [ema: 0.999817] 
[Epoch 218/288] [Batch 0/174] [D loss: 0.411143] [G loss: 0.179786] [ema: 0.999817] 
[Epoch 218/288] [Batch 100/174] [D loss: 0.438117] [G loss: 0.177527] [ema: 0.999818] 
[Epoch 219/288] [Batch 0/174] [D loss: 0.400798] [G loss: 0.190263] [ema: 0.999818] 
[Epoch 219/288] [Batch 100/174] [D loss: 0.412242] [G loss: 0.175573] [ema: 0.999819] 
[Epoch 220/288] [Batch 0/174] [D loss: 0.406447] [G loss: 0.183091] [ema: 0.999819] 
[Epoch 220/288] [Batch 100/174] [D loss: 0.361328] [G loss: 0.196328] [ema: 0.999819] 
[Epoch 221/288] [Batch 0/174] [D loss: 0.402651] [G loss: 0.181310] [ema: 0.999820] 
[Epoch 221/288] [Batch 100/174] [D loss: 0.368598] [G loss: 0.188318] [ema: 0.999820] 
[Epoch 222/288] [Batch 0/174] [D loss: 0.369585] [G loss: 0.188498] [ema: 0.999821] 
[Epoch 222/288] [Batch 100/174] [D loss: 0.408173] [G loss: 0.186076] [ema: 0.999821] 
[Epoch 223/288] [Batch 0/174] [D loss: 0.383322] [G loss: 0.188109] [ema: 0.999821] 
[Epoch 223/288] [Batch 100/174] [D loss: 0.388502] [G loss: 0.190059] [ema: 0.999822] 
[Epoch 224/288] [Batch 0/174] [D loss: 0.377177] [G loss: 0.186396] [ema: 0.999822] 
[Epoch 224/288] [Batch 100/174] [D loss: 0.356298] [G loss: 0.189333] [ema: 0.999823] 
[Epoch 225/288] [Batch 0/174] [D loss: 0.442603] [G loss: 0.182869] [ema: 0.999823] 
[Epoch 225/288] [Batch 100/174] [D loss: 0.427292] [G loss: 0.192039] [ema: 0.999823] 
[Epoch 226/288] [Batch 0/174] [D loss: 0.366471] [G loss: 0.192492] [ema: 0.999824] 
[Epoch 226/288] [Batch 100/174] [D loss: 0.406446] [G loss: 0.181591] [ema: 0.999824] 
[Epoch 227/288] [Batch 0/174] [D loss: 0.386167] [G loss: 0.183403] [ema: 0.999825] 
[Epoch 227/288] [Batch 100/174] [D loss: 0.382491] [G loss: 0.192135] [ema: 0.999825] 
[Epoch 228/288] [Batch 0/174] [D loss: 0.372761] [G loss: 0.190974] [ema: 0.999825] 
[Epoch 228/288] [Batch 100/174] [D loss: 0.385166] [G loss: 0.184766] [ema: 0.999826] 
[Epoch 229/288] [Batch 0/174] [D loss: 0.367691] [G loss: 0.194088] [ema: 0.999826] 
[Epoch 229/288] [Batch 100/174] [D loss: 0.405053] [G loss: 0.183398] [ema: 0.999826] 
[Epoch 230/288] [Batch 0/174] [D loss: 0.344110] [G loss: 0.187958] [ema: 0.999827] 
[Epoch 230/288] [Batch 100/174] [D loss: 0.408797] [G loss: 0.178859] [ema: 0.999827] 
[Epoch 231/288] [Batch 0/174] [D loss: 0.358965] [G loss: 0.203213] [ema: 0.999828] 
[Epoch 231/288] [Batch 100/174] [D loss: 0.397525] [G loss: 0.186249] [ema: 0.999828] 
[Epoch 232/288] [Batch 0/174] [D loss: 0.400113] [G loss: 0.187362] [ema: 0.999828] 
[Epoch 232/288] [Batch 100/174] [D loss: 0.388844] [G loss: 0.170682] [ema: 0.999829] 
[Epoch 233/288] [Batch 0/174] [D loss: 0.443458] [G loss: 0.182100] [ema: 0.999829] 
[Epoch 233/288] [Batch 100/174] [D loss: 0.407642] [G loss: 0.181955] [ema: 0.999829] 
[Epoch 234/288] [Batch 0/174] [D loss: 0.386167] [G loss: 0.187248] [ema: 0.999830] 
[Epoch 234/288] [Batch 100/174] [D loss: 0.405254] [G loss: 0.183084] [ema: 0.999830] 
[Epoch 235/288] [Batch 0/174] [D loss: 0.371140] [G loss: 0.190698] [ema: 0.999830] 
[Epoch 235/288] [Batch 100/174] [D loss: 0.390398] [G loss: 0.188452] [ema: 0.999831] 
[Epoch 236/288] [Batch 0/174] [D loss: 0.397533] [G loss: 0.184707] [ema: 0.999831] 
[Epoch 236/288] [Batch 100/174] [D loss: 0.359067] [G loss: 0.187428] [ema: 0.999832] 
[Epoch 237/288] [Batch 0/174] [D loss: 0.407082] [G loss: 0.177552] [ema: 0.999832] 
[Epoch 237/288] [Batch 100/174] [D loss: 0.408756] [G loss: 0.183066] [ema: 0.999832] 
[Epoch 238/288] [Batch 0/174] [D loss: 0.378994] [G loss: 0.187144] [ema: 0.999833] 
[Epoch 238/288] [Batch 100/174] [D loss: 0.375937] [G loss: 0.185908] [ema: 0.999833] 
[Epoch 239/288] [Batch 0/174] [D loss: 0.387850] [G loss: 0.184380] [ema: 0.999833] 
[Epoch 239/288] [Batch 100/174] [D loss: 0.410797] [G loss: 0.185181] [ema: 0.999834] 
[Epoch 240/288] [Batch 0/174] [D loss: 0.404984] [G loss: 0.184739] [ema: 0.999834] 
[Epoch 240/288] [Batch 100/174] [D loss: 0.403694] [G loss: 0.173930] [ema: 0.999834] 
[Epoch 241/288] [Batch 0/174] [D loss: 0.398496] [G loss: 0.185415] [ema: 0.999835] 
[Epoch 241/288] [Batch 100/174] [D loss: 0.361667] [G loss: 0.177308] [ema: 0.999835] 
[Epoch 242/288] [Batch 0/174] [D loss: 0.414638] [G loss: 0.180625] [ema: 0.999835] 
[Epoch 242/288] [Batch 100/174] [D loss: 0.361602] [G loss: 0.182126] [ema: 0.999836] 
[Epoch 243/288] [Batch 0/174] [D loss: 0.386220] [G loss: 0.178522] [ema: 0.999836] 
[Epoch 243/288] [Batch 100/174] [D loss: 0.377992] [G loss: 0.182436] [ema: 0.999836] 
[Epoch 244/288] [Batch 0/174] [D loss: 0.371719] [G loss: 0.190994] [ema: 0.999837] 
[Epoch 244/288] [Batch 100/174] [D loss: 0.380479] [G loss: 0.176699] [ema: 0.999837] 
[Epoch 245/288] [Batch 0/174] [D loss: 0.385555] [G loss: 0.194421] [ema: 0.999837] 
[Epoch 245/288] [Batch 100/174] [D loss: 0.386825] [G loss: 0.187783] [ema: 0.999838] 
[Epoch 246/288] [Batch 0/174] [D loss: 0.441806] [G loss: 0.170258] [ema: 0.999838] 
[Epoch 246/288] [Batch 100/174] [D loss: 0.387943] [G loss: 0.186969] [ema: 0.999838] 
[Epoch 247/288] [Batch 0/174] [D loss: 0.403359] [G loss: 0.189345] [ema: 0.999839] 
[Epoch 247/288] [Batch 100/174] [D loss: 0.398138] [G loss: 0.179846] [ema: 0.999839] 
[Epoch 248/288] [Batch 0/174] [D loss: 0.383386] [G loss: 0.188173] [ema: 0.999839] 
[Epoch 248/288] [Batch 100/174] [D loss: 0.370697] [G loss: 0.180016] [ema: 0.999840] 
[Epoch 249/288] [Batch 0/174] [D loss: 0.390206] [G loss: 0.183010] [ema: 0.999840] 
[Epoch 249/288] [Batch 100/174] [D loss: 0.412254] [G loss: 0.180131] [ema: 0.999840] 
[Epoch 250/288] [Batch 0/174] [D loss: 0.430937] [G loss: 0.185661] [ema: 0.999841] 
[Epoch 250/288] [Batch 100/174] [D loss: 0.407145] [G loss: 0.186394] [ema: 0.999841] 
[Epoch 251/288] [Batch 0/174] [D loss: 0.383627] [G loss: 0.196266] [ema: 0.999841] 
[Epoch 251/288] [Batch 100/174] [D loss: 0.406727] [G loss: 0.184159] [ema: 0.999842] 
[Epoch 252/288] [Batch 0/174] [D loss: 0.382694] [G loss: 0.190737] [ema: 0.999842] 
[Epoch 252/288] [Batch 100/174] [D loss: 0.347925] [G loss: 0.188494] [ema: 0.999842] 
[Epoch 253/288] [Batch 0/174] [D loss: 0.400734] [G loss: 0.194701] [ema: 0.999843] 
[Epoch 253/288] [Batch 100/174] [D loss: 0.397163] [G loss: 0.181627] [ema: 0.999843] 
[Epoch 254/288] [Batch 0/174] [D loss: 0.409108] [G loss: 0.188080] [ema: 0.999843] 
[Epoch 254/288] [Batch 100/174] [D loss: 0.397879] [G loss: 0.188818] [ema: 0.999844] 
[Epoch 255/288] [Batch 0/174] [D loss: 0.409115] [G loss: 0.182317] [ema: 0.999844] 
[Epoch 255/288] [Batch 100/174] [D loss: 0.373785] [G loss: 0.196273] [ema: 0.999844] 
[Epoch 256/288] [Batch 0/174] [D loss: 0.355473] [G loss: 0.191599] [ema: 0.999844] 
[Epoch 256/288] [Batch 100/174] [D loss: 0.362478] [G loss: 0.190671] [ema: 0.999845] 
[Epoch 257/288] [Batch 0/174] [D loss: 0.411712] [G loss: 0.188427] [ema: 0.999845] 
[Epoch 257/288] [Batch 100/174] [D loss: 0.405426] [G loss: 0.189837] [ema: 0.999845] 
[Epoch 258/288] [Batch 0/174] [D loss: 0.392954] [G loss: 0.184055] [ema: 0.999846] 
[Epoch 258/288] [Batch 100/174] [D loss: 0.408871] [G loss: 0.180858] [ema: 0.999846] 
[Epoch 259/288] [Batch 0/174] [D loss: 0.434853] [G loss: 0.189424] [ema: 0.999846] 
[Epoch 259/288] [Batch 100/174] [D loss: 0.396352] [G loss: 0.177521] [ema: 0.999847] 
[Epoch 260/288] [Batch 0/174] [D loss: 0.403390] [G loss: 0.186128] [ema: 0.999847] 
[Epoch 260/288] [Batch 100/174] [D loss: 0.366423] [G loss: 0.184358] [ema: 0.999847] 
[Epoch 261/288] [Batch 0/174] [D loss: 0.352366] [G loss: 0.196296] [ema: 0.999847] 
[Epoch 261/288] [Batch 100/174] [D loss: 0.356539] [G loss: 0.183078] [ema: 0.999848] 
[Epoch 262/288] [Batch 0/174] [D loss: 0.339697] [G loss: 0.183604] [ema: 0.999848] 
[Epoch 262/288] [Batch 100/174] [D loss: 0.360930] [G loss: 0.193040] [ema: 0.999848] 
[Epoch 263/288] [Batch 0/174] [D loss: 0.384419] [G loss: 0.187332] [ema: 0.999849] 
[Epoch 263/288] [Batch 100/174] [D loss: 0.398569] [G loss: 0.170260] [ema: 0.999849] 
[Epoch 264/288] [Batch 0/174] [D loss: 0.409428] [G loss: 0.184564] [ema: 0.999849] 
[Epoch 264/288] [Batch 100/174] [D loss: 0.374408] [G loss: 0.186431] [ema: 0.999849] 
[Epoch 265/288] [Batch 0/174] [D loss: 0.388559] [G loss: 0.184774] [ema: 0.999850] 
[Epoch 265/288] [Batch 100/174] [D loss: 0.384256] [G loss: 0.193919] [ema: 0.999850] 
[Epoch 266/288] [Batch 0/174] [D loss: 0.423054] [G loss: 0.187699] [ema: 0.999850] 
[Epoch 266/288] [Batch 100/174] [D loss: 0.370429] [G loss: 0.181673] [ema: 0.999851] 
[Epoch 267/288] [Batch 0/174] [D loss: 0.409978] [G loss: 0.184929] [ema: 0.999851] 
[Epoch 267/288] [Batch 100/174] [D loss: 0.334155] [G loss: 0.187062] [ema: 0.999851] 
[Epoch 268/288] [Batch 0/174] [D loss: 0.353012] [G loss: 0.183667] [ema: 0.999851] 
[Epoch 268/288] [Batch 100/174] [D loss: 0.372215] [G loss: 0.180610] [ema: 0.999852] 
[Epoch 269/288] [Batch 0/174] [D loss: 0.408392] [G loss: 0.195080] [ema: 0.999852] 
[Epoch 269/288] [Batch 100/174] [D loss: 0.397647] [G loss: 0.171410] [ema: 0.999852] 
[Epoch 270/288] [Batch 0/174] [D loss: 0.364368] [G loss: 0.187391] [ema: 0.999852] 
[Epoch 270/288] [Batch 100/174] [D loss: 0.411050] [G loss: 0.176389] [ema: 0.999853] 
[Epoch 271/288] [Batch 0/174] [D loss: 0.340576] [G loss: 0.182002] [ema: 0.999853] 
[Epoch 271/288] [Batch 100/174] [D loss: 0.395172] [G loss: 0.182249] [ema: 0.999853] 
[Epoch 272/288] [Batch 0/174] [D loss: 0.381033] [G loss: 0.194877] [ema: 0.999854] 
[Epoch 272/288] [Batch 100/174] [D loss: 0.364247] [G loss: 0.189777] [ema: 0.999854] 
[Epoch 273/288] [Batch 0/174] [D loss: 0.430718] [G loss: 0.184259] [ema: 0.999854] 
[Epoch 273/288] [Batch 100/174] [D loss: 0.381344] [G loss: 0.169845] [ema: 0.999854] 
[Epoch 274/288] [Batch 0/174] [D loss: 0.417716] [G loss: 0.183809] [ema: 0.999855] 
[Epoch 274/288] [Batch 100/174] [D loss: 0.381780] [G loss: 0.176348] [ema: 0.999855] 
[Epoch 275/288] [Batch 0/174] [D loss: 0.383427] [G loss: 0.181541] [ema: 0.999855] 
[Epoch 275/288] [Batch 100/174] [D loss: 0.416865] [G loss: 0.186232] [ema: 0.999855] 
[Epoch 276/288] [Batch 0/174] [D loss: 0.400625] [G loss: 0.183816] [ema: 0.999856] 
[Epoch 276/288] [Batch 100/174] [D loss: 0.399037] [G loss: 0.180249] [ema: 0.999856] 
[Epoch 277/288] [Batch 0/174] [D loss: 0.372165] [G loss: 0.187754] [ema: 0.999856] 
[Epoch 277/288] [Batch 100/174] [D loss: 0.366770] [G loss: 0.182949] [ema: 0.999856] 
[Epoch 278/288] [Batch 0/174] [D loss: 0.381377] [G loss: 0.188498] [ema: 0.999857] 
[Epoch 278/288] [Batch 100/174] [D loss: 0.417096] [G loss: 0.188250] [ema: 0.999857] 
[Epoch 279/288] [Batch 0/174] [D loss: 0.369208] [G loss: 0.188720] [ema: 0.999857] 
[Epoch 279/288] [Batch 100/174] [D loss: 0.364286] [G loss: 0.190850] [ema: 0.999858] 
[Epoch 280/288] [Batch 0/174] [D loss: 0.411457] [G loss: 0.198121] [ema: 0.999858] 
[Epoch 280/288] [Batch 100/174] [D loss: 0.365887] [G loss: 0.183898] [ema: 0.999858] 
[Epoch 281/288] [Batch 0/174] [D loss: 0.382382] [G loss: 0.187743] [ema: 0.999858] 
[Epoch 281/288] [Batch 100/174] [D loss: 0.411938] [G loss: 0.175512] [ema: 0.999859] 
[Epoch 282/288] [Batch 0/174] [D loss: 0.403322] [G loss: 0.193511] [ema: 0.999859] 
[Epoch 282/288] [Batch 100/174] [D loss: 0.370090] [G loss: 0.184588] [ema: 0.999859] 
[Epoch 283/288] [Batch 0/174] [D loss: 0.383186] [G loss: 0.182746] [ema: 0.999859] 
[Epoch 283/288] [Batch 100/174] [D loss: 0.406854] [G loss: 0.190001] [ema: 0.999860] 
[Epoch 284/288] [Batch 0/174] [D loss: 0.414202] [G loss: 0.184124] [ema: 0.999860] 
[Epoch 284/288] [Batch 100/174] [D loss: 0.371647] [G loss: 0.181005] [ema: 0.999860] 
[Epoch 285/288] [Batch 0/174] [D loss: 0.392660] [G loss: 0.187956] [ema: 0.999860] 
[Epoch 285/288] [Batch 100/174] [D loss: 0.388534] [G loss: 0.192634] [ema: 0.999861] 
[Epoch 286/288] [Batch 0/174] [D loss: 0.396415] [G loss: 0.186527] [ema: 0.999861] 
[Epoch 286/288] [Batch 100/174] [D loss: 0.416938] [G loss: 0.176851] [ema: 0.999861] 
[Epoch 287/288] [Batch 0/174] [D loss: 0.388696] [G loss: 0.193031] [ema: 0.999861] 
[Epoch 287/288] [Batch 100/174] [D loss: 0.409598] [G loss: 0.184436] [ema: 0.999861] 
