
 Starting training
Total of classes being trained: 6

['MotionSense_DAGHAR_Multiclass.csv', 'RealWorld_thigh_DAGHAR_Multiclass.csv', 'WISDM_DAGHAR_Multiclass.csv', 'UCI_DAGHAR_Multiclass.csv', 'RealWorld_waist_DAGHAR_Multiclass.csv', 'KuHar_DAGHAR_Multiclass.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
MotionSense_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
MotionSense_DAGHAR_Multiclass
daghar
return single class data and labels, class is MotionSense_DAGHAR_Multiclass
data shape is (3558, 3, 1, 60)
label shape is (3558,)
223
Epochs between checkpoint: 57



Saving checkpoint 1 in logs/daghar_split_dataset_50000_60_100/MotionSense_DAGHAR_Multiclass_50000_D_60_2024_10_25_14_09_47/Model



[Epoch 0/225] [Batch 0/223] [D loss: 1.072917] [G loss: 0.449546] [ema: 0.000000] 
[Epoch 0/225] [Batch 100/223] [D loss: 0.499110] [G loss: 0.189887] [ema: 0.933033] 
[Epoch 0/225] [Batch 200/223] [D loss: 0.566035] [G loss: 0.145964] [ema: 0.965936] 
[Epoch 1/225] [Batch 0/223] [D loss: 0.465399] [G loss: 0.151275] [ema: 0.969395] 
[Epoch 1/225] [Batch 100/223] [D loss: 0.476051] [G loss: 0.181041] [ema: 0.978769] 
[Epoch 1/225] [Batch 200/223] [D loss: 0.391452] [G loss: 0.171201] [ema: 0.983747] 
[Epoch 2/225] [Batch 0/223] [D loss: 0.481760] [G loss: 0.182950] [ema: 0.984579] 
[Epoch 2/225] [Batch 100/223] [D loss: 0.412485] [G loss: 0.212920] [ema: 0.987385] 
[Epoch 2/225] [Batch 200/223] [D loss: 0.464618] [G loss: 0.163460] [ema: 0.989328] 
[Epoch 3/225] [Batch 0/223] [D loss: 0.456045] [G loss: 0.169375] [ema: 0.989693] 
[Epoch 3/225] [Batch 100/223] [D loss: 0.449398] [G loss: 0.205875] [ema: 0.991027] 
[Epoch 3/225] [Batch 200/223] [D loss: 0.422884] [G loss: 0.162403] [ema: 0.992055] 
[Epoch 4/225] [Batch 0/223] [D loss: 0.463445] [G loss: 0.187509] [ema: 0.992259] 
[Epoch 4/225] [Batch 100/223] [D loss: 0.572442] [G loss: 0.157148] [ema: 0.993037] 
[Epoch 4/225] [Batch 200/223] [D loss: 0.396728] [G loss: 0.207363] [ema: 0.993673] 
[Epoch 5/225] [Batch 0/223] [D loss: 0.425269] [G loss: 0.158004] [ema: 0.993803] 
[Epoch 5/225] [Batch 100/223] [D loss: 0.479577] [G loss: 0.173000] [ema: 0.994311] 
[Epoch 5/225] [Batch 200/223] [D loss: 0.456218] [G loss: 0.155447] [ema: 0.994743] 
[Epoch 6/225] [Batch 0/223] [D loss: 0.410119] [G loss: 0.190928] [ema: 0.994833] 
[Epoch 6/225] [Batch 100/223] [D loss: 0.428668] [G loss: 0.185486] [ema: 0.995191] 
[Epoch 6/225] [Batch 200/223] [D loss: 0.388738] [G loss: 0.146735] [ema: 0.995503] 
[Epoch 7/225] [Batch 0/223] [D loss: 0.402374] [G loss: 0.177579] [ema: 0.995569] 
[Epoch 7/225] [Batch 100/223] [D loss: 0.399128] [G loss: 0.213363] [ema: 0.995836] 
[Epoch 7/225] [Batch 200/223] [D loss: 0.401749] [G loss: 0.188082] [ema: 0.996072] 
[Epoch 8/225] [Batch 0/223] [D loss: 0.412715] [G loss: 0.244182] [ema: 0.996122] 
[Epoch 8/225] [Batch 100/223] [D loss: 0.452533] [G loss: 0.176598] [ema: 0.996328] 
[Epoch 8/225] [Batch 200/223] [D loss: 0.387802] [G loss: 0.165181] [ema: 0.996512] 
[Epoch 9/225] [Batch 0/223] [D loss: 0.378410] [G loss: 0.219333] [ema: 0.996552] 
[Epoch 9/225] [Batch 100/223] [D loss: 0.352178] [G loss: 0.173368] [ema: 0.996716] 
[Epoch 9/225] [Batch 200/223] [D loss: 0.347175] [G loss: 0.218368] [ema: 0.996864] 
[Epoch 10/225] [Batch 0/223] [D loss: 0.351089] [G loss: 0.220048] [ema: 0.996897] 
[Epoch 10/225] [Batch 100/223] [D loss: 0.325914] [G loss: 0.165446] [ema: 0.997030] 
[Epoch 10/225] [Batch 200/223] [D loss: 0.345686] [G loss: 0.209878] [ema: 0.997152] 
[Epoch 11/225] [Batch 0/223] [D loss: 0.415288] [G loss: 0.170194] [ema: 0.997178] 
[Epoch 11/225] [Batch 100/223] [D loss: 0.418864] [G loss: 0.165583] [ema: 0.997289] 
[Epoch 11/225] [Batch 200/223] [D loss: 0.441078] [G loss: 0.152899] [ema: 0.997391] 
[Epoch 12/225] [Batch 0/223] [D loss: 0.392398] [G loss: 0.191369] [ema: 0.997413] 
[Epoch 12/225] [Batch 100/223] [D loss: 0.360475] [G loss: 0.224045] [ema: 0.997506] 
[Epoch 12/225] [Batch 200/223] [D loss: 0.372626] [G loss: 0.206977] [ema: 0.997593] 
[Epoch 13/225] [Batch 0/223] [D loss: 0.378845] [G loss: 0.187755] [ema: 0.997612] 
[Epoch 13/225] [Batch 100/223] [D loss: 0.393067] [G loss: 0.174594] [ema: 0.997691] 
[Epoch 13/225] [Batch 200/223] [D loss: 0.413836] [G loss: 0.192622] [ema: 0.997766] 
[Epoch 14/225] [Batch 0/223] [D loss: 0.423509] [G loss: 0.189678] [ema: 0.997782] 
[Epoch 14/225] [Batch 100/223] [D loss: 0.371277] [G loss: 0.184093] [ema: 0.997851] 
[Epoch 14/225] [Batch 200/223] [D loss: 0.393929] [G loss: 0.182330] [ema: 0.997916] 
[Epoch 15/225] [Batch 0/223] [D loss: 0.370498] [G loss: 0.192234] [ema: 0.997930] 
[Epoch 15/225] [Batch 100/223] [D loss: 0.387217] [G loss: 0.185490] [ema: 0.997990] 
[Epoch 15/225] [Batch 200/223] [D loss: 0.434449] [G loss: 0.153563] [ema: 0.998047] 
[Epoch 16/225] [Batch 0/223] [D loss: 0.456139] [G loss: 0.173315] [ema: 0.998059] 
[Epoch 16/225] [Batch 100/223] [D loss: 0.385193] [G loss: 0.188104] [ema: 0.998112] 
[Epoch 16/225] [Batch 200/223] [D loss: 0.422365] [G loss: 0.172338] [ema: 0.998162] 
[Epoch 17/225] [Batch 0/223] [D loss: 0.400926] [G loss: 0.173335] [ema: 0.998173] 
[Epoch 17/225] [Batch 100/223] [D loss: 0.407287] [G loss: 0.154164] [ema: 0.998220] 
[Epoch 17/225] [Batch 200/223] [D loss: 0.409919] [G loss: 0.200786] [ema: 0.998265] 
[Epoch 18/225] [Batch 0/223] [D loss: 0.395729] [G loss: 0.159597] [ema: 0.998275] 
[Epoch 18/225] [Batch 100/223] [D loss: 0.345481] [G loss: 0.198106] [ema: 0.998317] 
[Epoch 18/225] [Batch 200/223] [D loss: 0.436276] [G loss: 0.202046] [ema: 0.998356] 
[Epoch 19/225] [Batch 0/223] [D loss: 0.353248] [G loss: 0.182446] [ema: 0.998365] 
[Epoch 19/225] [Batch 100/223] [D loss: 0.376764] [G loss: 0.225673] [ema: 0.998403] 
[Epoch 19/225] [Batch 200/223] [D loss: 0.406843] [G loss: 0.186970] [ema: 0.998439] 
[Epoch 20/225] [Batch 0/223] [D loss: 0.402578] [G loss: 0.205013] [ema: 0.998447] 
[Epoch 20/225] [Batch 100/223] [D loss: 0.373290] [G loss: 0.172298] [ema: 0.998481] 
[Epoch 20/225] [Batch 200/223] [D loss: 0.375670] [G loss: 0.206271] [ema: 0.998514] 
[Epoch 21/225] [Batch 0/223] [D loss: 0.421240] [G loss: 0.170949] [ema: 0.998521] 
[Epoch 21/225] [Batch 100/223] [D loss: 0.393018] [G loss: 0.183806] [ema: 0.998552] 
[Epoch 21/225] [Batch 200/223] [D loss: 0.384573] [G loss: 0.192670] [ema: 0.998581] 
[Epoch 22/225] [Batch 0/223] [D loss: 0.416126] [G loss: 0.202136] [ema: 0.998588] 
[Epoch 22/225] [Batch 100/223] [D loss: 0.374857] [G loss: 0.181605] [ema: 0.998616] 
[Epoch 22/225] [Batch 200/223] [D loss: 0.385434] [G loss: 0.197059] [ema: 0.998643] 
[Epoch 23/225] [Batch 0/223] [D loss: 0.424881] [G loss: 0.193055] [ema: 0.998649] 
[Epoch 23/225] [Batch 100/223] [D loss: 0.384034] [G loss: 0.174381] [ema: 0.998675] 
[Epoch 23/225] [Batch 200/223] [D loss: 0.287837] [G loss: 0.210659] [ema: 0.998700] 
[Epoch 24/225] [Batch 0/223] [D loss: 0.395919] [G loss: 0.199868] [ema: 0.998706] 
[Epoch 24/225] [Batch 100/223] [D loss: 0.381930] [G loss: 0.176311] [ema: 0.998729] 
[Epoch 24/225] [Batch 200/223] [D loss: 0.440444] [G loss: 0.182847] [ema: 0.998752] 
[Epoch 25/225] [Batch 0/223] [D loss: 0.434112] [G loss: 0.211064] [ema: 0.998757] 
[Epoch 25/225] [Batch 100/223] [D loss: 0.355872] [G loss: 0.173645] [ema: 0.998779] 
[Epoch 25/225] [Batch 200/223] [D loss: 0.412614] [G loss: 0.192921] [ema: 0.998800] 
[Epoch 26/225] [Batch 0/223] [D loss: 0.367069] [G loss: 0.174467] [ema: 0.998805] 
[Epoch 26/225] [Batch 100/223] [D loss: 0.398781] [G loss: 0.200666] [ema: 0.998825] 
[Epoch 26/225] [Batch 200/223] [D loss: 0.385663] [G loss: 0.173968] [ema: 0.998845] 
[Epoch 27/225] [Batch 0/223] [D loss: 0.402127] [G loss: 0.222154] [ema: 0.998849] 
[Epoch 27/225] [Batch 100/223] [D loss: 0.362341] [G loss: 0.172858] [ema: 0.998868] 
[Epoch 27/225] [Batch 200/223] [D loss: 0.371798] [G loss: 0.192701] [ema: 0.998886] 
[Epoch 28/225] [Batch 0/223] [D loss: 0.440527] [G loss: 0.195056] [ema: 0.998891] 
[Epoch 28/225] [Batch 100/223] [D loss: 0.381011] [G loss: 0.232562] [ema: 0.998908] 
[Epoch 28/225] [Batch 200/223] [D loss: 0.393403] [G loss: 0.188671] [ema: 0.998925] 
[Epoch 29/225] [Batch 0/223] [D loss: 0.390125] [G loss: 0.204435] [ema: 0.998929] 
[Epoch 29/225] [Batch 100/223] [D loss: 0.405750] [G loss: 0.225778] [ema: 0.998945] 
[Epoch 29/225] [Batch 200/223] [D loss: 0.368901] [G loss: 0.167117] [ema: 0.998961] 
[Epoch 30/225] [Batch 0/223] [D loss: 0.386499] [G loss: 0.199331] [ema: 0.998964] 
[Epoch 30/225] [Batch 100/223] [D loss: 0.404838] [G loss: 0.185634] [ema: 0.998980] 
[Epoch 30/225] [Batch 200/223] [D loss: 0.374149] [G loss: 0.180881] [ema: 0.998994] 
[Epoch 31/225] [Batch 0/223] [D loss: 0.358351] [G loss: 0.190621] [ema: 0.998998] 
[Epoch 31/225] [Batch 100/223] [D loss: 0.326463] [G loss: 0.187738] [ema: 0.999012] 
[Epoch 31/225] [Batch 200/223] [D loss: 0.321869] [G loss: 0.203090] [ema: 0.999026] 
[Epoch 32/225] [Batch 0/223] [D loss: 0.351032] [G loss: 0.139396] [ema: 0.999029] 
[Epoch 32/225] [Batch 100/223] [D loss: 0.418230] [G loss: 0.182199] [ema: 0.999043] 
[Epoch 32/225] [Batch 200/223] [D loss: 0.419739] [G loss: 0.216380] [ema: 0.999056] 
[Epoch 33/225] [Batch 0/223] [D loss: 0.355620] [G loss: 0.225045] [ema: 0.999059] 
[Epoch 33/225] [Batch 100/223] [D loss: 0.400180] [G loss: 0.210396] [ema: 0.999071] 
[Epoch 33/225] [Batch 200/223] [D loss: 0.433532] [G loss: 0.213536] [ema: 0.999083] 
[Epoch 34/225] [Batch 0/223] [D loss: 0.381486] [G loss: 0.197128] [ema: 0.999086] 
[Epoch 34/225] [Batch 100/223] [D loss: 0.384626] [G loss: 0.180623] [ema: 0.999098] 
[Epoch 34/225] [Batch 200/223] [D loss: 0.373469] [G loss: 0.215563] [ema: 0.999110] 
[Epoch 35/225] [Batch 0/223] [D loss: 0.397556] [G loss: 0.214638] [ema: 0.999112] 
[Epoch 35/225] [Batch 100/223] [D loss: 0.410814] [G loss: 0.173328] [ema: 0.999124] 
[Epoch 35/225] [Batch 200/223] [D loss: 0.327167] [G loss: 0.188010] [ema: 0.999134] 
[Epoch 36/225] [Batch 0/223] [D loss: 0.376168] [G loss: 0.216311] [ema: 0.999137] 
[Epoch 36/225] [Batch 100/223] [D loss: 0.356139] [G loss: 0.186071] [ema: 0.999148] 
[Epoch 36/225] [Batch 200/223] [D loss: 0.367573] [G loss: 0.202805] [ema: 0.999158] 
[Epoch 37/225] [Batch 0/223] [D loss: 0.325606] [G loss: 0.174936] [ema: 0.999160] 
[Epoch 37/225] [Batch 100/223] [D loss: 0.397043] [G loss: 0.183731] [ema: 0.999170] 
[Epoch 37/225] [Batch 200/223] [D loss: 0.347996] [G loss: 0.218404] [ema: 0.999180] 
[Epoch 38/225] [Batch 0/223] [D loss: 0.382541] [G loss: 0.194034] [ema: 0.999182] 
[Epoch 38/225] [Batch 100/223] [D loss: 0.383455] [G loss: 0.208351] [ema: 0.999192] 
[Epoch 38/225] [Batch 200/223] [D loss: 0.408889] [G loss: 0.191849] [ema: 0.999201] 
[Epoch 39/225] [Batch 0/223] [D loss: 0.359977] [G loss: 0.199468] [ema: 0.999203] 
[Epoch 39/225] [Batch 100/223] [D loss: 0.378074] [G loss: 0.200133] [ema: 0.999212] 
[Epoch 39/225] [Batch 200/223] [D loss: 0.438589] [G loss: 0.194072] [ema: 0.999221] 
[Epoch 40/225] [Batch 0/223] [D loss: 0.344574] [G loss: 0.208615] [ema: 0.999223] 
[Epoch 40/225] [Batch 100/223] [D loss: 0.383619] [G loss: 0.181828] [ema: 0.999232] 
[Epoch 40/225] [Batch 200/223] [D loss: 0.455172] [G loss: 0.193528] [ema: 0.999240] 
[Epoch 41/225] [Batch 0/223] [D loss: 0.362020] [G loss: 0.187714] [ema: 0.999242] 
[Epoch 41/225] [Batch 100/223] [D loss: 0.418457] [G loss: 0.219357] [ema: 0.999250] 
[Epoch 41/225] [Batch 200/223] [D loss: 0.358054] [G loss: 0.199107] [ema: 0.999258] 
[Epoch 42/225] [Batch 0/223] [D loss: 0.369330] [G loss: 0.230262] [ema: 0.999260] 
[Epoch 42/225] [Batch 100/223] [D loss: 0.285636] [G loss: 0.224882] [ema: 0.999268] 
[Epoch 42/225] [Batch 200/223] [D loss: 0.412430] [G loss: 0.178446] [ema: 0.999276] 
[Epoch 43/225] [Batch 0/223] [D loss: 0.339509] [G loss: 0.193905] [ema: 0.999277] 
[Epoch 43/225] [Batch 100/223] [D loss: 0.426676] [G loss: 0.187965] [ema: 0.999285] 
[Epoch 43/225] [Batch 200/223] [D loss: 0.387365] [G loss: 0.177181] [ema: 0.999292] 
[Epoch 44/225] [Batch 0/223] [D loss: 0.386634] [G loss: 0.189493] [ema: 0.999294] 
[Epoch 44/225] [Batch 100/223] [D loss: 0.390155] [G loss: 0.211387] [ema: 0.999301] 
[Epoch 44/225] [Batch 200/223] [D loss: 0.375772] [G loss: 0.191749] [ema: 0.999308] 
[Epoch 45/225] [Batch 0/223] [D loss: 0.381296] [G loss: 0.203443] [ema: 0.999310] 
[Epoch 45/225] [Batch 100/223] [D loss: 0.372068] [G loss: 0.178302] [ema: 0.999316] 
[Epoch 45/225] [Batch 200/223] [D loss: 0.388601] [G loss: 0.187719] [ema: 0.999323] 
[Epoch 46/225] [Batch 0/223] [D loss: 0.363353] [G loss: 0.245870] [ema: 0.999325] 
[Epoch 46/225] [Batch 100/223] [D loss: 0.361867] [G loss: 0.187837] [ema: 0.999331] 
[Epoch 46/225] [Batch 200/223] [D loss: 0.378317] [G loss: 0.166228] [ema: 0.999337] 
[Epoch 47/225] [Batch 0/223] [D loss: 0.353339] [G loss: 0.205139] [ema: 0.999339] 
[Epoch 47/225] [Batch 100/223] [D loss: 0.368794] [G loss: 0.213777] [ema: 0.999345] 
[Epoch 47/225] [Batch 200/223] [D loss: 0.364974] [G loss: 0.211994] [ema: 0.999351] 
[Epoch 48/225] [Batch 0/223] [D loss: 0.330962] [G loss: 0.179228] [ema: 0.999353] 
[Epoch 48/225] [Batch 100/223] [D loss: 0.419716] [G loss: 0.193612] [ema: 0.999359] 
[Epoch 48/225] [Batch 200/223] [D loss: 0.337788] [G loss: 0.161998] [ema: 0.999365] 
[Epoch 49/225] [Batch 0/223] [D loss: 0.468134] [G loss: 0.192425] [ema: 0.999366] 
[Epoch 49/225] [Batch 100/223] [D loss: 0.439177] [G loss: 0.169928] [ema: 0.999372] 
[Epoch 49/225] [Batch 200/223] [D loss: 0.410258] [G loss: 0.143112] [ema: 0.999377] 
[Epoch 50/225] [Batch 0/223] [D loss: 0.396142] [G loss: 0.192382] [ema: 0.999379] 
[Epoch 50/225] [Batch 100/223] [D loss: 0.396862] [G loss: 0.184523] [ema: 0.999384] 
[Epoch 50/225] [Batch 200/223] [D loss: 0.387188] [G loss: 0.156765] [ema: 0.999389] 
[Epoch 51/225] [Batch 0/223] [D loss: 0.425813] [G loss: 0.172388] [ema: 0.999391] 
[Epoch 51/225] [Batch 100/223] [D loss: 0.434830] [G loss: 0.168011] [ema: 0.999396] 
[Epoch 51/225] [Batch 200/223] [D loss: 0.485026] [G loss: 0.151860] [ema: 0.999401] 
[Epoch 52/225] [Batch 0/223] [D loss: 0.441181] [G loss: 0.158059] [ema: 0.999402] 
[Epoch 52/225] [Batch 100/223] [D loss: 0.499354] [G loss: 0.147240] [ema: 0.999408] 
[Epoch 52/225] [Batch 200/223] [D loss: 0.445615] [G loss: 0.154541] [ema: 0.999413] 
[Epoch 53/225] [Batch 0/223] [D loss: 0.467039] [G loss: 0.141797] [ema: 0.999414] 
[Epoch 53/225] [Batch 100/223] [D loss: 0.541773] [G loss: 0.142007] [ema: 0.999419] 
[Epoch 53/225] [Batch 200/223] [D loss: 0.530778] [G loss: 0.156788] [ema: 0.999423] 
[Epoch 54/225] [Batch 0/223] [D loss: 0.443057] [G loss: 0.146835] [ema: 0.999425] 
[Epoch 54/225] [Batch 100/223] [D loss: 0.459521] [G loss: 0.158811] [ema: 0.999429] 
[Epoch 54/225] [Batch 200/223] [D loss: 0.458186] [G loss: 0.139820] [ema: 0.999434] 
[Epoch 55/225] [Batch 0/223] [D loss: 0.478876] [G loss: 0.186647] [ema: 0.999435] 
[Epoch 55/225] [Batch 100/223] [D loss: 0.495198] [G loss: 0.137103] [ema: 0.999440] 
[Epoch 55/225] [Batch 200/223] [D loss: 0.430519] [G loss: 0.121653] [ema: 0.999444] 
[Epoch 56/225] [Batch 0/223] [D loss: 0.473645] [G loss: 0.154906] [ema: 0.999445] 
[Epoch 56/225] [Batch 100/223] [D loss: 0.451040] [G loss: 0.152176] [ema: 0.999450] 
[Epoch 56/225] [Batch 200/223] [D loss: 0.431921] [G loss: 0.176689] [ema: 0.999454] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_60_100/MotionSense_DAGHAR_Multiclass_50000_D_60_2024_10_25_14_09_47/Model



[Epoch 57/225] [Batch 0/223] [D loss: 0.442691] [G loss: 0.169233] [ema: 0.999455] 
[Epoch 57/225] [Batch 100/223] [D loss: 0.459101] [G loss: 0.143220] [ema: 0.999459] 
[Epoch 57/225] [Batch 200/223] [D loss: 0.448891] [G loss: 0.151874] [ema: 0.999463] 
[Epoch 58/225] [Batch 0/223] [D loss: 0.409800] [G loss: 0.197978] [ema: 0.999464] 
[Epoch 58/225] [Batch 100/223] [D loss: 0.410683] [G loss: 0.159689] [ema: 0.999468] 
[Epoch 58/225] [Batch 200/223] [D loss: 0.433602] [G loss: 0.150666] [ema: 0.999472] 
[Epoch 59/225] [Batch 0/223] [D loss: 0.432792] [G loss: 0.168603] [ema: 0.999473] 
[Epoch 59/225] [Batch 100/223] [D loss: 0.474942] [G loss: 0.126657] [ema: 0.999477] 
[Epoch 59/225] [Batch 200/223] [D loss: 0.444103] [G loss: 0.182220] [ema: 0.999481] 
[Epoch 60/225] [Batch 0/223] [D loss: 0.414833] [G loss: 0.162999] [ema: 0.999482] 
[Epoch 60/225] [Batch 100/223] [D loss: 0.478273] [G loss: 0.158216] [ema: 0.999486] 
[Epoch 60/225] [Batch 200/223] [D loss: 0.423099] [G loss: 0.171137] [ema: 0.999490] 
[Epoch 61/225] [Batch 0/223] [D loss: 0.413085] [G loss: 0.171140] [ema: 0.999491] 
[Epoch 61/225] [Batch 100/223] [D loss: 0.452009] [G loss: 0.173079] [ema: 0.999494] 
[Epoch 61/225] [Batch 200/223] [D loss: 0.442490] [G loss: 0.182552] [ema: 0.999498] 
[Epoch 62/225] [Batch 0/223] [D loss: 0.436208] [G loss: 0.201040] [ema: 0.999499] 
[Epoch 62/225] [Batch 100/223] [D loss: 0.447325] [G loss: 0.180635] [ema: 0.999502] 
[Epoch 62/225] [Batch 200/223] [D loss: 0.425399] [G loss: 0.173923] [ema: 0.999506] 
[Epoch 63/225] [Batch 0/223] [D loss: 0.424772] [G loss: 0.164538] [ema: 0.999507] 
[Epoch 63/225] [Batch 100/223] [D loss: 0.496444] [G loss: 0.172394] [ema: 0.999510] 
[Epoch 63/225] [Batch 200/223] [D loss: 0.467309] [G loss: 0.166200] [ema: 0.999514] 
[Epoch 64/225] [Batch 0/223] [D loss: 0.431551] [G loss: 0.174442] [ema: 0.999514] 
[Epoch 64/225] [Batch 100/223] [D loss: 0.467064] [G loss: 0.163142] [ema: 0.999518] 
[Epoch 64/225] [Batch 200/223] [D loss: 0.377062] [G loss: 0.157089] [ema: 0.999521] 
[Epoch 65/225] [Batch 0/223] [D loss: 0.425424] [G loss: 0.210308] [ema: 0.999522] 
[Epoch 65/225] [Batch 100/223] [D loss: 0.451658] [G loss: 0.188912] [ema: 0.999525] 
[Epoch 65/225] [Batch 200/223] [D loss: 0.431891] [G loss: 0.178015] [ema: 0.999528] 
[Epoch 66/225] [Batch 0/223] [D loss: 0.515121] [G loss: 0.163235] [ema: 0.999529] 
[Epoch 66/225] [Batch 100/223] [D loss: 0.355491] [G loss: 0.199061] [ema: 0.999532] 
[Epoch 66/225] [Batch 200/223] [D loss: 0.447533] [G loss: 0.177402] [ema: 0.999535] 
[Epoch 67/225] [Batch 0/223] [D loss: 0.424866] [G loss: 0.176890] [ema: 0.999536] 
[Epoch 67/225] [Batch 100/223] [D loss: 0.427692] [G loss: 0.150677] [ema: 0.999539] 
[Epoch 67/225] [Batch 200/223] [D loss: 0.394893] [G loss: 0.155433] [ema: 0.999542] 
[Epoch 68/225] [Batch 0/223] [D loss: 0.402685] [G loss: 0.179345] [ema: 0.999543] 
[Epoch 68/225] [Batch 100/223] [D loss: 0.372297] [G loss: 0.158778] [ema: 0.999546] 
[Epoch 68/225] [Batch 200/223] [D loss: 0.431435] [G loss: 0.171153] [ema: 0.999549] 
[Epoch 69/225] [Batch 0/223] [D loss: 0.445830] [G loss: 0.184330] [ema: 0.999550] 
[Epoch 69/225] [Batch 100/223] [D loss: 0.406918] [G loss: 0.183821] [ema: 0.999553] 
[Epoch 69/225] [Batch 200/223] [D loss: 0.374169] [G loss: 0.176973] [ema: 0.999555] 
[Epoch 70/225] [Batch 0/223] [D loss: 0.381555] [G loss: 0.225661] [ema: 0.999556] 
[Epoch 70/225] [Batch 100/223] [D loss: 0.375795] [G loss: 0.169838] [ema: 0.999559] 
[Epoch 70/225] [Batch 200/223] [D loss: 0.426847] [G loss: 0.177637] [ema: 0.999562] 
[Epoch 71/225] [Batch 0/223] [D loss: 0.459655] [G loss: 0.184024] [ema: 0.999562] 
[Epoch 71/225] [Batch 100/223] [D loss: 0.373113] [G loss: 0.183125] [ema: 0.999565] 
[Epoch 71/225] [Batch 200/223] [D loss: 0.390307] [G loss: 0.174945] [ema: 0.999568] 
[Epoch 72/225] [Batch 0/223] [D loss: 0.427861] [G loss: 0.162067] [ema: 0.999568] 
[Epoch 72/225] [Batch 100/223] [D loss: 0.371595] [G loss: 0.190631] [ema: 0.999571] 
[Epoch 72/225] [Batch 200/223] [D loss: 0.360379] [G loss: 0.203454] [ema: 0.999574] 
[Epoch 73/225] [Batch 0/223] [D loss: 0.322968] [G loss: 0.191067] [ema: 0.999574] 
[Epoch 73/225] [Batch 100/223] [D loss: 0.348946] [G loss: 0.194615] [ema: 0.999577] 
[Epoch 73/225] [Batch 200/223] [D loss: 0.359941] [G loss: 0.191603] [ema: 0.999579] 
[Epoch 74/225] [Batch 0/223] [D loss: 0.407721] [G loss: 0.181625] [ema: 0.999580] 
[Epoch 74/225] [Batch 100/223] [D loss: 0.377185] [G loss: 0.194594] [ema: 0.999583] 
[Epoch 74/225] [Batch 200/223] [D loss: 0.431689] [G loss: 0.188648] [ema: 0.999585] 
[Epoch 75/225] [Batch 0/223] [D loss: 0.361819] [G loss: 0.216893] [ema: 0.999586] 
[Epoch 75/225] [Batch 100/223] [D loss: 0.358024] [G loss: 0.188359] [ema: 0.999588] 
[Epoch 75/225] [Batch 200/223] [D loss: 0.375038] [G loss: 0.208663] [ema: 0.999591] 
[Epoch 76/225] [Batch 0/223] [D loss: 0.353741] [G loss: 0.194417] [ema: 0.999591] 
[Epoch 76/225] [Batch 100/223] [D loss: 0.380072] [G loss: 0.159464] [ema: 0.999593] 
[Epoch 76/225] [Batch 200/223] [D loss: 0.397948] [G loss: 0.185512] [ema: 0.999596] 
[Epoch 77/225] [Batch 0/223] [D loss: 0.439623] [G loss: 0.193595] [ema: 0.999596] 
[Epoch 77/225] [Batch 100/223] [D loss: 0.412736] [G loss: 0.165590] [ema: 0.999599] 
[Epoch 77/225] [Batch 200/223] [D loss: 0.397095] [G loss: 0.190442] [ema: 0.999601] 
[Epoch 78/225] [Batch 0/223] [D loss: 0.399622] [G loss: 0.185039] [ema: 0.999602] 
[Epoch 78/225] [Batch 100/223] [D loss: 0.398370] [G loss: 0.167397] [ema: 0.999604] 
[Epoch 78/225] [Batch 200/223] [D loss: 0.396813] [G loss: 0.199736] [ema: 0.999606] 
[Epoch 79/225] [Batch 0/223] [D loss: 0.386601] [G loss: 0.191886] [ema: 0.999607] 
[Epoch 79/225] [Batch 100/223] [D loss: 0.390984] [G loss: 0.212166] [ema: 0.999609] 
[Epoch 79/225] [Batch 200/223] [D loss: 0.420188] [G loss: 0.167540] [ema: 0.999611] 
[Epoch 80/225] [Batch 0/223] [D loss: 0.418656] [G loss: 0.199835] [ema: 0.999612] 
[Epoch 80/225] [Batch 100/223] [D loss: 0.356579] [G loss: 0.202226] [ema: 0.999614] 
[Epoch 80/225] [Batch 200/223] [D loss: 0.356746] [G loss: 0.207385] [ema: 0.999616] 
[Epoch 81/225] [Batch 0/223] [D loss: 0.436911] [G loss: 0.163266] [ema: 0.999616] 
[Epoch 81/225] [Batch 100/223] [D loss: 0.392626] [G loss: 0.178547] [ema: 0.999618] 
[Epoch 81/225] [Batch 200/223] [D loss: 0.386835] [G loss: 0.202329] [ema: 0.999621] 
[Epoch 82/225] [Batch 0/223] [D loss: 0.418236] [G loss: 0.223731] [ema: 0.999621] 
[Epoch 82/225] [Batch 100/223] [D loss: 0.442226] [G loss: 0.168860] [ema: 0.999623] 
[Epoch 82/225] [Batch 200/223] [D loss: 0.438280] [G loss: 0.206268] [ema: 0.999625] 
[Epoch 83/225] [Batch 0/223] [D loss: 0.389230] [G loss: 0.194255] [ema: 0.999626] 
[Epoch 83/225] [Batch 100/223] [D loss: 0.329277] [G loss: 0.194195] [ema: 0.999628] 
[Epoch 83/225] [Batch 200/223] [D loss: 0.380996] [G loss: 0.198630] [ema: 0.999630] 
[Epoch 84/225] [Batch 0/223] [D loss: 0.315539] [G loss: 0.194387] [ema: 0.999630] 
[Epoch 84/225] [Batch 100/223] [D loss: 0.399645] [G loss: 0.198277] [ema: 0.999632] 
[Epoch 84/225] [Batch 200/223] [D loss: 0.415947] [G loss: 0.180918] [ema: 0.999634] 
[Epoch 85/225] [Batch 0/223] [D loss: 0.365204] [G loss: 0.203981] [ema: 0.999634] 
[Epoch 85/225] [Batch 100/223] [D loss: 0.401396] [G loss: 0.199635] [ema: 0.999636] 
[Epoch 85/225] [Batch 200/223] [D loss: 0.378607] [G loss: 0.203679] [ema: 0.999638] 
[Epoch 86/225] [Batch 0/223] [D loss: 0.312588] [G loss: 0.236271] [ema: 0.999639] 
[Epoch 86/225] [Batch 100/223] [D loss: 0.393342] [G loss: 0.206712] [ema: 0.999641] 
[Epoch 86/225] [Batch 200/223] [D loss: 0.286339] [G loss: 0.216211] [ema: 0.999642] 
[Epoch 87/225] [Batch 0/223] [D loss: 0.393036] [G loss: 0.185921] [ema: 0.999643] 
[Epoch 87/225] [Batch 100/223] [D loss: 0.356735] [G loss: 0.203313] [ema: 0.999645] 
[Epoch 87/225] [Batch 200/223] [D loss: 0.381169] [G loss: 0.200452] [ema: 0.999646] 
[Epoch 88/225] [Batch 0/223] [D loss: 0.370942] [G loss: 0.193342] [ema: 0.999647] 
[Epoch 88/225] [Batch 100/223] [D loss: 0.400628] [G loss: 0.191456] [ema: 0.999649] 
[Epoch 88/225] [Batch 200/223] [D loss: 0.396986] [G loss: 0.169885] [ema: 0.999650] 
[Epoch 89/225] [Batch 0/223] [D loss: 0.366935] [G loss: 0.217035] [ema: 0.999651] 
[Epoch 89/225] [Batch 100/223] [D loss: 0.361716] [G loss: 0.185321] [ema: 0.999653] 
[Epoch 89/225] [Batch 200/223] [D loss: 0.396103] [G loss: 0.186606] [ema: 0.999654] 
[Epoch 90/225] [Batch 0/223] [D loss: 0.360773] [G loss: 0.199034] [ema: 0.999655] 
[Epoch 90/225] [Batch 100/223] [D loss: 0.434847] [G loss: 0.202834] [ema: 0.999656] 
[Epoch 90/225] [Batch 200/223] [D loss: 0.409107] [G loss: 0.178238] [ema: 0.999658] 
[Epoch 91/225] [Batch 0/223] [D loss: 0.390225] [G loss: 0.215343] [ema: 0.999658] 
[Epoch 91/225] [Batch 100/223] [D loss: 0.334435] [G loss: 0.190473] [ema: 0.999660] 
[Epoch 91/225] [Batch 200/223] [D loss: 0.359157] [G loss: 0.202292] [ema: 0.999662] 
[Epoch 92/225] [Batch 0/223] [D loss: 0.349261] [G loss: 0.230569] [ema: 0.999662] 
[Epoch 92/225] [Batch 100/223] [D loss: 0.346576] [G loss: 0.197260] [ema: 0.999664] 
[Epoch 92/225] [Batch 200/223] [D loss: 0.353929] [G loss: 0.206752] [ema: 0.999665] 
[Epoch 93/225] [Batch 0/223] [D loss: 0.386413] [G loss: 0.216278] [ema: 0.999666] 
[Epoch 93/225] [Batch 100/223] [D loss: 0.346894] [G loss: 0.208420] [ema: 0.999667] 
[Epoch 93/225] [Batch 200/223] [D loss: 0.401051] [G loss: 0.177864] [ema: 0.999669] 
[Epoch 94/225] [Batch 0/223] [D loss: 0.377232] [G loss: 0.197236] [ema: 0.999669] 
[Epoch 94/225] [Batch 100/223] [D loss: 0.378668] [G loss: 0.207556] [ema: 0.999671] 
[Epoch 94/225] [Batch 200/223] [D loss: 0.326325] [G loss: 0.227521] [ema: 0.999673] 
[Epoch 95/225] [Batch 0/223] [D loss: 0.389773] [G loss: 0.224608] [ema: 0.999673] 
[Epoch 95/225] [Batch 100/223] [D loss: 0.338468] [G loss: 0.194321] [ema: 0.999674] 
[Epoch 95/225] [Batch 200/223] [D loss: 0.313881] [G loss: 0.205499] [ema: 0.999676] 
[Epoch 96/225] [Batch 0/223] [D loss: 0.344986] [G loss: 0.233835] [ema: 0.999676] 
[Epoch 96/225] [Batch 100/223] [D loss: 0.309755] [G loss: 0.235603] [ema: 0.999678] 
[Epoch 96/225] [Batch 200/223] [D loss: 0.361685] [G loss: 0.173868] [ema: 0.999679] 
[Epoch 97/225] [Batch 0/223] [D loss: 0.343642] [G loss: 0.196922] [ema: 0.999680] 
[Epoch 97/225] [Batch 100/223] [D loss: 0.337953] [G loss: 0.221139] [ema: 0.999681] 
[Epoch 97/225] [Batch 200/223] [D loss: 0.365912] [G loss: 0.218586] [ema: 0.999683] 
[Epoch 98/225] [Batch 0/223] [D loss: 0.365556] [G loss: 0.198877] [ema: 0.999683] 
[Epoch 98/225] [Batch 100/223] [D loss: 0.310957] [G loss: 0.222539] [ema: 0.999684] 
[Epoch 98/225] [Batch 200/223] [D loss: 0.368776] [G loss: 0.219208] [ema: 0.999686] 
[Epoch 99/225] [Batch 0/223] [D loss: 0.359647] [G loss: 0.196624] [ema: 0.999686] 
[Epoch 99/225] [Batch 100/223] [D loss: 0.367060] [G loss: 0.221371] [ema: 0.999687] 
[Epoch 99/225] [Batch 200/223] [D loss: 0.328494] [G loss: 0.197110] [ema: 0.999689] 
[Epoch 100/225] [Batch 0/223] [D loss: 0.355357] [G loss: 0.228332] [ema: 0.999689] 
[Epoch 100/225] [Batch 100/223] [D loss: 0.328714] [G loss: 0.186349] [ema: 0.999691] 
[Epoch 100/225] [Batch 200/223] [D loss: 0.411987] [G loss: 0.191127] [ema: 0.999692] 
[Epoch 101/225] [Batch 0/223] [D loss: 0.374665] [G loss: 0.204289] [ema: 0.999692] 
[Epoch 101/225] [Batch 100/223] [D loss: 0.313467] [G loss: 0.216934] [ema: 0.999694] 
[Epoch 101/225] [Batch 200/223] [D loss: 0.370199] [G loss: 0.196982] [ema: 0.999695] 
[Epoch 102/225] [Batch 0/223] [D loss: 0.404158] [G loss: 0.193248] [ema: 0.999695] 
[Epoch 102/225] [Batch 100/223] [D loss: 0.333733] [G loss: 0.218710] [ema: 0.999697] 
[Epoch 102/225] [Batch 200/223] [D loss: 0.362942] [G loss: 0.182986] [ema: 0.999698] 
[Epoch 103/225] [Batch 0/223] [D loss: 0.319591] [G loss: 0.220126] [ema: 0.999698] 
[Epoch 103/225] [Batch 100/223] [D loss: 0.366435] [G loss: 0.212178] [ema: 0.999700] 
[Epoch 103/225] [Batch 200/223] [D loss: 0.368068] [G loss: 0.224446] [ema: 0.999701] 
[Epoch 104/225] [Batch 0/223] [D loss: 0.363011] [G loss: 0.235392] [ema: 0.999701] 
[Epoch 104/225] [Batch 100/223] [D loss: 0.408455] [G loss: 0.194709] [ema: 0.999702] 
[Epoch 104/225] [Batch 200/223] [D loss: 0.322504] [G loss: 0.209421] [ema: 0.999704] 
[Epoch 105/225] [Batch 0/223] [D loss: 0.375156] [G loss: 0.208731] [ema: 0.999704] 
[Epoch 105/225] [Batch 100/223] [D loss: 0.323616] [G loss: 0.221398] [ema: 0.999705] 
[Epoch 105/225] [Batch 200/223] [D loss: 0.330748] [G loss: 0.199048] [ema: 0.999707] 
[Epoch 106/225] [Batch 0/223] [D loss: 0.332406] [G loss: 0.188351] [ema: 0.999707] 
[Epoch 106/225] [Batch 100/223] [D loss: 0.308334] [G loss: 0.174529] [ema: 0.999708] 
[Epoch 106/225] [Batch 200/223] [D loss: 0.341372] [G loss: 0.215219] [ema: 0.999709] 
[Epoch 107/225] [Batch 0/223] [D loss: 0.389568] [G loss: 0.206543] [ema: 0.999710] 
[Epoch 107/225] [Batch 100/223] [D loss: 0.348423] [G loss: 0.222701] [ema: 0.999711] 
[Epoch 107/225] [Batch 200/223] [D loss: 0.357326] [G loss: 0.170293] [ema: 0.999712] 
[Epoch 108/225] [Batch 0/223] [D loss: 0.367390] [G loss: 0.208796] [ema: 0.999712] 
[Epoch 108/225] [Batch 100/223] [D loss: 0.317432] [G loss: 0.197540] [ema: 0.999713] 
[Epoch 108/225] [Batch 200/223] [D loss: 0.356420] [G loss: 0.199915] [ema: 0.999715] 
[Epoch 109/225] [Batch 0/223] [D loss: 0.295872] [G loss: 0.226657] [ema: 0.999715] 
[Epoch 109/225] [Batch 100/223] [D loss: 0.324846] [G loss: 0.197009] [ema: 0.999716] 
[Epoch 109/225] [Batch 200/223] [D loss: 0.338851] [G loss: 0.207141] [ema: 0.999717] 
[Epoch 110/225] [Batch 0/223] [D loss: 0.349768] [G loss: 0.224155] [ema: 0.999717] 
[Epoch 110/225] [Batch 100/223] [D loss: 0.337291] [G loss: 0.210560] [ema: 0.999719] 
[Epoch 110/225] [Batch 200/223] [D loss: 0.394509] [G loss: 0.189512] [ema: 0.999720] 
[Epoch 111/225] [Batch 0/223] [D loss: 0.364106] [G loss: 0.215971] [ema: 0.999720] 
[Epoch 111/225] [Batch 100/223] [D loss: 0.394147] [G loss: 0.219614] [ema: 0.999721] 
[Epoch 111/225] [Batch 200/223] [D loss: 0.375156] [G loss: 0.219333] [ema: 0.999722] 
[Epoch 112/225] [Batch 0/223] [D loss: 0.325528] [G loss: 0.242522] [ema: 0.999723] 
[Epoch 112/225] [Batch 100/223] [D loss: 0.334123] [G loss: 0.213430] [ema: 0.999724] 
[Epoch 112/225] [Batch 200/223] [D loss: 0.319505] [G loss: 0.217046] [ema: 0.999725] 
[Epoch 113/225] [Batch 0/223] [D loss: 0.395840] [G loss: 0.203186] [ema: 0.999725] 
[Epoch 113/225] [Batch 100/223] [D loss: 0.308726] [G loss: 0.190368] [ema: 0.999726] 
[Epoch 113/225] [Batch 200/223] [D loss: 0.372537] [G loss: 0.178319] [ema: 0.999727] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_60_100/MotionSense_DAGHAR_Multiclass_50000_D_60_2024_10_25_14_09_47/Model



[Epoch 114/225] [Batch 0/223] [D loss: 0.354919] [G loss: 0.227750] [ema: 0.999727] 
[Epoch 114/225] [Batch 100/223] [D loss: 0.325951] [G loss: 0.205093] [ema: 0.999728] 
[Epoch 114/225] [Batch 200/223] [D loss: 0.388730] [G loss: 0.203522] [ema: 0.999730] 
[Epoch 115/225] [Batch 0/223] [D loss: 0.354724] [G loss: 0.178486] [ema: 0.999730] 
[Epoch 115/225] [Batch 100/223] [D loss: 0.328802] [G loss: 0.208026] [ema: 0.999731] 
[Epoch 115/225] [Batch 200/223] [D loss: 0.295760] [G loss: 0.225804] [ema: 0.999732] 
[Epoch 116/225] [Batch 0/223] [D loss: 0.334991] [G loss: 0.231151] [ema: 0.999732] 
[Epoch 116/225] [Batch 100/223] [D loss: 0.397616] [G loss: 0.223140] [ema: 0.999733] 
[Epoch 116/225] [Batch 200/223] [D loss: 0.338431] [G loss: 0.197807] [ema: 0.999734] 
[Epoch 117/225] [Batch 0/223] [D loss: 0.300132] [G loss: 0.240407] [ema: 0.999734] 
[Epoch 117/225] [Batch 100/223] [D loss: 0.363469] [G loss: 0.178677] [ema: 0.999735] 
[Epoch 117/225] [Batch 200/223] [D loss: 0.350333] [G loss: 0.211841] [ema: 0.999736] 
[Epoch 118/225] [Batch 0/223] [D loss: 0.331902] [G loss: 0.215492] [ema: 0.999737] 
[Epoch 118/225] [Batch 100/223] [D loss: 0.343480] [G loss: 0.186261] [ema: 0.999738] 
[Epoch 118/225] [Batch 200/223] [D loss: 0.394025] [G loss: 0.191531] [ema: 0.999739] 
[Epoch 119/225] [Batch 0/223] [D loss: 0.309320] [G loss: 0.203984] [ema: 0.999739] 
[Epoch 119/225] [Batch 100/223] [D loss: 0.333952] [G loss: 0.204169] [ema: 0.999740] 
[Epoch 119/225] [Batch 200/223] [D loss: 0.380797] [G loss: 0.201358] [ema: 0.999741] 
[Epoch 120/225] [Batch 0/223] [D loss: 0.377174] [G loss: 0.193299] [ema: 0.999741] 
[Epoch 120/225] [Batch 100/223] [D loss: 0.301680] [G loss: 0.191267] [ema: 0.999742] 
[Epoch 120/225] [Batch 200/223] [D loss: 0.288253] [G loss: 0.205910] [ema: 0.999743] 
[Epoch 121/225] [Batch 0/223] [D loss: 0.319418] [G loss: 0.224441] [ema: 0.999743] 
[Epoch 121/225] [Batch 100/223] [D loss: 0.316677] [G loss: 0.213897] [ema: 0.999744] 
[Epoch 121/225] [Batch 200/223] [D loss: 0.342109] [G loss: 0.194560] [ema: 0.999745] 
[Epoch 122/225] [Batch 0/223] [D loss: 0.342327] [G loss: 0.238225] [ema: 0.999745] 
[Epoch 122/225] [Batch 100/223] [D loss: 0.328157] [G loss: 0.194876] [ema: 0.999746] 
[Epoch 122/225] [Batch 200/223] [D loss: 0.340727] [G loss: 0.220070] [ema: 0.999747] 
[Epoch 123/225] [Batch 0/223] [D loss: 0.384382] [G loss: 0.192680] [ema: 0.999747] 
[Epoch 123/225] [Batch 100/223] [D loss: 0.386242] [G loss: 0.231530] [ema: 0.999748] 
[Epoch 123/225] [Batch 200/223] [D loss: 0.334165] [G loss: 0.212577] [ema: 0.999749] 
[Epoch 124/225] [Batch 0/223] [D loss: 0.321072] [G loss: 0.201686] [ema: 0.999749] 
[Epoch 124/225] [Batch 100/223] [D loss: 0.305958] [G loss: 0.214521] [ema: 0.999750] 
[Epoch 124/225] [Batch 200/223] [D loss: 0.348220] [G loss: 0.215807] [ema: 0.999751] 
[Epoch 125/225] [Batch 0/223] [D loss: 0.352914] [G loss: 0.205200] [ema: 0.999751] 
[Epoch 125/225] [Batch 100/223] [D loss: 0.366614] [G loss: 0.198823] [ema: 0.999752] 
[Epoch 125/225] [Batch 200/223] [D loss: 0.326695] [G loss: 0.241036] [ema: 0.999753] 
[Epoch 126/225] [Batch 0/223] [D loss: 0.299175] [G loss: 0.219417] [ema: 0.999753] 
[Epoch 126/225] [Batch 100/223] [D loss: 0.323815] [G loss: 0.206456] [ema: 0.999754] 
[Epoch 126/225] [Batch 200/223] [D loss: 0.325393] [G loss: 0.210446] [ema: 0.999755] 
[Epoch 127/225] [Batch 0/223] [D loss: 0.327444] [G loss: 0.201085] [ema: 0.999755] 
[Epoch 127/225] [Batch 100/223] [D loss: 0.314837] [G loss: 0.180468] [ema: 0.999756] 
[Epoch 127/225] [Batch 200/223] [D loss: 0.350822] [G loss: 0.221099] [ema: 0.999757] 
[Epoch 128/225] [Batch 0/223] [D loss: 0.322938] [G loss: 0.224429] [ema: 0.999757] 
[Epoch 128/225] [Batch 100/223] [D loss: 0.383607] [G loss: 0.218999] [ema: 0.999758] 
[Epoch 128/225] [Batch 200/223] [D loss: 0.348822] [G loss: 0.203598] [ema: 0.999759] 
[Epoch 129/225] [Batch 0/223] [D loss: 0.363164] [G loss: 0.202873] [ema: 0.999759] 
[Epoch 129/225] [Batch 100/223] [D loss: 0.288103] [G loss: 0.202022] [ema: 0.999760] 
[Epoch 129/225] [Batch 200/223] [D loss: 0.304725] [G loss: 0.180246] [ema: 0.999761] 
[Epoch 130/225] [Batch 0/223] [D loss: 0.316192] [G loss: 0.203642] [ema: 0.999761] 
[Epoch 130/225] [Batch 100/223] [D loss: 0.315410] [G loss: 0.180681] [ema: 0.999762] 
[Epoch 130/225] [Batch 200/223] [D loss: 0.356535] [G loss: 0.198653] [ema: 0.999763] 
[Epoch 131/225] [Batch 0/223] [D loss: 0.303942] [G loss: 0.209846] [ema: 0.999763] 
[Epoch 131/225] [Batch 100/223] [D loss: 0.309465] [G loss: 0.241487] [ema: 0.999764] 
[Epoch 131/225] [Batch 200/223] [D loss: 0.340471] [G loss: 0.190711] [ema: 0.999764] 
[Epoch 132/225] [Batch 0/223] [D loss: 0.351692] [G loss: 0.229209] [ema: 0.999765] 
[Epoch 132/225] [Batch 100/223] [D loss: 0.397925] [G loss: 0.195706] [ema: 0.999765] 
[Epoch 132/225] [Batch 200/223] [D loss: 0.316626] [G loss: 0.212501] [ema: 0.999766] 
[Epoch 133/225] [Batch 0/223] [D loss: 0.345348] [G loss: 0.212787] [ema: 0.999766] 
[Epoch 133/225] [Batch 100/223] [D loss: 0.352526] [G loss: 0.231179] [ema: 0.999767] 
[Epoch 133/225] [Batch 200/223] [D loss: 0.308722] [G loss: 0.231544] [ema: 0.999768] 
[Epoch 134/225] [Batch 0/223] [D loss: 0.361438] [G loss: 0.198851] [ema: 0.999768] 
[Epoch 134/225] [Batch 100/223] [D loss: 0.354700] [G loss: 0.176162] [ema: 0.999769] 
[Epoch 134/225] [Batch 200/223] [D loss: 0.309345] [G loss: 0.201128] [ema: 0.999770] 
[Epoch 135/225] [Batch 0/223] [D loss: 0.320773] [G loss: 0.232279] [ema: 0.999770] 
[Epoch 135/225] [Batch 100/223] [D loss: 0.347125] [G loss: 0.212688] [ema: 0.999771] 
[Epoch 135/225] [Batch 200/223] [D loss: 0.329272] [G loss: 0.203548] [ema: 0.999771] 
[Epoch 136/225] [Batch 0/223] [D loss: 0.364516] [G loss: 0.227960] [ema: 0.999771] 
[Epoch 136/225] [Batch 100/223] [D loss: 0.351387] [G loss: 0.209630] [ema: 0.999772] 
[Epoch 136/225] [Batch 200/223] [D loss: 0.359147] [G loss: 0.236374] [ema: 0.999773] 
[Epoch 137/225] [Batch 0/223] [D loss: 0.360074] [G loss: 0.210503] [ema: 0.999773] 
[Epoch 137/225] [Batch 100/223] [D loss: 0.320341] [G loss: 0.211064] [ema: 0.999774] 
[Epoch 137/225] [Batch 200/223] [D loss: 0.312912] [G loss: 0.195980] [ema: 0.999775] 
[Epoch 138/225] [Batch 0/223] [D loss: 0.362531] [G loss: 0.229635] [ema: 0.999775] 
[Epoch 138/225] [Batch 100/223] [D loss: 0.361252] [G loss: 0.237856] [ema: 0.999776] 
[Epoch 138/225] [Batch 200/223] [D loss: 0.324138] [G loss: 0.205381] [ema: 0.999776] 
[Epoch 139/225] [Batch 0/223] [D loss: 0.346401] [G loss: 0.203122] [ema: 0.999776] 
[Epoch 139/225] [Batch 100/223] [D loss: 0.341463] [G loss: 0.213247] [ema: 0.999777] 
[Epoch 139/225] [Batch 200/223] [D loss: 0.303915] [G loss: 0.222610] [ema: 0.999778] 
[Epoch 140/225] [Batch 0/223] [D loss: 0.349461] [G loss: 0.204469] [ema: 0.999778] 
[Epoch 140/225] [Batch 100/223] [D loss: 0.291855] [G loss: 0.215803] [ema: 0.999779] 
[Epoch 140/225] [Batch 200/223] [D loss: 0.338319] [G loss: 0.229133] [ema: 0.999779] 
[Epoch 141/225] [Batch 0/223] [D loss: 0.323656] [G loss: 0.217134] [ema: 0.999780] 
[Epoch 141/225] [Batch 100/223] [D loss: 0.328914] [G loss: 0.198076] [ema: 0.999780] 
[Epoch 141/225] [Batch 200/223] [D loss: 0.428230] [G loss: 0.179152] [ema: 0.999781] 
[Epoch 142/225] [Batch 0/223] [D loss: 0.363014] [G loss: 0.192707] [ema: 0.999781] 
[Epoch 142/225] [Batch 100/223] [D loss: 0.370976] [G loss: 0.188542] [ema: 0.999782] 
[Epoch 142/225] [Batch 200/223] [D loss: 0.339864] [G loss: 0.190804] [ema: 0.999783] 
[Epoch 143/225] [Batch 0/223] [D loss: 0.348791] [G loss: 0.212987] [ema: 0.999783] 
[Epoch 143/225] [Batch 100/223] [D loss: 0.344846] [G loss: 0.214370] [ema: 0.999783] 
[Epoch 143/225] [Batch 200/223] [D loss: 0.366772] [G loss: 0.203445] [ema: 0.999784] 
[Epoch 144/225] [Batch 0/223] [D loss: 0.374537] [G loss: 0.209473] [ema: 0.999784] 
[Epoch 144/225] [Batch 100/223] [D loss: 0.329827] [G loss: 0.217690] [ema: 0.999785] 
[Epoch 144/225] [Batch 200/223] [D loss: 0.326219] [G loss: 0.223951] [ema: 0.999786] 
[Epoch 145/225] [Batch 0/223] [D loss: 0.325234] [G loss: 0.216760] [ema: 0.999786] 
[Epoch 145/225] [Batch 100/223] [D loss: 0.367993] [G loss: 0.201482] [ema: 0.999786] 
[Epoch 145/225] [Batch 200/223] [D loss: 0.309812] [G loss: 0.206868] [ema: 0.999787] 
[Epoch 146/225] [Batch 0/223] [D loss: 0.348508] [G loss: 0.221206] [ema: 0.999787] 
[Epoch 146/225] [Batch 100/223] [D loss: 0.374001] [G loss: 0.183257] [ema: 0.999788] 
[Epoch 146/225] [Batch 200/223] [D loss: 0.324534] [G loss: 0.199966] [ema: 0.999788] 
[Epoch 147/225] [Batch 0/223] [D loss: 0.311247] [G loss: 0.232177] [ema: 0.999789] 
[Epoch 147/225] [Batch 100/223] [D loss: 0.322291] [G loss: 0.212611] [ema: 0.999789] 
[Epoch 147/225] [Batch 200/223] [D loss: 0.304059] [G loss: 0.194229] [ema: 0.999790] 
[Epoch 148/225] [Batch 0/223] [D loss: 0.358690] [G loss: 0.178905] [ema: 0.999790] 
[Epoch 148/225] [Batch 100/223] [D loss: 0.354685] [G loss: 0.207406] [ema: 0.999791] 
[Epoch 148/225] [Batch 200/223] [D loss: 0.351991] [G loss: 0.203295] [ema: 0.999791] 
[Epoch 149/225] [Batch 0/223] [D loss: 0.354460] [G loss: 0.231433] [ema: 0.999791] 
[Epoch 149/225] [Batch 100/223] [D loss: 0.347897] [G loss: 0.220808] [ema: 0.999792] 
[Epoch 149/225] [Batch 200/223] [D loss: 0.404383] [G loss: 0.178948] [ema: 0.999793] 
[Epoch 150/225] [Batch 0/223] [D loss: 0.355396] [G loss: 0.209849] [ema: 0.999793] 
[Epoch 150/225] [Batch 100/223] [D loss: 0.290063] [G loss: 0.206716] [ema: 0.999793] 
[Epoch 150/225] [Batch 200/223] [D loss: 0.337739] [G loss: 0.211537] [ema: 0.999794] 
[Epoch 151/225] [Batch 0/223] [D loss: 0.362359] [G loss: 0.210741] [ema: 0.999794] 
[Epoch 151/225] [Batch 100/223] [D loss: 0.311528] [G loss: 0.216096] [ema: 0.999795] 
[Epoch 151/225] [Batch 200/223] [D loss: 0.305832] [G loss: 0.230708] [ema: 0.999795] 
[Epoch 152/225] [Batch 0/223] [D loss: 0.351118] [G loss: 0.194834] [ema: 0.999796] 
[Epoch 152/225] [Batch 100/223] [D loss: 0.340774] [G loss: 0.200480] [ema: 0.999796] 
[Epoch 152/225] [Batch 200/223] [D loss: 0.366880] [G loss: 0.217522] [ema: 0.999797] 
[Epoch 153/225] [Batch 0/223] [D loss: 0.326880] [G loss: 0.199527] [ema: 0.999797] 
[Epoch 153/225] [Batch 100/223] [D loss: 0.359503] [G loss: 0.213864] [ema: 0.999797] 
[Epoch 153/225] [Batch 200/223] [D loss: 0.323020] [G loss: 0.217424] [ema: 0.999798] 
[Epoch 154/225] [Batch 0/223] [D loss: 0.315968] [G loss: 0.211813] [ema: 0.999798] 
[Epoch 154/225] [Batch 100/223] [D loss: 0.306049] [G loss: 0.179631] [ema: 0.999799] 
[Epoch 154/225] [Batch 200/223] [D loss: 0.326146] [G loss: 0.209360] [ema: 0.999799] 
[Epoch 155/225] [Batch 0/223] [D loss: 0.315729] [G loss: 0.210075] [ema: 0.999799] 
[Epoch 155/225] [Batch 100/223] [D loss: 0.397669] [G loss: 0.213774] [ema: 0.999800] 
[Epoch 155/225] [Batch 200/223] [D loss: 0.320308] [G loss: 0.222191] [ema: 0.999801] 
[Epoch 156/225] [Batch 0/223] [D loss: 0.326313] [G loss: 0.188803] [ema: 0.999801] 
[Epoch 156/225] [Batch 100/223] [D loss: 0.345965] [G loss: 0.190226] [ema: 0.999801] 
[Epoch 156/225] [Batch 200/223] [D loss: 0.343114] [G loss: 0.197549] [ema: 0.999802] 
[Epoch 157/225] [Batch 0/223] [D loss: 0.352426] [G loss: 0.223351] [ema: 0.999802] 
[Epoch 157/225] [Batch 100/223] [D loss: 0.309035] [G loss: 0.211717] [ema: 0.999803] 
[Epoch 157/225] [Batch 200/223] [D loss: 0.334123] [G loss: 0.198575] [ema: 0.999803] 
[Epoch 158/225] [Batch 0/223] [D loss: 0.326159] [G loss: 0.224057] [ema: 0.999803] 
[Epoch 158/225] [Batch 100/223] [D loss: 0.307322] [G loss: 0.233278] [ema: 0.999804] 
[Epoch 158/225] [Batch 200/223] [D loss: 0.312676] [G loss: 0.202292] [ema: 0.999804] 
[Epoch 159/225] [Batch 0/223] [D loss: 0.307836] [G loss: 0.218590] [ema: 0.999805] 
[Epoch 159/225] [Batch 100/223] [D loss: 0.364890] [G loss: 0.226244] [ema: 0.999805] 
[Epoch 159/225] [Batch 200/223] [D loss: 0.315604] [G loss: 0.200169] [ema: 0.999806] 
[Epoch 160/225] [Batch 0/223] [D loss: 0.342844] [G loss: 0.202916] [ema: 0.999806] 
[Epoch 160/225] [Batch 100/223] [D loss: 0.352005] [G loss: 0.231998] [ema: 0.999806] 
[Epoch 160/225] [Batch 200/223] [D loss: 0.343755] [G loss: 0.220342] [ema: 0.999807] 
[Epoch 161/225] [Batch 0/223] [D loss: 0.376360] [G loss: 0.206125] [ema: 0.999807] 
[Epoch 161/225] [Batch 100/223] [D loss: 0.354651] [G loss: 0.219045] [ema: 0.999807] 
[Epoch 161/225] [Batch 200/223] [D loss: 0.359716] [G loss: 0.200792] [ema: 0.999808] 
[Epoch 162/225] [Batch 0/223] [D loss: 0.327420] [G loss: 0.225443] [ema: 0.999808] 
[Epoch 162/225] [Batch 100/223] [D loss: 0.335574] [G loss: 0.222231] [ema: 0.999809] 
[Epoch 162/225] [Batch 200/223] [D loss: 0.353761] [G loss: 0.201463] [ema: 0.999809] 
[Epoch 163/225] [Batch 0/223] [D loss: 0.359633] [G loss: 0.208055] [ema: 0.999809] 
[Epoch 163/225] [Batch 100/223] [D loss: 0.342632] [G loss: 0.172512] [ema: 0.999810] 
[Epoch 163/225] [Batch 200/223] [D loss: 0.328413] [G loss: 0.225074] [ema: 0.999810] 
[Epoch 164/225] [Batch 0/223] [D loss: 0.379214] [G loss: 0.212072] [ema: 0.999810] 
[Epoch 164/225] [Batch 100/223] [D loss: 0.354996] [G loss: 0.208018] [ema: 0.999811] 
[Epoch 164/225] [Batch 200/223] [D loss: 0.301480] [G loss: 0.208875] [ema: 0.999812] 
[Epoch 165/225] [Batch 0/223] [D loss: 0.316727] [G loss: 0.216197] [ema: 0.999812] 
[Epoch 165/225] [Batch 100/223] [D loss: 0.424301] [G loss: 0.216894] [ema: 0.999812] 
[Epoch 165/225] [Batch 200/223] [D loss: 0.364323] [G loss: 0.232215] [ema: 0.999813] 
[Epoch 166/225] [Batch 0/223] [D loss: 0.355245] [G loss: 0.205084] [ema: 0.999813] 
[Epoch 166/225] [Batch 100/223] [D loss: 0.292035] [G loss: 0.233242] [ema: 0.999813] 
[Epoch 166/225] [Batch 200/223] [D loss: 0.304650] [G loss: 0.222221] [ema: 0.999814] 
[Epoch 167/225] [Batch 0/223] [D loss: 0.329961] [G loss: 0.221418] [ema: 0.999814] 
[Epoch 167/225] [Batch 100/223] [D loss: 0.320265] [G loss: 0.207013] [ema: 0.999814] 
[Epoch 167/225] [Batch 200/223] [D loss: 0.343942] [G loss: 0.211056] [ema: 0.999815] 
[Epoch 168/225] [Batch 0/223] [D loss: 0.307524] [G loss: 0.241159] [ema: 0.999815] 
[Epoch 168/225] [Batch 100/223] [D loss: 0.339828] [G loss: 0.225364] [ema: 0.999815] 
[Epoch 168/225] [Batch 200/223] [D loss: 0.332592] [G loss: 0.211139] [ema: 0.999816] 
[Epoch 169/225] [Batch 0/223] [D loss: 0.285741] [G loss: 0.237779] [ema: 0.999816] 
[Epoch 169/225] [Batch 100/223] [D loss: 0.340816] [G loss: 0.227377] [ema: 0.999817] 
[Epoch 169/225] [Batch 200/223] [D loss: 0.330669] [G loss: 0.206040] [ema: 0.999817] 
[Epoch 170/225] [Batch 0/223] [D loss: 0.319035] [G loss: 0.218563] [ema: 0.999817] 
[Epoch 170/225] [Batch 100/223] [D loss: 0.340336] [G loss: 0.217835] [ema: 0.999818] 
[Epoch 170/225] [Batch 200/223] [D loss: 0.319334] [G loss: 0.215491] [ema: 0.999818] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_60_100/MotionSense_DAGHAR_Multiclass_50000_D_60_2024_10_25_14_09_47/Model



[Epoch 171/225] [Batch 0/223] [D loss: 0.348146] [G loss: 0.235468] [ema: 0.999818] 
[Epoch 171/225] [Batch 100/223] [D loss: 0.346997] [G loss: 0.223116] [ema: 0.999819] 
[Epoch 171/225] [Batch 200/223] [D loss: 0.381183] [G loss: 0.219718] [ema: 0.999819] 
[Epoch 172/225] [Batch 0/223] [D loss: 0.302019] [G loss: 0.203131] [ema: 0.999819] 
[Epoch 172/225] [Batch 100/223] [D loss: 0.327229] [G loss: 0.208571] [ema: 0.999820] 
[Epoch 172/225] [Batch 200/223] [D loss: 0.300299] [G loss: 0.212900] [ema: 0.999820] 
[Epoch 173/225] [Batch 0/223] [D loss: 0.325973] [G loss: 0.222712] [ema: 0.999820] 
[Epoch 173/225] [Batch 100/223] [D loss: 0.315580] [G loss: 0.208037] [ema: 0.999821] 
[Epoch 173/225] [Batch 200/223] [D loss: 0.304365] [G loss: 0.237732] [ema: 0.999821] 
[Epoch 174/225] [Batch 0/223] [D loss: 0.336599] [G loss: 0.246390] [ema: 0.999821] 
[Epoch 174/225] [Batch 100/223] [D loss: 0.322717] [G loss: 0.222772] [ema: 0.999822] 
[Epoch 174/225] [Batch 200/223] [D loss: 0.309856] [G loss: 0.223931] [ema: 0.999822] 
[Epoch 175/225] [Batch 0/223] [D loss: 0.325587] [G loss: 0.219588] [ema: 0.999822] 
[Epoch 175/225] [Batch 100/223] [D loss: 0.311215] [G loss: 0.229458] [ema: 0.999823] 
[Epoch 175/225] [Batch 200/223] [D loss: 0.361816] [G loss: 0.222459] [ema: 0.999823] 
[Epoch 176/225] [Batch 0/223] [D loss: 0.317497] [G loss: 0.223052] [ema: 0.999823] 
[Epoch 176/225] [Batch 100/223] [D loss: 0.319162] [G loss: 0.197414] [ema: 0.999824] 
[Epoch 176/225] [Batch 200/223] [D loss: 0.356762] [G loss: 0.199030] [ema: 0.999824] 
[Epoch 177/225] [Batch 0/223] [D loss: 0.339649] [G loss: 0.232047] [ema: 0.999824] 
[Epoch 177/225] [Batch 100/223] [D loss: 0.355697] [G loss: 0.233830] [ema: 0.999825] 
[Epoch 177/225] [Batch 200/223] [D loss: 0.398444] [G loss: 0.204754] [ema: 0.999825] 
[Epoch 178/225] [Batch 0/223] [D loss: 0.374104] [G loss: 0.246228] [ema: 0.999825] 
[Epoch 178/225] [Batch 100/223] [D loss: 0.348043] [G loss: 0.197373] [ema: 0.999826] 
[Epoch 178/225] [Batch 200/223] [D loss: 0.347346] [G loss: 0.252284] [ema: 0.999826] 
[Epoch 179/225] [Batch 0/223] [D loss: 0.311372] [G loss: 0.203792] [ema: 0.999826] 
[Epoch 179/225] [Batch 100/223] [D loss: 0.273676] [G loss: 0.222334] [ema: 0.999827] 
[Epoch 179/225] [Batch 200/223] [D loss: 0.358009] [G loss: 0.214474] [ema: 0.999827] 
[Epoch 180/225] [Batch 0/223] [D loss: 0.339431] [G loss: 0.202899] [ema: 0.999827] 
[Epoch 180/225] [Batch 100/223] [D loss: 0.330289] [G loss: 0.210941] [ema: 0.999828] 
[Epoch 180/225] [Batch 200/223] [D loss: 0.360674] [G loss: 0.208320] [ema: 0.999828] 
[Epoch 181/225] [Batch 0/223] [D loss: 0.332395] [G loss: 0.227806] [ema: 0.999828] 
[Epoch 181/225] [Batch 100/223] [D loss: 0.339965] [G loss: 0.217185] [ema: 0.999829] 
[Epoch 181/225] [Batch 200/223] [D loss: 0.348355] [G loss: 0.209500] [ema: 0.999829] 
[Epoch 182/225] [Batch 0/223] [D loss: 0.318779] [G loss: 0.204046] [ema: 0.999829] 
[Epoch 182/225] [Batch 100/223] [D loss: 0.331518] [G loss: 0.225085] [ema: 0.999830] 
[Epoch 182/225] [Batch 200/223] [D loss: 0.313167] [G loss: 0.233266] [ema: 0.999830] 
[Epoch 183/225] [Batch 0/223] [D loss: 0.394881] [G loss: 0.211099] [ema: 0.999830] 
[Epoch 183/225] [Batch 100/223] [D loss: 0.401758] [G loss: 0.208920] [ema: 0.999831] 
[Epoch 183/225] [Batch 200/223] [D loss: 0.296494] [G loss: 0.225910] [ema: 0.999831] 
[Epoch 184/225] [Batch 0/223] [D loss: 0.409201] [G loss: 0.207342] [ema: 0.999831] 
[Epoch 184/225] [Batch 100/223] [D loss: 0.321466] [G loss: 0.222764] [ema: 0.999831] 
[Epoch 184/225] [Batch 200/223] [D loss: 0.269964] [G loss: 0.229329] [ema: 0.999832] 
[Epoch 185/225] [Batch 0/223] [D loss: 0.372868] [G loss: 0.192144] [ema: 0.999832] 
[Epoch 185/225] [Batch 100/223] [D loss: 0.291781] [G loss: 0.241460] [ema: 0.999832] 
[Epoch 185/225] [Batch 200/223] [D loss: 0.343386] [G loss: 0.184785] [ema: 0.999833] 
[Epoch 186/225] [Batch 0/223] [D loss: 0.322450] [G loss: 0.221832] [ema: 0.999833] 
[Epoch 186/225] [Batch 100/223] [D loss: 0.423904] [G loss: 0.213509] [ema: 0.999833] 
[Epoch 186/225] [Batch 200/223] [D loss: 0.353622] [G loss: 0.227863] [ema: 0.999834] 
[Epoch 187/225] [Batch 0/223] [D loss: 0.322178] [G loss: 0.237001] [ema: 0.999834] 
[Epoch 187/225] [Batch 100/223] [D loss: 0.282980] [G loss: 0.232680] [ema: 0.999834] 
[Epoch 187/225] [Batch 200/223] [D loss: 0.312769] [G loss: 0.197015] [ema: 0.999835] 
[Epoch 188/225] [Batch 0/223] [D loss: 0.343081] [G loss: 0.174669] [ema: 0.999835] 
[Epoch 188/225] [Batch 100/223] [D loss: 0.373297] [G loss: 0.187941] [ema: 0.999835] 
[Epoch 188/225] [Batch 200/223] [D loss: 0.324789] [G loss: 0.218457] [ema: 0.999835] 
[Epoch 189/225] [Batch 0/223] [D loss: 0.358258] [G loss: 0.222907] [ema: 0.999836] 
[Epoch 189/225] [Batch 100/223] [D loss: 0.326022] [G loss: 0.216944] [ema: 0.999836] 
[Epoch 189/225] [Batch 200/223] [D loss: 0.335485] [G loss: 0.228156] [ema: 0.999836] 
[Epoch 190/225] [Batch 0/223] [D loss: 0.307655] [G loss: 0.232617] [ema: 0.999836] 
[Epoch 190/225] [Batch 100/223] [D loss: 0.302550] [G loss: 0.197145] [ema: 0.999837] 
[Epoch 190/225] [Batch 200/223] [D loss: 0.353755] [G loss: 0.213398] [ema: 0.999837] 
[Epoch 191/225] [Batch 0/223] [D loss: 0.392510] [G loss: 0.219030] [ema: 0.999837] 
[Epoch 191/225] [Batch 100/223] [D loss: 0.344117] [G loss: 0.227522] [ema: 0.999838] 
[Epoch 191/225] [Batch 200/223] [D loss: 0.306388] [G loss: 0.197579] [ema: 0.999838] 
[Epoch 192/225] [Batch 0/223] [D loss: 0.423872] [G loss: 0.206278] [ema: 0.999838] 
[Epoch 192/225] [Batch 100/223] [D loss: 0.396206] [G loss: 0.203643] [ema: 0.999839] 
[Epoch 192/225] [Batch 200/223] [D loss: 0.339908] [G loss: 0.212541] [ema: 0.999839] 
[Epoch 193/225] [Batch 0/223] [D loss: 0.338272] [G loss: 0.224730] [ema: 0.999839] 
[Epoch 193/225] [Batch 100/223] [D loss: 0.315779] [G loss: 0.202799] [ema: 0.999839] 
[Epoch 193/225] [Batch 200/223] [D loss: 0.316158] [G loss: 0.199882] [ema: 0.999840] 
[Epoch 194/225] [Batch 0/223] [D loss: 0.320022] [G loss: 0.212246] [ema: 0.999840] 
[Epoch 194/225] [Batch 100/223] [D loss: 0.343745] [G loss: 0.197025] [ema: 0.999840] 
[Epoch 194/225] [Batch 200/223] [D loss: 0.368790] [G loss: 0.190887] [ema: 0.999841] 
[Epoch 195/225] [Batch 0/223] [D loss: 0.347114] [G loss: 0.227801] [ema: 0.999841] 
[Epoch 195/225] [Batch 100/223] [D loss: 0.296760] [G loss: 0.230627] [ema: 0.999841] 
[Epoch 195/225] [Batch 200/223] [D loss: 0.352501] [G loss: 0.202694] [ema: 0.999841] 
[Epoch 196/225] [Batch 0/223] [D loss: 0.324447] [G loss: 0.205538] [ema: 0.999841] 
[Epoch 196/225] [Batch 100/223] [D loss: 0.415590] [G loss: 0.201155] [ema: 0.999842] 
[Epoch 196/225] [Batch 200/223] [D loss: 0.308725] [G loss: 0.233807] [ema: 0.999842] 
[Epoch 197/225] [Batch 0/223] [D loss: 0.347029] [G loss: 0.221567] [ema: 0.999842] 
[Epoch 197/225] [Batch 100/223] [D loss: 0.343120] [G loss: 0.222718] [ema: 0.999843] 
[Epoch 197/225] [Batch 200/223] [D loss: 0.374858] [G loss: 0.225003] [ema: 0.999843] 
[Epoch 198/225] [Batch 0/223] [D loss: 0.287929] [G loss: 0.222155] [ema: 0.999843] 
[Epoch 198/225] [Batch 100/223] [D loss: 0.359567] [G loss: 0.230695] [ema: 0.999843] 
[Epoch 198/225] [Batch 200/223] [D loss: 0.287420] [G loss: 0.230724] [ema: 0.999844] 
[Epoch 199/225] [Batch 0/223] [D loss: 0.387673] [G loss: 0.219928] [ema: 0.999844] 
[Epoch 199/225] [Batch 100/223] [D loss: 0.321344] [G loss: 0.243980] [ema: 0.999844] 
[Epoch 199/225] [Batch 200/223] [D loss: 0.314984] [G loss: 0.195184] [ema: 0.999845] 
[Epoch 200/225] [Batch 0/223] [D loss: 0.301568] [G loss: 0.217548] [ema: 0.999845] 
[Epoch 200/225] [Batch 100/223] [D loss: 0.321032] [G loss: 0.229306] [ema: 0.999845] 
[Epoch 200/225] [Batch 200/223] [D loss: 0.340416] [G loss: 0.217063] [ema: 0.999845] 
[Epoch 201/225] [Batch 0/223] [D loss: 0.325991] [G loss: 0.242165] [ema: 0.999845] 
[Epoch 201/225] [Batch 100/223] [D loss: 0.357368] [G loss: 0.209821] [ema: 0.999846] 
[Epoch 201/225] [Batch 200/223] [D loss: 0.360330] [G loss: 0.205405] [ema: 0.999846] 
[Epoch 202/225] [Batch 0/223] [D loss: 0.368117] [G loss: 0.228043] [ema: 0.999846] 
[Epoch 202/225] [Batch 100/223] [D loss: 0.368052] [G loss: 0.237900] [ema: 0.999846] 
[Epoch 202/225] [Batch 200/223] [D loss: 0.337598] [G loss: 0.222701] [ema: 0.999847] 
[Epoch 203/225] [Batch 0/223] [D loss: 0.310048] [G loss: 0.214645] [ema: 0.999847] 
[Epoch 203/225] [Batch 100/223] [D loss: 0.310604] [G loss: 0.232848] [ema: 0.999847] 
[Epoch 203/225] [Batch 200/223] [D loss: 0.290132] [G loss: 0.215404] [ema: 0.999848] 
[Epoch 204/225] [Batch 0/223] [D loss: 0.298196] [G loss: 0.219429] [ema: 0.999848] 
[Epoch 204/225] [Batch 100/223] [D loss: 0.320499] [G loss: 0.217728] [ema: 0.999848] 
[Epoch 204/225] [Batch 200/223] [D loss: 0.337263] [G loss: 0.227589] [ema: 0.999848] 
[Epoch 205/225] [Batch 0/223] [D loss: 0.362166] [G loss: 0.240391] [ema: 0.999848] 
[Epoch 205/225] [Batch 100/223] [D loss: 0.329194] [G loss: 0.223330] [ema: 0.999849] 
[Epoch 205/225] [Batch 200/223] [D loss: 0.365861] [G loss: 0.212092] [ema: 0.999849] 
[Epoch 206/225] [Batch 0/223] [D loss: 0.319917] [G loss: 0.217594] [ema: 0.999849] 
[Epoch 206/225] [Batch 100/223] [D loss: 0.359569] [G loss: 0.204609] [ema: 0.999849] 
[Epoch 206/225] [Batch 200/223] [D loss: 0.301842] [G loss: 0.212347] [ema: 0.999850] 
[Epoch 207/225] [Batch 0/223] [D loss: 0.331497] [G loss: 0.247014] [ema: 0.999850] 
[Epoch 207/225] [Batch 100/223] [D loss: 0.312422] [G loss: 0.241693] [ema: 0.999850] 
[Epoch 207/225] [Batch 200/223] [D loss: 0.330822] [G loss: 0.211482] [ema: 0.999851] 
[Epoch 208/225] [Batch 0/223] [D loss: 0.294061] [G loss: 0.243647] [ema: 0.999851] 
[Epoch 208/225] [Batch 100/223] [D loss: 0.358301] [G loss: 0.234865] [ema: 0.999851] 
[Epoch 208/225] [Batch 200/223] [D loss: 0.389117] [G loss: 0.220023] [ema: 0.999851] 
[Epoch 209/225] [Batch 0/223] [D loss: 0.337931] [G loss: 0.234349] [ema: 0.999851] 
[Epoch 209/225] [Batch 100/223] [D loss: 0.313335] [G loss: 0.223448] [ema: 0.999852] 
[Epoch 209/225] [Batch 200/223] [D loss: 0.324378] [G loss: 0.202604] [ema: 0.999852] 
[Epoch 210/225] [Batch 0/223] [D loss: 0.332412] [G loss: 0.210625] [ema: 0.999852] 
[Epoch 210/225] [Batch 100/223] [D loss: 0.370243] [G loss: 0.216227] [ema: 0.999852] 
[Epoch 210/225] [Batch 200/223] [D loss: 0.333254] [G loss: 0.201656] [ema: 0.999853] 
[Epoch 211/225] [Batch 0/223] [D loss: 0.302209] [G loss: 0.196179] [ema: 0.999853] 
[Epoch 211/225] [Batch 100/223] [D loss: 0.313642] [G loss: 0.201463] [ema: 0.999853] 
[Epoch 211/225] [Batch 200/223] [D loss: 0.354016] [G loss: 0.206503] [ema: 0.999853] 
[Epoch 212/225] [Batch 0/223] [D loss: 0.350575] [G loss: 0.214561] [ema: 0.999853] 
[Epoch 212/225] [Batch 100/223] [D loss: 0.309425] [G loss: 0.215570] [ema: 0.999854] 
[Epoch 212/225] [Batch 200/223] [D loss: 0.334705] [G loss: 0.231343] [ema: 0.999854] 
[Epoch 213/225] [Batch 0/223] [D loss: 0.301251] [G loss: 0.220727] [ema: 0.999854] 
[Epoch 213/225] [Batch 100/223] [D loss: 0.334137] [G loss: 0.222034] [ema: 0.999854] 
[Epoch 213/225] [Batch 200/223] [D loss: 0.341946] [G loss: 0.213449] [ema: 0.999855] 
[Epoch 214/225] [Batch 0/223] [D loss: 0.301623] [G loss: 0.210737] [ema: 0.999855] 
[Epoch 214/225] [Batch 100/223] [D loss: 0.363724] [G loss: 0.211596] [ema: 0.999855] 
[Epoch 214/225] [Batch 200/223] [D loss: 0.324203] [G loss: 0.214317] [ema: 0.999855] 
[Epoch 215/225] [Batch 0/223] [D loss: 0.379444] [G loss: 0.223256] [ema: 0.999855] 
[Epoch 215/225] [Batch 100/223] [D loss: 0.316761] [G loss: 0.180158] [ema: 0.999856] 
[Epoch 215/225] [Batch 200/223] [D loss: 0.335645] [G loss: 0.219986] [ema: 0.999856] 
[Epoch 216/225] [Batch 0/223] [D loss: 0.277698] [G loss: 0.229727] [ema: 0.999856] 
[Epoch 216/225] [Batch 100/223] [D loss: 0.348792] [G loss: 0.213966] [ema: 0.999856] 
[Epoch 216/225] [Batch 200/223] [D loss: 0.344048] [G loss: 0.229611] [ema: 0.999857] 
[Epoch 217/225] [Batch 0/223] [D loss: 0.328061] [G loss: 0.213399] [ema: 0.999857] 
[Epoch 217/225] [Batch 100/223] [D loss: 0.301954] [G loss: 0.203700] [ema: 0.999857] 
[Epoch 217/225] [Batch 200/223] [D loss: 0.301350] [G loss: 0.206649] [ema: 0.999857] 
[Epoch 218/225] [Batch 0/223] [D loss: 0.337756] [G loss: 0.206017] [ema: 0.999857] 
[Epoch 218/225] [Batch 100/223] [D loss: 0.382492] [G loss: 0.243509] [ema: 0.999858] 
[Epoch 218/225] [Batch 200/223] [D loss: 0.330217] [G loss: 0.217159] [ema: 0.999858] 
[Epoch 219/225] [Batch 0/223] [D loss: 0.327962] [G loss: 0.227189] [ema: 0.999858] 
[Epoch 219/225] [Batch 100/223] [D loss: 0.321183] [G loss: 0.219630] [ema: 0.999858] 
[Epoch 219/225] [Batch 200/223] [D loss: 0.306501] [G loss: 0.211190] [ema: 0.999859] 
[Epoch 220/225] [Batch 0/223] [D loss: 0.364048] [G loss: 0.204016] [ema: 0.999859] 
[Epoch 220/225] [Batch 100/223] [D loss: 0.332989] [G loss: 0.219728] [ema: 0.999859] 
[Epoch 220/225] [Batch 200/223] [D loss: 0.312520] [G loss: 0.228106] [ema: 0.999859] 
[Epoch 221/225] [Batch 0/223] [D loss: 0.353528] [G loss: 0.230794] [ema: 0.999859] 
[Epoch 221/225] [Batch 100/223] [D loss: 0.299530] [G loss: 0.229652] [ema: 0.999860] 
[Epoch 221/225] [Batch 200/223] [D loss: 0.327427] [G loss: 0.222147] [ema: 0.999860] 
[Epoch 222/225] [Batch 0/223] [D loss: 0.345404] [G loss: 0.205735] [ema: 0.999860] 
[Epoch 222/225] [Batch 100/223] [D loss: 0.331301] [G loss: 0.236758] [ema: 0.999860] 
[Epoch 222/225] [Batch 200/223] [D loss: 0.381550] [G loss: 0.220738] [ema: 0.999861] 
[Epoch 223/225] [Batch 0/223] [D loss: 0.331671] [G loss: 0.232437] [ema: 0.999861] 
[Epoch 223/225] [Batch 100/223] [D loss: 0.320763] [G loss: 0.215562] [ema: 0.999861] 
[Epoch 223/225] [Batch 200/223] [D loss: 0.329109] [G loss: 0.210220] [ema: 0.999861] 
[Epoch 224/225] [Batch 0/223] [D loss: 0.285470] [G loss: 0.215619] [ema: 0.999861] 
[Epoch 224/225] [Batch 100/223] [D loss: 0.307877] [G loss: 0.215320] [ema: 0.999862] 
[Epoch 224/225] [Batch 200/223] [D loss: 0.322314] [G loss: 0.229435] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
RealWorld_thigh_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
RealWorld_thigh_DAGHAR_Multiclass
daghar
return single class data and labels, class is RealWorld_thigh_DAGHAR_Multiclass
data shape is (10338, 3, 1, 60)
label shape is (10338,)
647
Epochs between checkpoint: 20



Saving checkpoint 1 in logs/daghar_split_dataset_50000_60_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_60_2024_10_25_14_45_28/Model



[Epoch 0/78] [Batch 0/647] [D loss: 1.383771] [G loss: 0.420760] [ema: 0.000000] 
[Epoch 0/78] [Batch 100/647] [D loss: 0.533874] [G loss: 0.166219] [ema: 0.933033] 
[Epoch 0/78] [Batch 200/647] [D loss: 0.630994] [G loss: 0.145102] [ema: 0.965936] 
[Epoch 0/78] [Batch 300/647] [D loss: 0.438393] [G loss: 0.161801] [ema: 0.977160] 
[Epoch 0/78] [Batch 400/647] [D loss: 0.450182] [G loss: 0.145138] [ema: 0.982821] 
[Epoch 0/78] [Batch 500/647] [D loss: 0.451357] [G loss: 0.161532] [ema: 0.986233] 
[Epoch 0/78] [Batch 600/647] [D loss: 0.486843] [G loss: 0.154611] [ema: 0.988514] 
[Epoch 1/78] [Batch 0/647] [D loss: 0.445365] [G loss: 0.171008] [ema: 0.989344] 
[Epoch 1/78] [Batch 100/647] [D loss: 0.485261] [G loss: 0.149079] [ema: 0.990764] 
[Epoch 1/78] [Batch 200/647] [D loss: 0.388759] [G loss: 0.170191] [ema: 0.991850] 
[Epoch 1/78] [Batch 300/647] [D loss: 0.425887] [G loss: 0.173757] [ema: 0.992707] 
[Epoch 1/78] [Batch 400/647] [D loss: 0.395132] [G loss: 0.193007] [ema: 0.993402] 
[Epoch 1/78] [Batch 500/647] [D loss: 0.504628] [G loss: 0.145803] [ema: 0.993975] 
[Epoch 1/78] [Batch 600/647] [D loss: 0.441923] [G loss: 0.174902] [ema: 0.994457] 
[Epoch 2/78] [Batch 0/647] [D loss: 0.424335] [G loss: 0.172406] [ema: 0.994658] 
[Epoch 2/78] [Batch 100/647] [D loss: 0.414773] [G loss: 0.153195] [ema: 0.995040] 
[Epoch 2/78] [Batch 200/647] [D loss: 0.451490] [G loss: 0.171325] [ema: 0.995371] 
[Epoch 2/78] [Batch 300/647] [D loss: 0.418586] [G loss: 0.192384] [ema: 0.995661] 
[Epoch 2/78] [Batch 400/647] [D loss: 0.347430] [G loss: 0.216558] [ema: 0.995917] 
[Epoch 2/78] [Batch 500/647] [D loss: 0.384899] [G loss: 0.171060] [ema: 0.996144] 
[Epoch 2/78] [Batch 600/647] [D loss: 0.384783] [G loss: 0.223307] [ema: 0.996347] 
[Epoch 3/78] [Batch 0/647] [D loss: 0.371888] [G loss: 0.238712] [ema: 0.996435] 
[Epoch 3/78] [Batch 100/647] [D loss: 0.431990] [G loss: 0.179723] [ema: 0.996610] 
[Epoch 3/78] [Batch 200/647] [D loss: 0.391948] [G loss: 0.195683] [ema: 0.996768] 
[Epoch 3/78] [Batch 300/647] [D loss: 0.398563] [G loss: 0.214170] [ema: 0.996912] 
[Epoch 3/78] [Batch 400/647] [D loss: 0.436001] [G loss: 0.156783] [ema: 0.997043] 
[Epoch 3/78] [Batch 500/647] [D loss: 0.411255] [G loss: 0.159526] [ema: 0.997164] 
[Epoch 3/78] [Batch 600/647] [D loss: 0.432092] [G loss: 0.153144] [ema: 0.997276] 
[Epoch 4/78] [Batch 0/647] [D loss: 0.422766] [G loss: 0.164198] [ema: 0.997325] 
[Epoch 4/78] [Batch 100/647] [D loss: 0.402357] [G loss: 0.143935] [ema: 0.997425] 
[Epoch 4/78] [Batch 200/647] [D loss: 0.349377] [G loss: 0.194045] [ema: 0.997517] 
[Epoch 4/78] [Batch 300/647] [D loss: 0.413862] [G loss: 0.185750] [ema: 0.997603] 
[Epoch 4/78] [Batch 400/647] [D loss: 0.400446] [G loss: 0.196684] [ema: 0.997683] 
[Epoch 4/78] [Batch 500/647] [D loss: 0.432786] [G loss: 0.186280] [ema: 0.997758] 
[Epoch 4/78] [Batch 600/647] [D loss: 0.396900] [G loss: 0.170779] [ema: 0.997828] 
[Epoch 5/78] [Batch 0/647] [D loss: 0.412194] [G loss: 0.184942] [ema: 0.997860] 
[Epoch 5/78] [Batch 100/647] [D loss: 0.440360] [G loss: 0.162277] [ema: 0.997924] 
[Epoch 5/78] [Batch 200/647] [D loss: 0.431734] [G loss: 0.199812] [ema: 0.997984] 
[Epoch 5/78] [Batch 300/647] [D loss: 0.409095] [G loss: 0.195108] [ema: 0.998041] 
[Epoch 5/78] [Batch 400/647] [D loss: 0.417670] [G loss: 0.182983] [ema: 0.998095] 
[Epoch 5/78] [Batch 500/647] [D loss: 0.395698] [G loss: 0.189195] [ema: 0.998146] 
[Epoch 5/78] [Batch 600/647] [D loss: 0.393360] [G loss: 0.189622] [ema: 0.998194] 
[Epoch 6/78] [Batch 0/647] [D loss: 0.423370] [G loss: 0.180155] [ema: 0.998216] 
[Epoch 6/78] [Batch 100/647] [D loss: 0.429797] [G loss: 0.177202] [ema: 0.998261] 
[Epoch 6/78] [Batch 200/647] [D loss: 0.400723] [G loss: 0.166637] [ema: 0.998303] 
[Epoch 6/78] [Batch 300/647] [D loss: 0.399279] [G loss: 0.184564] [ema: 0.998344] 
[Epoch 6/78] [Batch 400/647] [D loss: 0.414254] [G loss: 0.171489] [ema: 0.998383] 
[Epoch 6/78] [Batch 500/647] [D loss: 0.387356] [G loss: 0.185586] [ema: 0.998419] 
[Epoch 6/78] [Batch 600/647] [D loss: 0.406700] [G loss: 0.194302] [ema: 0.998455] 
[Epoch 7/78] [Batch 0/647] [D loss: 0.389725] [G loss: 0.197397] [ema: 0.998471] 
[Epoch 7/78] [Batch 100/647] [D loss: 0.350481] [G loss: 0.182926] [ema: 0.998504] 
[Epoch 7/78] [Batch 200/647] [D loss: 0.433327] [G loss: 0.167322] [ema: 0.998535] 
[Epoch 7/78] [Batch 300/647] [D loss: 0.421643] [G loss: 0.185519] [ema: 0.998566] 
[Epoch 7/78] [Batch 400/647] [D loss: 0.418911] [G loss: 0.165105] [ema: 0.998595] 
[Epoch 7/78] [Batch 500/647] [D loss: 0.425651] [G loss: 0.180197] [ema: 0.998623] 
[Epoch 7/78] [Batch 600/647] [D loss: 0.432307] [G loss: 0.167210] [ema: 0.998649] 
[Epoch 8/78] [Batch 0/647] [D loss: 0.444274] [G loss: 0.158242] [ema: 0.998662] 
[Epoch 8/78] [Batch 100/647] [D loss: 0.415110] [G loss: 0.181752] [ema: 0.998687] 
[Epoch 8/78] [Batch 200/647] [D loss: 0.406614] [G loss: 0.186368] [ema: 0.998711] 
[Epoch 8/78] [Batch 300/647] [D loss: 0.433375] [G loss: 0.170124] [ema: 0.998735] 
[Epoch 8/78] [Batch 400/647] [D loss: 0.405138] [G loss: 0.161451] [ema: 0.998758] 
[Epoch 8/78] [Batch 500/647] [D loss: 0.381888] [G loss: 0.193154] [ema: 0.998780] 
[Epoch 8/78] [Batch 600/647] [D loss: 0.391232] [G loss: 0.179042] [ema: 0.998801] 
[Epoch 9/78] [Batch 0/647] [D loss: 0.399669] [G loss: 0.176479] [ema: 0.998810] 
[Epoch 9/78] [Batch 100/647] [D loss: 0.363063] [G loss: 0.179364] [ema: 0.998830] 
[Epoch 9/78] [Batch 200/647] [D loss: 0.416095] [G loss: 0.151032] [ema: 0.998850] 
[Epoch 9/78] [Batch 300/647] [D loss: 0.342859] [G loss: 0.181603] [ema: 0.998869] 
[Epoch 9/78] [Batch 400/647] [D loss: 0.371296] [G loss: 0.190587] [ema: 0.998887] 
[Epoch 9/78] [Batch 500/647] [D loss: 0.380237] [G loss: 0.165584] [ema: 0.998904] 
[Epoch 9/78] [Batch 600/647] [D loss: 0.400135] [G loss: 0.162384] [ema: 0.998921] 
[Epoch 10/78] [Batch 0/647] [D loss: 0.396658] [G loss: 0.188325] [ema: 0.998929] 
[Epoch 10/78] [Batch 100/647] [D loss: 0.389097] [G loss: 0.171436] [ema: 0.998946] 
[Epoch 10/78] [Batch 200/647] [D loss: 0.435532] [G loss: 0.159422] [ema: 0.998961] 
[Epoch 10/78] [Batch 300/647] [D loss: 0.410538] [G loss: 0.180852] [ema: 0.998977] 
[Epoch 10/78] [Batch 400/647] [D loss: 0.390836] [G loss: 0.185343] [ema: 0.998992] 
[Epoch 10/78] [Batch 500/647] [D loss: 0.441761] [G loss: 0.158793] [ema: 0.999006] 
[Epoch 10/78] [Batch 600/647] [D loss: 0.423621] [G loss: 0.188916] [ema: 0.999020] 
[Epoch 11/78] [Batch 0/647] [D loss: 0.387304] [G loss: 0.206271] [ema: 0.999027] 
[Epoch 11/78] [Batch 100/647] [D loss: 0.411127] [G loss: 0.156152] [ema: 0.999040] 
[Epoch 11/78] [Batch 200/647] [D loss: 0.344211] [G loss: 0.175620] [ema: 0.999053] 
[Epoch 11/78] [Batch 300/647] [D loss: 0.433362] [G loss: 0.172941] [ema: 0.999066] 
[Epoch 11/78] [Batch 400/647] [D loss: 0.401738] [G loss: 0.180089] [ema: 0.999078] 
[Epoch 11/78] [Batch 500/647] [D loss: 0.409772] [G loss: 0.166005] [ema: 0.999090] 
[Epoch 11/78] [Batch 600/647] [D loss: 0.414910] [G loss: 0.180351] [ema: 0.999102] 
[Epoch 12/78] [Batch 0/647] [D loss: 0.392130] [G loss: 0.196951] [ema: 0.999108] 
[Epoch 12/78] [Batch 100/647] [D loss: 0.399348] [G loss: 0.177528] [ema: 0.999119] 
[Epoch 12/78] [Batch 200/647] [D loss: 0.405263] [G loss: 0.181114] [ema: 0.999130] 
[Epoch 12/78] [Batch 300/647] [D loss: 0.378636] [G loss: 0.177420] [ema: 0.999141] 
[Epoch 12/78] [Batch 400/647] [D loss: 0.387477] [G loss: 0.184916] [ema: 0.999151] 
[Epoch 12/78] [Batch 500/647] [D loss: 0.401933] [G loss: 0.168249] [ema: 0.999162] 
[Epoch 12/78] [Batch 600/647] [D loss: 0.368280] [G loss: 0.176719] [ema: 0.999172] 
[Epoch 13/78] [Batch 0/647] [D loss: 0.417881] [G loss: 0.184993] [ema: 0.999176] 
[Epoch 13/78] [Batch 100/647] [D loss: 0.402862] [G loss: 0.190693] [ema: 0.999186] 
[Epoch 13/78] [Batch 200/647] [D loss: 0.354808] [G loss: 0.195068] [ema: 0.999195] 
[Epoch 13/78] [Batch 300/647] [D loss: 0.360494] [G loss: 0.190388] [ema: 0.999205] 
[Epoch 13/78] [Batch 400/647] [D loss: 0.367545] [G loss: 0.188793] [ema: 0.999214] 
[Epoch 13/78] [Batch 500/647] [D loss: 0.411494] [G loss: 0.164897] [ema: 0.999222] 
[Epoch 13/78] [Batch 600/647] [D loss: 0.382753] [G loss: 0.192121] [ema: 0.999231] 
[Epoch 14/78] [Batch 0/647] [D loss: 0.457003] [G loss: 0.189472] [ema: 0.999235] 
[Epoch 14/78] [Batch 100/647] [D loss: 0.387994] [G loss: 0.180358] [ema: 0.999243] 
[Epoch 14/78] [Batch 200/647] [D loss: 0.373100] [G loss: 0.184621] [ema: 0.999252] 
[Epoch 14/78] [Batch 300/647] [D loss: 0.398361] [G loss: 0.182536] [ema: 0.999260] 
[Epoch 14/78] [Batch 400/647] [D loss: 0.401993] [G loss: 0.180601] [ema: 0.999267] 
[Epoch 14/78] [Batch 500/647] [D loss: 0.389546] [G loss: 0.169831] [ema: 0.999275] 
[Epoch 14/78] [Batch 600/647] [D loss: 0.335457] [G loss: 0.198075] [ema: 0.999283] 
[Epoch 15/78] [Batch 0/647] [D loss: 0.389241] [G loss: 0.181376] [ema: 0.999286] 
[Epoch 15/78] [Batch 100/647] [D loss: 0.446693] [G loss: 0.175723] [ema: 0.999293] 
[Epoch 15/78] [Batch 200/647] [D loss: 0.394598] [G loss: 0.175855] [ema: 0.999300] 
[Epoch 15/78] [Batch 300/647] [D loss: 0.394791] [G loss: 0.178375] [ema: 0.999307] 
[Epoch 15/78] [Batch 400/647] [D loss: 0.373705] [G loss: 0.193717] [ema: 0.999314] 
[Epoch 15/78] [Batch 500/647] [D loss: 0.395322] [G loss: 0.166047] [ema: 0.999321] 
[Epoch 15/78] [Batch 600/647] [D loss: 0.407296] [G loss: 0.176219] [ema: 0.999328] 
[Epoch 16/78] [Batch 0/647] [D loss: 0.375052] [G loss: 0.188664] [ema: 0.999331] 
[Epoch 16/78] [Batch 100/647] [D loss: 0.359741] [G loss: 0.174745] [ema: 0.999337] 
[Epoch 16/78] [Batch 200/647] [D loss: 0.365519] [G loss: 0.172323] [ema: 0.999343] 
[Epoch 16/78] [Batch 300/647] [D loss: 0.408729] [G loss: 0.171514] [ema: 0.999349] 
[Epoch 16/78] [Batch 400/647] [D loss: 0.387723] [G loss: 0.190209] [ema: 0.999356] 
[Epoch 16/78] [Batch 500/647] [D loss: 0.425166] [G loss: 0.183197] [ema: 0.999361] 
[Epoch 16/78] [Batch 600/647] [D loss: 0.347399] [G loss: 0.191642] [ema: 0.999367] 
[Epoch 17/78] [Batch 0/647] [D loss: 0.362563] [G loss: 0.197535] [ema: 0.999370] 
[Epoch 17/78] [Batch 100/647] [D loss: 0.408603] [G loss: 0.176039] [ema: 0.999376] 
[Epoch 17/78] [Batch 200/647] [D loss: 0.396581] [G loss: 0.183951] [ema: 0.999381] 
[Epoch 17/78] [Batch 300/647] [D loss: 0.389835] [G loss: 0.163141] [ema: 0.999387] 
[Epoch 17/78] [Batch 400/647] [D loss: 0.348103] [G loss: 0.192484] [ema: 0.999392] 
[Epoch 17/78] [Batch 500/647] [D loss: 0.405642] [G loss: 0.178302] [ema: 0.999397] 
[Epoch 17/78] [Batch 600/647] [D loss: 0.365163] [G loss: 0.168277] [ema: 0.999403] 
[Epoch 18/78] [Batch 0/647] [D loss: 0.365461] [G loss: 0.190954] [ema: 0.999405] 
[Epoch 18/78] [Batch 100/647] [D loss: 0.351615] [G loss: 0.198737] [ema: 0.999410] 
[Epoch 18/78] [Batch 200/647] [D loss: 0.390817] [G loss: 0.162451] [ema: 0.999415] 
[Epoch 18/78] [Batch 300/647] [D loss: 0.393657] [G loss: 0.183011] [ema: 0.999420] 
[Epoch 18/78] [Batch 400/647] [D loss: 0.372636] [G loss: 0.203649] [ema: 0.999425] 
[Epoch 18/78] [Batch 500/647] [D loss: 0.319042] [G loss: 0.187929] [ema: 0.999429] 
[Epoch 18/78] [Batch 600/647] [D loss: 0.404171] [G loss: 0.185111] [ema: 0.999434] 
[Epoch 19/78] [Batch 0/647] [D loss: 0.368579] [G loss: 0.185250] [ema: 0.999436] 
[Epoch 19/78] [Batch 100/647] [D loss: 0.400283] [G loss: 0.177501] [ema: 0.999441] 
[Epoch 19/78] [Batch 200/647] [D loss: 0.367519] [G loss: 0.187126] [ema: 0.999445] 
[Epoch 19/78] [Batch 300/647] [D loss: 0.343426] [G loss: 0.186063] [ema: 0.999450] 
[Epoch 19/78] [Batch 400/647] [D loss: 0.358630] [G loss: 0.190855] [ema: 0.999454] 
[Epoch 19/78] [Batch 500/647] [D loss: 0.413883] [G loss: 0.183680] [ema: 0.999458] 
[Epoch 19/78] [Batch 600/647] [D loss: 0.361448] [G loss: 0.196999] [ema: 0.999463] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_60_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_60_2024_10_25_14_45_28/Model



[Epoch 20/78] [Batch 0/647] [D loss: 0.389057] [G loss: 0.193595] [ema: 0.999464] 
[Epoch 20/78] [Batch 100/647] [D loss: 0.375805] [G loss: 0.181388] [ema: 0.999469] 
[Epoch 20/78] [Batch 200/647] [D loss: 0.358842] [G loss: 0.191775] [ema: 0.999473] 
[Epoch 20/78] [Batch 300/647] [D loss: 0.400982] [G loss: 0.187362] [ema: 0.999477] 
[Epoch 20/78] [Batch 400/647] [D loss: 0.374912] [G loss: 0.172407] [ema: 0.999481] 
[Epoch 20/78] [Batch 500/647] [D loss: 0.381542] [G loss: 0.181848] [ema: 0.999484] 
[Epoch 20/78] [Batch 600/647] [D loss: 0.432591] [G loss: 0.176773] [ema: 0.999488] 
[Epoch 21/78] [Batch 0/647] [D loss: 0.377558] [G loss: 0.215576] [ema: 0.999490] 
[Epoch 21/78] [Batch 100/647] [D loss: 0.405259] [G loss: 0.170717] [ema: 0.999494] 
[Epoch 21/78] [Batch 200/647] [D loss: 0.445162] [G loss: 0.163954] [ema: 0.999497] 
[Epoch 21/78] [Batch 300/647] [D loss: 0.388048] [G loss: 0.193291] [ema: 0.999501] 
[Epoch 21/78] [Batch 400/647] [D loss: 0.383599] [G loss: 0.185624] [ema: 0.999505] 
[Epoch 21/78] [Batch 500/647] [D loss: 0.370249] [G loss: 0.190032] [ema: 0.999508] 
[Epoch 21/78] [Batch 600/647] [D loss: 0.375499] [G loss: 0.184560] [ema: 0.999512] 
[Epoch 22/78] [Batch 0/647] [D loss: 0.386618] [G loss: 0.193485] [ema: 0.999513] 
[Epoch 22/78] [Batch 100/647] [D loss: 0.353458] [G loss: 0.187244] [ema: 0.999517] 
[Epoch 22/78] [Batch 200/647] [D loss: 0.348200] [G loss: 0.200187] [ema: 0.999520] 
[Epoch 22/78] [Batch 300/647] [D loss: 0.394105] [G loss: 0.175828] [ema: 0.999523] 
[Epoch 22/78] [Batch 400/647] [D loss: 0.392252] [G loss: 0.193041] [ema: 0.999526] 
[Epoch 22/78] [Batch 500/647] [D loss: 0.369269] [G loss: 0.198871] [ema: 0.999530] 
[Epoch 22/78] [Batch 600/647] [D loss: 0.325306] [G loss: 0.190664] [ema: 0.999533] 
[Epoch 23/78] [Batch 0/647] [D loss: 0.345390] [G loss: 0.196584] [ema: 0.999534] 
[Epoch 23/78] [Batch 100/647] [D loss: 0.392261] [G loss: 0.177404] [ema: 0.999537] 
[Epoch 23/78] [Batch 200/647] [D loss: 0.433459] [G loss: 0.192580] [ema: 0.999540] 
[Epoch 23/78] [Batch 300/647] [D loss: 0.378214] [G loss: 0.178427] [ema: 0.999544] 
[Epoch 23/78] [Batch 400/647] [D loss: 0.402151] [G loss: 0.190598] [ema: 0.999547] 
[Epoch 23/78] [Batch 500/647] [D loss: 0.359297] [G loss: 0.194677] [ema: 0.999549] 
[Epoch 23/78] [Batch 600/647] [D loss: 0.393971] [G loss: 0.169477] [ema: 0.999552] 
[Epoch 24/78] [Batch 0/647] [D loss: 0.417546] [G loss: 0.186274] [ema: 0.999554] 
[Epoch 24/78] [Batch 100/647] [D loss: 0.350827] [G loss: 0.194916] [ema: 0.999557] 
[Epoch 24/78] [Batch 200/647] [D loss: 0.380094] [G loss: 0.171670] [ema: 0.999559] 
[Epoch 24/78] [Batch 300/647] [D loss: 0.367084] [G loss: 0.190934] [ema: 0.999562] 
[Epoch 24/78] [Batch 400/647] [D loss: 0.389378] [G loss: 0.193014] [ema: 0.999565] 
[Epoch 24/78] [Batch 500/647] [D loss: 0.385233] [G loss: 0.186573] [ema: 0.999568] 
[Epoch 24/78] [Batch 600/647] [D loss: 0.335974] [G loss: 0.186118] [ema: 0.999570] 
[Epoch 25/78] [Batch 0/647] [D loss: 0.383156] [G loss: 0.176369] [ema: 0.999572] 
[Epoch 25/78] [Batch 100/647] [D loss: 0.345463] [G loss: 0.187186] [ema: 0.999574] 
[Epoch 25/78] [Batch 200/647] [D loss: 0.393493] [G loss: 0.171884] [ema: 0.999577] 
[Epoch 25/78] [Batch 300/647] [D loss: 0.380276] [G loss: 0.161811] [ema: 0.999579] 
[Epoch 25/78] [Batch 400/647] [D loss: 0.369246] [G loss: 0.195560] [ema: 0.999582] 
[Epoch 25/78] [Batch 500/647] [D loss: 0.335605] [G loss: 0.194644] [ema: 0.999584] 
[Epoch 25/78] [Batch 600/647] [D loss: 0.378240] [G loss: 0.189235] [ema: 0.999587] 
[Epoch 26/78] [Batch 0/647] [D loss: 0.339771] [G loss: 0.198841] [ema: 0.999588] 
[Epoch 26/78] [Batch 100/647] [D loss: 0.363264] [G loss: 0.188614] [ema: 0.999590] 
[Epoch 26/78] [Batch 200/647] [D loss: 0.353669] [G loss: 0.185003] [ema: 0.999593] 
[Epoch 26/78] [Batch 300/647] [D loss: 0.408320] [G loss: 0.189666] [ema: 0.999595] 
[Epoch 26/78] [Batch 400/647] [D loss: 0.375478] [G loss: 0.196688] [ema: 0.999598] 
[Epoch 26/78] [Batch 500/647] [D loss: 0.371091] [G loss: 0.176006] [ema: 0.999600] 
[Epoch 26/78] [Batch 600/647] [D loss: 0.362087] [G loss: 0.197263] [ema: 0.999602] 
[Epoch 27/78] [Batch 0/647] [D loss: 0.364163] [G loss: 0.190425] [ema: 0.999603] 
[Epoch 27/78] [Batch 100/647] [D loss: 0.420277] [G loss: 0.186699] [ema: 0.999606] 
[Epoch 27/78] [Batch 200/647] [D loss: 0.380187] [G loss: 0.195010] [ema: 0.999608] 
[Epoch 27/78] [Batch 300/647] [D loss: 0.360392] [G loss: 0.191181] [ema: 0.999610] 
[Epoch 27/78] [Batch 400/647] [D loss: 0.337364] [G loss: 0.193165] [ema: 0.999612] 
[Epoch 27/78] [Batch 500/647] [D loss: 0.352842] [G loss: 0.191759] [ema: 0.999614] 
[Epoch 27/78] [Batch 600/647] [D loss: 0.393761] [G loss: 0.178201] [ema: 0.999616] 
[Epoch 28/78] [Batch 0/647] [D loss: 0.389224] [G loss: 0.211180] [ema: 0.999617] 
[Epoch 28/78] [Batch 100/647] [D loss: 0.394024] [G loss: 0.180634] [ema: 0.999620] 
[Epoch 28/78] [Batch 200/647] [D loss: 0.409241] [G loss: 0.189764] [ema: 0.999622] 
[Epoch 28/78] [Batch 300/647] [D loss: 0.366664] [G loss: 0.191512] [ema: 0.999624] 
[Epoch 28/78] [Batch 400/647] [D loss: 0.386040] [G loss: 0.208305] [ema: 0.999626] 
[Epoch 28/78] [Batch 500/647] [D loss: 0.389225] [G loss: 0.189804] [ema: 0.999628] 
[Epoch 28/78] [Batch 600/647] [D loss: 0.387535] [G loss: 0.188217] [ema: 0.999630] 
[Epoch 29/78] [Batch 0/647] [D loss: 0.373095] [G loss: 0.181173] [ema: 0.999631] 
[Epoch 29/78] [Batch 100/647] [D loss: 0.355907] [G loss: 0.198487] [ema: 0.999633] 
[Epoch 29/78] [Batch 200/647] [D loss: 0.382503] [G loss: 0.167204] [ema: 0.999635] 
[Epoch 29/78] [Batch 300/647] [D loss: 0.334512] [G loss: 0.203219] [ema: 0.999636] 
[Epoch 29/78] [Batch 400/647] [D loss: 0.378797] [G loss: 0.191834] [ema: 0.999638] 
[Epoch 29/78] [Batch 500/647] [D loss: 0.395202] [G loss: 0.192284] [ema: 0.999640] 
[Epoch 29/78] [Batch 600/647] [D loss: 0.346600] [G loss: 0.199366] [ema: 0.999642] 
[Epoch 30/78] [Batch 0/647] [D loss: 0.416679] [G loss: 0.191774] [ema: 0.999643] 
[Epoch 30/78] [Batch 100/647] [D loss: 0.393068] [G loss: 0.194220] [ema: 0.999645] 
[Epoch 30/78] [Batch 200/647] [D loss: 0.361199] [G loss: 0.192541] [ema: 0.999647] 
[Epoch 30/78] [Batch 300/647] [D loss: 0.379835] [G loss: 0.194922] [ema: 0.999648] 
[Epoch 30/78] [Batch 400/647] [D loss: 0.377201] [G loss: 0.184358] [ema: 0.999650] 
[Epoch 30/78] [Batch 500/647] [D loss: 0.344359] [G loss: 0.209716] [ema: 0.999652] 
[Epoch 30/78] [Batch 600/647] [D loss: 0.352798] [G loss: 0.201337] [ema: 0.999654] 
[Epoch 31/78] [Batch 0/647] [D loss: 0.356078] [G loss: 0.197331] [ema: 0.999654] 
[Epoch 31/78] [Batch 100/647] [D loss: 0.385614] [G loss: 0.198837] [ema: 0.999656] 
[Epoch 31/78] [Batch 200/647] [D loss: 0.358576] [G loss: 0.174262] [ema: 0.999658] 
[Epoch 31/78] [Batch 300/647] [D loss: 0.358384] [G loss: 0.198529] [ema: 0.999660] 
[Epoch 31/78] [Batch 400/647] [D loss: 0.352502] [G loss: 0.197772] [ema: 0.999661] 
[Epoch 31/78] [Batch 500/647] [D loss: 0.374466] [G loss: 0.198511] [ema: 0.999663] 
[Epoch 31/78] [Batch 600/647] [D loss: 0.371936] [G loss: 0.222314] [ema: 0.999665] 
[Epoch 32/78] [Batch 0/647] [D loss: 0.314549] [G loss: 0.230512] [ema: 0.999665] 
[Epoch 32/78] [Batch 100/647] [D loss: 0.326916] [G loss: 0.193651] [ema: 0.999667] 
[Epoch 32/78] [Batch 200/647] [D loss: 0.457953] [G loss: 0.184574] [ema: 0.999668] 
[Epoch 32/78] [Batch 300/647] [D loss: 0.403194] [G loss: 0.195793] [ema: 0.999670] 
[Epoch 32/78] [Batch 400/647] [D loss: 0.355034] [G loss: 0.196389] [ema: 0.999672] 
[Epoch 32/78] [Batch 500/647] [D loss: 0.353098] [G loss: 0.194639] [ema: 0.999673] 
[Epoch 32/78] [Batch 600/647] [D loss: 0.345803] [G loss: 0.198626] [ema: 0.999675] 
[Epoch 33/78] [Batch 0/647] [D loss: 0.323432] [G loss: 0.234770] [ema: 0.999675] 
[Epoch 33/78] [Batch 100/647] [D loss: 0.404283] [G loss: 0.187713] [ema: 0.999677] 
[Epoch 33/78] [Batch 200/647] [D loss: 0.342190] [G loss: 0.201021] [ema: 0.999678] 
[Epoch 33/78] [Batch 300/647] [D loss: 0.376911] [G loss: 0.207194] [ema: 0.999680] 
[Epoch 33/78] [Batch 400/647] [D loss: 0.373329] [G loss: 0.194233] [ema: 0.999681] 
[Epoch 33/78] [Batch 500/647] [D loss: 0.343345] [G loss: 0.187305] [ema: 0.999683] 
[Epoch 33/78] [Batch 600/647] [D loss: 0.405041] [G loss: 0.189885] [ema: 0.999684] 
[Epoch 34/78] [Batch 0/647] [D loss: 0.386693] [G loss: 0.195145] [ema: 0.999685] 
[Epoch 34/78] [Batch 100/647] [D loss: 0.411897] [G loss: 0.208203] [ema: 0.999686] 
[Epoch 34/78] [Batch 200/647] [D loss: 0.367730] [G loss: 0.197413] [ema: 0.999688] 
[Epoch 34/78] [Batch 300/647] [D loss: 0.353179] [G loss: 0.167318] [ema: 0.999689] 
[Epoch 34/78] [Batch 400/647] [D loss: 0.432376] [G loss: 0.198129] [ema: 0.999691] 
[Epoch 34/78] [Batch 500/647] [D loss: 0.364470] [G loss: 0.185479] [ema: 0.999692] 
[Epoch 34/78] [Batch 600/647] [D loss: 0.377342] [G loss: 0.190500] [ema: 0.999693] 
[Epoch 35/78] [Batch 0/647] [D loss: 0.379845] [G loss: 0.179902] [ema: 0.999694] 
[Epoch 35/78] [Batch 100/647] [D loss: 0.344805] [G loss: 0.194465] [ema: 0.999695] 
[Epoch 35/78] [Batch 200/647] [D loss: 0.364073] [G loss: 0.204504] [ema: 0.999697] 
[Epoch 35/78] [Batch 300/647] [D loss: 0.328954] [G loss: 0.201304] [ema: 0.999698] 
[Epoch 35/78] [Batch 400/647] [D loss: 0.441945] [G loss: 0.171053] [ema: 0.999699] 
[Epoch 35/78] [Batch 500/647] [D loss: 0.391432] [G loss: 0.179641] [ema: 0.999701] 
[Epoch 35/78] [Batch 600/647] [D loss: 0.346735] [G loss: 0.195932] [ema: 0.999702] 
[Epoch 36/78] [Batch 0/647] [D loss: 0.375530] [G loss: 0.185303] [ema: 0.999702] 
[Epoch 36/78] [Batch 100/647] [D loss: 0.332953] [G loss: 0.200748] [ema: 0.999704] 
[Epoch 36/78] [Batch 200/647] [D loss: 0.386995] [G loss: 0.186918] [ema: 0.999705] 
[Epoch 36/78] [Batch 300/647] [D loss: 0.371956] [G loss: 0.204815] [ema: 0.999706] 
[Epoch 36/78] [Batch 400/647] [D loss: 0.325209] [G loss: 0.201849] [ema: 0.999707] 
[Epoch 36/78] [Batch 500/647] [D loss: 0.339439] [G loss: 0.208536] [ema: 0.999709] 
[Epoch 36/78] [Batch 600/647] [D loss: 0.379952] [G loss: 0.176471] [ema: 0.999710] 
[Epoch 37/78] [Batch 0/647] [D loss: 0.328850] [G loss: 0.215022] [ema: 0.999710] 
[Epoch 37/78] [Batch 100/647] [D loss: 0.386725] [G loss: 0.192071] [ema: 0.999712] 
[Epoch 37/78] [Batch 200/647] [D loss: 0.357386] [G loss: 0.188934] [ema: 0.999713] 
[Epoch 37/78] [Batch 300/647] [D loss: 0.354162] [G loss: 0.204846] [ema: 0.999714] 
[Epoch 37/78] [Batch 400/647] [D loss: 0.366747] [G loss: 0.186820] [ema: 0.999715] 
[Epoch 37/78] [Batch 500/647] [D loss: 0.345515] [G loss: 0.201427] [ema: 0.999716] 
[Epoch 37/78] [Batch 600/647] [D loss: 0.362135] [G loss: 0.189421] [ema: 0.999718] 
[Epoch 38/78] [Batch 0/647] [D loss: 0.409575] [G loss: 0.194351] [ema: 0.999718] 
[Epoch 38/78] [Batch 100/647] [D loss: 0.432903] [G loss: 0.189354] [ema: 0.999719] 
[Epoch 38/78] [Batch 200/647] [D loss: 0.380574] [G loss: 0.192368] [ema: 0.999720] 
[Epoch 38/78] [Batch 300/647] [D loss: 0.382536] [G loss: 0.195195] [ema: 0.999722] 
[Epoch 38/78] [Batch 400/647] [D loss: 0.368199] [G loss: 0.187107] [ema: 0.999723] 
[Epoch 38/78] [Batch 500/647] [D loss: 0.376802] [G loss: 0.215589] [ema: 0.999724] 
[Epoch 38/78] [Batch 600/647] [D loss: 0.386184] [G loss: 0.187378] [ema: 0.999725] 
[Epoch 39/78] [Batch 0/647] [D loss: 0.309351] [G loss: 0.203549] [ema: 0.999725] 
[Epoch 39/78] [Batch 100/647] [D loss: 0.367357] [G loss: 0.199358] [ema: 0.999726] 
[Epoch 39/78] [Batch 200/647] [D loss: 0.335333] [G loss: 0.199436] [ema: 0.999727] 
[Epoch 39/78] [Batch 300/647] [D loss: 0.362678] [G loss: 0.214359] [ema: 0.999729] 
[Epoch 39/78] [Batch 400/647] [D loss: 0.402098] [G loss: 0.185316] [ema: 0.999730] 
[Epoch 39/78] [Batch 500/647] [D loss: 0.330589] [G loss: 0.208087] [ema: 0.999731] 
[Epoch 39/78] [Batch 600/647] [D loss: 0.373538] [G loss: 0.185985] [ema: 0.999732] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_60_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_60_2024_10_25_14_45_28/Model



[Epoch 40/78] [Batch 0/647] [D loss: 0.354733] [G loss: 0.209697] [ema: 0.999732] 
[Epoch 40/78] [Batch 100/647] [D loss: 0.316787] [G loss: 0.211768] [ema: 0.999733] 
[Epoch 40/78] [Batch 200/647] [D loss: 0.365522] [G loss: 0.205110] [ema: 0.999734] 
[Epoch 40/78] [Batch 300/647] [D loss: 0.350127] [G loss: 0.196636] [ema: 0.999735] 
[Epoch 40/78] [Batch 400/647] [D loss: 0.374086] [G loss: 0.177522] [ema: 0.999736] 
[Epoch 40/78] [Batch 500/647] [D loss: 0.378667] [G loss: 0.187777] [ema: 0.999737] 
[Epoch 40/78] [Batch 600/647] [D loss: 0.351351] [G loss: 0.186068] [ema: 0.999738] 
[Epoch 41/78] [Batch 0/647] [D loss: 0.357656] [G loss: 0.204711] [ema: 0.999739] 
[Epoch 41/78] [Batch 100/647] [D loss: 0.353791] [G loss: 0.208296] [ema: 0.999740] 
[Epoch 41/78] [Batch 200/647] [D loss: 0.367600] [G loss: 0.180916] [ema: 0.999741] 
[Epoch 41/78] [Batch 300/647] [D loss: 0.394657] [G loss: 0.200969] [ema: 0.999742] 
[Epoch 41/78] [Batch 400/647] [D loss: 0.346580] [G loss: 0.201819] [ema: 0.999743] 
[Epoch 41/78] [Batch 500/647] [D loss: 0.402203] [G loss: 0.213752] [ema: 0.999744] 
[Epoch 41/78] [Batch 600/647] [D loss: 0.337880] [G loss: 0.208185] [ema: 0.999745] 
[Epoch 42/78] [Batch 0/647] [D loss: 0.408888] [G loss: 0.192971] [ema: 0.999745] 
[Epoch 42/78] [Batch 100/647] [D loss: 0.384135] [G loss: 0.209236] [ema: 0.999746] 
[Epoch 42/78] [Batch 200/647] [D loss: 0.388451] [G loss: 0.209590] [ema: 0.999747] 
[Epoch 42/78] [Batch 300/647] [D loss: 0.333095] [G loss: 0.200532] [ema: 0.999748] 
[Epoch 42/78] [Batch 400/647] [D loss: 0.340738] [G loss: 0.208263] [ema: 0.999749] 
[Epoch 42/78] [Batch 500/647] [D loss: 0.356664] [G loss: 0.197447] [ema: 0.999750] 
[Epoch 42/78] [Batch 600/647] [D loss: 0.348262] [G loss: 0.180658] [ema: 0.999750] 
[Epoch 43/78] [Batch 0/647] [D loss: 0.382532] [G loss: 0.213935] [ema: 0.999751] 
[Epoch 43/78] [Batch 100/647] [D loss: 0.358508] [G loss: 0.209255] [ema: 0.999752] 
[Epoch 43/78] [Batch 200/647] [D loss: 0.317432] [G loss: 0.185324] [ema: 0.999753] 
[Epoch 43/78] [Batch 300/647] [D loss: 0.305440] [G loss: 0.179374] [ema: 0.999754] 
[Epoch 43/78] [Batch 400/647] [D loss: 0.331171] [G loss: 0.187493] [ema: 0.999754] 
[Epoch 43/78] [Batch 500/647] [D loss: 0.395853] [G loss: 0.204776] [ema: 0.999755] 
[Epoch 43/78] [Batch 600/647] [D loss: 0.390252] [G loss: 0.196112] [ema: 0.999756] 
[Epoch 44/78] [Batch 0/647] [D loss: 0.352764] [G loss: 0.223677] [ema: 0.999757] 
[Epoch 44/78] [Batch 100/647] [D loss: 0.382679] [G loss: 0.202339] [ema: 0.999757] 
[Epoch 44/78] [Batch 200/647] [D loss: 0.377574] [G loss: 0.202550] [ema: 0.999758] 
[Epoch 44/78] [Batch 300/647] [D loss: 0.322262] [G loss: 0.222659] [ema: 0.999759] 
[Epoch 44/78] [Batch 400/647] [D loss: 0.389632] [G loss: 0.172368] [ema: 0.999760] 
[Epoch 44/78] [Batch 500/647] [D loss: 0.310780] [G loss: 0.207636] [ema: 0.999761] 
[Epoch 44/78] [Batch 600/647] [D loss: 0.406394] [G loss: 0.165510] [ema: 0.999762] 
[Epoch 45/78] [Batch 0/647] [D loss: 0.331840] [G loss: 0.238783] [ema: 0.999762] 
[Epoch 45/78] [Batch 100/647] [D loss: 0.394124] [G loss: 0.133440] [ema: 0.999763] 
[Epoch 45/78] [Batch 200/647] [D loss: 0.376125] [G loss: 0.186949] [ema: 0.999764] 
[Epoch 45/78] [Batch 300/647] [D loss: 0.366701] [G loss: 0.206028] [ema: 0.999764] 
[Epoch 45/78] [Batch 400/647] [D loss: 0.368518] [G loss: 0.193173] [ema: 0.999765] 
[Epoch 45/78] [Batch 500/647] [D loss: 0.350000] [G loss: 0.186003] [ema: 0.999766] 
[Epoch 45/78] [Batch 600/647] [D loss: 0.343290] [G loss: 0.195122] [ema: 0.999767] 
[Epoch 46/78] [Batch 0/647] [D loss: 0.406652] [G loss: 0.193593] [ema: 0.999767] 
[Epoch 46/78] [Batch 100/647] [D loss: 0.347241] [G loss: 0.193768] [ema: 0.999768] 
[Epoch 46/78] [Batch 200/647] [D loss: 0.372413] [G loss: 0.205706] [ema: 0.999769] 
[Epoch 46/78] [Batch 300/647] [D loss: 0.337406] [G loss: 0.200444] [ema: 0.999769] 
[Epoch 46/78] [Batch 400/647] [D loss: 0.344764] [G loss: 0.191486] [ema: 0.999770] 
[Epoch 46/78] [Batch 500/647] [D loss: 0.333968] [G loss: 0.205412] [ema: 0.999771] 
[Epoch 46/78] [Batch 600/647] [D loss: 0.348562] [G loss: 0.200013] [ema: 0.999772] 
[Epoch 47/78] [Batch 0/647] [D loss: 0.354473] [G loss: 0.193479] [ema: 0.999772] 
[Epoch 47/78] [Batch 100/647] [D loss: 0.341390] [G loss: 0.195103] [ema: 0.999773] 
[Epoch 47/78] [Batch 200/647] [D loss: 0.353890] [G loss: 0.200915] [ema: 0.999774] 
[Epoch 47/78] [Batch 300/647] [D loss: 0.337240] [G loss: 0.198812] [ema: 0.999774] 
[Epoch 47/78] [Batch 400/647] [D loss: 0.372082] [G loss: 0.202409] [ema: 0.999775] 
[Epoch 47/78] [Batch 500/647] [D loss: 0.384018] [G loss: 0.186378] [ema: 0.999776] 
[Epoch 47/78] [Batch 600/647] [D loss: 0.327012] [G loss: 0.198605] [ema: 0.999776] 
[Epoch 48/78] [Batch 0/647] [D loss: 0.380935] [G loss: 0.202935] [ema: 0.999777] 
[Epoch 48/78] [Batch 100/647] [D loss: 0.376405] [G loss: 0.193567] [ema: 0.999778] 
[Epoch 48/78] [Batch 200/647] [D loss: 0.349679] [G loss: 0.173804] [ema: 0.999778] 
[Epoch 48/78] [Batch 300/647] [D loss: 0.369974] [G loss: 0.192619] [ema: 0.999779] 
[Epoch 48/78] [Batch 400/647] [D loss: 0.398005] [G loss: 0.204706] [ema: 0.999780] 
[Epoch 48/78] [Batch 500/647] [D loss: 0.375478] [G loss: 0.188281] [ema: 0.999780] 
[Epoch 48/78] [Batch 600/647] [D loss: 0.342654] [G loss: 0.201507] [ema: 0.999781] 
[Epoch 49/78] [Batch 0/647] [D loss: 0.364098] [G loss: 0.196828] [ema: 0.999781] 
[Epoch 49/78] [Batch 100/647] [D loss: 0.379964] [G loss: 0.208227] [ema: 0.999782] 
[Epoch 49/78] [Batch 200/647] [D loss: 0.361504] [G loss: 0.192603] [ema: 0.999783] 
[Epoch 49/78] [Batch 300/647] [D loss: 0.398877] [G loss: 0.182887] [ema: 0.999783] 
[Epoch 49/78] [Batch 400/647] [D loss: 0.348945] [G loss: 0.198555] [ema: 0.999784] 
[Epoch 49/78] [Batch 500/647] [D loss: 0.337392] [G loss: 0.187897] [ema: 0.999785] 
[Epoch 49/78] [Batch 600/647] [D loss: 0.377568] [G loss: 0.181928] [ema: 0.999785] 
[Epoch 50/78] [Batch 0/647] [D loss: 0.414128] [G loss: 0.188039] [ema: 0.999786] 
[Epoch 50/78] [Batch 100/647] [D loss: 0.356078] [G loss: 0.188632] [ema: 0.999786] 
[Epoch 50/78] [Batch 200/647] [D loss: 0.351993] [G loss: 0.180969] [ema: 0.999787] 
[Epoch 50/78] [Batch 300/647] [D loss: 0.354745] [G loss: 0.184003] [ema: 0.999788] 
[Epoch 50/78] [Batch 400/647] [D loss: 0.341425] [G loss: 0.193531] [ema: 0.999788] 
[Epoch 50/78] [Batch 500/647] [D loss: 0.359037] [G loss: 0.201348] [ema: 0.999789] 
[Epoch 50/78] [Batch 600/647] [D loss: 0.353733] [G loss: 0.191707] [ema: 0.999790] 
[Epoch 51/78] [Batch 0/647] [D loss: 0.414082] [G loss: 0.175103] [ema: 0.999790] 
[Epoch 51/78] [Batch 100/647] [D loss: 0.374757] [G loss: 0.189369] [ema: 0.999791] 
[Epoch 51/78] [Batch 200/647] [D loss: 0.410791] [G loss: 0.189834] [ema: 0.999791] 
[Epoch 51/78] [Batch 300/647] [D loss: 0.386837] [G loss: 0.186474] [ema: 0.999792] 
[Epoch 51/78] [Batch 400/647] [D loss: 0.434662] [G loss: 0.188657] [ema: 0.999792] 
[Epoch 51/78] [Batch 500/647] [D loss: 0.364786] [G loss: 0.174125] [ema: 0.999793] 
[Epoch 51/78] [Batch 600/647] [D loss: 0.342498] [G loss: 0.184429] [ema: 0.999794] 
[Epoch 52/78] [Batch 0/647] [D loss: 0.343257] [G loss: 0.188839] [ema: 0.999794] 
[Epoch 52/78] [Batch 100/647] [D loss: 0.366546] [G loss: 0.195969] [ema: 0.999795] 
[Epoch 52/78] [Batch 200/647] [D loss: 0.363979] [G loss: 0.200514] [ema: 0.999795] 
[Epoch 52/78] [Batch 300/647] [D loss: 0.385679] [G loss: 0.167778] [ema: 0.999796] 
[Epoch 52/78] [Batch 400/647] [D loss: 0.343176] [G loss: 0.215404] [ema: 0.999796] 
[Epoch 52/78] [Batch 500/647] [D loss: 0.354315] [G loss: 0.182854] [ema: 0.999797] 
[Epoch 52/78] [Batch 600/647] [D loss: 0.345800] [G loss: 0.202102] [ema: 0.999798] 
[Epoch 53/78] [Batch 0/647] [D loss: 0.380194] [G loss: 0.207744] [ema: 0.999798] 
[Epoch 53/78] [Batch 100/647] [D loss: 0.374588] [G loss: 0.200346] [ema: 0.999798] 
[Epoch 53/78] [Batch 200/647] [D loss: 0.440761] [G loss: 0.189653] [ema: 0.999799] 
[Epoch 53/78] [Batch 300/647] [D loss: 0.389991] [G loss: 0.187305] [ema: 0.999800] 
[Epoch 53/78] [Batch 400/647] [D loss: 0.370047] [G loss: 0.178009] [ema: 0.999800] 
[Epoch 53/78] [Batch 500/647] [D loss: 0.362441] [G loss: 0.189661] [ema: 0.999801] 
[Epoch 53/78] [Batch 600/647] [D loss: 0.335492] [G loss: 0.206610] [ema: 0.999801] 
[Epoch 54/78] [Batch 0/647] [D loss: 0.410062] [G loss: 0.184498] [ema: 0.999802] 
[Epoch 54/78] [Batch 100/647] [D loss: 0.407342] [G loss: 0.189627] [ema: 0.999802] 
[Epoch 54/78] [Batch 200/647] [D loss: 0.458109] [G loss: 0.168102] [ema: 0.999803] 
[Epoch 54/78] [Batch 300/647] [D loss: 0.374062] [G loss: 0.197248] [ema: 0.999803] 
[Epoch 54/78] [Batch 400/647] [D loss: 0.375985] [G loss: 0.197556] [ema: 0.999804] 
[Epoch 54/78] [Batch 500/647] [D loss: 0.387941] [G loss: 0.200969] [ema: 0.999804] 
[Epoch 54/78] [Batch 600/647] [D loss: 0.341931] [G loss: 0.195151] [ema: 0.999805] 
[Epoch 55/78] [Batch 0/647] [D loss: 0.382776] [G loss: 0.196596] [ema: 0.999805] 
[Epoch 55/78] [Batch 100/647] [D loss: 0.389315] [G loss: 0.194153] [ema: 0.999806] 
[Epoch 55/78] [Batch 200/647] [D loss: 0.366225] [G loss: 0.201859] [ema: 0.999806] 
[Epoch 55/78] [Batch 300/647] [D loss: 0.322712] [G loss: 0.197597] [ema: 0.999807] 
[Epoch 55/78] [Batch 400/647] [D loss: 0.363576] [G loss: 0.186545] [ema: 0.999807] 
[Epoch 55/78] [Batch 500/647] [D loss: 0.375975] [G loss: 0.200305] [ema: 0.999808] 
[Epoch 55/78] [Batch 600/647] [D loss: 0.371437] [G loss: 0.177412] [ema: 0.999808] 
[Epoch 56/78] [Batch 0/647] [D loss: 0.373435] [G loss: 0.184521] [ema: 0.999809] 
[Epoch 56/78] [Batch 100/647] [D loss: 0.375484] [G loss: 0.159438] [ema: 0.999809] 
[Epoch 56/78] [Batch 200/647] [D loss: 0.372444] [G loss: 0.188017] [ema: 0.999810] 
[Epoch 56/78] [Batch 300/647] [D loss: 0.426313] [G loss: 0.188742] [ema: 0.999810] 
[Epoch 56/78] [Batch 400/647] [D loss: 0.348492] [G loss: 0.200694] [ema: 0.999811] 
[Epoch 56/78] [Batch 500/647] [D loss: 0.410066] [G loss: 0.182319] [ema: 0.999811] 
[Epoch 56/78] [Batch 600/647] [D loss: 0.346143] [G loss: 0.176073] [ema: 0.999812] 
[Epoch 57/78] [Batch 0/647] [D loss: 0.408883] [G loss: 0.193024] [ema: 0.999812] 
[Epoch 57/78] [Batch 100/647] [D loss: 0.391382] [G loss: 0.178895] [ema: 0.999813] 
[Epoch 57/78] [Batch 200/647] [D loss: 0.364921] [G loss: 0.181074] [ema: 0.999813] 
[Epoch 57/78] [Batch 300/647] [D loss: 0.411029] [G loss: 0.190505] [ema: 0.999814] 
[Epoch 57/78] [Batch 400/647] [D loss: 0.433633] [G loss: 0.170821] [ema: 0.999814] 
[Epoch 57/78] [Batch 500/647] [D loss: 0.372229] [G loss: 0.178804] [ema: 0.999815] 
[Epoch 57/78] [Batch 600/647] [D loss: 0.454688] [G loss: 0.172470] [ema: 0.999815] 
[Epoch 58/78] [Batch 0/647] [D loss: 0.362663] [G loss: 0.190719] [ema: 0.999815] 
[Epoch 58/78] [Batch 100/647] [D loss: 0.346948] [G loss: 0.203834] [ema: 0.999816] 
[Epoch 58/78] [Batch 200/647] [D loss: 0.393854] [G loss: 0.181060] [ema: 0.999816] 
[Epoch 58/78] [Batch 300/647] [D loss: 0.376824] [G loss: 0.187739] [ema: 0.999817] 
[Epoch 58/78] [Batch 400/647] [D loss: 0.373906] [G loss: 0.197208] [ema: 0.999817] 
[Epoch 58/78] [Batch 500/647] [D loss: 0.417047] [G loss: 0.184728] [ema: 0.999818] 
[Epoch 58/78] [Batch 600/647] [D loss: 0.365085] [G loss: 0.165207] [ema: 0.999818] 
[Epoch 59/78] [Batch 0/647] [D loss: 0.365690] [G loss: 0.187476] [ema: 0.999818] 
[Epoch 59/78] [Batch 100/647] [D loss: 0.392091] [G loss: 0.191959] [ema: 0.999819] 
[Epoch 59/78] [Batch 200/647] [D loss: 0.408900] [G loss: 0.167131] [ema: 0.999819] 
[Epoch 59/78] [Batch 300/647] [D loss: 0.401671] [G loss: 0.180320] [ema: 0.999820] 
[Epoch 59/78] [Batch 400/647] [D loss: 0.356918] [G loss: 0.188216] [ema: 0.999820] 
[Epoch 59/78] [Batch 500/647] [D loss: 0.383321] [G loss: 0.168860] [ema: 0.999821] 
[Epoch 59/78] [Batch 600/647] [D loss: 0.400987] [G loss: 0.182093] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_60_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_60_2024_10_25_14_45_28/Model



[Epoch 60/78] [Batch 0/647] [D loss: 0.412331] [G loss: 0.191098] [ema: 0.999821] 
[Epoch 60/78] [Batch 100/647] [D loss: 0.377261] [G loss: 0.178820] [ema: 0.999822] 
[Epoch 60/78] [Batch 200/647] [D loss: 0.415252] [G loss: 0.184656] [ema: 0.999822] 
[Epoch 60/78] [Batch 300/647] [D loss: 0.357266] [G loss: 0.193959] [ema: 0.999823] 
[Epoch 60/78] [Batch 400/647] [D loss: 0.369649] [G loss: 0.187038] [ema: 0.999823] 
[Epoch 60/78] [Batch 500/647] [D loss: 0.363298] [G loss: 0.186372] [ema: 0.999824] 
[Epoch 60/78] [Batch 600/647] [D loss: 0.357868] [G loss: 0.167937] [ema: 0.999824] 
[Epoch 61/78] [Batch 0/647] [D loss: 0.397400] [G loss: 0.181903] [ema: 0.999824] 
[Epoch 61/78] [Batch 100/647] [D loss: 0.452867] [G loss: 0.175928] [ema: 0.999825] 
[Epoch 61/78] [Batch 200/647] [D loss: 0.390697] [G loss: 0.203625] [ema: 0.999825] 
[Epoch 61/78] [Batch 300/647] [D loss: 0.348248] [G loss: 0.203826] [ema: 0.999826] 
[Epoch 61/78] [Batch 400/647] [D loss: 0.385068] [G loss: 0.177514] [ema: 0.999826] 
[Epoch 61/78] [Batch 500/647] [D loss: 0.394942] [G loss: 0.188563] [ema: 0.999827] 
[Epoch 61/78] [Batch 600/647] [D loss: 0.354626] [G loss: 0.179351] [ema: 0.999827] 
[Epoch 62/78] [Batch 0/647] [D loss: 0.417412] [G loss: 0.198260] [ema: 0.999827] 
[Epoch 62/78] [Batch 100/647] [D loss: 0.329180] [G loss: 0.189142] [ema: 0.999828] 
[Epoch 62/78] [Batch 200/647] [D loss: 0.432056] [G loss: 0.185158] [ema: 0.999828] 
[Epoch 62/78] [Batch 300/647] [D loss: 0.411719] [G loss: 0.165843] [ema: 0.999829] 
[Epoch 62/78] [Batch 400/647] [D loss: 0.427453] [G loss: 0.175792] [ema: 0.999829] 
[Epoch 62/78] [Batch 500/647] [D loss: 0.382319] [G loss: 0.197606] [ema: 0.999829] 
[Epoch 62/78] [Batch 600/647] [D loss: 0.368718] [G loss: 0.182054] [ema: 0.999830] 
[Epoch 63/78] [Batch 0/647] [D loss: 0.399924] [G loss: 0.182217] [ema: 0.999830] 
[Epoch 63/78] [Batch 100/647] [D loss: 0.376308] [G loss: 0.186801] [ema: 0.999830] 
[Epoch 63/78] [Batch 200/647] [D loss: 0.395971] [G loss: 0.185714] [ema: 0.999831] 
[Epoch 63/78] [Batch 300/647] [D loss: 0.401020] [G loss: 0.168393] [ema: 0.999831] 
[Epoch 63/78] [Batch 400/647] [D loss: 0.362538] [G loss: 0.192015] [ema: 0.999832] 
[Epoch 63/78] [Batch 500/647] [D loss: 0.384501] [G loss: 0.193107] [ema: 0.999832] 
[Epoch 63/78] [Batch 600/647] [D loss: 0.405513] [G loss: 0.194010] [ema: 0.999832] 
[Epoch 64/78] [Batch 0/647] [D loss: 0.395036] [G loss: 0.198869] [ema: 0.999833] 
[Epoch 64/78] [Batch 100/647] [D loss: 0.401868] [G loss: 0.172649] [ema: 0.999833] 
[Epoch 64/78] [Batch 200/647] [D loss: 0.434099] [G loss: 0.179712] [ema: 0.999833] 
[Epoch 64/78] [Batch 300/647] [D loss: 0.440136] [G loss: 0.205699] [ema: 0.999834] 
[Epoch 64/78] [Batch 400/647] [D loss: 0.359242] [G loss: 0.175595] [ema: 0.999834] 
[Epoch 64/78] [Batch 500/647] [D loss: 0.365658] [G loss: 0.184777] [ema: 0.999835] 
[Epoch 64/78] [Batch 600/647] [D loss: 0.362608] [G loss: 0.176162] [ema: 0.999835] 
[Epoch 65/78] [Batch 0/647] [D loss: 0.394187] [G loss: 0.186900] [ema: 0.999835] 
[Epoch 65/78] [Batch 100/647] [D loss: 0.390388] [G loss: 0.166053] [ema: 0.999836] 
[Epoch 65/78] [Batch 200/647] [D loss: 0.364796] [G loss: 0.174235] [ema: 0.999836] 
[Epoch 65/78] [Batch 300/647] [D loss: 0.438991] [G loss: 0.183496] [ema: 0.999836] 
[Epoch 65/78] [Batch 400/647] [D loss: 0.388541] [G loss: 0.183267] [ema: 0.999837] 
[Epoch 65/78] [Batch 500/647] [D loss: 0.383163] [G loss: 0.174320] [ema: 0.999837] 
[Epoch 65/78] [Batch 600/647] [D loss: 0.397981] [G loss: 0.181633] [ema: 0.999838] 
[Epoch 66/78] [Batch 0/647] [D loss: 0.378367] [G loss: 0.196253] [ema: 0.999838] 
[Epoch 66/78] [Batch 100/647] [D loss: 0.401631] [G loss: 0.173870] [ema: 0.999838] 
[Epoch 66/78] [Batch 200/647] [D loss: 0.387757] [G loss: 0.184727] [ema: 0.999838] 
[Epoch 66/78] [Batch 300/647] [D loss: 0.384377] [G loss: 0.174305] [ema: 0.999839] 
[Epoch 66/78] [Batch 400/647] [D loss: 0.392718] [G loss: 0.200700] [ema: 0.999839] 
[Epoch 66/78] [Batch 500/647] [D loss: 0.438613] [G loss: 0.183805] [ema: 0.999840] 
[Epoch 66/78] [Batch 600/647] [D loss: 0.386012] [G loss: 0.179761] [ema: 0.999840] 
[Epoch 67/78] [Batch 0/647] [D loss: 0.464799] [G loss: 0.170962] [ema: 0.999840] 
[Epoch 67/78] [Batch 100/647] [D loss: 0.378233] [G loss: 0.178519] [ema: 0.999840] 
[Epoch 67/78] [Batch 200/647] [D loss: 0.352081] [G loss: 0.192371] [ema: 0.999841] 
[Epoch 67/78] [Batch 300/647] [D loss: 0.340567] [G loss: 0.194344] [ema: 0.999841] 
[Epoch 67/78] [Batch 400/647] [D loss: 0.363959] [G loss: 0.175886] [ema: 0.999842] 
[Epoch 67/78] [Batch 500/647] [D loss: 0.370993] [G loss: 0.180970] [ema: 0.999842] 
[Epoch 67/78] [Batch 600/647] [D loss: 0.379733] [G loss: 0.173515] [ema: 0.999842] 
[Epoch 68/78] [Batch 0/647] [D loss: 0.391078] [G loss: 0.181848] [ema: 0.999842] 
[Epoch 68/78] [Batch 100/647] [D loss: 0.438050] [G loss: 0.168866] [ema: 0.999843] 
[Epoch 68/78] [Batch 200/647] [D loss: 0.388578] [G loss: 0.205415] [ema: 0.999843] 
[Epoch 68/78] [Batch 300/647] [D loss: 0.398913] [G loss: 0.183519] [ema: 0.999844] 
[Epoch 68/78] [Batch 400/647] [D loss: 0.430732] [G loss: 0.170639] [ema: 0.999844] 
[Epoch 68/78] [Batch 500/647] [D loss: 0.395801] [G loss: 0.178645] [ema: 0.999844] 
[Epoch 68/78] [Batch 600/647] [D loss: 0.385123] [G loss: 0.194322] [ema: 0.999845] 
[Epoch 69/78] [Batch 0/647] [D loss: 0.380451] [G loss: 0.188329] [ema: 0.999845] 
[Epoch 69/78] [Batch 100/647] [D loss: 0.414464] [G loss: 0.179671] [ema: 0.999845] 
[Epoch 69/78] [Batch 200/647] [D loss: 0.393268] [G loss: 0.179884] [ema: 0.999845] 
[Epoch 69/78] [Batch 300/647] [D loss: 0.417531] [G loss: 0.177372] [ema: 0.999846] 
[Epoch 69/78] [Batch 400/647] [D loss: 0.345213] [G loss: 0.193709] [ema: 0.999846] 
[Epoch 69/78] [Batch 500/647] [D loss: 0.384934] [G loss: 0.170210] [ema: 0.999846] 
[Epoch 69/78] [Batch 600/647] [D loss: 0.412233] [G loss: 0.190187] [ema: 0.999847] 
[Epoch 70/78] [Batch 0/647] [D loss: 0.343753] [G loss: 0.195371] [ema: 0.999847] 
[Epoch 70/78] [Batch 100/647] [D loss: 0.386154] [G loss: 0.174928] [ema: 0.999847] 
[Epoch 70/78] [Batch 200/647] [D loss: 0.376314] [G loss: 0.161659] [ema: 0.999848] 
[Epoch 70/78] [Batch 300/647] [D loss: 0.394043] [G loss: 0.178558] [ema: 0.999848] 
[Epoch 70/78] [Batch 400/647] [D loss: 0.398772] [G loss: 0.164530] [ema: 0.999848] 
[Epoch 70/78] [Batch 500/647] [D loss: 0.404063] [G loss: 0.181455] [ema: 0.999849] 
[Epoch 70/78] [Batch 600/647] [D loss: 0.359012] [G loss: 0.185817] [ema: 0.999849] 
[Epoch 71/78] [Batch 0/647] [D loss: 0.420714] [G loss: 0.191854] [ema: 0.999849] 
[Epoch 71/78] [Batch 100/647] [D loss: 0.379627] [G loss: 0.196262] [ema: 0.999849] 
[Epoch 71/78] [Batch 200/647] [D loss: 0.405338] [G loss: 0.172848] [ema: 0.999850] 
[Epoch 71/78] [Batch 300/647] [D loss: 0.401993] [G loss: 0.177942] [ema: 0.999850] 
[Epoch 71/78] [Batch 400/647] [D loss: 0.362717] [G loss: 0.186952] [ema: 0.999850] 
[Epoch 71/78] [Batch 500/647] [D loss: 0.397505] [G loss: 0.173431] [ema: 0.999851] 
[Epoch 71/78] [Batch 600/647] [D loss: 0.397472] [G loss: 0.187219] [ema: 0.999851] 
[Epoch 72/78] [Batch 0/647] [D loss: 0.367122] [G loss: 0.172621] [ema: 0.999851] 
[Epoch 72/78] [Batch 100/647] [D loss: 0.413405] [G loss: 0.172891] [ema: 0.999852] 
[Epoch 72/78] [Batch 200/647] [D loss: 0.360269] [G loss: 0.187923] [ema: 0.999852] 
[Epoch 72/78] [Batch 300/647] [D loss: 0.385375] [G loss: 0.184602] [ema: 0.999852] 
[Epoch 72/78] [Batch 400/647] [D loss: 0.425510] [G loss: 0.173801] [ema: 0.999852] 
[Epoch 72/78] [Batch 500/647] [D loss: 0.396936] [G loss: 0.171269] [ema: 0.999853] 
[Epoch 72/78] [Batch 600/647] [D loss: 0.386247] [G loss: 0.180079] [ema: 0.999853] 
[Epoch 73/78] [Batch 0/647] [D loss: 0.383186] [G loss: 0.180469] [ema: 0.999853] 
[Epoch 73/78] [Batch 100/647] [D loss: 0.386940] [G loss: 0.185858] [ema: 0.999854] 
[Epoch 73/78] [Batch 200/647] [D loss: 0.371428] [G loss: 0.187474] [ema: 0.999854] 
[Epoch 73/78] [Batch 300/647] [D loss: 0.393313] [G loss: 0.174424] [ema: 0.999854] 
[Epoch 73/78] [Batch 400/647] [D loss: 0.366167] [G loss: 0.181807] [ema: 0.999854] 
[Epoch 73/78] [Batch 500/647] [D loss: 0.423723] [G loss: 0.184982] [ema: 0.999855] 
[Epoch 73/78] [Batch 600/647] [D loss: 0.366250] [G loss: 0.196503] [ema: 0.999855] 
[Epoch 74/78] [Batch 0/647] [D loss: 0.426238] [G loss: 0.181423] [ema: 0.999855] 
[Epoch 74/78] [Batch 100/647] [D loss: 0.387549] [G loss: 0.169739] [ema: 0.999856] 
[Epoch 74/78] [Batch 200/647] [D loss: 0.356657] [G loss: 0.180237] [ema: 0.999856] 
[Epoch 74/78] [Batch 300/647] [D loss: 0.402276] [G loss: 0.181254] [ema: 0.999856] 
[Epoch 74/78] [Batch 400/647] [D loss: 0.397541] [G loss: 0.185046] [ema: 0.999856] 
[Epoch 74/78] [Batch 500/647] [D loss: 0.403804] [G loss: 0.182744] [ema: 0.999857] 
[Epoch 74/78] [Batch 600/647] [D loss: 0.367793] [G loss: 0.182203] [ema: 0.999857] 
[Epoch 75/78] [Batch 0/647] [D loss: 0.412198] [G loss: 0.180419] [ema: 0.999857] 
[Epoch 75/78] [Batch 100/647] [D loss: 0.395391] [G loss: 0.171537] [ema: 0.999857] 
[Epoch 75/78] [Batch 200/647] [D loss: 0.393642] [G loss: 0.166548] [ema: 0.999858] 
[Epoch 75/78] [Batch 300/647] [D loss: 0.413951] [G loss: 0.180077] [ema: 0.999858] 
[Epoch 75/78] [Batch 400/647] [D loss: 0.410210] [G loss: 0.172437] [ema: 0.999858] 
[Epoch 75/78] [Batch 500/647] [D loss: 0.374971] [G loss: 0.172774] [ema: 0.999859] 
[Epoch 75/78] [Batch 600/647] [D loss: 0.407080] [G loss: 0.177686] [ema: 0.999859] 
[Epoch 76/78] [Batch 0/647] [D loss: 0.400899] [G loss: 0.190206] [ema: 0.999859] 
[Epoch 76/78] [Batch 100/647] [D loss: 0.418079] [G loss: 0.172021] [ema: 0.999859] 
[Epoch 76/78] [Batch 200/647] [D loss: 0.355135] [G loss: 0.170720] [ema: 0.999860] 
[Epoch 76/78] [Batch 300/647] [D loss: 0.388629] [G loss: 0.176734] [ema: 0.999860] 
[Epoch 76/78] [Batch 400/647] [D loss: 0.412724] [G loss: 0.181712] [ema: 0.999860] 
[Epoch 76/78] [Batch 500/647] [D loss: 0.429474] [G loss: 0.176893] [ema: 0.999860] 
[Epoch 76/78] [Batch 600/647] [D loss: 0.380919] [G loss: 0.180358] [ema: 0.999861] 
[Epoch 77/78] [Batch 0/647] [D loss: 0.451029] [G loss: 0.178513] [ema: 0.999861] 
[Epoch 77/78] [Batch 100/647] [D loss: 0.394185] [G loss: 0.179731] [ema: 0.999861] 
[Epoch 77/78] [Batch 200/647] [D loss: 0.379133] [G loss: 0.179135] [ema: 0.999861] 
[Epoch 77/78] [Batch 300/647] [D loss: 0.384381] [G loss: 0.184018] [ema: 0.999862] 
[Epoch 77/78] [Batch 400/647] [D loss: 0.433218] [G loss: 0.161972] [ema: 0.999862] 
[Epoch 77/78] [Batch 500/647] [D loss: 0.369202] [G loss: 0.181722] [ema: 0.999862] 
[Epoch 77/78] [Batch 600/647] [D loss: 0.391067] [G loss: 0.170876] [ema: 0.999863] 

----------------------------------------------------------------------------------------------------

 Starting individual training
WISDM_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
WISDM_DAGHAR_Multiclass
daghar
return single class data and labels, class is WISDM_DAGHAR_Multiclass
data shape is (8748, 3, 1, 60)
label shape is (8748,)
547
Epochs between checkpoint: 23



Saving checkpoint 1 in logs/daghar_split_dataset_50000_60_100/WISDM_DAGHAR_Multiclass_50000_D_60_2024_10_25_15_19_34/Model



[Epoch 0/92] [Batch 0/547] [D loss: 1.250115] [G loss: 0.441236] [ema: 0.000000] 
[Epoch 0/92] [Batch 100/547] [D loss: 0.650164] [G loss: 0.179921] [ema: 0.933033] 
[Epoch 0/92] [Batch 200/547] [D loss: 0.518765] [G loss: 0.148880] [ema: 0.965936] 
[Epoch 0/92] [Batch 300/547] [D loss: 0.505665] [G loss: 0.170462] [ema: 0.977160] 
[Epoch 0/92] [Batch 400/547] [D loss: 0.508105] [G loss: 0.137308] [ema: 0.982821] 
[Epoch 0/92] [Batch 500/547] [D loss: 0.457551] [G loss: 0.168704] [ema: 0.986233] 
[Epoch 1/92] [Batch 0/547] [D loss: 0.436266] [G loss: 0.197765] [ema: 0.987408] 
[Epoch 1/92] [Batch 100/547] [D loss: 0.502829] [G loss: 0.183715] [ema: 0.989344] 
[Epoch 1/92] [Batch 200/547] [D loss: 0.444000] [G loss: 0.166544] [ema: 0.990764] 
[Epoch 1/92] [Batch 300/547] [D loss: 0.503676] [G loss: 0.137166] [ema: 0.991850] 
[Epoch 1/92] [Batch 400/547] [D loss: 0.500382] [G loss: 0.131222] [ema: 0.992707] 
[Epoch 1/92] [Batch 500/547] [D loss: 0.462231] [G loss: 0.164311] [ema: 0.993402] 
[Epoch 2/92] [Batch 0/547] [D loss: 0.460707] [G loss: 0.191688] [ema: 0.993684] 
[Epoch 2/92] [Batch 100/547] [D loss: 0.522651] [G loss: 0.129013] [ema: 0.994212] 
[Epoch 2/92] [Batch 200/547] [D loss: 0.482345] [G loss: 0.153446] [ema: 0.994658] 
[Epoch 2/92] [Batch 300/547] [D loss: 0.520027] [G loss: 0.129242] [ema: 0.995040] 
[Epoch 2/92] [Batch 400/547] [D loss: 0.419358] [G loss: 0.175593] [ema: 0.995371] 
[Epoch 2/92] [Batch 500/547] [D loss: 0.502732] [G loss: 0.146627] [ema: 0.995661] 
[Epoch 3/92] [Batch 0/547] [D loss: 0.488434] [G loss: 0.170162] [ema: 0.995785] 
[Epoch 3/92] [Batch 100/547] [D loss: 0.472753] [G loss: 0.116598] [ema: 0.996027] 
[Epoch 3/92] [Batch 200/547] [D loss: 0.443596] [G loss: 0.166127] [ema: 0.996242] 
[Epoch 3/92] [Batch 300/547] [D loss: 0.479685] [G loss: 0.160674] [ema: 0.996435] 
[Epoch 3/92] [Batch 400/547] [D loss: 0.395301] [G loss: 0.159212] [ema: 0.996610] 
[Epoch 3/92] [Batch 500/547] [D loss: 0.431464] [G loss: 0.176511] [ema: 0.996768] 
[Epoch 4/92] [Batch 0/547] [D loss: 0.425493] [G loss: 0.182018] [ema: 0.996837] 
[Epoch 4/92] [Batch 100/547] [D loss: 0.496972] [G loss: 0.148284] [ema: 0.996975] 
[Epoch 4/92] [Batch 200/547] [D loss: 0.455361] [G loss: 0.160262] [ema: 0.997102] 
[Epoch 4/92] [Batch 300/547] [D loss: 0.433669] [G loss: 0.172403] [ema: 0.997218] 
[Epoch 4/92] [Batch 400/547] [D loss: 0.404739] [G loss: 0.167704] [ema: 0.997325] 
[Epoch 4/92] [Batch 500/547] [D loss: 0.443854] [G loss: 0.157555] [ema: 0.997425] 
[Epoch 5/92] [Batch 0/547] [D loss: 0.467344] [G loss: 0.162849] [ema: 0.997469] 
[Epoch 5/92] [Batch 100/547] [D loss: 0.449229] [G loss: 0.151426] [ema: 0.997558] 
[Epoch 5/92] [Batch 200/547] [D loss: 0.417510] [G loss: 0.165982] [ema: 0.997641] 
[Epoch 5/92] [Batch 300/547] [D loss: 0.451142] [G loss: 0.167512] [ema: 0.997719] 
[Epoch 5/92] [Batch 400/547] [D loss: 0.440414] [G loss: 0.174403] [ema: 0.997791] 
[Epoch 5/92] [Batch 500/547] [D loss: 0.418780] [G loss: 0.184443] [ema: 0.997860] 
[Epoch 6/92] [Batch 0/547] [D loss: 0.431600] [G loss: 0.191320] [ema: 0.997890] 
[Epoch 6/92] [Batch 100/547] [D loss: 0.443893] [G loss: 0.156749] [ema: 0.997953] 
[Epoch 6/92] [Batch 200/547] [D loss: 0.475373] [G loss: 0.181275] [ema: 0.998011] 
[Epoch 6/92] [Batch 300/547] [D loss: 0.441922] [G loss: 0.154931] [ema: 0.998067] 
[Epoch 6/92] [Batch 400/547] [D loss: 0.428086] [G loss: 0.152762] [ema: 0.998119] 
[Epoch 6/92] [Batch 500/547] [D loss: 0.447066] [G loss: 0.163097] [ema: 0.998169] 
[Epoch 7/92] [Batch 0/547] [D loss: 0.458970] [G loss: 0.160428] [ema: 0.998191] 
[Epoch 7/92] [Batch 100/547] [D loss: 0.383752] [G loss: 0.163936] [ema: 0.998237] 
[Epoch 7/92] [Batch 200/547] [D loss: 0.443302] [G loss: 0.166851] [ema: 0.998281] 
[Epoch 7/92] [Batch 300/547] [D loss: 0.418184] [G loss: 0.155250] [ema: 0.998323] 
[Epoch 7/92] [Batch 400/547] [D loss: 0.467303] [G loss: 0.140451] [ema: 0.998362] 
[Epoch 7/92] [Batch 500/547] [D loss: 0.459058] [G loss: 0.139097] [ema: 0.998400] 
[Epoch 8/92] [Batch 0/547] [D loss: 0.446456] [G loss: 0.166650] [ema: 0.998417] 
[Epoch 8/92] [Batch 100/547] [D loss: 0.421134] [G loss: 0.162521] [ema: 0.998453] 
[Epoch 8/92] [Batch 200/547] [D loss: 0.461161] [G loss: 0.164219] [ema: 0.998486] 
[Epoch 8/92] [Batch 300/547] [D loss: 0.435062] [G loss: 0.150338] [ema: 0.998519] 
[Epoch 8/92] [Batch 400/547] [D loss: 0.448367] [G loss: 0.161728] [ema: 0.998550] 
[Epoch 8/92] [Batch 500/547] [D loss: 0.414421] [G loss: 0.158876] [ema: 0.998579] 
[Epoch 9/92] [Batch 0/547] [D loss: 0.400268] [G loss: 0.176392] [ema: 0.998593] 
[Epoch 9/92] [Batch 100/547] [D loss: 0.480510] [G loss: 0.146258] [ema: 0.998621] 
[Epoch 9/92] [Batch 200/547] [D loss: 0.404526] [G loss: 0.186047] [ema: 0.998648] 
[Epoch 9/92] [Batch 300/547] [D loss: 0.438904] [G loss: 0.159831] [ema: 0.998674] 
[Epoch 9/92] [Batch 400/547] [D loss: 0.441019] [G loss: 0.166454] [ema: 0.998699] 
[Epoch 9/92] [Batch 500/547] [D loss: 0.430170] [G loss: 0.155747] [ema: 0.998723] 
[Epoch 10/92] [Batch 0/547] [D loss: 0.480415] [G loss: 0.158140] [ema: 0.998734] 
[Epoch 10/92] [Batch 100/547] [D loss: 0.434554] [G loss: 0.164613] [ema: 0.998756] 
[Epoch 10/92] [Batch 200/547] [D loss: 0.463575] [G loss: 0.157798] [ema: 0.998778] 
[Epoch 10/92] [Batch 300/547] [D loss: 0.443562] [G loss: 0.157025] [ema: 0.998799] 
[Epoch 10/92] [Batch 400/547] [D loss: 0.458504] [G loss: 0.158814] [ema: 0.998820] 
[Epoch 10/92] [Batch 500/547] [D loss: 0.422178] [G loss: 0.169336] [ema: 0.998840] 
[Epoch 11/92] [Batch 0/547] [D loss: 0.419656] [G loss: 0.168114] [ema: 0.998849] 
[Epoch 11/92] [Batch 100/547] [D loss: 0.420884] [G loss: 0.171799] [ema: 0.998867] 
[Epoch 11/92] [Batch 200/547] [D loss: 0.427499] [G loss: 0.164945] [ema: 0.998886] 
[Epoch 11/92] [Batch 300/547] [D loss: 0.433664] [G loss: 0.175333] [ema: 0.998903] 
[Epoch 11/92] [Batch 400/547] [D loss: 0.444555] [G loss: 0.161208] [ema: 0.998920] 
[Epoch 11/92] [Batch 500/547] [D loss: 0.469516] [G loss: 0.158869] [ema: 0.998937] 
[Epoch 12/92] [Batch 0/547] [D loss: 0.465773] [G loss: 0.164570] [ema: 0.998945] 
[Epoch 12/92] [Batch 100/547] [D loss: 0.419970] [G loss: 0.171233] [ema: 0.998960] 
[Epoch 12/92] [Batch 200/547] [D loss: 0.432343] [G loss: 0.171170] [ema: 0.998976] 
[Epoch 12/92] [Batch 300/547] [D loss: 0.425132] [G loss: 0.177101] [ema: 0.998991] 
[Epoch 12/92] [Batch 400/547] [D loss: 0.452709] [G loss: 0.154357] [ema: 0.999005] 
[Epoch 12/92] [Batch 500/547] [D loss: 0.407979] [G loss: 0.171092] [ema: 0.999019] 
[Epoch 13/92] [Batch 0/547] [D loss: 0.417148] [G loss: 0.169693] [ema: 0.999026] 
[Epoch 13/92] [Batch 100/547] [D loss: 0.406972] [G loss: 0.166203] [ema: 0.999039] 
[Epoch 13/92] [Batch 200/547] [D loss: 0.399067] [G loss: 0.170611] [ema: 0.999052] 
[Epoch 13/92] [Batch 300/547] [D loss: 0.439165] [G loss: 0.159598] [ema: 0.999065] 
[Epoch 13/92] [Batch 400/547] [D loss: 0.435545] [G loss: 0.155816] [ema: 0.999078] 
[Epoch 13/92] [Batch 500/547] [D loss: 0.389069] [G loss: 0.173109] [ema: 0.999090] 
[Epoch 14/92] [Batch 0/547] [D loss: 0.430956] [G loss: 0.168805] [ema: 0.999095] 
[Epoch 14/92] [Batch 100/547] [D loss: 0.420943] [G loss: 0.173984] [ema: 0.999107] 
[Epoch 14/92] [Batch 200/547] [D loss: 0.405406] [G loss: 0.164843] [ema: 0.999118] 
[Epoch 14/92] [Batch 300/547] [D loss: 0.421517] [G loss: 0.145614] [ema: 0.999129] 
[Epoch 14/92] [Batch 400/547] [D loss: 0.429087] [G loss: 0.166105] [ema: 0.999140] 
[Epoch 14/92] [Batch 500/547] [D loss: 0.407799] [G loss: 0.178099] [ema: 0.999151] 
[Epoch 15/92] [Batch 0/547] [D loss: 0.400989] [G loss: 0.205223] [ema: 0.999156] 
[Epoch 15/92] [Batch 100/547] [D loss: 0.423598] [G loss: 0.164011] [ema: 0.999166] 
[Epoch 15/92] [Batch 200/547] [D loss: 0.406806] [G loss: 0.181343] [ema: 0.999176] 
[Epoch 15/92] [Batch 300/547] [D loss: 0.427710] [G loss: 0.173358] [ema: 0.999185] 
[Epoch 15/92] [Batch 400/547] [D loss: 0.399808] [G loss: 0.171088] [ema: 0.999195] 
[Epoch 15/92] [Batch 500/547] [D loss: 0.438023] [G loss: 0.155325] [ema: 0.999204] 
[Epoch 16/92] [Batch 0/547] [D loss: 0.430326] [G loss: 0.181436] [ema: 0.999208] 
[Epoch 16/92] [Batch 100/547] [D loss: 0.418465] [G loss: 0.185391] [ema: 0.999217] 
[Epoch 16/92] [Batch 200/547] [D loss: 0.438975] [G loss: 0.169267] [ema: 0.999226] 
[Epoch 16/92] [Batch 300/547] [D loss: 0.391079] [G loss: 0.161483] [ema: 0.999235] 
[Epoch 16/92] [Batch 400/547] [D loss: 0.419908] [G loss: 0.169647] [ema: 0.999243] 
[Epoch 16/92] [Batch 500/547] [D loss: 0.414828] [G loss: 0.188475] [ema: 0.999251] 
[Epoch 17/92] [Batch 0/547] [D loss: 0.385277] [G loss: 0.168152] [ema: 0.999255] 
[Epoch 17/92] [Batch 100/547] [D loss: 0.369666] [G loss: 0.179495] [ema: 0.999263] 
[Epoch 17/92] [Batch 200/547] [D loss: 0.393310] [G loss: 0.177043] [ema: 0.999271] 
[Epoch 17/92] [Batch 300/547] [D loss: 0.422244] [G loss: 0.158924] [ema: 0.999278] 
[Epoch 17/92] [Batch 400/547] [D loss: 0.411727] [G loss: 0.156070] [ema: 0.999286] 
[Epoch 17/92] [Batch 500/547] [D loss: 0.429753] [G loss: 0.170705] [ema: 0.999293] 
[Epoch 18/92] [Batch 0/547] [D loss: 0.435697] [G loss: 0.192428] [ema: 0.999296] 
[Epoch 18/92] [Batch 100/547] [D loss: 0.402453] [G loss: 0.168949] [ema: 0.999303] 
[Epoch 18/92] [Batch 200/547] [D loss: 0.450644] [G loss: 0.181287] [ema: 0.999310] 
[Epoch 18/92] [Batch 300/547] [D loss: 0.422177] [G loss: 0.168256] [ema: 0.999317] 
[Epoch 18/92] [Batch 400/547] [D loss: 0.431783] [G loss: 0.170221] [ema: 0.999324] 
[Epoch 18/92] [Batch 500/547] [D loss: 0.444820] [G loss: 0.174196] [ema: 0.999330] 
[Epoch 19/92] [Batch 0/547] [D loss: 0.383005] [G loss: 0.164592] [ema: 0.999333] 
[Epoch 19/92] [Batch 100/547] [D loss: 0.430043] [G loss: 0.172961] [ema: 0.999340] 
[Epoch 19/92] [Batch 200/547] [D loss: 0.410880] [G loss: 0.163396] [ema: 0.999346] 
[Epoch 19/92] [Batch 300/547] [D loss: 0.382318] [G loss: 0.186356] [ema: 0.999352] 
[Epoch 19/92] [Batch 400/547] [D loss: 0.432033] [G loss: 0.165918] [ema: 0.999358] 
[Epoch 19/92] [Batch 500/547] [D loss: 0.449707] [G loss: 0.174087] [ema: 0.999364] 
[Epoch 20/92] [Batch 0/547] [D loss: 0.407304] [G loss: 0.170822] [ema: 0.999367] 
[Epoch 20/92] [Batch 100/547] [D loss: 0.414010] [G loss: 0.172284] [ema: 0.999372] 
[Epoch 20/92] [Batch 200/547] [D loss: 0.418673] [G loss: 0.167603] [ema: 0.999378] 
[Epoch 20/92] [Batch 300/547] [D loss: 0.343425] [G loss: 0.187626] [ema: 0.999384] 
[Epoch 20/92] [Batch 400/547] [D loss: 0.435887] [G loss: 0.191174] [ema: 0.999389] 
[Epoch 20/92] [Batch 500/547] [D loss: 0.401099] [G loss: 0.169615] [ema: 0.999394] 
[Epoch 21/92] [Batch 0/547] [D loss: 0.399030] [G loss: 0.181730] [ema: 0.999397] 
[Epoch 21/92] [Batch 100/547] [D loss: 0.380821] [G loss: 0.190573] [ema: 0.999402] 
[Epoch 21/92] [Batch 200/547] [D loss: 0.415879] [G loss: 0.166503] [ema: 0.999407] 
[Epoch 21/92] [Batch 300/547] [D loss: 0.468264] [G loss: 0.156354] [ema: 0.999412] 
[Epoch 21/92] [Batch 400/547] [D loss: 0.403892] [G loss: 0.162121] [ema: 0.999417] 
[Epoch 21/92] [Batch 500/547] [D loss: 0.364612] [G loss: 0.174295] [ema: 0.999422] 
[Epoch 22/92] [Batch 0/547] [D loss: 0.401152] [G loss: 0.179957] [ema: 0.999424] 
[Epoch 22/92] [Batch 100/547] [D loss: 0.428370] [G loss: 0.169171] [ema: 0.999429] 
[Epoch 22/92] [Batch 200/547] [D loss: 0.395530] [G loss: 0.172469] [ema: 0.999434] 
[Epoch 22/92] [Batch 300/547] [D loss: 0.405494] [G loss: 0.172844] [ema: 0.999438] 
[Epoch 22/92] [Batch 400/547] [D loss: 0.381041] [G loss: 0.175255] [ema: 0.999443] 
[Epoch 22/92] [Batch 500/547] [D loss: 0.476609] [G loss: 0.177815] [ema: 0.999447] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_60_100/WISDM_DAGHAR_Multiclass_50000_D_60_2024_10_25_15_19_34/Model



[Epoch 23/92] [Batch 0/547] [D loss: 0.444968] [G loss: 0.183591] [ema: 0.999449] 
[Epoch 23/92] [Batch 100/547] [D loss: 0.427525] [G loss: 0.173057] [ema: 0.999454] 
[Epoch 23/92] [Batch 200/547] [D loss: 0.395226] [G loss: 0.173683] [ema: 0.999458] 
[Epoch 23/92] [Batch 300/547] [D loss: 0.415680] [G loss: 0.174196] [ema: 0.999462] 
[Epoch 23/92] [Batch 400/547] [D loss: 0.418712] [G loss: 0.198698] [ema: 0.999466] 
[Epoch 23/92] [Batch 500/547] [D loss: 0.393672] [G loss: 0.176996] [ema: 0.999470] 
[Epoch 24/92] [Batch 0/547] [D loss: 0.412313] [G loss: 0.181588] [ema: 0.999472] 
[Epoch 24/92] [Batch 100/547] [D loss: 0.403001] [G loss: 0.157976] [ema: 0.999476] 
[Epoch 24/92] [Batch 200/547] [D loss: 0.411024] [G loss: 0.195543] [ema: 0.999480] 
[Epoch 24/92] [Batch 300/547] [D loss: 0.431010] [G loss: 0.146311] [ema: 0.999484] 
[Epoch 24/92] [Batch 400/547] [D loss: 0.380171] [G loss: 0.167919] [ema: 0.999488] 
[Epoch 24/92] [Batch 500/547] [D loss: 0.394509] [G loss: 0.178295] [ema: 0.999492] 
[Epoch 25/92] [Batch 0/547] [D loss: 0.382023] [G loss: 0.181797] [ema: 0.999493] 
[Epoch 25/92] [Batch 100/547] [D loss: 0.424852] [G loss: 0.169616] [ema: 0.999497] 
[Epoch 25/92] [Batch 200/547] [D loss: 0.433975] [G loss: 0.174708] [ema: 0.999501] 
[Epoch 25/92] [Batch 300/547] [D loss: 0.424930] [G loss: 0.179060] [ema: 0.999504] 
[Epoch 25/92] [Batch 400/547] [D loss: 0.432162] [G loss: 0.166516] [ema: 0.999508] 
[Epoch 25/92] [Batch 500/547] [D loss: 0.405262] [G loss: 0.169422] [ema: 0.999511] 
[Epoch 26/92] [Batch 0/547] [D loss: 0.409577] [G loss: 0.170548] [ema: 0.999513] 
[Epoch 26/92] [Batch 100/547] [D loss: 0.410403] [G loss: 0.180268] [ema: 0.999516] 
[Epoch 26/92] [Batch 200/547] [D loss: 0.376493] [G loss: 0.187011] [ema: 0.999519] 
[Epoch 26/92] [Batch 300/547] [D loss: 0.408211] [G loss: 0.189139] [ema: 0.999523] 
[Epoch 26/92] [Batch 400/547] [D loss: 0.428417] [G loss: 0.159157] [ema: 0.999526] 
[Epoch 26/92] [Batch 500/547] [D loss: 0.390725] [G loss: 0.190632] [ema: 0.999529] 
[Epoch 27/92] [Batch 0/547] [D loss: 0.419463] [G loss: 0.199500] [ema: 0.999531] 
[Epoch 27/92] [Batch 100/547] [D loss: 0.381956] [G loss: 0.188636] [ema: 0.999534] 
[Epoch 27/92] [Batch 200/547] [D loss: 0.418647] [G loss: 0.159499] [ema: 0.999537] 
[Epoch 27/92] [Batch 300/547] [D loss: 0.418997] [G loss: 0.184005] [ema: 0.999540] 
[Epoch 27/92] [Batch 400/547] [D loss: 0.429470] [G loss: 0.179855] [ema: 0.999543] 
[Epoch 27/92] [Batch 500/547] [D loss: 0.398082] [G loss: 0.186912] [ema: 0.999546] 
[Epoch 28/92] [Batch 0/547] [D loss: 0.417643] [G loss: 0.173776] [ema: 0.999548] 
[Epoch 28/92] [Batch 100/547] [D loss: 0.405827] [G loss: 0.173200] [ema: 0.999550] 
[Epoch 28/92] [Batch 200/547] [D loss: 0.408202] [G loss: 0.165352] [ema: 0.999553] 
[Epoch 28/92] [Batch 300/547] [D loss: 0.399111] [G loss: 0.194774] [ema: 0.999556] 
[Epoch 28/92] [Batch 400/547] [D loss: 0.462135] [G loss: 0.168116] [ema: 0.999559] 
[Epoch 28/92] [Batch 500/547] [D loss: 0.355255] [G loss: 0.186218] [ema: 0.999562] 
[Epoch 29/92] [Batch 0/547] [D loss: 0.441783] [G loss: 0.173893] [ema: 0.999563] 
[Epoch 29/92] [Batch 100/547] [D loss: 0.372252] [G loss: 0.180057] [ema: 0.999566] 
[Epoch 29/92] [Batch 200/547] [D loss: 0.399383] [G loss: 0.167695] [ema: 0.999569] 
[Epoch 29/92] [Batch 300/547] [D loss: 0.386411] [G loss: 0.169240] [ema: 0.999571] 
[Epoch 29/92] [Batch 400/547] [D loss: 0.371404] [G loss: 0.175089] [ema: 0.999574] 
[Epoch 29/92] [Batch 500/547] [D loss: 0.412991] [G loss: 0.174576] [ema: 0.999576] 
[Epoch 30/92] [Batch 0/547] [D loss: 0.408525] [G loss: 0.188203] [ema: 0.999578] 
[Epoch 30/92] [Batch 100/547] [D loss: 0.397952] [G loss: 0.189727] [ema: 0.999580] 
[Epoch 30/92] [Batch 200/547] [D loss: 0.391140] [G loss: 0.166304] [ema: 0.999583] 
[Epoch 30/92] [Batch 300/547] [D loss: 0.412201] [G loss: 0.155475] [ema: 0.999585] 
[Epoch 30/92] [Batch 400/547] [D loss: 0.423694] [G loss: 0.171123] [ema: 0.999588] 
[Epoch 30/92] [Batch 500/547] [D loss: 0.421262] [G loss: 0.180474] [ema: 0.999590] 
[Epoch 31/92] [Batch 0/547] [D loss: 0.424387] [G loss: 0.187735] [ema: 0.999591] 
[Epoch 31/92] [Batch 100/547] [D loss: 0.428167] [G loss: 0.162233] [ema: 0.999594] 
[Epoch 31/92] [Batch 200/547] [D loss: 0.452191] [G loss: 0.179569] [ema: 0.999596] 
[Epoch 31/92] [Batch 300/547] [D loss: 0.395472] [G loss: 0.184918] [ema: 0.999598] 
[Epoch 31/92] [Batch 400/547] [D loss: 0.380828] [G loss: 0.166576] [ema: 0.999601] 
[Epoch 31/92] [Batch 500/547] [D loss: 0.377366] [G loss: 0.180231] [ema: 0.999603] 
[Epoch 32/92] [Batch 0/547] [D loss: 0.397383] [G loss: 0.188609] [ema: 0.999604] 
[Epoch 32/92] [Batch 100/547] [D loss: 0.443153] [G loss: 0.191274] [ema: 0.999606] 
[Epoch 32/92] [Batch 200/547] [D loss: 0.364261] [G loss: 0.190625] [ema: 0.999609] 
[Epoch 32/92] [Batch 300/547] [D loss: 0.370567] [G loss: 0.175833] [ema: 0.999611] 
[Epoch 32/92] [Batch 400/547] [D loss: 0.444608] [G loss: 0.175365] [ema: 0.999613] 
[Epoch 32/92] [Batch 500/547] [D loss: 0.423342] [G loss: 0.183526] [ema: 0.999615] 
[Epoch 33/92] [Batch 0/547] [D loss: 0.452683] [G loss: 0.175406] [ema: 0.999616] 
[Epoch 33/92] [Batch 100/547] [D loss: 0.357967] [G loss: 0.179848] [ema: 0.999618] 
[Epoch 33/92] [Batch 200/547] [D loss: 0.424345] [G loss: 0.172456] [ema: 0.999620] 
[Epoch 33/92] [Batch 300/547] [D loss: 0.401679] [G loss: 0.189772] [ema: 0.999622] 
[Epoch 33/92] [Batch 400/547] [D loss: 0.398009] [G loss: 0.173771] [ema: 0.999624] 
[Epoch 33/92] [Batch 500/547] [D loss: 0.416575] [G loss: 0.161775] [ema: 0.999626] 
[Epoch 34/92] [Batch 0/547] [D loss: 0.453609] [G loss: 0.180338] [ema: 0.999627] 
[Epoch 34/92] [Batch 100/547] [D loss: 0.423466] [G loss: 0.169944] [ema: 0.999629] 
[Epoch 34/92] [Batch 200/547] [D loss: 0.411677] [G loss: 0.177149] [ema: 0.999631] 
[Epoch 34/92] [Batch 300/547] [D loss: 0.429082] [G loss: 0.164164] [ema: 0.999633] 
[Epoch 34/92] [Batch 400/547] [D loss: 0.405891] [G loss: 0.174136] [ema: 0.999635] 
[Epoch 34/92] [Batch 500/547] [D loss: 0.444462] [G loss: 0.159480] [ema: 0.999637] 
[Epoch 35/92] [Batch 0/547] [D loss: 0.391453] [G loss: 0.175117] [ema: 0.999638] 
[Epoch 35/92] [Batch 100/547] [D loss: 0.432362] [G loss: 0.173843] [ema: 0.999640] 
[Epoch 35/92] [Batch 200/547] [D loss: 0.393865] [G loss: 0.188318] [ema: 0.999642] 
[Epoch 35/92] [Batch 300/547] [D loss: 0.405863] [G loss: 0.177376] [ema: 0.999644] 
[Epoch 35/92] [Batch 400/547] [D loss: 0.386308] [G loss: 0.169630] [ema: 0.999645] 
[Epoch 35/92] [Batch 500/547] [D loss: 0.402379] [G loss: 0.178113] [ema: 0.999647] 
[Epoch 36/92] [Batch 0/547] [D loss: 0.346204] [G loss: 0.195202] [ema: 0.999648] 
[Epoch 36/92] [Batch 100/547] [D loss: 0.381797] [G loss: 0.188670] [ema: 0.999650] 
[Epoch 36/92] [Batch 200/547] [D loss: 0.428019] [G loss: 0.178993] [ema: 0.999652] 
[Epoch 36/92] [Batch 300/547] [D loss: 0.427563] [G loss: 0.159248] [ema: 0.999653] 
[Epoch 36/92] [Batch 400/547] [D loss: 0.411863] [G loss: 0.177861] [ema: 0.999655] 
[Epoch 36/92] [Batch 500/547] [D loss: 0.451996] [G loss: 0.164320] [ema: 0.999657] 
[Epoch 37/92] [Batch 0/547] [D loss: 0.391645] [G loss: 0.187740] [ema: 0.999658] 
[Epoch 37/92] [Batch 100/547] [D loss: 0.387805] [G loss: 0.182575] [ema: 0.999659] 
[Epoch 37/92] [Batch 200/547] [D loss: 0.420809] [G loss: 0.189677] [ema: 0.999661] 
[Epoch 37/92] [Batch 300/547] [D loss: 0.379084] [G loss: 0.166831] [ema: 0.999663] 
[Epoch 37/92] [Batch 400/547] [D loss: 0.392920] [G loss: 0.163068] [ema: 0.999664] 
[Epoch 37/92] [Batch 500/547] [D loss: 0.407756] [G loss: 0.173601] [ema: 0.999666] 
[Epoch 38/92] [Batch 0/547] [D loss: 0.415147] [G loss: 0.177812] [ema: 0.999667] 
[Epoch 38/92] [Batch 100/547] [D loss: 0.375536] [G loss: 0.192338] [ema: 0.999668] 
[Epoch 38/92] [Batch 200/547] [D loss: 0.422723] [G loss: 0.177161] [ema: 0.999670] 
[Epoch 38/92] [Batch 300/547] [D loss: 0.424516] [G loss: 0.179105] [ema: 0.999671] 
[Epoch 38/92] [Batch 400/547] [D loss: 0.404583] [G loss: 0.196233] [ema: 0.999673] 
[Epoch 38/92] [Batch 500/547] [D loss: 0.367369] [G loss: 0.182042] [ema: 0.999674] 
[Epoch 39/92] [Batch 0/547] [D loss: 0.416194] [G loss: 0.178657] [ema: 0.999675] 
[Epoch 39/92] [Batch 100/547] [D loss: 0.465630] [G loss: 0.172947] [ema: 0.999677] 
[Epoch 39/92] [Batch 200/547] [D loss: 0.391686] [G loss: 0.178390] [ema: 0.999678] 
[Epoch 39/92] [Batch 300/547] [D loss: 0.435779] [G loss: 0.167312] [ema: 0.999680] 
[Epoch 39/92] [Batch 400/547] [D loss: 0.385453] [G loss: 0.197082] [ema: 0.999681] 
[Epoch 39/92] [Batch 500/547] [D loss: 0.386806] [G loss: 0.180037] [ema: 0.999683] 
[Epoch 40/92] [Batch 0/547] [D loss: 0.427585] [G loss: 0.187356] [ema: 0.999683] 
[Epoch 40/92] [Batch 100/547] [D loss: 0.411870] [G loss: 0.194826] [ema: 0.999685] 
[Epoch 40/92] [Batch 200/547] [D loss: 0.426142] [G loss: 0.162236] [ema: 0.999686] 
[Epoch 40/92] [Batch 300/547] [D loss: 0.400083] [G loss: 0.167632] [ema: 0.999688] 
[Epoch 40/92] [Batch 400/547] [D loss: 0.434840] [G loss: 0.162739] [ema: 0.999689] 
[Epoch 40/92] [Batch 500/547] [D loss: 0.453465] [G loss: 0.162347] [ema: 0.999690] 
[Epoch 41/92] [Batch 0/547] [D loss: 0.403502] [G loss: 0.157604] [ema: 0.999691] 
[Epoch 41/92] [Batch 100/547] [D loss: 0.411564] [G loss: 0.176395] [ema: 0.999692] 
[Epoch 41/92] [Batch 200/547] [D loss: 0.405244] [G loss: 0.170042] [ema: 0.999694] 
[Epoch 41/92] [Batch 300/547] [D loss: 0.406322] [G loss: 0.176403] [ema: 0.999695] 
[Epoch 41/92] [Batch 400/547] [D loss: 0.414254] [G loss: 0.177959] [ema: 0.999696] 
[Epoch 41/92] [Batch 500/547] [D loss: 0.400504] [G loss: 0.174147] [ema: 0.999698] 
[Epoch 42/92] [Batch 0/547] [D loss: 0.344671] [G loss: 0.187254] [ema: 0.999698] 
[Epoch 42/92] [Batch 100/547] [D loss: 0.414642] [G loss: 0.178641] [ema: 0.999700] 
[Epoch 42/92] [Batch 200/547] [D loss: 0.459046] [G loss: 0.149320] [ema: 0.999701] 
[Epoch 42/92] [Batch 300/547] [D loss: 0.414632] [G loss: 0.168918] [ema: 0.999702] 
[Epoch 42/92] [Batch 400/547] [D loss: 0.370601] [G loss: 0.188078] [ema: 0.999703] 
[Epoch 42/92] [Batch 500/547] [D loss: 0.419393] [G loss: 0.184127] [ema: 0.999705] 
[Epoch 43/92] [Batch 0/547] [D loss: 0.439129] [G loss: 0.181903] [ema: 0.999705] 
[Epoch 43/92] [Batch 100/547] [D loss: 0.410214] [G loss: 0.172686] [ema: 0.999707] 
[Epoch 43/92] [Batch 200/547] [D loss: 0.427991] [G loss: 0.189514] [ema: 0.999708] 
[Epoch 43/92] [Batch 300/547] [D loss: 0.453978] [G loss: 0.154101] [ema: 0.999709] 
[Epoch 43/92] [Batch 400/547] [D loss: 0.404901] [G loss: 0.183520] [ema: 0.999710] 
[Epoch 43/92] [Batch 500/547] [D loss: 0.402251] [G loss: 0.185406] [ema: 0.999711] 
[Epoch 44/92] [Batch 0/547] [D loss: 0.405509] [G loss: 0.171268] [ema: 0.999712] 
[Epoch 44/92] [Batch 100/547] [D loss: 0.430467] [G loss: 0.169322] [ema: 0.999713] 
[Epoch 44/92] [Batch 200/547] [D loss: 0.414589] [G loss: 0.180497] [ema: 0.999714] 
[Epoch 44/92] [Batch 300/547] [D loss: 0.391491] [G loss: 0.186491] [ema: 0.999716] 
[Epoch 44/92] [Batch 400/547] [D loss: 0.417181] [G loss: 0.176100] [ema: 0.999717] 
[Epoch 44/92] [Batch 500/547] [D loss: 0.378908] [G loss: 0.171537] [ema: 0.999718] 
[Epoch 45/92] [Batch 0/547] [D loss: 0.416578] [G loss: 0.183329] [ema: 0.999718] 
[Epoch 45/92] [Batch 100/547] [D loss: 0.393270] [G loss: 0.190248] [ema: 0.999720] 
[Epoch 45/92] [Batch 200/547] [D loss: 0.412643] [G loss: 0.163200] [ema: 0.999721] 
[Epoch 45/92] [Batch 300/547] [D loss: 0.406480] [G loss: 0.185555] [ema: 0.999722] 
[Epoch 45/92] [Batch 400/547] [D loss: 0.409553] [G loss: 0.171151] [ema: 0.999723] 
[Epoch 45/92] [Batch 500/547] [D loss: 0.365873] [G loss: 0.170104] [ema: 0.999724] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_60_100/WISDM_DAGHAR_Multiclass_50000_D_60_2024_10_25_15_19_34/Model



[Epoch 46/92] [Batch 0/547] [D loss: 0.409924] [G loss: 0.175198] [ema: 0.999725] 
[Epoch 46/92] [Batch 100/547] [D loss: 0.377108] [G loss: 0.191184] [ema: 0.999726] 
[Epoch 46/92] [Batch 200/547] [D loss: 0.439466] [G loss: 0.188135] [ema: 0.999727] 
[Epoch 46/92] [Batch 300/547] [D loss: 0.418029] [G loss: 0.179110] [ema: 0.999728] 
[Epoch 46/92] [Batch 400/547] [D loss: 0.441993] [G loss: 0.186241] [ema: 0.999729] 
[Epoch 46/92] [Batch 500/547] [D loss: 0.423552] [G loss: 0.175550] [ema: 0.999730] 
[Epoch 47/92] [Batch 0/547] [D loss: 0.388505] [G loss: 0.190121] [ema: 0.999730] 
[Epoch 47/92] [Batch 100/547] [D loss: 0.399495] [G loss: 0.172669] [ema: 0.999731] 
[Epoch 47/92] [Batch 200/547] [D loss: 0.386982] [G loss: 0.180928] [ema: 0.999733] 
[Epoch 47/92] [Batch 300/547] [D loss: 0.431284] [G loss: 0.167045] [ema: 0.999734] 
[Epoch 47/92] [Batch 400/547] [D loss: 0.378967] [G loss: 0.182973] [ema: 0.999735] 
[Epoch 47/92] [Batch 500/547] [D loss: 0.380966] [G loss: 0.193424] [ema: 0.999736] 
[Epoch 48/92] [Batch 0/547] [D loss: 0.408257] [G loss: 0.183994] [ema: 0.999736] 
[Epoch 48/92] [Batch 100/547] [D loss: 0.438091] [G loss: 0.158444] [ema: 0.999737] 
[Epoch 48/92] [Batch 200/547] [D loss: 0.436184] [G loss: 0.183508] [ema: 0.999738] 
[Epoch 48/92] [Batch 300/547] [D loss: 0.438776] [G loss: 0.164347] [ema: 0.999739] 
[Epoch 48/92] [Batch 400/547] [D loss: 0.412123] [G loss: 0.185343] [ema: 0.999740] 
[Epoch 48/92] [Batch 500/547] [D loss: 0.405028] [G loss: 0.175307] [ema: 0.999741] 
[Epoch 49/92] [Batch 0/547] [D loss: 0.387934] [G loss: 0.194967] [ema: 0.999741] 
[Epoch 49/92] [Batch 100/547] [D loss: 0.430511] [G loss: 0.152700] [ema: 0.999742] 
[Epoch 49/92] [Batch 200/547] [D loss: 0.394569] [G loss: 0.173247] [ema: 0.999743] 
[Epoch 49/92] [Batch 300/547] [D loss: 0.418748] [G loss: 0.173547] [ema: 0.999744] 
[Epoch 49/92] [Batch 400/547] [D loss: 0.419648] [G loss: 0.178762] [ema: 0.999745] 
[Epoch 49/92] [Batch 500/547] [D loss: 0.399623] [G loss: 0.173064] [ema: 0.999746] 
[Epoch 50/92] [Batch 0/547] [D loss: 0.433356] [G loss: 0.167110] [ema: 0.999747] 
[Epoch 50/92] [Batch 100/547] [D loss: 0.416147] [G loss: 0.184859] [ema: 0.999748] 
[Epoch 50/92] [Batch 200/547] [D loss: 0.377693] [G loss: 0.172691] [ema: 0.999748] 
[Epoch 50/92] [Batch 300/547] [D loss: 0.396601] [G loss: 0.171932] [ema: 0.999749] 
[Epoch 50/92] [Batch 400/547] [D loss: 0.487498] [G loss: 0.171196] [ema: 0.999750] 
[Epoch 50/92] [Batch 500/547] [D loss: 0.394332] [G loss: 0.179144] [ema: 0.999751] 
[Epoch 51/92] [Batch 0/547] [D loss: 0.430212] [G loss: 0.176880] [ema: 0.999752] 
[Epoch 51/92] [Batch 100/547] [D loss: 0.396887] [G loss: 0.182009] [ema: 0.999752] 
[Epoch 51/92] [Batch 200/547] [D loss: 0.440551] [G loss: 0.163945] [ema: 0.999753] 
[Epoch 51/92] [Batch 300/547] [D loss: 0.430860] [G loss: 0.160555] [ema: 0.999754] 
[Epoch 51/92] [Batch 400/547] [D loss: 0.375720] [G loss: 0.182286] [ema: 0.999755] 
[Epoch 51/92] [Batch 500/547] [D loss: 0.422346] [G loss: 0.175552] [ema: 0.999756] 
[Epoch 52/92] [Batch 0/547] [D loss: 0.409486] [G loss: 0.157708] [ema: 0.999756] 
[Epoch 52/92] [Batch 100/547] [D loss: 0.428305] [G loss: 0.156091] [ema: 0.999757] 
[Epoch 52/92] [Batch 200/547] [D loss: 0.425578] [G loss: 0.182311] [ema: 0.999758] 
[Epoch 52/92] [Batch 300/547] [D loss: 0.411710] [G loss: 0.164304] [ema: 0.999759] 
[Epoch 52/92] [Batch 400/547] [D loss: 0.454274] [G loss: 0.182272] [ema: 0.999760] 
[Epoch 52/92] [Batch 500/547] [D loss: 0.418505] [G loss: 0.176302] [ema: 0.999761] 
[Epoch 53/92] [Batch 0/547] [D loss: 0.448864] [G loss: 0.176941] [ema: 0.999761] 
[Epoch 53/92] [Batch 100/547] [D loss: 0.430313] [G loss: 0.147894] [ema: 0.999762] 
[Epoch 53/92] [Batch 200/547] [D loss: 0.400790] [G loss: 0.195251] [ema: 0.999763] 
[Epoch 53/92] [Batch 300/547] [D loss: 0.382064] [G loss: 0.184863] [ema: 0.999763] 
[Epoch 53/92] [Batch 400/547] [D loss: 0.418242] [G loss: 0.156119] [ema: 0.999764] 
[Epoch 53/92] [Batch 500/547] [D loss: 0.429712] [G loss: 0.164829] [ema: 0.999765] 
[Epoch 54/92] [Batch 0/547] [D loss: 0.418360] [G loss: 0.179931] [ema: 0.999765] 
[Epoch 54/92] [Batch 100/547] [D loss: 0.403287] [G loss: 0.175639] [ema: 0.999766] 
[Epoch 54/92] [Batch 200/547] [D loss: 0.407291] [G loss: 0.184116] [ema: 0.999767] 
[Epoch 54/92] [Batch 300/547] [D loss: 0.457821] [G loss: 0.177000] [ema: 0.999768] 
[Epoch 54/92] [Batch 400/547] [D loss: 0.408609] [G loss: 0.178947] [ema: 0.999768] 
[Epoch 54/92] [Batch 500/547] [D loss: 0.430123] [G loss: 0.174061] [ema: 0.999769] 
[Epoch 55/92] [Batch 0/547] [D loss: 0.451077] [G loss: 0.167124] [ema: 0.999770] 
[Epoch 55/92] [Batch 100/547] [D loss: 0.471705] [G loss: 0.182370] [ema: 0.999770] 
[Epoch 55/92] [Batch 200/547] [D loss: 0.431972] [G loss: 0.169021] [ema: 0.999771] 
[Epoch 55/92] [Batch 300/547] [D loss: 0.431132] [G loss: 0.166495] [ema: 0.999772] 
[Epoch 55/92] [Batch 400/547] [D loss: 0.434244] [G loss: 0.176635] [ema: 0.999773] 
[Epoch 55/92] [Batch 500/547] [D loss: 0.452271] [G loss: 0.163104] [ema: 0.999773] 
[Epoch 56/92] [Batch 0/547] [D loss: 0.428174] [G loss: 0.171687] [ema: 0.999774] 
[Epoch 56/92] [Batch 100/547] [D loss: 0.413480] [G loss: 0.157602] [ema: 0.999774] 
[Epoch 56/92] [Batch 200/547] [D loss: 0.416781] [G loss: 0.174741] [ema: 0.999775] 
[Epoch 56/92] [Batch 300/547] [D loss: 0.422568] [G loss: 0.149157] [ema: 0.999776] 
[Epoch 56/92] [Batch 400/547] [D loss: 0.437283] [G loss: 0.169685] [ema: 0.999777] 
[Epoch 56/92] [Batch 500/547] [D loss: 0.423075] [G loss: 0.181834] [ema: 0.999777] 
[Epoch 57/92] [Batch 0/547] [D loss: 0.406193] [G loss: 0.172960] [ema: 0.999778] 
[Epoch 57/92] [Batch 100/547] [D loss: 0.426800] [G loss: 0.161778] [ema: 0.999778] 
[Epoch 57/92] [Batch 200/547] [D loss: 0.431176] [G loss: 0.155925] [ema: 0.999779] 
[Epoch 57/92] [Batch 300/547] [D loss: 0.376503] [G loss: 0.170156] [ema: 0.999780] 
[Epoch 57/92] [Batch 400/547] [D loss: 0.412099] [G loss: 0.160616] [ema: 0.999781] 
[Epoch 57/92] [Batch 500/547] [D loss: 0.441567] [G loss: 0.178303] [ema: 0.999781] 
[Epoch 58/92] [Batch 0/547] [D loss: 0.404121] [G loss: 0.170294] [ema: 0.999782] 
[Epoch 58/92] [Batch 100/547] [D loss: 0.451985] [G loss: 0.185326] [ema: 0.999782] 
[Epoch 58/92] [Batch 200/547] [D loss: 0.432974] [G loss: 0.166416] [ema: 0.999783] 
[Epoch 58/92] [Batch 300/547] [D loss: 0.411534] [G loss: 0.151937] [ema: 0.999784] 
[Epoch 58/92] [Batch 400/547] [D loss: 0.409221] [G loss: 0.180276] [ema: 0.999784] 
[Epoch 58/92] [Batch 500/547] [D loss: 0.440107] [G loss: 0.163408] [ema: 0.999785] 
[Epoch 59/92] [Batch 0/547] [D loss: 0.410684] [G loss: 0.163141] [ema: 0.999785] 
[Epoch 59/92] [Batch 100/547] [D loss: 0.476088] [G loss: 0.175123] [ema: 0.999786] 
[Epoch 59/92] [Batch 200/547] [D loss: 0.469630] [G loss: 0.147548] [ema: 0.999787] 
[Epoch 59/92] [Batch 300/547] [D loss: 0.432980] [G loss: 0.149868] [ema: 0.999787] 
[Epoch 59/92] [Batch 400/547] [D loss: 0.463605] [G loss: 0.166497] [ema: 0.999788] 
[Epoch 59/92] [Batch 500/547] [D loss: 0.407079] [G loss: 0.166876] [ema: 0.999789] 
[Epoch 60/92] [Batch 0/547] [D loss: 0.449295] [G loss: 0.156268] [ema: 0.999789] 
[Epoch 60/92] [Batch 100/547] [D loss: 0.447674] [G loss: 0.164022] [ema: 0.999789] 
[Epoch 60/92] [Batch 200/547] [D loss: 0.403983] [G loss: 0.159951] [ema: 0.999790] 
[Epoch 60/92] [Batch 300/547] [D loss: 0.458586] [G loss: 0.167631] [ema: 0.999791] 
[Epoch 60/92] [Batch 400/547] [D loss: 0.455301] [G loss: 0.149463] [ema: 0.999791] 
[Epoch 60/92] [Batch 500/547] [D loss: 0.446330] [G loss: 0.159691] [ema: 0.999792] 
[Epoch 61/92] [Batch 0/547] [D loss: 0.418898] [G loss: 0.159787] [ema: 0.999792] 
[Epoch 61/92] [Batch 100/547] [D loss: 0.418926] [G loss: 0.168453] [ema: 0.999793] 
[Epoch 61/92] [Batch 200/547] [D loss: 0.455909] [G loss: 0.173108] [ema: 0.999794] 
[Epoch 61/92] [Batch 300/547] [D loss: 0.464247] [G loss: 0.156074] [ema: 0.999794] 
[Epoch 61/92] [Batch 400/547] [D loss: 0.413092] [G loss: 0.163615] [ema: 0.999795] 
[Epoch 61/92] [Batch 500/547] [D loss: 0.395677] [G loss: 0.173448] [ema: 0.999795] 
[Epoch 62/92] [Batch 0/547] [D loss: 0.423010] [G loss: 0.160351] [ema: 0.999796] 
[Epoch 62/92] [Batch 100/547] [D loss: 0.408949] [G loss: 0.177457] [ema: 0.999796] 
[Epoch 62/92] [Batch 200/547] [D loss: 0.423849] [G loss: 0.170403] [ema: 0.999797] 
[Epoch 62/92] [Batch 300/547] [D loss: 0.444011] [G loss: 0.167430] [ema: 0.999797] 
[Epoch 62/92] [Batch 400/547] [D loss: 0.420977] [G loss: 0.168943] [ema: 0.999798] 
[Epoch 62/92] [Batch 500/547] [D loss: 0.419434] [G loss: 0.165327] [ema: 0.999799] 
[Epoch 63/92] [Batch 0/547] [D loss: 0.401960] [G loss: 0.161692] [ema: 0.999799] 
[Epoch 63/92] [Batch 100/547] [D loss: 0.434227] [G loss: 0.163583] [ema: 0.999799] 
[Epoch 63/92] [Batch 200/547] [D loss: 0.430973] [G loss: 0.162622] [ema: 0.999800] 
[Epoch 63/92] [Batch 300/547] [D loss: 0.442306] [G loss: 0.159057] [ema: 0.999801] 
[Epoch 63/92] [Batch 400/547] [D loss: 0.442789] [G loss: 0.170424] [ema: 0.999801] 
[Epoch 63/92] [Batch 500/547] [D loss: 0.424545] [G loss: 0.162984] [ema: 0.999802] 
[Epoch 64/92] [Batch 0/547] [D loss: 0.480154] [G loss: 0.174132] [ema: 0.999802] 
[Epoch 64/92] [Batch 100/547] [D loss: 0.446465] [G loss: 0.156294] [ema: 0.999803] 
[Epoch 64/92] [Batch 200/547] [D loss: 0.392793] [G loss: 0.158029] [ema: 0.999803] 
[Epoch 64/92] [Batch 300/547] [D loss: 0.441998] [G loss: 0.157004] [ema: 0.999804] 
[Epoch 64/92] [Batch 400/547] [D loss: 0.421413] [G loss: 0.163190] [ema: 0.999804] 
[Epoch 64/92] [Batch 500/547] [D loss: 0.455167] [G loss: 0.163903] [ema: 0.999805] 
[Epoch 65/92] [Batch 0/547] [D loss: 0.459148] [G loss: 0.169688] [ema: 0.999805] 
[Epoch 65/92] [Batch 100/547] [D loss: 0.467891] [G loss: 0.161871] [ema: 0.999806] 
[Epoch 65/92] [Batch 200/547] [D loss: 0.417805] [G loss: 0.157256] [ema: 0.999806] 
[Epoch 65/92] [Batch 300/547] [D loss: 0.452780] [G loss: 0.159151] [ema: 0.999807] 
[Epoch 65/92] [Batch 400/547] [D loss: 0.436683] [G loss: 0.161095] [ema: 0.999807] 
[Epoch 65/92] [Batch 500/547] [D loss: 0.373208] [G loss: 0.182418] [ema: 0.999808] 
[Epoch 66/92] [Batch 0/547] [D loss: 0.450677] [G loss: 0.166873] [ema: 0.999808] 
[Epoch 66/92] [Batch 100/547] [D loss: 0.444279] [G loss: 0.157875] [ema: 0.999809] 
[Epoch 66/92] [Batch 200/547] [D loss: 0.438879] [G loss: 0.160344] [ema: 0.999809] 
[Epoch 66/92] [Batch 300/547] [D loss: 0.419754] [G loss: 0.169261] [ema: 0.999810] 
[Epoch 66/92] [Batch 400/547] [D loss: 0.444410] [G loss: 0.167929] [ema: 0.999810] 
[Epoch 66/92] [Batch 500/547] [D loss: 0.469499] [G loss: 0.158514] [ema: 0.999811] 
[Epoch 67/92] [Batch 0/547] [D loss: 0.449403] [G loss: 0.154400] [ema: 0.999811] 
[Epoch 67/92] [Batch 100/547] [D loss: 0.458639] [G loss: 0.160767] [ema: 0.999811] 
[Epoch 67/92] [Batch 200/547] [D loss: 0.417710] [G loss: 0.164936] [ema: 0.999812] 
[Epoch 67/92] [Batch 300/547] [D loss: 0.489402] [G loss: 0.160724] [ema: 0.999812] 
[Epoch 67/92] [Batch 400/547] [D loss: 0.416103] [G loss: 0.166940] [ema: 0.999813] 
[Epoch 67/92] [Batch 500/547] [D loss: 0.431135] [G loss: 0.165381] [ema: 0.999813] 
[Epoch 68/92] [Batch 0/547] [D loss: 0.420452] [G loss: 0.172878] [ema: 0.999814] 
[Epoch 68/92] [Batch 100/547] [D loss: 0.375648] [G loss: 0.162167] [ema: 0.999814] 
[Epoch 68/92] [Batch 200/547] [D loss: 0.403685] [G loss: 0.151894] [ema: 0.999815] 
[Epoch 68/92] [Batch 300/547] [D loss: 0.425345] [G loss: 0.166054] [ema: 0.999815] 
[Epoch 68/92] [Batch 400/547] [D loss: 0.478889] [G loss: 0.160710] [ema: 0.999816] 
[Epoch 68/92] [Batch 500/547] [D loss: 0.491182] [G loss: 0.158561] [ema: 0.999816] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_60_100/WISDM_DAGHAR_Multiclass_50000_D_60_2024_10_25_15_19_34/Model



[Epoch 69/92] [Batch 0/547] [D loss: 0.458059] [G loss: 0.171044] [ema: 0.999816] 
[Epoch 69/92] [Batch 100/547] [D loss: 0.451631] [G loss: 0.180147] [ema: 0.999817] 
[Epoch 69/92] [Batch 200/547] [D loss: 0.394666] [G loss: 0.174441] [ema: 0.999817] 
[Epoch 69/92] [Batch 300/547] [D loss: 0.445796] [G loss: 0.173282] [ema: 0.999818] 
[Epoch 69/92] [Batch 400/547] [D loss: 0.451782] [G loss: 0.151924] [ema: 0.999818] 
[Epoch 69/92] [Batch 500/547] [D loss: 0.396740] [G loss: 0.169260] [ema: 0.999819] 
[Epoch 70/92] [Batch 0/547] [D loss: 0.431609] [G loss: 0.158465] [ema: 0.999819] 
[Epoch 70/92] [Batch 100/547] [D loss: 0.461103] [G loss: 0.159179] [ema: 0.999819] 
[Epoch 70/92] [Batch 200/547] [D loss: 0.422837] [G loss: 0.163692] [ema: 0.999820] 
[Epoch 70/92] [Batch 300/547] [D loss: 0.446620] [G loss: 0.170106] [ema: 0.999820] 
[Epoch 70/92] [Batch 400/547] [D loss: 0.425438] [G loss: 0.161657] [ema: 0.999821] 
[Epoch 70/92] [Batch 500/547] [D loss: 0.419277] [G loss: 0.170497] [ema: 0.999821] 
[Epoch 71/92] [Batch 0/547] [D loss: 0.408660] [G loss: 0.158502] [ema: 0.999822] 
[Epoch 71/92] [Batch 100/547] [D loss: 0.425589] [G loss: 0.161414] [ema: 0.999822] 
[Epoch 71/92] [Batch 200/547] [D loss: 0.425061] [G loss: 0.167723] [ema: 0.999822] 
[Epoch 71/92] [Batch 300/547] [D loss: 0.417045] [G loss: 0.160250] [ema: 0.999823] 
[Epoch 71/92] [Batch 400/547] [D loss: 0.459856] [G loss: 0.162809] [ema: 0.999823] 
[Epoch 71/92] [Batch 500/547] [D loss: 0.456631] [G loss: 0.162450] [ema: 0.999824] 
[Epoch 72/92] [Batch 0/547] [D loss: 0.431710] [G loss: 0.177052] [ema: 0.999824] 
[Epoch 72/92] [Batch 100/547] [D loss: 0.449884] [G loss: 0.147678] [ema: 0.999824] 
[Epoch 72/92] [Batch 200/547] [D loss: 0.404278] [G loss: 0.176392] [ema: 0.999825] 
[Epoch 72/92] [Batch 300/547] [D loss: 0.419860] [G loss: 0.164993] [ema: 0.999825] 
[Epoch 72/92] [Batch 400/547] [D loss: 0.452794] [G loss: 0.167401] [ema: 0.999826] 
[Epoch 72/92] [Batch 500/547] [D loss: 0.431230] [G loss: 0.175723] [ema: 0.999826] 
[Epoch 73/92] [Batch 0/547] [D loss: 0.422051] [G loss: 0.169133] [ema: 0.999826] 
[Epoch 73/92] [Batch 100/547] [D loss: 0.429838] [G loss: 0.148630] [ema: 0.999827] 
[Epoch 73/92] [Batch 200/547] [D loss: 0.398409] [G loss: 0.169101] [ema: 0.999827] 
[Epoch 73/92] [Batch 300/547] [D loss: 0.405357] [G loss: 0.171910] [ema: 0.999828] 
[Epoch 73/92] [Batch 400/547] [D loss: 0.429772] [G loss: 0.169323] [ema: 0.999828] 
[Epoch 73/92] [Batch 500/547] [D loss: 0.449671] [G loss: 0.169674] [ema: 0.999829] 
[Epoch 74/92] [Batch 0/547] [D loss: 0.405705] [G loss: 0.159353] [ema: 0.999829] 
[Epoch 74/92] [Batch 100/547] [D loss: 0.457462] [G loss: 0.164745] [ema: 0.999829] 
[Epoch 74/92] [Batch 200/547] [D loss: 0.441315] [G loss: 0.164150] [ema: 0.999830] 
[Epoch 74/92] [Batch 300/547] [D loss: 0.449125] [G loss: 0.156698] [ema: 0.999830] 
[Epoch 74/92] [Batch 400/547] [D loss: 0.389447] [G loss: 0.167798] [ema: 0.999830] 
[Epoch 74/92] [Batch 500/547] [D loss: 0.444611] [G loss: 0.170609] [ema: 0.999831] 
[Epoch 75/92] [Batch 0/547] [D loss: 0.413648] [G loss: 0.166050] [ema: 0.999831] 
[Epoch 75/92] [Batch 100/547] [D loss: 0.432304] [G loss: 0.166615] [ema: 0.999831] 
[Epoch 75/92] [Batch 200/547] [D loss: 0.439172] [G loss: 0.147223] [ema: 0.999832] 
[Epoch 75/92] [Batch 300/547] [D loss: 0.437689] [G loss: 0.169337] [ema: 0.999832] 
[Epoch 75/92] [Batch 400/547] [D loss: 0.444247] [G loss: 0.174242] [ema: 0.999833] 
[Epoch 75/92] [Batch 500/547] [D loss: 0.437391] [G loss: 0.172194] [ema: 0.999833] 
[Epoch 76/92] [Batch 0/547] [D loss: 0.431936] [G loss: 0.171905] [ema: 0.999833] 
[Epoch 76/92] [Batch 100/547] [D loss: 0.458897] [G loss: 0.169757] [ema: 0.999834] 
[Epoch 76/92] [Batch 200/547] [D loss: 0.452419] [G loss: 0.154327] [ema: 0.999834] 
[Epoch 76/92] [Batch 300/547] [D loss: 0.438667] [G loss: 0.157166] [ema: 0.999834] 
[Epoch 76/92] [Batch 400/547] [D loss: 0.431859] [G loss: 0.170456] [ema: 0.999835] 
[Epoch 76/92] [Batch 500/547] [D loss: 0.442915] [G loss: 0.166413] [ema: 0.999835] 
[Epoch 77/92] [Batch 0/547] [D loss: 0.428915] [G loss: 0.171287] [ema: 0.999835] 
[Epoch 77/92] [Batch 100/547] [D loss: 0.480162] [G loss: 0.161795] [ema: 0.999836] 
[Epoch 77/92] [Batch 200/547] [D loss: 0.462751] [G loss: 0.159770] [ema: 0.999836] 
[Epoch 77/92] [Batch 300/547] [D loss: 0.420842] [G loss: 0.163906] [ema: 0.999837] 
[Epoch 77/92] [Batch 400/547] [D loss: 0.448815] [G loss: 0.174508] [ema: 0.999837] 
[Epoch 77/92] [Batch 500/547] [D loss: 0.438198] [G loss: 0.166875] [ema: 0.999837] 
[Epoch 78/92] [Batch 0/547] [D loss: 0.421007] [G loss: 0.155764] [ema: 0.999838] 
[Epoch 78/92] [Batch 100/547] [D loss: 0.453982] [G loss: 0.156500] [ema: 0.999838] 
[Epoch 78/92] [Batch 200/547] [D loss: 0.441950] [G loss: 0.171115] [ema: 0.999838] 
[Epoch 78/92] [Batch 300/547] [D loss: 0.460069] [G loss: 0.161192] [ema: 0.999839] 
[Epoch 78/92] [Batch 400/547] [D loss: 0.444801] [G loss: 0.166150] [ema: 0.999839] 
[Epoch 78/92] [Batch 500/547] [D loss: 0.421649] [G loss: 0.157764] [ema: 0.999839] 
[Epoch 79/92] [Batch 0/547] [D loss: 0.478718] [G loss: 0.167215] [ema: 0.999840] 
[Epoch 79/92] [Batch 100/547] [D loss: 0.425165] [G loss: 0.175990] [ema: 0.999840] 
[Epoch 79/92] [Batch 200/547] [D loss: 0.438146] [G loss: 0.162462] [ema: 0.999840] 
[Epoch 79/92] [Batch 300/547] [D loss: 0.455507] [G loss: 0.155452] [ema: 0.999841] 
[Epoch 79/92] [Batch 400/547] [D loss: 0.486990] [G loss: 0.155330] [ema: 0.999841] 
[Epoch 79/92] [Batch 500/547] [D loss: 0.424555] [G loss: 0.161773] [ema: 0.999841] 
[Epoch 80/92] [Batch 0/547] [D loss: 0.447662] [G loss: 0.165878] [ema: 0.999842] 
[Epoch 80/92] [Batch 100/547] [D loss: 0.425867] [G loss: 0.158508] [ema: 0.999842] 
[Epoch 80/92] [Batch 200/547] [D loss: 0.405525] [G loss: 0.163863] [ema: 0.999842] 
[Epoch 80/92] [Batch 300/547] [D loss: 0.497435] [G loss: 0.154216] [ema: 0.999843] 
[Epoch 80/92] [Batch 400/547] [D loss: 0.460128] [G loss: 0.161799] [ema: 0.999843] 
[Epoch 80/92] [Batch 500/547] [D loss: 0.410948] [G loss: 0.177262] [ema: 0.999843] 
[Epoch 81/92] [Batch 0/547] [D loss: 0.404590] [G loss: 0.152081] [ema: 0.999844] 
[Epoch 81/92] [Batch 100/547] [D loss: 0.414225] [G loss: 0.166341] [ema: 0.999844] 
[Epoch 81/92] [Batch 200/547] [D loss: 0.407846] [G loss: 0.168162] [ema: 0.999844] 
[Epoch 81/92] [Batch 300/547] [D loss: 0.449982] [G loss: 0.163080] [ema: 0.999845] 
[Epoch 81/92] [Batch 400/547] [D loss: 0.451938] [G loss: 0.184206] [ema: 0.999845] 
[Epoch 81/92] [Batch 500/547] [D loss: 0.458052] [G loss: 0.169114] [ema: 0.999845] 
[Epoch 82/92] [Batch 0/547] [D loss: 0.446292] [G loss: 0.174072] [ema: 0.999845] 
[Epoch 82/92] [Batch 100/547] [D loss: 0.450566] [G loss: 0.157100] [ema: 0.999846] 
[Epoch 82/92] [Batch 200/547] [D loss: 0.392107] [G loss: 0.178970] [ema: 0.999846] 
[Epoch 82/92] [Batch 300/547] [D loss: 0.422068] [G loss: 0.160303] [ema: 0.999847] 
[Epoch 82/92] [Batch 400/547] [D loss: 0.423162] [G loss: 0.157797] [ema: 0.999847] 
[Epoch 82/92] [Batch 500/547] [D loss: 0.414551] [G loss: 0.164287] [ema: 0.999847] 
[Epoch 83/92] [Batch 0/547] [D loss: 0.441313] [G loss: 0.156000] [ema: 0.999847] 
[Epoch 83/92] [Batch 100/547] [D loss: 0.454578] [G loss: 0.162960] [ema: 0.999848] 
[Epoch 83/92] [Batch 200/547] [D loss: 0.474580] [G loss: 0.149514] [ema: 0.999848] 
[Epoch 83/92] [Batch 300/547] [D loss: 0.459195] [G loss: 0.164472] [ema: 0.999848] 
[Epoch 83/92] [Batch 400/547] [D loss: 0.455765] [G loss: 0.161380] [ema: 0.999849] 
[Epoch 83/92] [Batch 500/547] [D loss: 0.472635] [G loss: 0.161721] [ema: 0.999849] 
[Epoch 84/92] [Batch 0/547] [D loss: 0.453265] [G loss: 0.168179] [ema: 0.999849] 
[Epoch 84/92] [Batch 100/547] [D loss: 0.391231] [G loss: 0.167345] [ema: 0.999849] 
[Epoch 84/92] [Batch 200/547] [D loss: 0.456866] [G loss: 0.161442] [ema: 0.999850] 
[Epoch 84/92] [Batch 300/547] [D loss: 0.413062] [G loss: 0.164880] [ema: 0.999850] 
[Epoch 84/92] [Batch 400/547] [D loss: 0.460939] [G loss: 0.168197] [ema: 0.999850] 
[Epoch 84/92] [Batch 500/547] [D loss: 0.411557] [G loss: 0.168804] [ema: 0.999851] 
[Epoch 85/92] [Batch 0/547] [D loss: 0.452831] [G loss: 0.152129] [ema: 0.999851] 
[Epoch 85/92] [Batch 100/547] [D loss: 0.433851] [G loss: 0.151188] [ema: 0.999851] 
[Epoch 85/92] [Batch 200/547] [D loss: 0.404190] [G loss: 0.174051] [ema: 0.999852] 
[Epoch 85/92] [Batch 300/547] [D loss: 0.432294] [G loss: 0.155373] [ema: 0.999852] 
[Epoch 85/92] [Batch 400/547] [D loss: 0.454078] [G loss: 0.157365] [ema: 0.999852] 
[Epoch 85/92] [Batch 500/547] [D loss: 0.467092] [G loss: 0.170319] [ema: 0.999853] 
[Epoch 86/92] [Batch 0/547] [D loss: 0.393464] [G loss: 0.167903] [ema: 0.999853] 
[Epoch 86/92] [Batch 100/547] [D loss: 0.451307] [G loss: 0.155520] [ema: 0.999853] 
[Epoch 86/92] [Batch 200/547] [D loss: 0.434340] [G loss: 0.163846] [ema: 0.999853] 
[Epoch 86/92] [Batch 300/547] [D loss: 0.434352] [G loss: 0.151073] [ema: 0.999854] 
[Epoch 86/92] [Batch 400/547] [D loss: 0.486716] [G loss: 0.156198] [ema: 0.999854] 
[Epoch 86/92] [Batch 500/547] [D loss: 0.425341] [G loss: 0.163151] [ema: 0.999854] 
[Epoch 87/92] [Batch 0/547] [D loss: 0.422285] [G loss: 0.181904] [ema: 0.999854] 
[Epoch 87/92] [Batch 100/547] [D loss: 0.419218] [G loss: 0.169388] [ema: 0.999855] 
[Epoch 87/92] [Batch 200/547] [D loss: 0.443155] [G loss: 0.164451] [ema: 0.999855] 
[Epoch 87/92] [Batch 300/547] [D loss: 0.444342] [G loss: 0.171015] [ema: 0.999855] 
[Epoch 87/92] [Batch 400/547] [D loss: 0.396527] [G loss: 0.153182] [ema: 0.999856] 
[Epoch 87/92] [Batch 500/547] [D loss: 0.442345] [G loss: 0.167446] [ema: 0.999856] 
[Epoch 88/92] [Batch 0/547] [D loss: 0.456624] [G loss: 0.150884] [ema: 0.999856] 
[Epoch 88/92] [Batch 100/547] [D loss: 0.463640] [G loss: 0.158928] [ema: 0.999856] 
[Epoch 88/92] [Batch 200/547] [D loss: 0.457622] [G loss: 0.170552] [ema: 0.999857] 
[Epoch 88/92] [Batch 300/547] [D loss: 0.431608] [G loss: 0.163214] [ema: 0.999857] 
[Epoch 88/92] [Batch 400/547] [D loss: 0.431602] [G loss: 0.160272] [ema: 0.999857] 
[Epoch 88/92] [Batch 500/547] [D loss: 0.423261] [G loss: 0.170600] [ema: 0.999857] 
[Epoch 89/92] [Batch 0/547] [D loss: 0.423444] [G loss: 0.158141] [ema: 0.999858] 
[Epoch 89/92] [Batch 100/547] [D loss: 0.474242] [G loss: 0.157219] [ema: 0.999858] 
[Epoch 89/92] [Batch 200/547] [D loss: 0.488095] [G loss: 0.151375] [ema: 0.999858] 
[Epoch 89/92] [Batch 300/547] [D loss: 0.403331] [G loss: 0.160199] [ema: 0.999859] 
[Epoch 89/92] [Batch 400/547] [D loss: 0.446496] [G loss: 0.159964] [ema: 0.999859] 
[Epoch 89/92] [Batch 500/547] [D loss: 0.450257] [G loss: 0.167052] [ema: 0.999859] 
[Epoch 90/92] [Batch 0/547] [D loss: 0.439390] [G loss: 0.164013] [ema: 0.999859] 
[Epoch 90/92] [Batch 100/547] [D loss: 0.439727] [G loss: 0.158905] [ema: 0.999859] 
[Epoch 90/92] [Batch 200/547] [D loss: 0.419975] [G loss: 0.156710] [ema: 0.999860] 
[Epoch 90/92] [Batch 300/547] [D loss: 0.397478] [G loss: 0.176604] [ema: 0.999860] 
[Epoch 90/92] [Batch 400/547] [D loss: 0.400891] [G loss: 0.161748] [ema: 0.999860] 
[Epoch 90/92] [Batch 500/547] [D loss: 0.426967] [G loss: 0.159431] [ema: 0.999861] 
[Epoch 91/92] [Batch 0/547] [D loss: 0.454313] [G loss: 0.171541] [ema: 0.999861] 
[Epoch 91/92] [Batch 100/547] [D loss: 0.462918] [G loss: 0.170108] [ema: 0.999861] 
[Epoch 91/92] [Batch 200/547] [D loss: 0.451736] [G loss: 0.174077] [ema: 0.999861] 
[Epoch 91/92] [Batch 300/547] [D loss: 0.451987] [G loss: 0.162388] [ema: 0.999862] 
[Epoch 91/92] [Batch 400/547] [D loss: 0.479865] [G loss: 0.165030] [ema: 0.999862] 
[Epoch 91/92] [Batch 500/547] [D loss: 0.435842] [G loss: 0.162765] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
UCI_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
UCI_DAGHAR_Multiclass
daghar
return single class data and labels, class is UCI_DAGHAR_Multiclass
data shape is (2420, 3, 1, 60)
label shape is (2420,)
152
Epochs between checkpoint: 83



Saving checkpoint 1 in logs/daghar_split_dataset_50000_60_100/UCI_DAGHAR_Multiclass_50000_D_60_2024_10_25_15_53_51/Model



[Epoch 0/329] [Batch 0/152] [D loss: 0.921360] [G loss: 0.403151] [ema: 0.000000] 
[Epoch 0/329] [Batch 100/152] [D loss: 0.474746] [G loss: 0.158833] [ema: 0.933033] 
[Epoch 1/329] [Batch 0/152] [D loss: 0.499961] [G loss: 0.146745] [ema: 0.955422] 
[Epoch 1/329] [Batch 100/152] [D loss: 0.407937] [G loss: 0.188633] [ema: 0.972869] 
[Epoch 2/329] [Batch 0/152] [D loss: 0.467207] [G loss: 0.191207] [ema: 0.977457] 
[Epoch 2/329] [Batch 100/152] [D loss: 0.505588] [G loss: 0.138803] [ema: 0.982989] 
[Epoch 3/329] [Batch 0/152] [D loss: 0.412475] [G loss: 0.165899] [ema: 0.984914] 
[Epoch 3/329] [Batch 100/152] [D loss: 0.509218] [G loss: 0.122039] [ema: 0.987611] 
[Epoch 4/329] [Batch 0/152] [D loss: 0.442956] [G loss: 0.175501] [ema: 0.988664] 
[Epoch 4/329] [Batch 100/152] [D loss: 0.414547] [G loss: 0.170873] [ema: 0.990258] 
[Epoch 5/329] [Batch 0/152] [D loss: 0.457243] [G loss: 0.196825] [ema: 0.990921] 
[Epoch 5/329] [Batch 100/152] [D loss: 0.439413] [G loss: 0.137589] [ema: 0.991973] 
[Epoch 6/329] [Batch 0/152] [D loss: 0.460299] [G loss: 0.205422] [ema: 0.992429] 
[Epoch 6/329] [Batch 100/152] [D loss: 0.462222] [G loss: 0.126257] [ema: 0.993174] 
[Epoch 7/329] [Batch 0/152] [D loss: 0.483914] [G loss: 0.175673] [ema: 0.993507] 
[Epoch 7/329] [Batch 100/152] [D loss: 0.402986] [G loss: 0.181206] [ema: 0.994063] 
[Epoch 8/329] [Batch 0/152] [D loss: 0.484304] [G loss: 0.137481] [ema: 0.994316] 
[Epoch 8/329] [Batch 100/152] [D loss: 0.434611] [G loss: 0.159944] [ema: 0.994747] 
[Epoch 9/329] [Batch 0/152] [D loss: 0.433555] [G loss: 0.170729] [ema: 0.994946] 
[Epoch 9/329] [Batch 100/152] [D loss: 0.442800] [G loss: 0.142834] [ema: 0.995289] 
[Epoch 10/329] [Batch 0/152] [D loss: 0.443259] [G loss: 0.173148] [ema: 0.995450] 
[Epoch 10/329] [Batch 100/152] [D loss: 0.446663] [G loss: 0.159391] [ema: 0.995730] 
[Epoch 11/329] [Batch 0/152] [D loss: 0.455592] [G loss: 0.137527] [ema: 0.995863] 
[Epoch 11/329] [Batch 100/152] [D loss: 0.429232] [G loss: 0.144589] [ema: 0.996096] 
[Epoch 12/329] [Batch 0/152] [D loss: 0.387050] [G loss: 0.213211] [ema: 0.996207] 
[Epoch 12/329] [Batch 100/152] [D loss: 0.436960] [G loss: 0.156646] [ema: 0.996404] 
[Epoch 13/329] [Batch 0/152] [D loss: 0.368477] [G loss: 0.197209] [ema: 0.996498] 
[Epoch 13/329] [Batch 100/152] [D loss: 0.414813] [G loss: 0.163231] [ema: 0.996667] 
[Epoch 14/329] [Batch 0/152] [D loss: 0.415378] [G loss: 0.169709] [ema: 0.996748] 
[Epoch 14/329] [Batch 100/152] [D loss: 0.378635] [G loss: 0.201500] [ema: 0.996894] 
[Epoch 15/329] [Batch 0/152] [D loss: 0.427420] [G loss: 0.202102] [ema: 0.996964] 
[Epoch 15/329] [Batch 100/152] [D loss: 0.364447] [G loss: 0.190057] [ema: 0.997092] 
[Epoch 16/329] [Batch 0/152] [D loss: 0.404246] [G loss: 0.193813] [ema: 0.997154] 
[Epoch 16/329] [Batch 100/152] [D loss: 0.386098] [G loss: 0.180512] [ema: 0.997266] 
[Epoch 17/329] [Batch 0/152] [D loss: 0.363178] [G loss: 0.197257] [ema: 0.997321] 
[Epoch 17/329] [Batch 100/152] [D loss: 0.391596] [G loss: 0.174163] [ema: 0.997421] 
[Epoch 18/329] [Batch 0/152] [D loss: 0.413256] [G loss: 0.208078] [ema: 0.997470] 
[Epoch 18/329] [Batch 100/152] [D loss: 0.410468] [G loss: 0.164650] [ema: 0.997559] 
[Epoch 19/329] [Batch 0/152] [D loss: 0.439103] [G loss: 0.195403] [ema: 0.997603] 
[Epoch 19/329] [Batch 100/152] [D loss: 0.399374] [G loss: 0.202825] [ema: 0.997683] 
[Epoch 20/329] [Batch 0/152] [D loss: 0.397762] [G loss: 0.195731] [ema: 0.997723] 
[Epoch 20/329] [Batch 100/152] [D loss: 0.442017] [G loss: 0.159579] [ema: 0.997795] 
[Epoch 21/329] [Batch 0/152] [D loss: 0.400967] [G loss: 0.232815] [ema: 0.997831] 
[Epoch 21/329] [Batch 100/152] [D loss: 0.378845] [G loss: 0.176034] [ema: 0.997897] 
[Epoch 22/329] [Batch 0/152] [D loss: 0.367197] [G loss: 0.168790] [ema: 0.997929] 
[Epoch 22/329] [Batch 100/152] [D loss: 0.370980] [G loss: 0.193760] [ema: 0.997989] 
[Epoch 23/329] [Batch 0/152] [D loss: 0.400847] [G loss: 0.179787] [ema: 0.998019] 
[Epoch 23/329] [Batch 100/152] [D loss: 0.403464] [G loss: 0.179955] [ema: 0.998074] 
[Epoch 24/329] [Batch 0/152] [D loss: 0.398254] [G loss: 0.201573] [ema: 0.998102] 
[Epoch 24/329] [Batch 100/152] [D loss: 0.411361] [G loss: 0.153480] [ema: 0.998152] 
[Epoch 25/329] [Batch 0/152] [D loss: 0.449523] [G loss: 0.203149] [ema: 0.998178] 
[Epoch 25/329] [Batch 100/152] [D loss: 0.448343] [G loss: 0.174342] [ema: 0.998224] 
[Epoch 26/329] [Batch 0/152] [D loss: 0.436439] [G loss: 0.204113] [ema: 0.998248] 
[Epoch 26/329] [Batch 100/152] [D loss: 0.428305] [G loss: 0.196746] [ema: 0.998291] 
[Epoch 27/329] [Batch 0/152] [D loss: 0.409198] [G loss: 0.174023] [ema: 0.998312] 
[Epoch 27/329] [Batch 100/152] [D loss: 0.439150] [G loss: 0.172461] [ema: 0.998353] 
[Epoch 28/329] [Batch 0/152] [D loss: 0.430604] [G loss: 0.186398] [ema: 0.998373] 
[Epoch 28/329] [Batch 100/152] [D loss: 0.439944] [G loss: 0.164946] [ema: 0.998410] 
[Epoch 29/329] [Batch 0/152] [D loss: 0.447288] [G loss: 0.198345] [ema: 0.998429] 
[Epoch 29/329] [Batch 100/152] [D loss: 0.410912] [G loss: 0.191727] [ema: 0.998464] 
[Epoch 30/329] [Batch 0/152] [D loss: 0.359772] [G loss: 0.202392] [ema: 0.998481] 
[Epoch 30/329] [Batch 100/152] [D loss: 0.384967] [G loss: 0.192827] [ema: 0.998514] 
[Epoch 31/329] [Batch 0/152] [D loss: 0.368948] [G loss: 0.190548] [ema: 0.998530] 
[Epoch 31/329] [Batch 100/152] [D loss: 0.421001] [G loss: 0.176414] [ema: 0.998561] 
[Epoch 32/329] [Batch 0/152] [D loss: 0.342007] [G loss: 0.187918] [ema: 0.998576] 
[Epoch 32/329] [Batch 100/152] [D loss: 0.402336] [G loss: 0.166105] [ema: 0.998605] 
[Epoch 33/329] [Batch 0/152] [D loss: 0.366333] [G loss: 0.201348] [ema: 0.998619] 
[Epoch 33/329] [Batch 100/152] [D loss: 0.347979] [G loss: 0.194485] [ema: 0.998646] 
[Epoch 34/329] [Batch 0/152] [D loss: 0.428043] [G loss: 0.181228] [ema: 0.998660] 
[Epoch 34/329] [Batch 100/152] [D loss: 0.387563] [G loss: 0.172952] [ema: 0.998685] 
[Epoch 35/329] [Batch 0/152] [D loss: 0.412101] [G loss: 0.189698] [ema: 0.998698] 
[Epoch 35/329] [Batch 100/152] [D loss: 0.354290] [G loss: 0.194421] [ema: 0.998722] 
[Epoch 36/329] [Batch 0/152] [D loss: 0.357115] [G loss: 0.205177] [ema: 0.998734] 
[Epoch 36/329] [Batch 100/152] [D loss: 0.420948] [G loss: 0.170237] [ema: 0.998757] 
[Epoch 37/329] [Batch 0/152] [D loss: 0.384440] [G loss: 0.177016] [ema: 0.998768] 
[Epoch 37/329] [Batch 100/152] [D loss: 0.427316] [G loss: 0.182344] [ema: 0.998790] 
[Epoch 38/329] [Batch 0/152] [D loss: 0.395282] [G loss: 0.165185] [ema: 0.998801] 
[Epoch 38/329] [Batch 100/152] [D loss: 0.356296] [G loss: 0.197227] [ema: 0.998821] 
[Epoch 39/329] [Batch 0/152] [D loss: 0.415739] [G loss: 0.203891] [ema: 0.998831] 
[Epoch 39/329] [Batch 100/152] [D loss: 0.371541] [G loss: 0.181912] [ema: 0.998851] 
[Epoch 40/329] [Batch 0/152] [D loss: 0.380017] [G loss: 0.178171] [ema: 0.998861] 
[Epoch 40/329] [Batch 100/152] [D loss: 0.400826] [G loss: 0.185098] [ema: 0.998879] 
[Epoch 41/329] [Batch 0/152] [D loss: 0.368269] [G loss: 0.180370] [ema: 0.998888] 
[Epoch 41/329] [Batch 100/152] [D loss: 0.396969] [G loss: 0.187779] [ema: 0.998906] 
[Epoch 42/329] [Batch 0/152] [D loss: 0.365148] [G loss: 0.203645] [ema: 0.998915] 
[Epoch 42/329] [Batch 100/152] [D loss: 0.405956] [G loss: 0.184908] [ema: 0.998932] 
[Epoch 43/329] [Batch 0/152] [D loss: 0.351804] [G loss: 0.201259] [ema: 0.998940] 
[Epoch 43/329] [Batch 100/152] [D loss: 0.371151] [G loss: 0.181402] [ema: 0.998956] 
[Epoch 44/329] [Batch 0/152] [D loss: 0.384967] [G loss: 0.192488] [ema: 0.998964] 
[Epoch 44/329] [Batch 100/152] [D loss: 0.359847] [G loss: 0.188699] [ema: 0.998979] 
[Epoch 45/329] [Batch 0/152] [D loss: 0.361357] [G loss: 0.189355] [ema: 0.998987] 
[Epoch 45/329] [Batch 100/152] [D loss: 0.384299] [G loss: 0.179873] [ema: 0.999002] 
[Epoch 46/329] [Batch 0/152] [D loss: 0.429589] [G loss: 0.190628] [ema: 0.999009] 
[Epoch 46/329] [Batch 100/152] [D loss: 0.402629] [G loss: 0.172883] [ema: 0.999023] 
[Epoch 47/329] [Batch 0/152] [D loss: 0.420935] [G loss: 0.187130] [ema: 0.999030] 
[Epoch 47/329] [Batch 100/152] [D loss: 0.355018] [G loss: 0.205006] [ema: 0.999044] 
[Epoch 48/329] [Batch 0/152] [D loss: 0.416221] [G loss: 0.177863] [ema: 0.999050] 
[Epoch 48/329] [Batch 100/152] [D loss: 0.362854] [G loss: 0.206870] [ema: 0.999063] 
[Epoch 49/329] [Batch 0/152] [D loss: 0.412926] [G loss: 0.189745] [ema: 0.999070] 
[Epoch 49/329] [Batch 100/152] [D loss: 0.389370] [G loss: 0.183186] [ema: 0.999082] 
[Epoch 50/329] [Batch 0/152] [D loss: 0.384556] [G loss: 0.184508] [ema: 0.999088] 
[Epoch 50/329] [Batch 100/152] [D loss: 0.360537] [G loss: 0.180501] [ema: 0.999100] 
[Epoch 51/329] [Batch 0/152] [D loss: 0.375375] [G loss: 0.182552] [ema: 0.999106] 
[Epoch 51/329] [Batch 100/152] [D loss: 0.404892] [G loss: 0.169967] [ema: 0.999118] 
[Epoch 52/329] [Batch 0/152] [D loss: 0.412666] [G loss: 0.177164] [ema: 0.999123] 
[Epoch 52/329] [Batch 100/152] [D loss: 0.373034] [G loss: 0.205669] [ema: 0.999134] 
[Epoch 53/329] [Batch 0/152] [D loss: 0.423080] [G loss: 0.204403] [ema: 0.999140] 
[Epoch 53/329] [Batch 100/152] [D loss: 0.380849] [G loss: 0.205590] [ema: 0.999150] 
[Epoch 54/329] [Batch 0/152] [D loss: 0.358506] [G loss: 0.194512] [ema: 0.999156] 
[Epoch 54/329] [Batch 100/152] [D loss: 0.356020] [G loss: 0.195293] [ema: 0.999166] 
[Epoch 55/329] [Batch 0/152] [D loss: 0.364661] [G loss: 0.190147] [ema: 0.999171] 
[Epoch 55/329] [Batch 100/152] [D loss: 0.389353] [G loss: 0.185963] [ema: 0.999181] 
[Epoch 56/329] [Batch 0/152] [D loss: 0.382095] [G loss: 0.187596] [ema: 0.999186] 
[Epoch 56/329] [Batch 100/152] [D loss: 0.376921] [G loss: 0.200201] [ema: 0.999195] 
[Epoch 57/329] [Batch 0/152] [D loss: 0.353460] [G loss: 0.197375] [ema: 0.999200] 
[Epoch 57/329] [Batch 100/152] [D loss: 0.333674] [G loss: 0.197083] [ema: 0.999209] 
[Epoch 58/329] [Batch 0/152] [D loss: 0.347163] [G loss: 0.198647] [ema: 0.999214] 
[Epoch 58/329] [Batch 100/152] [D loss: 0.376306] [G loss: 0.204101] [ema: 0.999223] 
[Epoch 59/329] [Batch 0/152] [D loss: 0.322731] [G loss: 0.210526] [ema: 0.999227] 
[Epoch 59/329] [Batch 100/152] [D loss: 0.354836] [G loss: 0.191758] [ema: 0.999236] 
[Epoch 60/329] [Batch 0/152] [D loss: 0.395544] [G loss: 0.197826] [ema: 0.999240] 
[Epoch 60/329] [Batch 100/152] [D loss: 0.390237] [G loss: 0.190756] [ema: 0.999248] 
[Epoch 61/329] [Batch 0/152] [D loss: 0.380910] [G loss: 0.195058] [ema: 0.999253] 
[Epoch 61/329] [Batch 100/152] [D loss: 0.379728] [G loss: 0.185291] [ema: 0.999261] 
[Epoch 62/329] [Batch 0/152] [D loss: 0.355305] [G loss: 0.205432] [ema: 0.999265] 
[Epoch 62/329] [Batch 100/152] [D loss: 0.335066] [G loss: 0.195269] [ema: 0.999272] 
[Epoch 63/329] [Batch 0/152] [D loss: 0.359188] [G loss: 0.194421] [ema: 0.999276] 
[Epoch 63/329] [Batch 100/152] [D loss: 0.326587] [G loss: 0.189812] [ema: 0.999284] 
[Epoch 64/329] [Batch 0/152] [D loss: 0.407523] [G loss: 0.199140] [ema: 0.999288] 
[Epoch 64/329] [Batch 100/152] [D loss: 0.405797] [G loss: 0.188602] [ema: 0.999295] 
[Epoch 65/329] [Batch 0/152] [D loss: 0.335196] [G loss: 0.209620] [ema: 0.999299] 
[Epoch 65/329] [Batch 100/152] [D loss: 0.353653] [G loss: 0.199580] [ema: 0.999306] 
[Epoch 66/329] [Batch 0/152] [D loss: 0.361127] [G loss: 0.215681] [ema: 0.999309] 
[Epoch 66/329] [Batch 100/152] [D loss: 0.394069] [G loss: 0.183273] [ema: 0.999316] 
[Epoch 67/329] [Batch 0/152] [D loss: 0.361278] [G loss: 0.181357] [ema: 0.999320] 
[Epoch 67/329] [Batch 100/152] [D loss: 0.359923] [G loss: 0.194327] [ema: 0.999326] 
[Epoch 68/329] [Batch 0/152] [D loss: 0.356814] [G loss: 0.189713] [ema: 0.999330] 
[Epoch 68/329] [Batch 100/152] [D loss: 0.368715] [G loss: 0.199315] [ema: 0.999336] 
[Epoch 69/329] [Batch 0/152] [D loss: 0.367790] [G loss: 0.193880] [ema: 0.999339] 
[Epoch 69/329] [Batch 100/152] [D loss: 0.397205] [G loss: 0.193102] [ema: 0.999346] 
[Epoch 70/329] [Batch 0/152] [D loss: 0.380497] [G loss: 0.200357] [ema: 0.999349] 
[Epoch 70/329] [Batch 100/152] [D loss: 0.388349] [G loss: 0.197015] [ema: 0.999355] 
[Epoch 71/329] [Batch 0/152] [D loss: 0.344585] [G loss: 0.208061] [ema: 0.999358] 
[Epoch 71/329] [Batch 100/152] [D loss: 0.394970] [G loss: 0.182969] [ema: 0.999364] 
[Epoch 72/329] [Batch 0/152] [D loss: 0.417350] [G loss: 0.177631] [ema: 0.999367] 
[Epoch 72/329] [Batch 100/152] [D loss: 0.361254] [G loss: 0.195952] [ema: 0.999373] 
[Epoch 73/329] [Batch 0/152] [D loss: 0.383868] [G loss: 0.194683] [ema: 0.999376] 
[Epoch 73/329] [Batch 100/152] [D loss: 0.380968] [G loss: 0.194529] [ema: 0.999381] 
[Epoch 74/329] [Batch 0/152] [D loss: 0.390440] [G loss: 0.203069] [ema: 0.999384] 
[Epoch 74/329] [Batch 100/152] [D loss: 0.313316] [G loss: 0.191929] [ema: 0.999389] 
[Epoch 75/329] [Batch 0/152] [D loss: 0.323602] [G loss: 0.230846] [ema: 0.999392] 
[Epoch 75/329] [Batch 100/152] [D loss: 0.370669] [G loss: 0.187235] [ema: 0.999397] 
[Epoch 76/329] [Batch 0/152] [D loss: 0.379707] [G loss: 0.193209] [ema: 0.999400] 
[Epoch 76/329] [Batch 100/152] [D loss: 0.429312] [G loss: 0.182253] [ema: 0.999405] 
[Epoch 77/329] [Batch 0/152] [D loss: 0.323082] [G loss: 0.217693] [ema: 0.999408] 
[Epoch 77/329] [Batch 100/152] [D loss: 0.393533] [G loss: 0.183070] [ema: 0.999413] 
[Epoch 78/329] [Batch 0/152] [D loss: 0.404529] [G loss: 0.200739] [ema: 0.999416] 
[Epoch 78/329] [Batch 100/152] [D loss: 0.386149] [G loss: 0.188430] [ema: 0.999420] 
[Epoch 79/329] [Batch 0/152] [D loss: 0.389587] [G loss: 0.202494] [ema: 0.999423] 
[Epoch 79/329] [Batch 100/152] [D loss: 0.450305] [G loss: 0.197678] [ema: 0.999428] 
[Epoch 80/329] [Batch 0/152] [D loss: 0.358093] [G loss: 0.200689] [ema: 0.999430] 
[Epoch 80/329] [Batch 100/152] [D loss: 0.346328] [G loss: 0.198236] [ema: 0.999435] 
[Epoch 81/329] [Batch 0/152] [D loss: 0.394716] [G loss: 0.191530] [ema: 0.999437] 
[Epoch 81/329] [Batch 100/152] [D loss: 0.338826] [G loss: 0.191060] [ema: 0.999442] 
[Epoch 82/329] [Batch 0/152] [D loss: 0.407999] [G loss: 0.178686] [ema: 0.999444] 
[Epoch 82/329] [Batch 100/152] [D loss: 0.345454] [G loss: 0.204541] [ema: 0.999448] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_60_100/UCI_DAGHAR_Multiclass_50000_D_60_2024_10_25_15_53_51/Model



[Epoch 83/329] [Batch 0/152] [D loss: 0.354062] [G loss: 0.197042] [ema: 0.999451] 
[Epoch 83/329] [Batch 100/152] [D loss: 0.369927] [G loss: 0.200859] [ema: 0.999455] 
[Epoch 84/329] [Batch 0/152] [D loss: 0.461592] [G loss: 0.193242] [ema: 0.999457] 
[Epoch 84/329] [Batch 100/152] [D loss: 0.362353] [G loss: 0.199917] [ema: 0.999461] 
[Epoch 85/329] [Batch 0/152] [D loss: 0.393055] [G loss: 0.198105] [ema: 0.999464] 
[Epoch 85/329] [Batch 100/152] [D loss: 0.405116] [G loss: 0.195676] [ema: 0.999468] 
[Epoch 86/329] [Batch 0/152] [D loss: 0.339255] [G loss: 0.188421] [ema: 0.999470] 
[Epoch 86/329] [Batch 100/152] [D loss: 0.386049] [G loss: 0.189153] [ema: 0.999474] 
[Epoch 87/329] [Batch 0/152] [D loss: 0.360874] [G loss: 0.208403] [ema: 0.999476] 
[Epoch 87/329] [Batch 100/152] [D loss: 0.349614] [G loss: 0.203518] [ema: 0.999480] 
[Epoch 88/329] [Batch 0/152] [D loss: 0.389021] [G loss: 0.203267] [ema: 0.999482] 
[Epoch 88/329] [Batch 100/152] [D loss: 0.395513] [G loss: 0.180401] [ema: 0.999486] 
[Epoch 89/329] [Batch 0/152] [D loss: 0.356697] [G loss: 0.181311] [ema: 0.999488] 
[Epoch 89/329] [Batch 100/152] [D loss: 0.375583] [G loss: 0.202502] [ema: 0.999492] 
[Epoch 90/329] [Batch 0/152] [D loss: 0.350194] [G loss: 0.206293] [ema: 0.999493] 
[Epoch 90/329] [Batch 100/152] [D loss: 0.394223] [G loss: 0.188741] [ema: 0.999497] 
[Epoch 91/329] [Batch 0/152] [D loss: 0.411951] [G loss: 0.212031] [ema: 0.999499] 
[Epoch 91/329] [Batch 100/152] [D loss: 0.357376] [G loss: 0.191297] [ema: 0.999503] 
[Epoch 92/329] [Batch 0/152] [D loss: 0.379788] [G loss: 0.213253] [ema: 0.999504] 
[Epoch 92/329] [Batch 100/152] [D loss: 0.369743] [G loss: 0.182219] [ema: 0.999508] 
[Epoch 93/329] [Batch 0/152] [D loss: 0.378589] [G loss: 0.200853] [ema: 0.999510] 
[Epoch 93/329] [Batch 100/152] [D loss: 0.415514] [G loss: 0.189527] [ema: 0.999513] 
[Epoch 94/329] [Batch 0/152] [D loss: 0.416589] [G loss: 0.203494] [ema: 0.999515] 
[Epoch 94/329] [Batch 100/152] [D loss: 0.353363] [G loss: 0.196953] [ema: 0.999518] 
[Epoch 95/329] [Batch 0/152] [D loss: 0.345616] [G loss: 0.220015] [ema: 0.999520] 
[Epoch 95/329] [Batch 100/152] [D loss: 0.379005] [G loss: 0.177648] [ema: 0.999523] 
[Epoch 96/329] [Batch 0/152] [D loss: 0.338472] [G loss: 0.197929] [ema: 0.999525] 
[Epoch 96/329] [Batch 100/152] [D loss: 0.406859] [G loss: 0.191494] [ema: 0.999528] 
[Epoch 97/329] [Batch 0/152] [D loss: 0.399942] [G loss: 0.196606] [ema: 0.999530] 
[Epoch 97/329] [Batch 100/152] [D loss: 0.391051] [G loss: 0.197634] [ema: 0.999533] 
[Epoch 98/329] [Batch 0/152] [D loss: 0.347796] [G loss: 0.178776] [ema: 0.999535] 
[Epoch 98/329] [Batch 100/152] [D loss: 0.390216] [G loss: 0.180422] [ema: 0.999538] 
[Epoch 99/329] [Batch 0/152] [D loss: 0.407116] [G loss: 0.191875] [ema: 0.999539] 
[Epoch 99/329] [Batch 100/152] [D loss: 0.326919] [G loss: 0.211727] [ema: 0.999543] 
[Epoch 100/329] [Batch 0/152] [D loss: 0.376139] [G loss: 0.190044] [ema: 0.999544] 
[Epoch 100/329] [Batch 100/152] [D loss: 0.398157] [G loss: 0.176329] [ema: 0.999547] 
[Epoch 101/329] [Batch 0/152] [D loss: 0.397851] [G loss: 0.195395] [ema: 0.999549] 
[Epoch 101/329] [Batch 100/152] [D loss: 0.388057] [G loss: 0.194309] [ema: 0.999552] 
[Epoch 102/329] [Batch 0/152] [D loss: 0.407528] [G loss: 0.189440] [ema: 0.999553] 
[Epoch 102/329] [Batch 100/152] [D loss: 0.370795] [G loss: 0.186899] [ema: 0.999556] 
[Epoch 103/329] [Batch 0/152] [D loss: 0.380254] [G loss: 0.186879] [ema: 0.999557] 
[Epoch 103/329] [Batch 100/152] [D loss: 0.395861] [G loss: 0.204874] [ema: 0.999560] 
[Epoch 104/329] [Batch 0/152] [D loss: 0.340913] [G loss: 0.194054] [ema: 0.999562] 
[Epoch 104/329] [Batch 100/152] [D loss: 0.334769] [G loss: 0.206227] [ema: 0.999564] 
[Epoch 105/329] [Batch 0/152] [D loss: 0.407462] [G loss: 0.203360] [ema: 0.999566] 
[Epoch 105/329] [Batch 100/152] [D loss: 0.343171] [G loss: 0.185558] [ema: 0.999568] 
[Epoch 106/329] [Batch 0/152] [D loss: 0.340188] [G loss: 0.219184] [ema: 0.999570] 
[Epoch 106/329] [Batch 100/152] [D loss: 0.370123] [G loss: 0.190890] [ema: 0.999573] 
[Epoch 107/329] [Batch 0/152] [D loss: 0.403008] [G loss: 0.185644] [ema: 0.999574] 
[Epoch 107/329] [Batch 100/152] [D loss: 0.367854] [G loss: 0.175966] [ema: 0.999577] 
[Epoch 108/329] [Batch 0/152] [D loss: 0.328165] [G loss: 0.197578] [ema: 0.999578] 
[Epoch 108/329] [Batch 100/152] [D loss: 0.365577] [G loss: 0.187848] [ema: 0.999580] 
[Epoch 109/329] [Batch 0/152] [D loss: 0.368506] [G loss: 0.208193] [ema: 0.999582] 
[Epoch 109/329] [Batch 100/152] [D loss: 0.380424] [G loss: 0.173719] [ema: 0.999584] 
[Epoch 110/329] [Batch 0/152] [D loss: 0.405363] [G loss: 0.197136] [ema: 0.999586] 
[Epoch 110/329] [Batch 100/152] [D loss: 0.360730] [G loss: 0.188657] [ema: 0.999588] 
[Epoch 111/329] [Batch 0/152] [D loss: 0.383858] [G loss: 0.207066] [ema: 0.999589] 
[Epoch 111/329] [Batch 100/152] [D loss: 0.389546] [G loss: 0.184248] [ema: 0.999592] 
[Epoch 112/329] [Batch 0/152] [D loss: 0.374143] [G loss: 0.196656] [ema: 0.999593] 
[Epoch 112/329] [Batch 100/152] [D loss: 0.403519] [G loss: 0.190002] [ema: 0.999595] 
[Epoch 113/329] [Batch 0/152] [D loss: 0.385249] [G loss: 0.202596] [ema: 0.999597] 
[Epoch 113/329] [Batch 100/152] [D loss: 0.400564] [G loss: 0.201465] [ema: 0.999599] 
[Epoch 114/329] [Batch 0/152] [D loss: 0.390556] [G loss: 0.188459] [ema: 0.999600] 
[Epoch 114/329] [Batch 100/152] [D loss: 0.346106] [G loss: 0.187638] [ema: 0.999602] 
[Epoch 115/329] [Batch 0/152] [D loss: 0.376560] [G loss: 0.196198] [ema: 0.999604] 
[Epoch 115/329] [Batch 100/152] [D loss: 0.352232] [G loss: 0.192843] [ema: 0.999606] 
[Epoch 116/329] [Batch 0/152] [D loss: 0.423563] [G loss: 0.189903] [ema: 0.999607] 
[Epoch 116/329] [Batch 100/152] [D loss: 0.371071] [G loss: 0.198289] [ema: 0.999609] 
[Epoch 117/329] [Batch 0/152] [D loss: 0.364384] [G loss: 0.188933] [ema: 0.999610] 
[Epoch 117/329] [Batch 100/152] [D loss: 0.371287] [G loss: 0.185872] [ema: 0.999612] 
[Epoch 118/329] [Batch 0/152] [D loss: 0.369544] [G loss: 0.199813] [ema: 0.999614] 
[Epoch 118/329] [Batch 100/152] [D loss: 0.339766] [G loss: 0.182128] [ema: 0.999616] 
[Epoch 119/329] [Batch 0/152] [D loss: 0.402249] [G loss: 0.204428] [ema: 0.999617] 
[Epoch 119/329] [Batch 100/152] [D loss: 0.406870] [G loss: 0.182949] [ema: 0.999619] 
[Epoch 120/329] [Batch 0/152] [D loss: 0.394860] [G loss: 0.198766] [ema: 0.999620] 
[Epoch 120/329] [Batch 100/152] [D loss: 0.344581] [G loss: 0.196568] [ema: 0.999622] 
[Epoch 121/329] [Batch 0/152] [D loss: 0.407297] [G loss: 0.193358] [ema: 0.999623] 
[Epoch 121/329] [Batch 100/152] [D loss: 0.386544] [G loss: 0.171175] [ema: 0.999625] 
[Epoch 122/329] [Batch 0/152] [D loss: 0.368685] [G loss: 0.205121] [ema: 0.999626] 
[Epoch 122/329] [Batch 100/152] [D loss: 0.356532] [G loss: 0.192239] [ema: 0.999628] 
[Epoch 123/329] [Batch 0/152] [D loss: 0.367087] [G loss: 0.197023] [ema: 0.999629] 
[Epoch 123/329] [Batch 100/152] [D loss: 0.337356] [G loss: 0.203630] [ema: 0.999631] 
[Epoch 124/329] [Batch 0/152] [D loss: 0.351232] [G loss: 0.202967] [ema: 0.999632] 
[Epoch 124/329] [Batch 100/152] [D loss: 0.377749] [G loss: 0.174565] [ema: 0.999634] 
[Epoch 125/329] [Batch 0/152] [D loss: 0.375248] [G loss: 0.194586] [ema: 0.999635] 
[Epoch 125/329] [Batch 100/152] [D loss: 0.352286] [G loss: 0.201041] [ema: 0.999637] 
[Epoch 126/329] [Batch 0/152] [D loss: 0.343243] [G loss: 0.187727] [ema: 0.999638] 
[Epoch 126/329] [Batch 100/152] [D loss: 0.375789] [G loss: 0.182873] [ema: 0.999640] 
[Epoch 127/329] [Batch 0/152] [D loss: 0.390834] [G loss: 0.203060] [ema: 0.999641] 
[Epoch 127/329] [Batch 100/152] [D loss: 0.363506] [G loss: 0.201913] [ema: 0.999643] 
[Epoch 128/329] [Batch 0/152] [D loss: 0.386766] [G loss: 0.201912] [ema: 0.999644] 
[Epoch 128/329] [Batch 100/152] [D loss: 0.367563] [G loss: 0.189216] [ema: 0.999646] 
[Epoch 129/329] [Batch 0/152] [D loss: 0.366589] [G loss: 0.197776] [ema: 0.999647] 
[Epoch 129/329] [Batch 100/152] [D loss: 0.359960] [G loss: 0.187442] [ema: 0.999648] 
[Epoch 130/329] [Batch 0/152] [D loss: 0.408874] [G loss: 0.177737] [ema: 0.999649] 
[Epoch 130/329] [Batch 100/152] [D loss: 0.382363] [G loss: 0.187585] [ema: 0.999651] 
[Epoch 131/329] [Batch 0/152] [D loss: 0.361617] [G loss: 0.181114] [ema: 0.999652] 
[Epoch 131/329] [Batch 100/152] [D loss: 0.360127] [G loss: 0.195092] [ema: 0.999654] 
[Epoch 132/329] [Batch 0/152] [D loss: 0.388800] [G loss: 0.206505] [ema: 0.999655] 
[Epoch 132/329] [Batch 100/152] [D loss: 0.328043] [G loss: 0.204577] [ema: 0.999656] 
[Epoch 133/329] [Batch 0/152] [D loss: 0.408387] [G loss: 0.193384] [ema: 0.999657] 
[Epoch 133/329] [Batch 100/152] [D loss: 0.397175] [G loss: 0.186316] [ema: 0.999659] 
[Epoch 134/329] [Batch 0/152] [D loss: 0.382238] [G loss: 0.210150] [ema: 0.999660] 
[Epoch 134/329] [Batch 100/152] [D loss: 0.341513] [G loss: 0.199210] [ema: 0.999661] 
[Epoch 135/329] [Batch 0/152] [D loss: 0.385668] [G loss: 0.215792] [ema: 0.999662] 
[Epoch 135/329] [Batch 100/152] [D loss: 0.332001] [G loss: 0.188155] [ema: 0.999664] 
[Epoch 136/329] [Batch 0/152] [D loss: 0.355643] [G loss: 0.197733] [ema: 0.999665] 
[Epoch 136/329] [Batch 100/152] [D loss: 0.346303] [G loss: 0.187751] [ema: 0.999666] 
[Epoch 137/329] [Batch 0/152] [D loss: 0.391094] [G loss: 0.186729] [ema: 0.999667] 
[Epoch 137/329] [Batch 100/152] [D loss: 0.327776] [G loss: 0.213950] [ema: 0.999669] 
[Epoch 138/329] [Batch 0/152] [D loss: 0.352944] [G loss: 0.202979] [ema: 0.999670] 
[Epoch 138/329] [Batch 100/152] [D loss: 0.367435] [G loss: 0.194189] [ema: 0.999671] 
[Epoch 139/329] [Batch 0/152] [D loss: 0.369826] [G loss: 0.197049] [ema: 0.999672] 
[Epoch 139/329] [Batch 100/152] [D loss: 0.381386] [G loss: 0.186910] [ema: 0.999674] 
[Epoch 140/329] [Batch 0/152] [D loss: 0.362928] [G loss: 0.182755] [ema: 0.999674] 
[Epoch 140/329] [Batch 100/152] [D loss: 0.354733] [G loss: 0.187697] [ema: 0.999676] 
[Epoch 141/329] [Batch 0/152] [D loss: 0.347515] [G loss: 0.202160] [ema: 0.999677] 
[Epoch 141/329] [Batch 100/152] [D loss: 0.382726] [G loss: 0.191296] [ema: 0.999678] 
[Epoch 142/329] [Batch 0/152] [D loss: 0.364407] [G loss: 0.206730] [ema: 0.999679] 
[Epoch 142/329] [Batch 100/152] [D loss: 0.394462] [G loss: 0.200862] [ema: 0.999680] 
[Epoch 143/329] [Batch 0/152] [D loss: 0.372458] [G loss: 0.200836] [ema: 0.999681] 
[Epoch 143/329] [Batch 100/152] [D loss: 0.368423] [G loss: 0.188628] [ema: 0.999683] 
[Epoch 144/329] [Batch 0/152] [D loss: 0.373881] [G loss: 0.196286] [ema: 0.999683] 
[Epoch 144/329] [Batch 100/152] [D loss: 0.377979] [G loss: 0.195357] [ema: 0.999685] 
[Epoch 145/329] [Batch 0/152] [D loss: 0.345743] [G loss: 0.197552] [ema: 0.999686] 
[Epoch 145/329] [Batch 100/152] [D loss: 0.396370] [G loss: 0.171802] [ema: 0.999687] 
[Epoch 146/329] [Batch 0/152] [D loss: 0.386636] [G loss: 0.195309] [ema: 0.999688] 
[Epoch 146/329] [Batch 100/152] [D loss: 0.334358] [G loss: 0.204817] [ema: 0.999689] 
[Epoch 147/329] [Batch 0/152] [D loss: 0.371556] [G loss: 0.205022] [ema: 0.999690] 
[Epoch 147/329] [Batch 100/152] [D loss: 0.363025] [G loss: 0.205097] [ema: 0.999691] 
[Epoch 148/329] [Batch 0/152] [D loss: 0.381303] [G loss: 0.197924] [ema: 0.999692] 
[Epoch 148/329] [Batch 100/152] [D loss: 0.461333] [G loss: 0.181411] [ema: 0.999693] 
[Epoch 149/329] [Batch 0/152] [D loss: 0.365446] [G loss: 0.188773] [ema: 0.999694] 
[Epoch 149/329] [Batch 100/152] [D loss: 0.396622] [G loss: 0.189277] [ema: 0.999695] 
[Epoch 150/329] [Batch 0/152] [D loss: 0.346940] [G loss: 0.188699] [ema: 0.999696] 
[Epoch 150/329] [Batch 100/152] [D loss: 0.337679] [G loss: 0.190516] [ema: 0.999697] 
[Epoch 151/329] [Batch 0/152] [D loss: 0.344686] [G loss: 0.211100] [ema: 0.999698] 
[Epoch 151/329] [Batch 100/152] [D loss: 0.343885] [G loss: 0.197373] [ema: 0.999699] 
[Epoch 152/329] [Batch 0/152] [D loss: 0.340665] [G loss: 0.198633] [ema: 0.999700] 
[Epoch 152/329] [Batch 100/152] [D loss: 0.375838] [G loss: 0.199336] [ema: 0.999701] 
[Epoch 153/329] [Batch 0/152] [D loss: 0.391455] [G loss: 0.199676] [ema: 0.999702] 
[Epoch 153/329] [Batch 100/152] [D loss: 0.423948] [G loss: 0.182564] [ema: 0.999703] 
[Epoch 154/329] [Batch 0/152] [D loss: 0.371454] [G loss: 0.197613] [ema: 0.999704] 
[Epoch 154/329] [Batch 100/152] [D loss: 0.382113] [G loss: 0.191557] [ema: 0.999705] 
[Epoch 155/329] [Batch 0/152] [D loss: 0.385689] [G loss: 0.196673] [ema: 0.999706] 
[Epoch 155/329] [Batch 100/152] [D loss: 0.404529] [G loss: 0.197716] [ema: 0.999707] 
[Epoch 156/329] [Batch 0/152] [D loss: 0.380466] [G loss: 0.197651] [ema: 0.999708] 
[Epoch 156/329] [Batch 100/152] [D loss: 0.392361] [G loss: 0.186110] [ema: 0.999709] 
[Epoch 157/329] [Batch 0/152] [D loss: 0.383532] [G loss: 0.196829] [ema: 0.999710] 
[Epoch 157/329] [Batch 100/152] [D loss: 0.347082] [G loss: 0.196421] [ema: 0.999711] 
[Epoch 158/329] [Batch 0/152] [D loss: 0.389799] [G loss: 0.181412] [ema: 0.999711] 
[Epoch 158/329] [Batch 100/152] [D loss: 0.363662] [G loss: 0.194177] [ema: 0.999713] 
[Epoch 159/329] [Batch 0/152] [D loss: 0.327695] [G loss: 0.198145] [ema: 0.999713] 
[Epoch 159/329] [Batch 100/152] [D loss: 0.321627] [G loss: 0.203314] [ema: 0.999714] 
[Epoch 160/329] [Batch 0/152] [D loss: 0.350692] [G loss: 0.206286] [ema: 0.999715] 
[Epoch 160/329] [Batch 100/152] [D loss: 0.382259] [G loss: 0.193298] [ema: 0.999716] 
[Epoch 161/329] [Batch 0/152] [D loss: 0.334439] [G loss: 0.198917] [ema: 0.999717] 
[Epoch 161/329] [Batch 100/152] [D loss: 0.360087] [G loss: 0.192838] [ema: 0.999718] 
[Epoch 162/329] [Batch 0/152] [D loss: 0.384456] [G loss: 0.202904] [ema: 0.999719] 
[Epoch 162/329] [Batch 100/152] [D loss: 0.326954] [G loss: 0.200897] [ema: 0.999720] 
[Epoch 163/329] [Batch 0/152] [D loss: 0.376556] [G loss: 0.192564] [ema: 0.999720] 
[Epoch 163/329] [Batch 100/152] [D loss: 0.372884] [G loss: 0.191550] [ema: 0.999721] 
[Epoch 164/329] [Batch 0/152] [D loss: 0.425502] [G loss: 0.183727] [ema: 0.999722] 
[Epoch 164/329] [Batch 100/152] [D loss: 0.379677] [G loss: 0.198623] [ema: 0.999723] 
[Epoch 165/329] [Batch 0/152] [D loss: 0.415314] [G loss: 0.191420] [ema: 0.999724] 
[Epoch 165/329] [Batch 100/152] [D loss: 0.389745] [G loss: 0.186719] [ema: 0.999725] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_60_100/UCI_DAGHAR_Multiclass_50000_D_60_2024_10_25_15_53_51/Model



[Epoch 166/329] [Batch 0/152] [D loss: 0.391412] [G loss: 0.193836] [ema: 0.999725] 
[Epoch 166/329] [Batch 100/152] [D loss: 0.391257] [G loss: 0.185673] [ema: 0.999726] 
[Epoch 167/329] [Batch 0/152] [D loss: 0.389999] [G loss: 0.200220] [ema: 0.999727] 
[Epoch 167/329] [Batch 100/152] [D loss: 0.354338] [G loss: 0.203465] [ema: 0.999728] 
[Epoch 168/329] [Batch 0/152] [D loss: 0.340210] [G loss: 0.180685] [ema: 0.999729] 
[Epoch 168/329] [Batch 100/152] [D loss: 0.340368] [G loss: 0.189510] [ema: 0.999730] 
[Epoch 169/329] [Batch 0/152] [D loss: 0.400007] [G loss: 0.185538] [ema: 0.999730] 
[Epoch 169/329] [Batch 100/152] [D loss: 0.344533] [G loss: 0.192424] [ema: 0.999731] 
[Epoch 170/329] [Batch 0/152] [D loss: 0.355995] [G loss: 0.209585] [ema: 0.999732] 
[Epoch 170/329] [Batch 100/152] [D loss: 0.343641] [G loss: 0.205018] [ema: 0.999733] 
[Epoch 171/329] [Batch 0/152] [D loss: 0.372282] [G loss: 0.207625] [ema: 0.999733] 
[Epoch 171/329] [Batch 100/152] [D loss: 0.389259] [G loss: 0.179727] [ema: 0.999734] 
[Epoch 172/329] [Batch 0/152] [D loss: 0.343891] [G loss: 0.191071] [ema: 0.999735] 
[Epoch 172/329] [Batch 100/152] [D loss: 0.371687] [G loss: 0.187482] [ema: 0.999736] 
[Epoch 173/329] [Batch 0/152] [D loss: 0.391757] [G loss: 0.191474] [ema: 0.999736] 
[Epoch 173/329] [Batch 100/152] [D loss: 0.403803] [G loss: 0.196438] [ema: 0.999737] 
[Epoch 174/329] [Batch 0/152] [D loss: 0.346685] [G loss: 0.201593] [ema: 0.999738] 
[Epoch 174/329] [Batch 100/152] [D loss: 0.366614] [G loss: 0.183863] [ema: 0.999739] 
[Epoch 175/329] [Batch 0/152] [D loss: 0.409869] [G loss: 0.189547] [ema: 0.999739] 
[Epoch 175/329] [Batch 100/152] [D loss: 0.372668] [G loss: 0.189526] [ema: 0.999740] 
[Epoch 176/329] [Batch 0/152] [D loss: 0.327896] [G loss: 0.202294] [ema: 0.999741] 
[Epoch 176/329] [Batch 100/152] [D loss: 0.386002] [G loss: 0.187407] [ema: 0.999742] 
[Epoch 177/329] [Batch 0/152] [D loss: 0.383232] [G loss: 0.196876] [ema: 0.999742] 
[Epoch 177/329] [Batch 100/152] [D loss: 0.338030] [G loss: 0.191958] [ema: 0.999743] 
[Epoch 178/329] [Batch 0/152] [D loss: 0.395339] [G loss: 0.183284] [ema: 0.999744] 
[Epoch 178/329] [Batch 100/152] [D loss: 0.367647] [G loss: 0.189290] [ema: 0.999745] 
[Epoch 179/329] [Batch 0/152] [D loss: 0.361520] [G loss: 0.209158] [ema: 0.999745] 
[Epoch 179/329] [Batch 100/152] [D loss: 0.352244] [G loss: 0.180709] [ema: 0.999746] 
[Epoch 180/329] [Batch 0/152] [D loss: 0.430739] [G loss: 0.209519] [ema: 0.999747] 
[Epoch 180/329] [Batch 100/152] [D loss: 0.344014] [G loss: 0.172354] [ema: 0.999748] 
[Epoch 181/329] [Batch 0/152] [D loss: 0.380447] [G loss: 0.174101] [ema: 0.999748] 
[Epoch 181/329] [Batch 100/152] [D loss: 0.347160] [G loss: 0.187609] [ema: 0.999749] 
[Epoch 182/329] [Batch 0/152] [D loss: 0.383666] [G loss: 0.198571] [ema: 0.999749] 
[Epoch 182/329] [Batch 100/152] [D loss: 0.360023] [G loss: 0.196833] [ema: 0.999750] 
[Epoch 183/329] [Batch 0/152] [D loss: 0.418275] [G loss: 0.188962] [ema: 0.999751] 
[Epoch 183/329] [Batch 100/152] [D loss: 0.321361] [G loss: 0.186172] [ema: 0.999752] 
[Epoch 184/329] [Batch 0/152] [D loss: 0.366995] [G loss: 0.188139] [ema: 0.999752] 
[Epoch 184/329] [Batch 100/152] [D loss: 0.383865] [G loss: 0.207491] [ema: 0.999753] 
[Epoch 185/329] [Batch 0/152] [D loss: 0.383974] [G loss: 0.200496] [ema: 0.999754] 
[Epoch 185/329] [Batch 100/152] [D loss: 0.335527] [G loss: 0.185102] [ema: 0.999754] 
[Epoch 186/329] [Batch 0/152] [D loss: 0.350310] [G loss: 0.194845] [ema: 0.999755] 
[Epoch 186/329] [Batch 100/152] [D loss: 0.371032] [G loss: 0.199353] [ema: 0.999756] 
[Epoch 187/329] [Batch 0/152] [D loss: 0.395673] [G loss: 0.197388] [ema: 0.999756] 
[Epoch 187/329] [Batch 100/152] [D loss: 0.366372] [G loss: 0.191510] [ema: 0.999757] 
[Epoch 188/329] [Batch 0/152] [D loss: 0.358908] [G loss: 0.200292] [ema: 0.999757] 
[Epoch 188/329] [Batch 100/152] [D loss: 0.360183] [G loss: 0.190183] [ema: 0.999758] 
[Epoch 189/329] [Batch 0/152] [D loss: 0.407816] [G loss: 0.203494] [ema: 0.999759] 
[Epoch 189/329] [Batch 100/152] [D loss: 0.418253] [G loss: 0.190382] [ema: 0.999760] 
[Epoch 190/329] [Batch 0/152] [D loss: 0.362814] [G loss: 0.201847] [ema: 0.999760] 
[Epoch 190/329] [Batch 100/152] [D loss: 0.349108] [G loss: 0.202590] [ema: 0.999761] 
[Epoch 191/329] [Batch 0/152] [D loss: 0.357811] [G loss: 0.209990] [ema: 0.999761] 
[Epoch 191/329] [Batch 100/152] [D loss: 0.378893] [G loss: 0.178259] [ema: 0.999762] 
[Epoch 192/329] [Batch 0/152] [D loss: 0.359367] [G loss: 0.206750] [ema: 0.999763] 
[Epoch 192/329] [Batch 100/152] [D loss: 0.344407] [G loss: 0.181135] [ema: 0.999763] 
[Epoch 193/329] [Batch 0/152] [D loss: 0.433081] [G loss: 0.187300] [ema: 0.999764] 
[Epoch 193/329] [Batch 100/152] [D loss: 0.375264] [G loss: 0.188735] [ema: 0.999765] 
[Epoch 194/329] [Batch 0/152] [D loss: 0.346853] [G loss: 0.189797] [ema: 0.999765] 
[Epoch 194/329] [Batch 100/152] [D loss: 0.381431] [G loss: 0.191736] [ema: 0.999766] 
[Epoch 195/329] [Batch 0/152] [D loss: 0.366279] [G loss: 0.203775] [ema: 0.999766] 
[Epoch 195/329] [Batch 100/152] [D loss: 0.415689] [G loss: 0.183271] [ema: 0.999767] 
[Epoch 196/329] [Batch 0/152] [D loss: 0.373315] [G loss: 0.195590] [ema: 0.999767] 
[Epoch 196/329] [Batch 100/152] [D loss: 0.331492] [G loss: 0.189466] [ema: 0.999768] 
[Epoch 197/329] [Batch 0/152] [D loss: 0.361448] [G loss: 0.203977] [ema: 0.999769] 
[Epoch 197/329] [Batch 100/152] [D loss: 0.391930] [G loss: 0.193493] [ema: 0.999769] 
[Epoch 198/329] [Batch 0/152] [D loss: 0.328990] [G loss: 0.201152] [ema: 0.999770] 
[Epoch 198/329] [Batch 100/152] [D loss: 0.355252] [G loss: 0.188825] [ema: 0.999770] 
[Epoch 199/329] [Batch 0/152] [D loss: 0.375497] [G loss: 0.192135] [ema: 0.999771] 
[Epoch 199/329] [Batch 100/152] [D loss: 0.362182] [G loss: 0.203948] [ema: 0.999772] 
[Epoch 200/329] [Batch 0/152] [D loss: 0.373255] [G loss: 0.192269] [ema: 0.999772] 
[Epoch 200/329] [Batch 100/152] [D loss: 0.375832] [G loss: 0.187395] [ema: 0.999773] 
[Epoch 201/329] [Batch 0/152] [D loss: 0.357682] [G loss: 0.184426] [ema: 0.999773] 
[Epoch 201/329] [Batch 100/152] [D loss: 0.370460] [G loss: 0.182732] [ema: 0.999774] 
[Epoch 202/329] [Batch 0/152] [D loss: 0.374402] [G loss: 0.197901] [ema: 0.999774] 
[Epoch 202/329] [Batch 100/152] [D loss: 0.436899] [G loss: 0.188597] [ema: 0.999775] 
[Epoch 203/329] [Batch 0/152] [D loss: 0.403943] [G loss: 0.199691] [ema: 0.999775] 
[Epoch 203/329] [Batch 100/152] [D loss: 0.335431] [G loss: 0.202006] [ema: 0.999776] 
[Epoch 204/329] [Batch 0/152] [D loss: 0.358934] [G loss: 0.197181] [ema: 0.999776] 
[Epoch 204/329] [Batch 100/152] [D loss: 0.395526] [G loss: 0.199459] [ema: 0.999777] 
[Epoch 205/329] [Batch 0/152] [D loss: 0.360520] [G loss: 0.195020] [ema: 0.999778] 
[Epoch 205/329] [Batch 100/152] [D loss: 0.409010] [G loss: 0.183630] [ema: 0.999778] 
[Epoch 206/329] [Batch 0/152] [D loss: 0.362058] [G loss: 0.191624] [ema: 0.999779] 
[Epoch 206/329] [Batch 100/152] [D loss: 0.337899] [G loss: 0.198987] [ema: 0.999779] 
[Epoch 207/329] [Batch 0/152] [D loss: 0.407100] [G loss: 0.199741] [ema: 0.999780] 
[Epoch 207/329] [Batch 100/152] [D loss: 0.387229] [G loss: 0.174651] [ema: 0.999780] 
[Epoch 208/329] [Batch 0/152] [D loss: 0.346875] [G loss: 0.162833] [ema: 0.999781] 
[Epoch 208/329] [Batch 100/152] [D loss: 0.402628] [G loss: 0.178787] [ema: 0.999781] 
[Epoch 209/329] [Batch 0/152] [D loss: 0.374540] [G loss: 0.207004] [ema: 0.999782] 
[Epoch 209/329] [Batch 100/152] [D loss: 0.371610] [G loss: 0.190252] [ema: 0.999783] 
[Epoch 210/329] [Batch 0/152] [D loss: 0.393126] [G loss: 0.204625] [ema: 0.999783] 
[Epoch 210/329] [Batch 100/152] [D loss: 0.380657] [G loss: 0.186562] [ema: 0.999784] 
[Epoch 211/329] [Batch 0/152] [D loss: 0.353465] [G loss: 0.199944] [ema: 0.999784] 
[Epoch 211/329] [Batch 100/152] [D loss: 0.404620] [G loss: 0.184972] [ema: 0.999785] 
[Epoch 212/329] [Batch 0/152] [D loss: 0.403736] [G loss: 0.197044] [ema: 0.999785] 
[Epoch 212/329] [Batch 100/152] [D loss: 0.373648] [G loss: 0.175318] [ema: 0.999786] 
[Epoch 213/329] [Batch 0/152] [D loss: 0.333902] [G loss: 0.205031] [ema: 0.999786] 
[Epoch 213/329] [Batch 100/152] [D loss: 0.387599] [G loss: 0.170786] [ema: 0.999787] 
[Epoch 214/329] [Batch 0/152] [D loss: 0.370298] [G loss: 0.204343] [ema: 0.999787] 
[Epoch 214/329] [Batch 100/152] [D loss: 0.381552] [G loss: 0.203462] [ema: 0.999788] 
[Epoch 215/329] [Batch 0/152] [D loss: 0.375306] [G loss: 0.192760] [ema: 0.999788] 
[Epoch 215/329] [Batch 100/152] [D loss: 0.390387] [G loss: 0.177529] [ema: 0.999789] 
[Epoch 216/329] [Batch 0/152] [D loss: 0.394031] [G loss: 0.188627] [ema: 0.999789] 
[Epoch 216/329] [Batch 100/152] [D loss: 0.387363] [G loss: 0.180821] [ema: 0.999790] 
[Epoch 217/329] [Batch 0/152] [D loss: 0.387783] [G loss: 0.184911] [ema: 0.999790] 
[Epoch 217/329] [Batch 100/152] [D loss: 0.415080] [G loss: 0.191640] [ema: 0.999791] 
[Epoch 218/329] [Batch 0/152] [D loss: 0.373749] [G loss: 0.177757] [ema: 0.999791] 
[Epoch 218/329] [Batch 100/152] [D loss: 0.395269] [G loss: 0.183419] [ema: 0.999791] 
[Epoch 219/329] [Batch 0/152] [D loss: 0.323483] [G loss: 0.196228] [ema: 0.999792] 
[Epoch 219/329] [Batch 100/152] [D loss: 0.370400] [G loss: 0.186172] [ema: 0.999792] 
[Epoch 220/329] [Batch 0/152] [D loss: 0.359676] [G loss: 0.174964] [ema: 0.999793] 
[Epoch 220/329] [Batch 100/152] [D loss: 0.407393] [G loss: 0.162823] [ema: 0.999793] 
[Epoch 221/329] [Batch 0/152] [D loss: 0.390584] [G loss: 0.180522] [ema: 0.999794] 
[Epoch 221/329] [Batch 100/152] [D loss: 0.330688] [G loss: 0.180396] [ema: 0.999794] 
[Epoch 222/329] [Batch 0/152] [D loss: 0.384923] [G loss: 0.168817] [ema: 0.999795] 
[Epoch 222/329] [Batch 100/152] [D loss: 0.392825] [G loss: 0.183083] [ema: 0.999795] 
[Epoch 223/329] [Batch 0/152] [D loss: 0.441941] [G loss: 0.171802] [ema: 0.999796] 
[Epoch 223/329] [Batch 100/152] [D loss: 0.375057] [G loss: 0.180645] [ema: 0.999796] 
[Epoch 224/329] [Batch 0/152] [D loss: 0.437150] [G loss: 0.171038] [ema: 0.999796] 
[Epoch 224/329] [Batch 100/152] [D loss: 0.459984] [G loss: 0.177098] [ema: 0.999797] 
[Epoch 225/329] [Batch 0/152] [D loss: 0.366281] [G loss: 0.189127] [ema: 0.999797] 
[Epoch 225/329] [Batch 100/152] [D loss: 0.413999] [G loss: 0.196761] [ema: 0.999798] 
[Epoch 226/329] [Batch 0/152] [D loss: 0.355659] [G loss: 0.177443] [ema: 0.999798] 
[Epoch 226/329] [Batch 100/152] [D loss: 0.394788] [G loss: 0.175366] [ema: 0.999799] 
[Epoch 227/329] [Batch 0/152] [D loss: 0.418885] [G loss: 0.181517] [ema: 0.999799] 
[Epoch 227/329] [Batch 100/152] [D loss: 0.405161] [G loss: 0.174415] [ema: 0.999800] 
[Epoch 228/329] [Batch 0/152] [D loss: 0.354814] [G loss: 0.178795] [ema: 0.999800] 
[Epoch 228/329] [Batch 100/152] [D loss: 0.425653] [G loss: 0.175579] [ema: 0.999801] 
[Epoch 229/329] [Batch 0/152] [D loss: 0.412060] [G loss: 0.186860] [ema: 0.999801] 
[Epoch 229/329] [Batch 100/152] [D loss: 0.395499] [G loss: 0.167505] [ema: 0.999801] 
[Epoch 230/329] [Batch 0/152] [D loss: 0.371773] [G loss: 0.201733] [ema: 0.999802] 
[Epoch 230/329] [Batch 100/152] [D loss: 0.418388] [G loss: 0.169591] [ema: 0.999802] 
[Epoch 231/329] [Batch 0/152] [D loss: 0.423563] [G loss: 0.183270] [ema: 0.999803] 
[Epoch 231/329] [Batch 100/152] [D loss: 0.439611] [G loss: 0.185786] [ema: 0.999803] 
[Epoch 232/329] [Batch 0/152] [D loss: 0.378682] [G loss: 0.174197] [ema: 0.999803] 
[Epoch 232/329] [Batch 100/152] [D loss: 0.383473] [G loss: 0.182238] [ema: 0.999804] 
[Epoch 233/329] [Batch 0/152] [D loss: 0.409568] [G loss: 0.171008] [ema: 0.999804] 
[Epoch 233/329] [Batch 100/152] [D loss: 0.412597] [G loss: 0.182381] [ema: 0.999805] 
[Epoch 234/329] [Batch 0/152] [D loss: 0.435143] [G loss: 0.173891] [ema: 0.999805] 
[Epoch 234/329] [Batch 100/152] [D loss: 0.431141] [G loss: 0.187597] [ema: 0.999806] 
[Epoch 235/329] [Batch 0/152] [D loss: 0.419528] [G loss: 0.187947] [ema: 0.999806] 
[Epoch 235/329] [Batch 100/152] [D loss: 0.423921] [G loss: 0.180266] [ema: 0.999807] 
[Epoch 236/329] [Batch 0/152] [D loss: 0.418668] [G loss: 0.187390] [ema: 0.999807] 
[Epoch 236/329] [Batch 100/152] [D loss: 0.368593] [G loss: 0.185775] [ema: 0.999807] 
[Epoch 237/329] [Batch 0/152] [D loss: 0.376289] [G loss: 0.184499] [ema: 0.999808] 
[Epoch 237/329] [Batch 100/152] [D loss: 0.399767] [G loss: 0.175766] [ema: 0.999808] 
[Epoch 238/329] [Batch 0/152] [D loss: 0.388977] [G loss: 0.190864] [ema: 0.999808] 
[Epoch 238/329] [Batch 100/152] [D loss: 0.355265] [G loss: 0.172246] [ema: 0.999809] 
[Epoch 239/329] [Batch 0/152] [D loss: 0.406294] [G loss: 0.177209] [ema: 0.999809] 
[Epoch 239/329] [Batch 100/152] [D loss: 0.384557] [G loss: 0.177380] [ema: 0.999810] 
[Epoch 240/329] [Batch 0/152] [D loss: 0.373367] [G loss: 0.195312] [ema: 0.999810] 
[Epoch 240/329] [Batch 100/152] [D loss: 0.361883] [G loss: 0.190061] [ema: 0.999811] 
[Epoch 241/329] [Batch 0/152] [D loss: 0.416873] [G loss: 0.184279] [ema: 0.999811] 
[Epoch 241/329] [Batch 100/152] [D loss: 0.384701] [G loss: 0.183102] [ema: 0.999811] 
[Epoch 242/329] [Batch 0/152] [D loss: 0.376749] [G loss: 0.199192] [ema: 0.999812] 
[Epoch 242/329] [Batch 100/152] [D loss: 0.361768] [G loss: 0.189298] [ema: 0.999812] 
[Epoch 243/329] [Batch 0/152] [D loss: 0.398620] [G loss: 0.190745] [ema: 0.999812] 
[Epoch 243/329] [Batch 100/152] [D loss: 0.369224] [G loss: 0.189880] [ema: 0.999813] 
[Epoch 244/329] [Batch 0/152] [D loss: 0.362723] [G loss: 0.187697] [ema: 0.999813] 
[Epoch 244/329] [Batch 100/152] [D loss: 0.383692] [G loss: 0.201810] [ema: 0.999814] 
[Epoch 245/329] [Batch 0/152] [D loss: 0.386812] [G loss: 0.190651] [ema: 0.999814] 
[Epoch 245/329] [Batch 100/152] [D loss: 0.401589] [G loss: 0.197526] [ema: 0.999814] 
[Epoch 246/329] [Batch 0/152] [D loss: 0.445048] [G loss: 0.174981] [ema: 0.999815] 
[Epoch 246/329] [Batch 100/152] [D loss: 0.420212] [G loss: 0.177005] [ema: 0.999815] 
[Epoch 247/329] [Batch 0/152] [D loss: 0.415294] [G loss: 0.177932] [ema: 0.999815] 
[Epoch 247/329] [Batch 100/152] [D loss: 0.408864] [G loss: 0.178377] [ema: 0.999816] 
[Epoch 248/329] [Batch 0/152] [D loss: 0.397380] [G loss: 0.177863] [ema: 0.999816] 
[Epoch 248/329] [Batch 100/152] [D loss: 0.424203] [G loss: 0.177762] [ema: 0.999817] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_60_100/UCI_DAGHAR_Multiclass_50000_D_60_2024_10_25_15_53_51/Model



[Epoch 249/329] [Batch 0/152] [D loss: 0.372036] [G loss: 0.175595] [ema: 0.999817] 
[Epoch 249/329] [Batch 100/152] [D loss: 0.380518] [G loss: 0.173918] [ema: 0.999817] 
[Epoch 250/329] [Batch 0/152] [D loss: 0.375079] [G loss: 0.173399] [ema: 0.999818] 
[Epoch 250/329] [Batch 100/152] [D loss: 0.373924] [G loss: 0.182174] [ema: 0.999818] 
[Epoch 251/329] [Batch 0/152] [D loss: 0.378725] [G loss: 0.189401] [ema: 0.999818] 
[Epoch 251/329] [Batch 100/152] [D loss: 0.385271] [G loss: 0.180478] [ema: 0.999819] 
[Epoch 252/329] [Batch 0/152] [D loss: 0.361301] [G loss: 0.176108] [ema: 0.999819] 
[Epoch 252/329] [Batch 100/152] [D loss: 0.380361] [G loss: 0.182101] [ema: 0.999820] 
[Epoch 253/329] [Batch 0/152] [D loss: 0.409230] [G loss: 0.205483] [ema: 0.999820] 
[Epoch 253/329] [Batch 100/152] [D loss: 0.399695] [G loss: 0.184082] [ema: 0.999820] 
[Epoch 254/329] [Batch 0/152] [D loss: 0.388441] [G loss: 0.188746] [ema: 0.999820] 
[Epoch 254/329] [Batch 100/152] [D loss: 0.426373] [G loss: 0.162842] [ema: 0.999821] 
[Epoch 255/329] [Batch 0/152] [D loss: 0.443808] [G loss: 0.181479] [ema: 0.999821] 
[Epoch 255/329] [Batch 100/152] [D loss: 0.384080] [G loss: 0.165655] [ema: 0.999822] 
[Epoch 256/329] [Batch 0/152] [D loss: 0.436577] [G loss: 0.163160] [ema: 0.999822] 
[Epoch 256/329] [Batch 100/152] [D loss: 0.391456] [G loss: 0.183295] [ema: 0.999822] 
[Epoch 257/329] [Batch 0/152] [D loss: 0.401812] [G loss: 0.170329] [ema: 0.999823] 
[Epoch 257/329] [Batch 100/152] [D loss: 0.375258] [G loss: 0.165757] [ema: 0.999823] 
[Epoch 258/329] [Batch 0/152] [D loss: 0.414078] [G loss: 0.172712] [ema: 0.999823] 
[Epoch 258/329] [Batch 100/152] [D loss: 0.434352] [G loss: 0.158051] [ema: 0.999824] 
[Epoch 259/329] [Batch 0/152] [D loss: 0.461120] [G loss: 0.180334] [ema: 0.999824] 
[Epoch 259/329] [Batch 100/152] [D loss: 0.412516] [G loss: 0.188934] [ema: 0.999824] 
[Epoch 260/329] [Batch 0/152] [D loss: 0.376855] [G loss: 0.191850] [ema: 0.999825] 
[Epoch 260/329] [Batch 100/152] [D loss: 0.444093] [G loss: 0.192998] [ema: 0.999825] 
[Epoch 261/329] [Batch 0/152] [D loss: 0.371772] [G loss: 0.178628] [ema: 0.999825] 
[Epoch 261/329] [Batch 100/152] [D loss: 0.435916] [G loss: 0.160434] [ema: 0.999826] 
[Epoch 262/329] [Batch 0/152] [D loss: 0.426420] [G loss: 0.180706] [ema: 0.999826] 
[Epoch 262/329] [Batch 100/152] [D loss: 0.408586] [G loss: 0.181127] [ema: 0.999826] 
[Epoch 263/329] [Batch 0/152] [D loss: 0.397233] [G loss: 0.171254] [ema: 0.999827] 
[Epoch 263/329] [Batch 100/152] [D loss: 0.413078] [G loss: 0.179627] [ema: 0.999827] 
[Epoch 264/329] [Batch 0/152] [D loss: 0.418289] [G loss: 0.176152] [ema: 0.999827] 
[Epoch 264/329] [Batch 100/152] [D loss: 0.377027] [G loss: 0.171342] [ema: 0.999828] 
[Epoch 265/329] [Batch 0/152] [D loss: 0.417115] [G loss: 0.178113] [ema: 0.999828] 
[Epoch 265/329] [Batch 100/152] [D loss: 0.423233] [G loss: 0.170630] [ema: 0.999828] 
[Epoch 266/329] [Batch 0/152] [D loss: 0.404809] [G loss: 0.169723] [ema: 0.999829] 
[Epoch 266/329] [Batch 100/152] [D loss: 0.412525] [G loss: 0.177462] [ema: 0.999829] 
[Epoch 267/329] [Batch 0/152] [D loss: 0.385847] [G loss: 0.200354] [ema: 0.999829] 
[Epoch 267/329] [Batch 100/152] [D loss: 0.404610] [G loss: 0.171555] [ema: 0.999830] 
[Epoch 268/329] [Batch 0/152] [D loss: 0.391267] [G loss: 0.177951] [ema: 0.999830] 
[Epoch 268/329] [Batch 100/152] [D loss: 0.434032] [G loss: 0.163820] [ema: 0.999830] 
[Epoch 269/329] [Batch 0/152] [D loss: 0.424257] [G loss: 0.159159] [ema: 0.999830] 
[Epoch 269/329] [Batch 100/152] [D loss: 0.368101] [G loss: 0.182402] [ema: 0.999831] 
[Epoch 270/329] [Batch 0/152] [D loss: 0.382913] [G loss: 0.182701] [ema: 0.999831] 
[Epoch 270/329] [Batch 100/152] [D loss: 0.386937] [G loss: 0.177705] [ema: 0.999832] 
[Epoch 271/329] [Batch 0/152] [D loss: 0.378442] [G loss: 0.184490] [ema: 0.999832] 
[Epoch 271/329] [Batch 100/152] [D loss: 0.374191] [G loss: 0.172477] [ema: 0.999832] 
[Epoch 272/329] [Batch 0/152] [D loss: 0.371800] [G loss: 0.194264] [ema: 0.999832] 
[Epoch 272/329] [Batch 100/152] [D loss: 0.366992] [G loss: 0.183531] [ema: 0.999833] 
[Epoch 273/329] [Batch 0/152] [D loss: 0.402221] [G loss: 0.177732] [ema: 0.999833] 
[Epoch 273/329] [Batch 100/152] [D loss: 0.398758] [G loss: 0.162753] [ema: 0.999833] 
[Epoch 274/329] [Batch 0/152] [D loss: 0.426001] [G loss: 0.185564] [ema: 0.999834] 
[Epoch 274/329] [Batch 100/152] [D loss: 0.388495] [G loss: 0.163159] [ema: 0.999834] 
[Epoch 275/329] [Batch 0/152] [D loss: 0.415715] [G loss: 0.165466] [ema: 0.999834] 
[Epoch 275/329] [Batch 100/152] [D loss: 0.391932] [G loss: 0.173153] [ema: 0.999835] 
[Epoch 276/329] [Batch 0/152] [D loss: 0.430818] [G loss: 0.187201] [ema: 0.999835] 
[Epoch 276/329] [Batch 100/152] [D loss: 0.386467] [G loss: 0.177020] [ema: 0.999835] 
[Epoch 277/329] [Batch 0/152] [D loss: 0.410447] [G loss: 0.168366] [ema: 0.999835] 
[Epoch 277/329] [Batch 100/152] [D loss: 0.400971] [G loss: 0.179541] [ema: 0.999836] 
[Epoch 278/329] [Batch 0/152] [D loss: 0.470768] [G loss: 0.169957] [ema: 0.999836] 
[Epoch 278/329] [Batch 100/152] [D loss: 0.400074] [G loss: 0.173475] [ema: 0.999836] 
[Epoch 279/329] [Batch 0/152] [D loss: 0.421061] [G loss: 0.178193] [ema: 0.999837] 
[Epoch 279/329] [Batch 100/152] [D loss: 0.401990] [G loss: 0.174397] [ema: 0.999837] 
[Epoch 280/329] [Batch 0/152] [D loss: 0.395923] [G loss: 0.177540] [ema: 0.999837] 
[Epoch 280/329] [Batch 100/152] [D loss: 0.398530] [G loss: 0.173343] [ema: 0.999838] 
[Epoch 281/329] [Batch 0/152] [D loss: 0.379749] [G loss: 0.187752] [ema: 0.999838] 
[Epoch 281/329] [Batch 100/152] [D loss: 0.454940] [G loss: 0.169117] [ema: 0.999838] 
[Epoch 282/329] [Batch 0/152] [D loss: 0.390309] [G loss: 0.187032] [ema: 0.999838] 
[Epoch 282/329] [Batch 100/152] [D loss: 0.349801] [G loss: 0.181153] [ema: 0.999839] 
[Epoch 283/329] [Batch 0/152] [D loss: 0.417743] [G loss: 0.184853] [ema: 0.999839] 
[Epoch 283/329] [Batch 100/152] [D loss: 0.397439] [G loss: 0.188923] [ema: 0.999839] 
[Epoch 284/329] [Batch 0/152] [D loss: 0.415426] [G loss: 0.179192] [ema: 0.999839] 
[Epoch 284/329] [Batch 100/152] [D loss: 0.385166] [G loss: 0.154357] [ema: 0.999840] 
[Epoch 285/329] [Batch 0/152] [D loss: 0.404502] [G loss: 0.168314] [ema: 0.999840] 
[Epoch 285/329] [Batch 100/152] [D loss: 0.430728] [G loss: 0.184570] [ema: 0.999840] 
[Epoch 286/329] [Batch 0/152] [D loss: 0.402955] [G loss: 0.167517] [ema: 0.999841] 
[Epoch 286/329] [Batch 100/152] [D loss: 0.404583] [G loss: 0.176736] [ema: 0.999841] 
[Epoch 287/329] [Batch 0/152] [D loss: 0.380050] [G loss: 0.197425] [ema: 0.999841] 
[Epoch 287/329] [Batch 100/152] [D loss: 0.406695] [G loss: 0.171437] [ema: 0.999841] 
[Epoch 288/329] [Batch 0/152] [D loss: 0.390484] [G loss: 0.172694] [ema: 0.999842] 
[Epoch 288/329] [Batch 100/152] [D loss: 0.428454] [G loss: 0.161029] [ema: 0.999842] 
[Epoch 289/329] [Batch 0/152] [D loss: 0.379852] [G loss: 0.190218] [ema: 0.999842] 
[Epoch 289/329] [Batch 100/152] [D loss: 0.383902] [G loss: 0.180914] [ema: 0.999843] 
[Epoch 290/329] [Batch 0/152] [D loss: 0.420373] [G loss: 0.178503] [ema: 0.999843] 
[Epoch 290/329] [Batch 100/152] [D loss: 0.405202] [G loss: 0.181382] [ema: 0.999843] 
[Epoch 291/329] [Batch 0/152] [D loss: 0.389367] [G loss: 0.176286] [ema: 0.999843] 
[Epoch 291/329] [Batch 100/152] [D loss: 0.406214] [G loss: 0.173777] [ema: 0.999844] 
[Epoch 292/329] [Batch 0/152] [D loss: 0.409887] [G loss: 0.172209] [ema: 0.999844] 
[Epoch 292/329] [Batch 100/152] [D loss: 0.417060] [G loss: 0.186782] [ema: 0.999844] 
[Epoch 293/329] [Batch 0/152] [D loss: 0.380888] [G loss: 0.179519] [ema: 0.999844] 
[Epoch 293/329] [Batch 100/152] [D loss: 0.372811] [G loss: 0.177800] [ema: 0.999845] 
[Epoch 294/329] [Batch 0/152] [D loss: 0.376474] [G loss: 0.180329] [ema: 0.999845] 
[Epoch 294/329] [Batch 100/152] [D loss: 0.394541] [G loss: 0.177755] [ema: 0.999845] 
[Epoch 295/329] [Batch 0/152] [D loss: 0.373213] [G loss: 0.186774] [ema: 0.999845] 
[Epoch 295/329] [Batch 100/152] [D loss: 0.410796] [G loss: 0.185702] [ema: 0.999846] 
[Epoch 296/329] [Batch 0/152] [D loss: 0.396292] [G loss: 0.181950] [ema: 0.999846] 
[Epoch 296/329] [Batch 100/152] [D loss: 0.390690] [G loss: 0.182932] [ema: 0.999846] 
[Epoch 297/329] [Batch 0/152] [D loss: 0.393652] [G loss: 0.177241] [ema: 0.999846] 
[Epoch 297/329] [Batch 100/152] [D loss: 0.411045] [G loss: 0.182391] [ema: 0.999847] 
[Epoch 298/329] [Batch 0/152] [D loss: 0.398796] [G loss: 0.180832] [ema: 0.999847] 
[Epoch 298/329] [Batch 100/152] [D loss: 0.380233] [G loss: 0.179490] [ema: 0.999847] 
[Epoch 299/329] [Batch 0/152] [D loss: 0.413438] [G loss: 0.164883] [ema: 0.999847] 
[Epoch 299/329] [Batch 100/152] [D loss: 0.407174] [G loss: 0.174052] [ema: 0.999848] 
[Epoch 300/329] [Batch 0/152] [D loss: 0.390808] [G loss: 0.173886] [ema: 0.999848] 
[Epoch 300/329] [Batch 100/152] [D loss: 0.406403] [G loss: 0.178073] [ema: 0.999848] 
[Epoch 301/329] [Batch 0/152] [D loss: 0.406751] [G loss: 0.176579] [ema: 0.999849] 
[Epoch 301/329] [Batch 100/152] [D loss: 0.403144] [G loss: 0.183576] [ema: 0.999849] 
[Epoch 302/329] [Batch 0/152] [D loss: 0.389840] [G loss: 0.188188] [ema: 0.999849] 
[Epoch 302/329] [Batch 100/152] [D loss: 0.382739] [G loss: 0.181612] [ema: 0.999849] 
[Epoch 303/329] [Batch 0/152] [D loss: 0.406111] [G loss: 0.174512] [ema: 0.999850] 
[Epoch 303/329] [Batch 100/152] [D loss: 0.364958] [G loss: 0.182795] [ema: 0.999850] 
[Epoch 304/329] [Batch 0/152] [D loss: 0.393633] [G loss: 0.182920] [ema: 0.999850] 
[Epoch 304/329] [Batch 100/152] [D loss: 0.375885] [G loss: 0.175055] [ema: 0.999850] 
[Epoch 305/329] [Batch 0/152] [D loss: 0.440793] [G loss: 0.164263] [ema: 0.999850] 
[Epoch 305/329] [Batch 100/152] [D loss: 0.405091] [G loss: 0.176660] [ema: 0.999851] 
[Epoch 306/329] [Batch 0/152] [D loss: 0.435275] [G loss: 0.167880] [ema: 0.999851] 
[Epoch 306/329] [Batch 100/152] [D loss: 0.445659] [G loss: 0.176775] [ema: 0.999851] 
[Epoch 307/329] [Batch 0/152] [D loss: 0.398746] [G loss: 0.172488] [ema: 0.999851] 
[Epoch 307/329] [Batch 100/152] [D loss: 0.404815] [G loss: 0.178290] [ema: 0.999852] 
[Epoch 308/329] [Batch 0/152] [D loss: 0.427004] [G loss: 0.182288] [ema: 0.999852] 
[Epoch 308/329] [Batch 100/152] [D loss: 0.380654] [G loss: 0.181628] [ema: 0.999852] 
[Epoch 309/329] [Batch 0/152] [D loss: 0.381692] [G loss: 0.191205] [ema: 0.999852] 
[Epoch 309/329] [Batch 100/152] [D loss: 0.388705] [G loss: 0.182367] [ema: 0.999853] 
[Epoch 310/329] [Batch 0/152] [D loss: 0.406687] [G loss: 0.183702] [ema: 0.999853] 
[Epoch 310/329] [Batch 100/152] [D loss: 0.421955] [G loss: 0.149324] [ema: 0.999853] 
[Epoch 311/329] [Batch 0/152] [D loss: 0.410596] [G loss: 0.182265] [ema: 0.999853] 
[Epoch 311/329] [Batch 100/152] [D loss: 0.404264] [G loss: 0.170654] [ema: 0.999854] 
[Epoch 312/329] [Batch 0/152] [D loss: 0.428637] [G loss: 0.161340] [ema: 0.999854] 
[Epoch 312/329] [Batch 100/152] [D loss: 0.374083] [G loss: 0.180892] [ema: 0.999854] 
[Epoch 313/329] [Batch 0/152] [D loss: 0.386977] [G loss: 0.166703] [ema: 0.999854] 
[Epoch 313/329] [Batch 100/152] [D loss: 0.372191] [G loss: 0.173986] [ema: 0.999855] 
[Epoch 314/329] [Batch 0/152] [D loss: 0.415152] [G loss: 0.174026] [ema: 0.999855] 
[Epoch 314/329] [Batch 100/152] [D loss: 0.389504] [G loss: 0.173234] [ema: 0.999855] 
[Epoch 315/329] [Batch 0/152] [D loss: 0.448210] [G loss: 0.164465] [ema: 0.999855] 
[Epoch 315/329] [Batch 100/152] [D loss: 0.401757] [G loss: 0.169545] [ema: 0.999856] 
[Epoch 316/329] [Batch 0/152] [D loss: 0.397612] [G loss: 0.177070] [ema: 0.999856] 
[Epoch 316/329] [Batch 100/152] [D loss: 0.398925] [G loss: 0.172953] [ema: 0.999856] 
[Epoch 317/329] [Batch 0/152] [D loss: 0.406129] [G loss: 0.183219] [ema: 0.999856] 
[Epoch 317/329] [Batch 100/152] [D loss: 0.424844] [G loss: 0.176524] [ema: 0.999856] 
[Epoch 318/329] [Batch 0/152] [D loss: 0.422602] [G loss: 0.177171] [ema: 0.999857] 
[Epoch 318/329] [Batch 100/152] [D loss: 0.432178] [G loss: 0.183367] [ema: 0.999857] 
[Epoch 319/329] [Batch 0/152] [D loss: 0.353859] [G loss: 0.192123] [ema: 0.999857] 
[Epoch 319/329] [Batch 100/152] [D loss: 0.409083] [G loss: 0.172836] [ema: 0.999857] 
[Epoch 320/329] [Batch 0/152] [D loss: 0.382990] [G loss: 0.187910] [ema: 0.999858] 
[Epoch 320/329] [Batch 100/152] [D loss: 0.379640] [G loss: 0.179814] [ema: 0.999858] 
[Epoch 321/329] [Batch 0/152] [D loss: 0.377094] [G loss: 0.180990] [ema: 0.999858] 
[Epoch 321/329] [Batch 100/152] [D loss: 0.401227] [G loss: 0.184503] [ema: 0.999858] 
[Epoch 322/329] [Batch 0/152] [D loss: 0.389075] [G loss: 0.172663] [ema: 0.999858] 
[Epoch 322/329] [Batch 100/152] [D loss: 0.405320] [G loss: 0.181748] [ema: 0.999859] 
[Epoch 323/329] [Batch 0/152] [D loss: 0.397718] [G loss: 0.193170] [ema: 0.999859] 
[Epoch 323/329] [Batch 100/152] [D loss: 0.392457] [G loss: 0.170775] [ema: 0.999859] 
[Epoch 324/329] [Batch 0/152] [D loss: 0.371751] [G loss: 0.176489] [ema: 0.999859] 
[Epoch 324/329] [Batch 100/152] [D loss: 0.375426] [G loss: 0.186923] [ema: 0.999860] 
[Epoch 325/329] [Batch 0/152] [D loss: 0.393612] [G loss: 0.179973] [ema: 0.999860] 
[Epoch 325/329] [Batch 100/152] [D loss: 0.413401] [G loss: 0.168783] [ema: 0.999860] 
[Epoch 326/329] [Batch 0/152] [D loss: 0.456981] [G loss: 0.177289] [ema: 0.999860] 
[Epoch 326/329] [Batch 100/152] [D loss: 0.437472] [G loss: 0.170863] [ema: 0.999860] 
[Epoch 327/329] [Batch 0/152] [D loss: 0.402282] [G loss: 0.175513] [ema: 0.999861] 
[Epoch 327/329] [Batch 100/152] [D loss: 0.407249] [G loss: 0.175193] [ema: 0.999861] 
[Epoch 328/329] [Batch 0/152] [D loss: 0.426816] [G loss: 0.186506] [ema: 0.999861] 
[Epoch 328/329] [Batch 100/152] [D loss: 0.430846] [G loss: 0.173807] [ema: 0.999861] 

----------------------------------------------------------------------------------------------------

 Starting individual training
RealWorld_waist_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
RealWorld_waist_DAGHAR_Multiclass
daghar
return single class data and labels, class is RealWorld_waist_DAGHAR_Multiclass
data shape is (10332, 3, 1, 60)
label shape is (10332,)
646
Epochs between checkpoint: 20



Saving checkpoint 1 in logs/daghar_split_dataset_50000_60_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_60_2024_10_25_16_30_38/Model



[Epoch 0/78] [Batch 0/646] [D loss: 1.159498] [G loss: 0.375937] [ema: 0.000000] 
[Epoch 0/78] [Batch 100/646] [D loss: 0.463433] [G loss: 0.213037] [ema: 0.933033] 
[Epoch 0/78] [Batch 200/646] [D loss: 0.560383] [G loss: 0.153544] [ema: 0.965936] 
[Epoch 0/78] [Batch 300/646] [D loss: 0.438204] [G loss: 0.155658] [ema: 0.977160] 
[Epoch 0/78] [Batch 400/646] [D loss: 0.485243] [G loss: 0.143608] [ema: 0.982821] 
[Epoch 0/78] [Batch 500/646] [D loss: 0.482065] [G loss: 0.178611] [ema: 0.986233] 
[Epoch 0/78] [Batch 600/646] [D loss: 0.441074] [G loss: 0.174011] [ema: 0.988514] 
[Epoch 1/78] [Batch 0/646] [D loss: 0.444551] [G loss: 0.175302] [ema: 0.989328] 
[Epoch 1/78] [Batch 100/646] [D loss: 0.509937] [G loss: 0.156101] [ema: 0.990752] 
[Epoch 1/78] [Batch 200/646] [D loss: 0.396959] [G loss: 0.147268] [ema: 0.991840] 
[Epoch 1/78] [Batch 300/646] [D loss: 0.438967] [G loss: 0.157089] [ema: 0.992700] 
[Epoch 1/78] [Batch 400/646] [D loss: 0.392331] [G loss: 0.197246] [ema: 0.993395] 
[Epoch 1/78] [Batch 500/646] [D loss: 0.416562] [G loss: 0.164299] [ema: 0.993970] 
[Epoch 1/78] [Batch 600/646] [D loss: 0.371536] [G loss: 0.204484] [ema: 0.994452] 
[Epoch 2/78] [Batch 0/646] [D loss: 0.447980] [G loss: 0.144341] [ema: 0.994649] 
[Epoch 2/78] [Batch 100/646] [D loss: 0.411831] [G loss: 0.161501] [ema: 0.995033] 
[Epoch 2/78] [Batch 200/646] [D loss: 0.405614] [G loss: 0.166130] [ema: 0.995365] 
[Epoch 2/78] [Batch 300/646] [D loss: 0.470035] [G loss: 0.176781] [ema: 0.995656] 
[Epoch 2/78] [Batch 400/646] [D loss: 0.439864] [G loss: 0.140497] [ema: 0.995912] 
[Epoch 2/78] [Batch 500/646] [D loss: 0.390698] [G loss: 0.192892] [ema: 0.996139] 
[Epoch 2/78] [Batch 600/646] [D loss: 0.492862] [G loss: 0.136590] [ema: 0.996343] 
[Epoch 3/78] [Batch 0/646] [D loss: 0.408553] [G loss: 0.188816] [ema: 0.996430] 
[Epoch 3/78] [Batch 100/646] [D loss: 0.434825] [G loss: 0.177236] [ema: 0.996605] 
[Epoch 3/78] [Batch 200/646] [D loss: 0.424392] [G loss: 0.195076] [ema: 0.996763] 
[Epoch 3/78] [Batch 300/646] [D loss: 0.398610] [G loss: 0.187540] [ema: 0.996908] 
[Epoch 3/78] [Batch 400/646] [D loss: 0.409313] [G loss: 0.195107] [ema: 0.997040] 
[Epoch 3/78] [Batch 500/646] [D loss: 0.405111] [G loss: 0.175295] [ema: 0.997161] 
[Epoch 3/78] [Batch 600/646] [D loss: 0.423435] [G loss: 0.162365] [ema: 0.997273] 
[Epoch 4/78] [Batch 0/646] [D loss: 0.432542] [G loss: 0.172807] [ema: 0.997321] 
[Epoch 4/78] [Batch 100/646] [D loss: 0.396055] [G loss: 0.201625] [ema: 0.997421] 
[Epoch 4/78] [Batch 200/646] [D loss: 0.402946] [G loss: 0.200859] [ema: 0.997513] 
[Epoch 4/78] [Batch 300/646] [D loss: 0.394088] [G loss: 0.171304] [ema: 0.997599] 
[Epoch 4/78] [Batch 400/646] [D loss: 0.399240] [G loss: 0.180691] [ema: 0.997680] 
[Epoch 4/78] [Batch 500/646] [D loss: 0.421935] [G loss: 0.138242] [ema: 0.997755] 
[Epoch 4/78] [Batch 600/646] [D loss: 0.409361] [G loss: 0.160873] [ema: 0.997825] 
[Epoch 5/78] [Batch 0/646] [D loss: 0.391862] [G loss: 0.196835] [ema: 0.997856] 
[Epoch 5/78] [Batch 100/646] [D loss: 0.348070] [G loss: 0.191080] [ema: 0.997921] 
[Epoch 5/78] [Batch 200/646] [D loss: 0.414222] [G loss: 0.187729] [ema: 0.997981] 
[Epoch 5/78] [Batch 300/646] [D loss: 0.401538] [G loss: 0.169015] [ema: 0.998038] 
[Epoch 5/78] [Batch 400/646] [D loss: 0.439299] [G loss: 0.180597] [ema: 0.998092] 
[Epoch 5/78] [Batch 500/646] [D loss: 0.428941] [G loss: 0.182291] [ema: 0.998143] 
[Epoch 5/78] [Batch 600/646] [D loss: 0.387821] [G loss: 0.180817] [ema: 0.998192] 
[Epoch 6/78] [Batch 0/646] [D loss: 0.390700] [G loss: 0.190298] [ema: 0.998213] 
[Epoch 6/78] [Batch 100/646] [D loss: 0.402677] [G loss: 0.188228] [ema: 0.998258] 
[Epoch 6/78] [Batch 200/646] [D loss: 0.383452] [G loss: 0.170286] [ema: 0.998301] 
[Epoch 6/78] [Batch 300/646] [D loss: 0.441474] [G loss: 0.165746] [ema: 0.998342] 
[Epoch 6/78] [Batch 400/646] [D loss: 0.395721] [G loss: 0.193212] [ema: 0.998380] 
[Epoch 6/78] [Batch 500/646] [D loss: 0.410839] [G loss: 0.172669] [ema: 0.998417] 
[Epoch 6/78] [Batch 600/646] [D loss: 0.446642] [G loss: 0.171441] [ema: 0.998453] 
[Epoch 7/78] [Batch 0/646] [D loss: 0.413178] [G loss: 0.183579] [ema: 0.998468] 
[Epoch 7/78] [Batch 100/646] [D loss: 0.412591] [G loss: 0.186241] [ema: 0.998501] 
[Epoch 7/78] [Batch 200/646] [D loss: 0.368873] [G loss: 0.171713] [ema: 0.998533] 
[Epoch 7/78] [Batch 300/646] [D loss: 0.390100] [G loss: 0.191438] [ema: 0.998564] 
[Epoch 7/78] [Batch 400/646] [D loss: 0.385200] [G loss: 0.199268] [ema: 0.998593] 
[Epoch 7/78] [Batch 500/646] [D loss: 0.434916] [G loss: 0.178737] [ema: 0.998621] 
[Epoch 7/78] [Batch 600/646] [D loss: 0.397486] [G loss: 0.170182] [ema: 0.998648] 
[Epoch 8/78] [Batch 0/646] [D loss: 0.412740] [G loss: 0.216808] [ema: 0.998660] 
[Epoch 8/78] [Batch 100/646] [D loss: 0.389665] [G loss: 0.151435] [ema: 0.998685] 
[Epoch 8/78] [Batch 200/646] [D loss: 0.391309] [G loss: 0.183249] [ema: 0.998710] 
[Epoch 8/78] [Batch 300/646] [D loss: 0.395053] [G loss: 0.168425] [ema: 0.998733] 
[Epoch 8/78] [Batch 400/646] [D loss: 0.388285] [G loss: 0.181478] [ema: 0.998756] 
[Epoch 8/78] [Batch 500/646] [D loss: 0.414277] [G loss: 0.172109] [ema: 0.998778] 
[Epoch 8/78] [Batch 600/646] [D loss: 0.392372] [G loss: 0.186459] [ema: 0.998799] 
[Epoch 9/78] [Batch 0/646] [D loss: 0.415494] [G loss: 0.175843] [ema: 0.998809] 
[Epoch 9/78] [Batch 100/646] [D loss: 0.427464] [G loss: 0.172274] [ema: 0.998829] 
[Epoch 9/78] [Batch 200/646] [D loss: 0.427667] [G loss: 0.182098] [ema: 0.998848] 
[Epoch 9/78] [Batch 300/646] [D loss: 0.413523] [G loss: 0.179427] [ema: 0.998867] 
[Epoch 9/78] [Batch 400/646] [D loss: 0.390913] [G loss: 0.169636] [ema: 0.998885] 
[Epoch 9/78] [Batch 500/646] [D loss: 0.421866] [G loss: 0.184158] [ema: 0.998903] 
[Epoch 9/78] [Batch 600/646] [D loss: 0.388021] [G loss: 0.175998] [ema: 0.998920] 
[Epoch 10/78] [Batch 0/646] [D loss: 0.384367] [G loss: 0.171374] [ema: 0.998928] 
[Epoch 10/78] [Batch 100/646] [D loss: 0.375965] [G loss: 0.203253] [ema: 0.998944] 
[Epoch 10/78] [Batch 200/646] [D loss: 0.323686] [G loss: 0.210482] [ema: 0.998960] 
[Epoch 10/78] [Batch 300/646] [D loss: 0.363827] [G loss: 0.193275] [ema: 0.998975] 
[Epoch 10/78] [Batch 400/646] [D loss: 0.421849] [G loss: 0.169340] [ema: 0.998990] 
[Epoch 10/78] [Batch 500/646] [D loss: 0.367913] [G loss: 0.196261] [ema: 0.999005] 
[Epoch 10/78] [Batch 600/646] [D loss: 0.380775] [G loss: 0.187393] [ema: 0.999019] 
[Epoch 11/78] [Batch 0/646] [D loss: 0.375973] [G loss: 0.196979] [ema: 0.999025] 
[Epoch 11/78] [Batch 100/646] [D loss: 0.390180] [G loss: 0.185597] [ema: 0.999039] 
[Epoch 11/78] [Batch 200/646] [D loss: 0.368787] [G loss: 0.177201] [ema: 0.999052] 
[Epoch 11/78] [Batch 300/646] [D loss: 0.384291] [G loss: 0.157991] [ema: 0.999065] 
[Epoch 11/78] [Batch 400/646] [D loss: 0.383831] [G loss: 0.182335] [ema: 0.999077] 
[Epoch 11/78] [Batch 500/646] [D loss: 0.408737] [G loss: 0.169374] [ema: 0.999089] 
[Epoch 11/78] [Batch 600/646] [D loss: 0.384080] [G loss: 0.187008] [ema: 0.999101] 
[Epoch 12/78] [Batch 0/646] [D loss: 0.396619] [G loss: 0.177525] [ema: 0.999106] 
[Epoch 12/78] [Batch 100/646] [D loss: 0.368563] [G loss: 0.185412] [ema: 0.999118] 
[Epoch 12/78] [Batch 200/646] [D loss: 0.387237] [G loss: 0.190245] [ema: 0.999129] 
[Epoch 12/78] [Batch 300/646] [D loss: 0.414034] [G loss: 0.180552] [ema: 0.999140] 
[Epoch 12/78] [Batch 400/646] [D loss: 0.395686] [G loss: 0.189902] [ema: 0.999150] 
[Epoch 12/78] [Batch 500/646] [D loss: 0.392767] [G loss: 0.182216] [ema: 0.999160] 
[Epoch 12/78] [Batch 600/646] [D loss: 0.379053] [G loss: 0.176228] [ema: 0.999170] 
[Epoch 13/78] [Batch 0/646] [D loss: 0.397684] [G loss: 0.176839] [ema: 0.999175] 
[Epoch 13/78] [Batch 100/646] [D loss: 0.401726] [G loss: 0.171927] [ema: 0.999185] 
[Epoch 13/78] [Batch 200/646] [D loss: 0.384333] [G loss: 0.195206] [ema: 0.999194] 
[Epoch 13/78] [Batch 300/646] [D loss: 0.419103] [G loss: 0.191127] [ema: 0.999203] 
[Epoch 13/78] [Batch 400/646] [D loss: 0.408456] [G loss: 0.173173] [ema: 0.999212] 
[Epoch 13/78] [Batch 500/646] [D loss: 0.431246] [G loss: 0.197529] [ema: 0.999221] 
[Epoch 13/78] [Batch 600/646] [D loss: 0.351256] [G loss: 0.188678] [ema: 0.999230] 
[Epoch 14/78] [Batch 0/646] [D loss: 0.359963] [G loss: 0.188102] [ema: 0.999234] 
[Epoch 14/78] [Batch 100/646] [D loss: 0.409059] [G loss: 0.186677] [ema: 0.999242] 
[Epoch 14/78] [Batch 200/646] [D loss: 0.356015] [G loss: 0.193009] [ema: 0.999250] 
[Epoch 14/78] [Batch 300/646] [D loss: 0.396979] [G loss: 0.171392] [ema: 0.999258] 
[Epoch 14/78] [Batch 400/646] [D loss: 0.375574] [G loss: 0.180917] [ema: 0.999266] 
[Epoch 14/78] [Batch 500/646] [D loss: 0.437213] [G loss: 0.170356] [ema: 0.999274] 
[Epoch 14/78] [Batch 600/646] [D loss: 0.359603] [G loss: 0.196938] [ema: 0.999282] 
[Epoch 15/78] [Batch 0/646] [D loss: 0.414981] [G loss: 0.192907] [ema: 0.999285] 
[Epoch 15/78] [Batch 100/646] [D loss: 0.366567] [G loss: 0.193145] [ema: 0.999292] 
[Epoch 15/78] [Batch 200/646] [D loss: 0.371251] [G loss: 0.189421] [ema: 0.999299] 
[Epoch 15/78] [Batch 300/646] [D loss: 0.392879] [G loss: 0.163353] [ema: 0.999306] 
[Epoch 15/78] [Batch 400/646] [D loss: 0.437005] [G loss: 0.158860] [ema: 0.999313] 
[Epoch 15/78] [Batch 500/646] [D loss: 0.393922] [G loss: 0.188584] [ema: 0.999320] 
[Epoch 15/78] [Batch 600/646] [D loss: 0.368436] [G loss: 0.178689] [ema: 0.999327] 
[Epoch 16/78] [Batch 0/646] [D loss: 0.425624] [G loss: 0.184265] [ema: 0.999330] 
[Epoch 16/78] [Batch 100/646] [D loss: 0.455130] [G loss: 0.170828] [ema: 0.999336] 
[Epoch 16/78] [Batch 200/646] [D loss: 0.346081] [G loss: 0.188825] [ema: 0.999342] 
[Epoch 16/78] [Batch 300/646] [D loss: 0.467946] [G loss: 0.171580] [ema: 0.999349] 
[Epoch 16/78] [Batch 400/646] [D loss: 0.376110] [G loss: 0.184542] [ema: 0.999355] 
[Epoch 16/78] [Batch 500/646] [D loss: 0.341180] [G loss: 0.194487] [ema: 0.999361] 
[Epoch 16/78] [Batch 600/646] [D loss: 0.387562] [G loss: 0.186618] [ema: 0.999366] 
[Epoch 17/78] [Batch 0/646] [D loss: 0.371513] [G loss: 0.189952] [ema: 0.999369] 
[Epoch 17/78] [Batch 100/646] [D loss: 0.396641] [G loss: 0.168163] [ema: 0.999375] 
[Epoch 17/78] [Batch 200/646] [D loss: 0.396603] [G loss: 0.183833] [ema: 0.999380] 
[Epoch 17/78] [Batch 300/646] [D loss: 0.383345] [G loss: 0.187720] [ema: 0.999386] 
[Epoch 17/78] [Batch 400/646] [D loss: 0.412523] [G loss: 0.173814] [ema: 0.999391] 
[Epoch 17/78] [Batch 500/646] [D loss: 0.393864] [G loss: 0.176591] [ema: 0.999397] 
[Epoch 17/78] [Batch 600/646] [D loss: 0.383297] [G loss: 0.166352] [ema: 0.999402] 
[Epoch 18/78] [Batch 0/646] [D loss: 0.342990] [G loss: 0.189093] [ema: 0.999404] 
[Epoch 18/78] [Batch 100/646] [D loss: 0.420028] [G loss: 0.171102] [ema: 0.999409] 
[Epoch 18/78] [Batch 200/646] [D loss: 0.377646] [G loss: 0.202187] [ema: 0.999414] 
[Epoch 18/78] [Batch 300/646] [D loss: 0.411845] [G loss: 0.177106] [ema: 0.999419] 
[Epoch 18/78] [Batch 400/646] [D loss: 0.369816] [G loss: 0.190634] [ema: 0.999424] 
[Epoch 18/78] [Batch 500/646] [D loss: 0.360662] [G loss: 0.191887] [ema: 0.999429] 
[Epoch 18/78] [Batch 600/646] [D loss: 0.381170] [G loss: 0.188425] [ema: 0.999433] 
[Epoch 19/78] [Batch 0/646] [D loss: 0.400881] [G loss: 0.182402] [ema: 0.999435] 
[Epoch 19/78] [Batch 100/646] [D loss: 0.437242] [G loss: 0.191322] [ema: 0.999440] 
[Epoch 19/78] [Batch 200/646] [D loss: 0.398064] [G loss: 0.197036] [ema: 0.999444] 
[Epoch 19/78] [Batch 300/646] [D loss: 0.357212] [G loss: 0.208214] [ema: 0.999449] 
[Epoch 19/78] [Batch 400/646] [D loss: 0.396520] [G loss: 0.185991] [ema: 0.999453] 
[Epoch 19/78] [Batch 500/646] [D loss: 0.403612] [G loss: 0.189939] [ema: 0.999458] 
[Epoch 19/78] [Batch 600/646] [D loss: 0.363879] [G loss: 0.180392] [ema: 0.999462] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_60_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_60_2024_10_25_16_30_38/Model



[Epoch 20/78] [Batch 0/646] [D loss: 0.396711] [G loss: 0.186329] [ema: 0.999464] 
[Epoch 20/78] [Batch 100/646] [D loss: 0.390795] [G loss: 0.189119] [ema: 0.999468] 
[Epoch 20/78] [Batch 200/646] [D loss: 0.404522] [G loss: 0.173112] [ema: 0.999472] 
[Epoch 20/78] [Batch 300/646] [D loss: 0.442958] [G loss: 0.167794] [ema: 0.999476] 
[Epoch 20/78] [Batch 400/646] [D loss: 0.375774] [G loss: 0.188272] [ema: 0.999480] 
[Epoch 20/78] [Batch 500/646] [D loss: 0.396269] [G loss: 0.177916] [ema: 0.999484] 
[Epoch 20/78] [Batch 600/646] [D loss: 0.404022] [G loss: 0.183806] [ema: 0.999487] 
[Epoch 21/78] [Batch 0/646] [D loss: 0.384418] [G loss: 0.170260] [ema: 0.999489] 
[Epoch 21/78] [Batch 100/646] [D loss: 0.368202] [G loss: 0.200122] [ema: 0.999493] 
[Epoch 21/78] [Batch 200/646] [D loss: 0.400174] [G loss: 0.188957] [ema: 0.999497] 
[Epoch 21/78] [Batch 300/646] [D loss: 0.345281] [G loss: 0.178553] [ema: 0.999500] 
[Epoch 21/78] [Batch 400/646] [D loss: 0.397905] [G loss: 0.198766] [ema: 0.999504] 
[Epoch 21/78] [Batch 500/646] [D loss: 0.403244] [G loss: 0.190324] [ema: 0.999507] 
[Epoch 21/78] [Batch 600/646] [D loss: 0.409217] [G loss: 0.184719] [ema: 0.999511] 
[Epoch 22/78] [Batch 0/646] [D loss: 0.406368] [G loss: 0.187003] [ema: 0.999512] 
[Epoch 22/78] [Batch 100/646] [D loss: 0.379257] [G loss: 0.195652] [ema: 0.999516] 
[Epoch 22/78] [Batch 200/646] [D loss: 0.398127] [G loss: 0.184359] [ema: 0.999519] 
[Epoch 22/78] [Batch 300/646] [D loss: 0.387500] [G loss: 0.186872] [ema: 0.999522] 
[Epoch 22/78] [Batch 400/646] [D loss: 0.404056] [G loss: 0.180382] [ema: 0.999526] 
[Epoch 22/78] [Batch 500/646] [D loss: 0.373987] [G loss: 0.185147] [ema: 0.999529] 
[Epoch 22/78] [Batch 600/646] [D loss: 0.410013] [G loss: 0.172335] [ema: 0.999532] 
[Epoch 23/78] [Batch 0/646] [D loss: 0.354110] [G loss: 0.187318] [ema: 0.999534] 
[Epoch 23/78] [Batch 100/646] [D loss: 0.409068] [G loss: 0.168005] [ema: 0.999537] 
[Epoch 23/78] [Batch 200/646] [D loss: 0.415610] [G loss: 0.187376] [ema: 0.999540] 
[Epoch 23/78] [Batch 300/646] [D loss: 0.415748] [G loss: 0.181780] [ema: 0.999543] 
[Epoch 23/78] [Batch 400/646] [D loss: 0.383770] [G loss: 0.192794] [ema: 0.999546] 
[Epoch 23/78] [Batch 500/646] [D loss: 0.422207] [G loss: 0.185364] [ema: 0.999549] 
[Epoch 23/78] [Batch 600/646] [D loss: 0.385711] [G loss: 0.176903] [ema: 0.999552] 
[Epoch 24/78] [Batch 0/646] [D loss: 0.378043] [G loss: 0.192241] [ema: 0.999553] 
[Epoch 24/78] [Batch 100/646] [D loss: 0.370247] [G loss: 0.188980] [ema: 0.999556] 
[Epoch 24/78] [Batch 200/646] [D loss: 0.416213] [G loss: 0.177489] [ema: 0.999559] 
[Epoch 24/78] [Batch 300/646] [D loss: 0.396731] [G loss: 0.188427] [ema: 0.999562] 
[Epoch 24/78] [Batch 400/646] [D loss: 0.395029] [G loss: 0.181079] [ema: 0.999564] 
[Epoch 24/78] [Batch 500/646] [D loss: 0.400666] [G loss: 0.188918] [ema: 0.999567] 
[Epoch 24/78] [Batch 600/646] [D loss: 0.382164] [G loss: 0.185737] [ema: 0.999570] 
[Epoch 25/78] [Batch 0/646] [D loss: 0.390210] [G loss: 0.194306] [ema: 0.999571] 
[Epoch 25/78] [Batch 100/646] [D loss: 0.377920] [G loss: 0.183890] [ema: 0.999574] 
[Epoch 25/78] [Batch 200/646] [D loss: 0.457455] [G loss: 0.181740] [ema: 0.999576] 
[Epoch 25/78] [Batch 300/646] [D loss: 0.354152] [G loss: 0.190757] [ema: 0.999579] 
[Epoch 25/78] [Batch 400/646] [D loss: 0.424115] [G loss: 0.173248] [ema: 0.999581] 
[Epoch 25/78] [Batch 500/646] [D loss: 0.416180] [G loss: 0.180342] [ema: 0.999584] 
[Epoch 25/78] [Batch 600/646] [D loss: 0.389156] [G loss: 0.186789] [ema: 0.999586] 
[Epoch 26/78] [Batch 0/646] [D loss: 0.359615] [G loss: 0.208407] [ema: 0.999587] 
[Epoch 26/78] [Batch 100/646] [D loss: 0.348406] [G loss: 0.179932] [ema: 0.999590] 
[Epoch 26/78] [Batch 200/646] [D loss: 0.365556] [G loss: 0.184602] [ema: 0.999592] 
[Epoch 26/78] [Batch 300/646] [D loss: 0.383271] [G loss: 0.184803] [ema: 0.999595] 
[Epoch 26/78] [Batch 400/646] [D loss: 0.404749] [G loss: 0.193233] [ema: 0.999597] 
[Epoch 26/78] [Batch 500/646] [D loss: 0.381420] [G loss: 0.192992] [ema: 0.999599] 
[Epoch 26/78] [Batch 600/646] [D loss: 0.367548] [G loss: 0.187548] [ema: 0.999602] 
[Epoch 27/78] [Batch 0/646] [D loss: 0.431244] [G loss: 0.182888] [ema: 0.999603] 
[Epoch 27/78] [Batch 100/646] [D loss: 0.409368] [G loss: 0.170237] [ema: 0.999605] 
[Epoch 27/78] [Batch 200/646] [D loss: 0.383303] [G loss: 0.166980] [ema: 0.999607] 
[Epoch 27/78] [Batch 300/646] [D loss: 0.355684] [G loss: 0.177146] [ema: 0.999609] 
[Epoch 27/78] [Batch 400/646] [D loss: 0.404959] [G loss: 0.193686] [ema: 0.999612] 
[Epoch 27/78] [Batch 500/646] [D loss: 0.378492] [G loss: 0.181383] [ema: 0.999614] 
[Epoch 27/78] [Batch 600/646] [D loss: 0.380228] [G loss: 0.182058] [ema: 0.999616] 
[Epoch 28/78] [Batch 0/646] [D loss: 0.409165] [G loss: 0.201409] [ema: 0.999617] 
[Epoch 28/78] [Batch 100/646] [D loss: 0.404874] [G loss: 0.195083] [ema: 0.999619] 
[Epoch 28/78] [Batch 200/646] [D loss: 0.368213] [G loss: 0.186736] [ema: 0.999621] 
[Epoch 28/78] [Batch 300/646] [D loss: 0.371825] [G loss: 0.198857] [ema: 0.999623] 
[Epoch 28/78] [Batch 400/646] [D loss: 0.415325] [G loss: 0.184813] [ema: 0.999625] 
[Epoch 28/78] [Batch 500/646] [D loss: 0.366159] [G loss: 0.179166] [ema: 0.999627] 
[Epoch 28/78] [Batch 600/646] [D loss: 0.395988] [G loss: 0.173326] [ema: 0.999629] 
[Epoch 29/78] [Batch 0/646] [D loss: 0.379765] [G loss: 0.188510] [ema: 0.999630] 
[Epoch 29/78] [Batch 100/646] [D loss: 0.357740] [G loss: 0.194551] [ema: 0.999632] 
[Epoch 29/78] [Batch 200/646] [D loss: 0.394570] [G loss: 0.172446] [ema: 0.999634] 
[Epoch 29/78] [Batch 300/646] [D loss: 0.440597] [G loss: 0.173022] [ema: 0.999636] 
[Epoch 29/78] [Batch 400/646] [D loss: 0.369352] [G loss: 0.190984] [ema: 0.999638] 
[Epoch 29/78] [Batch 500/646] [D loss: 0.357923] [G loss: 0.197116] [ema: 0.999640] 
[Epoch 29/78] [Batch 600/646] [D loss: 0.394224] [G loss: 0.183804] [ema: 0.999642] 
[Epoch 30/78] [Batch 0/646] [D loss: 0.361006] [G loss: 0.202252] [ema: 0.999642] 
[Epoch 30/78] [Batch 100/646] [D loss: 0.341604] [G loss: 0.178621] [ema: 0.999644] 
[Epoch 30/78] [Batch 200/646] [D loss: 0.400393] [G loss: 0.193942] [ema: 0.999646] 
[Epoch 30/78] [Batch 300/646] [D loss: 0.416046] [G loss: 0.190424] [ema: 0.999648] 
[Epoch 30/78] [Batch 400/646] [D loss: 0.396904] [G loss: 0.193759] [ema: 0.999650] 
[Epoch 30/78] [Batch 500/646] [D loss: 0.376881] [G loss: 0.179322] [ema: 0.999651] 
[Epoch 30/78] [Batch 600/646] [D loss: 0.365431] [G loss: 0.196163] [ema: 0.999653] 
[Epoch 31/78] [Batch 0/646] [D loss: 0.363444] [G loss: 0.210038] [ema: 0.999654] 
[Epoch 31/78] [Batch 100/646] [D loss: 0.396821] [G loss: 0.188514] [ema: 0.999656] 
[Epoch 31/78] [Batch 200/646] [D loss: 0.383074] [G loss: 0.183142] [ema: 0.999657] 
[Epoch 31/78] [Batch 300/646] [D loss: 0.375834] [G loss: 0.174909] [ema: 0.999659] 
[Epoch 31/78] [Batch 400/646] [D loss: 0.381024] [G loss: 0.178565] [ema: 0.999661] 
[Epoch 31/78] [Batch 500/646] [D loss: 0.333429] [G loss: 0.187552] [ema: 0.999662] 
[Epoch 31/78] [Batch 600/646] [D loss: 0.372513] [G loss: 0.185416] [ema: 0.999664] 
[Epoch 32/78] [Batch 0/646] [D loss: 0.427031] [G loss: 0.200502] [ema: 0.999665] 
[Epoch 32/78] [Batch 100/646] [D loss: 0.400656] [G loss: 0.159570] [ema: 0.999666] 
[Epoch 32/78] [Batch 200/646] [D loss: 0.376441] [G loss: 0.188341] [ema: 0.999668] 
[Epoch 32/78] [Batch 300/646] [D loss: 0.393867] [G loss: 0.183116] [ema: 0.999670] 
[Epoch 32/78] [Batch 400/646] [D loss: 0.402534] [G loss: 0.184739] [ema: 0.999671] 
[Epoch 32/78] [Batch 500/646] [D loss: 0.372010] [G loss: 0.175469] [ema: 0.999673] 
[Epoch 32/78] [Batch 600/646] [D loss: 0.340747] [G loss: 0.196881] [ema: 0.999674] 
[Epoch 33/78] [Batch 0/646] [D loss: 0.407950] [G loss: 0.179005] [ema: 0.999675] 
[Epoch 33/78] [Batch 100/646] [D loss: 0.355729] [G loss: 0.180362] [ema: 0.999676] 
[Epoch 33/78] [Batch 200/646] [D loss: 0.390890] [G loss: 0.192842] [ema: 0.999678] 
[Epoch 33/78] [Batch 300/646] [D loss: 0.404335] [G loss: 0.187060] [ema: 0.999679] 
[Epoch 33/78] [Batch 400/646] [D loss: 0.393652] [G loss: 0.192687] [ema: 0.999681] 
[Epoch 33/78] [Batch 500/646] [D loss: 0.350861] [G loss: 0.190066] [ema: 0.999682] 
[Epoch 33/78] [Batch 600/646] [D loss: 0.355249] [G loss: 0.191726] [ema: 0.999684] 
[Epoch 34/78] [Batch 0/646] [D loss: 0.402855] [G loss: 0.194980] [ema: 0.999684] 
[Epoch 34/78] [Batch 100/646] [D loss: 0.374474] [G loss: 0.194504] [ema: 0.999686] 
[Epoch 34/78] [Batch 200/646] [D loss: 0.390882] [G loss: 0.171378] [ema: 0.999687] 
[Epoch 34/78] [Batch 300/646] [D loss: 0.402609] [G loss: 0.185107] [ema: 0.999689] 
[Epoch 34/78] [Batch 400/646] [D loss: 0.389742] [G loss: 0.186154] [ema: 0.999690] 
[Epoch 34/78] [Batch 500/646] [D loss: 0.396415] [G loss: 0.188363] [ema: 0.999691] 
[Epoch 34/78] [Batch 600/646] [D loss: 0.345616] [G loss: 0.196404] [ema: 0.999693] 
[Epoch 35/78] [Batch 0/646] [D loss: 0.393304] [G loss: 0.197190] [ema: 0.999693] 
[Epoch 35/78] [Batch 100/646] [D loss: 0.393450] [G loss: 0.181003] [ema: 0.999695] 
[Epoch 35/78] [Batch 200/646] [D loss: 0.337827] [G loss: 0.208326] [ema: 0.999696] 
[Epoch 35/78] [Batch 300/646] [D loss: 0.385202] [G loss: 0.174961] [ema: 0.999697] 
[Epoch 35/78] [Batch 400/646] [D loss: 0.353497] [G loss: 0.191142] [ema: 0.999699] 
[Epoch 35/78] [Batch 500/646] [D loss: 0.361613] [G loss: 0.192379] [ema: 0.999700] 
[Epoch 35/78] [Batch 600/646] [D loss: 0.384659] [G loss: 0.184375] [ema: 0.999701] 
[Epoch 36/78] [Batch 0/646] [D loss: 0.435813] [G loss: 0.183229] [ema: 0.999702] 
[Epoch 36/78] [Batch 100/646] [D loss: 0.378776] [G loss: 0.186825] [ema: 0.999703] 
[Epoch 36/78] [Batch 200/646] [D loss: 0.388029] [G loss: 0.188946] [ema: 0.999705] 
[Epoch 36/78] [Batch 300/646] [D loss: 0.388056] [G loss: 0.178199] [ema: 0.999706] 
[Epoch 36/78] [Batch 400/646] [D loss: 0.402420] [G loss: 0.196902] [ema: 0.999707] 
[Epoch 36/78] [Batch 500/646] [D loss: 0.360832] [G loss: 0.191471] [ema: 0.999708] 
[Epoch 36/78] [Batch 600/646] [D loss: 0.408714] [G loss: 0.184623] [ema: 0.999709] 
[Epoch 37/78] [Batch 0/646] [D loss: 0.396557] [G loss: 0.181445] [ema: 0.999710] 
[Epoch 37/78] [Batch 100/646] [D loss: 0.404077] [G loss: 0.188408] [ema: 0.999711] 
[Epoch 37/78] [Batch 200/646] [D loss: 0.396841] [G loss: 0.203165] [ema: 0.999712] 
[Epoch 37/78] [Batch 300/646] [D loss: 0.374756] [G loss: 0.182476] [ema: 0.999714] 
[Epoch 37/78] [Batch 400/646] [D loss: 0.404999] [G loss: 0.179191] [ema: 0.999715] 
[Epoch 37/78] [Batch 500/646] [D loss: 0.401499] [G loss: 0.192956] [ema: 0.999716] 
[Epoch 37/78] [Batch 600/646] [D loss: 0.393127] [G loss: 0.194013] [ema: 0.999717] 
[Epoch 38/78] [Batch 0/646] [D loss: 0.411916] [G loss: 0.186252] [ema: 0.999718] 
[Epoch 38/78] [Batch 100/646] [D loss: 0.402049] [G loss: 0.181053] [ema: 0.999719] 
[Epoch 38/78] [Batch 200/646] [D loss: 0.346123] [G loss: 0.192516] [ema: 0.999720] 
[Epoch 38/78] [Batch 300/646] [D loss: 0.377736] [G loss: 0.184254] [ema: 0.999721] 
[Epoch 38/78] [Batch 400/646] [D loss: 0.360803] [G loss: 0.172185] [ema: 0.999722] 
[Epoch 38/78] [Batch 500/646] [D loss: 0.381383] [G loss: 0.181164] [ema: 0.999723] 
[Epoch 38/78] [Batch 600/646] [D loss: 0.378487] [G loss: 0.180429] [ema: 0.999724] 
[Epoch 39/78] [Batch 0/646] [D loss: 0.364947] [G loss: 0.203395] [ema: 0.999725] 
[Epoch 39/78] [Batch 100/646] [D loss: 0.364697] [G loss: 0.172229] [ema: 0.999726] 
[Epoch 39/78] [Batch 200/646] [D loss: 0.370801] [G loss: 0.182897] [ema: 0.999727] 
[Epoch 39/78] [Batch 300/646] [D loss: 0.360552] [G loss: 0.179116] [ema: 0.999728] 
[Epoch 39/78] [Batch 400/646] [D loss: 0.420082] [G loss: 0.197665] [ema: 0.999729] 
[Epoch 39/78] [Batch 500/646] [D loss: 0.336253] [G loss: 0.190589] [ema: 0.999730] 
[Epoch 39/78] [Batch 600/646] [D loss: 0.370651] [G loss: 0.183564] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_60_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_60_2024_10_25_16_30_38/Model



[Epoch 40/78] [Batch 0/646] [D loss: 0.403092] [G loss: 0.203262] [ema: 0.999732] 
[Epoch 40/78] [Batch 100/646] [D loss: 0.433770] [G loss: 0.187203] [ema: 0.999733] 
[Epoch 40/78] [Batch 200/646] [D loss: 0.402523] [G loss: 0.172732] [ema: 0.999734] 
[Epoch 40/78] [Batch 300/646] [D loss: 0.423275] [G loss: 0.190139] [ema: 0.999735] 
[Epoch 40/78] [Batch 400/646] [D loss: 0.428360] [G loss: 0.183540] [ema: 0.999736] 
[Epoch 40/78] [Batch 500/646] [D loss: 0.383295] [G loss: 0.198317] [ema: 0.999737] 
[Epoch 40/78] [Batch 600/646] [D loss: 0.380486] [G loss: 0.175462] [ema: 0.999738] 
[Epoch 41/78] [Batch 0/646] [D loss: 0.352944] [G loss: 0.185529] [ema: 0.999738] 
[Epoch 41/78] [Batch 100/646] [D loss: 0.384527] [G loss: 0.189269] [ema: 0.999739] 
[Epoch 41/78] [Batch 200/646] [D loss: 0.379124] [G loss: 0.178386] [ema: 0.999740] 
[Epoch 41/78] [Batch 300/646] [D loss: 0.394568] [G loss: 0.200128] [ema: 0.999741] 
[Epoch 41/78] [Batch 400/646] [D loss: 0.371272] [G loss: 0.177114] [ema: 0.999742] 
[Epoch 41/78] [Batch 500/646] [D loss: 0.339734] [G loss: 0.192878] [ema: 0.999743] 
[Epoch 41/78] [Batch 600/646] [D loss: 0.416505] [G loss: 0.176816] [ema: 0.999744] 
[Epoch 42/78] [Batch 0/646] [D loss: 0.365985] [G loss: 0.204018] [ema: 0.999745] 
[Epoch 42/78] [Batch 100/646] [D loss: 0.344686] [G loss: 0.195285] [ema: 0.999745] 
[Epoch 42/78] [Batch 200/646] [D loss: 0.362996] [G loss: 0.173972] [ema: 0.999746] 
[Epoch 42/78] [Batch 300/646] [D loss: 0.411060] [G loss: 0.188663] [ema: 0.999747] 
[Epoch 42/78] [Batch 400/646] [D loss: 0.349999] [G loss: 0.193802] [ema: 0.999748] 
[Epoch 42/78] [Batch 500/646] [D loss: 0.383860] [G loss: 0.193153] [ema: 0.999749] 
[Epoch 42/78] [Batch 600/646] [D loss: 0.398135] [G loss: 0.176026] [ema: 0.999750] 
[Epoch 43/78] [Batch 0/646] [D loss: 0.418155] [G loss: 0.188871] [ema: 0.999751] 
[Epoch 43/78] [Batch 100/646] [D loss: 0.337190] [G loss: 0.186027] [ema: 0.999751] 
[Epoch 43/78] [Batch 200/646] [D loss: 0.390400] [G loss: 0.178126] [ema: 0.999752] 
[Epoch 43/78] [Batch 300/646] [D loss: 0.370040] [G loss: 0.192815] [ema: 0.999753] 
[Epoch 43/78] [Batch 400/646] [D loss: 0.412950] [G loss: 0.181294] [ema: 0.999754] 
[Epoch 43/78] [Batch 500/646] [D loss: 0.357725] [G loss: 0.202440] [ema: 0.999755] 
[Epoch 43/78] [Batch 600/646] [D loss: 0.397064] [G loss: 0.175947] [ema: 0.999756] 
[Epoch 44/78] [Batch 0/646] [D loss: 0.359430] [G loss: 0.182815] [ema: 0.999756] 
[Epoch 44/78] [Batch 100/646] [D loss: 0.396207] [G loss: 0.178245] [ema: 0.999757] 
[Epoch 44/78] [Batch 200/646] [D loss: 0.419974] [G loss: 0.171497] [ema: 0.999758] 
[Epoch 44/78] [Batch 300/646] [D loss: 0.422100] [G loss: 0.190218] [ema: 0.999759] 
[Epoch 44/78] [Batch 400/646] [D loss: 0.363948] [G loss: 0.184405] [ema: 0.999760] 
[Epoch 44/78] [Batch 500/646] [D loss: 0.388231] [G loss: 0.172613] [ema: 0.999760] 
[Epoch 44/78] [Batch 600/646] [D loss: 0.371737] [G loss: 0.187953] [ema: 0.999761] 
[Epoch 45/78] [Batch 0/646] [D loss: 0.393031] [G loss: 0.184019] [ema: 0.999762] 
[Epoch 45/78] [Batch 100/646] [D loss: 0.400546] [G loss: 0.202910] [ema: 0.999762] 
[Epoch 45/78] [Batch 200/646] [D loss: 0.413169] [G loss: 0.189251] [ema: 0.999763] 
[Epoch 45/78] [Batch 300/646] [D loss: 0.346930] [G loss: 0.187007] [ema: 0.999764] 
[Epoch 45/78] [Batch 400/646] [D loss: 0.347154] [G loss: 0.188556] [ema: 0.999765] 
[Epoch 45/78] [Batch 500/646] [D loss: 0.375299] [G loss: 0.181527] [ema: 0.999766] 
[Epoch 45/78] [Batch 600/646] [D loss: 0.444390] [G loss: 0.174277] [ema: 0.999766] 
[Epoch 46/78] [Batch 0/646] [D loss: 0.365497] [G loss: 0.195860] [ema: 0.999767] 
[Epoch 46/78] [Batch 100/646] [D loss: 0.367323] [G loss: 0.191101] [ema: 0.999768] 
[Epoch 46/78] [Batch 200/646] [D loss: 0.384340] [G loss: 0.174881] [ema: 0.999768] 
[Epoch 46/78] [Batch 300/646] [D loss: 0.379303] [G loss: 0.179348] [ema: 0.999769] 
[Epoch 46/78] [Batch 400/646] [D loss: 0.366359] [G loss: 0.196640] [ema: 0.999770] 
[Epoch 46/78] [Batch 500/646] [D loss: 0.344543] [G loss: 0.193526] [ema: 0.999771] 
[Epoch 46/78] [Batch 600/646] [D loss: 0.357155] [G loss: 0.181189] [ema: 0.999771] 
[Epoch 47/78] [Batch 0/646] [D loss: 0.379333] [G loss: 0.192218] [ema: 0.999772] 
[Epoch 47/78] [Batch 100/646] [D loss: 0.385610] [G loss: 0.186162] [ema: 0.999772] 
[Epoch 47/78] [Batch 200/646] [D loss: 0.356700] [G loss: 0.178356] [ema: 0.999773] 
[Epoch 47/78] [Batch 300/646] [D loss: 0.378897] [G loss: 0.194769] [ema: 0.999774] 
[Epoch 47/78] [Batch 400/646] [D loss: 0.395910] [G loss: 0.180408] [ema: 0.999775] 
[Epoch 47/78] [Batch 500/646] [D loss: 0.393375] [G loss: 0.161732] [ema: 0.999775] 
[Epoch 47/78] [Batch 600/646] [D loss: 0.395245] [G loss: 0.175029] [ema: 0.999776] 
[Epoch 48/78] [Batch 0/646] [D loss: 0.397363] [G loss: 0.197281] [ema: 0.999776] 
[Epoch 48/78] [Batch 100/646] [D loss: 0.355028] [G loss: 0.174058] [ema: 0.999777] 
[Epoch 48/78] [Batch 200/646] [D loss: 0.368437] [G loss: 0.181030] [ema: 0.999778] 
[Epoch 48/78] [Batch 300/646] [D loss: 0.444249] [G loss: 0.191145] [ema: 0.999779] 
[Epoch 48/78] [Batch 400/646] [D loss: 0.423483] [G loss: 0.187609] [ema: 0.999779] 
[Epoch 48/78] [Batch 500/646] [D loss: 0.367866] [G loss: 0.186043] [ema: 0.999780] 
[Epoch 48/78] [Batch 600/646] [D loss: 0.406089] [G loss: 0.180289] [ema: 0.999781] 
[Epoch 49/78] [Batch 0/646] [D loss: 0.365835] [G loss: 0.186421] [ema: 0.999781] 
[Epoch 49/78] [Batch 100/646] [D loss: 0.373232] [G loss: 0.167204] [ema: 0.999782] 
[Epoch 49/78] [Batch 200/646] [D loss: 0.378998] [G loss: 0.178403] [ema: 0.999782] 
[Epoch 49/78] [Batch 300/646] [D loss: 0.360970] [G loss: 0.184815] [ema: 0.999783] 
[Epoch 49/78] [Batch 400/646] [D loss: 0.408189] [G loss: 0.179255] [ema: 0.999784] 
[Epoch 49/78] [Batch 500/646] [D loss: 0.382276] [G loss: 0.192318] [ema: 0.999784] 
[Epoch 49/78] [Batch 600/646] [D loss: 0.357632] [G loss: 0.192672] [ema: 0.999785] 
[Epoch 50/78] [Batch 0/646] [D loss: 0.377469] [G loss: 0.190786] [ema: 0.999785] 
[Epoch 50/78] [Batch 100/646] [D loss: 0.349375] [G loss: 0.190913] [ema: 0.999786] 
[Epoch 50/78] [Batch 200/646] [D loss: 0.361117] [G loss: 0.160208] [ema: 0.999787] 
[Epoch 50/78] [Batch 300/646] [D loss: 0.433369] [G loss: 0.191299] [ema: 0.999787] 
[Epoch 50/78] [Batch 400/646] [D loss: 0.349147] [G loss: 0.207069] [ema: 0.999788] 
[Epoch 50/78] [Batch 500/646] [D loss: 0.345206] [G loss: 0.198332] [ema: 0.999789] 
[Epoch 50/78] [Batch 600/646] [D loss: 0.391162] [G loss: 0.173983] [ema: 0.999789] 
[Epoch 51/78] [Batch 0/646] [D loss: 0.366582] [G loss: 0.178954] [ema: 0.999790] 
[Epoch 51/78] [Batch 100/646] [D loss: 0.368469] [G loss: 0.186603] [ema: 0.999790] 
[Epoch 51/78] [Batch 200/646] [D loss: 0.425288] [G loss: 0.176243] [ema: 0.999791] 
[Epoch 51/78] [Batch 300/646] [D loss: 0.337453] [G loss: 0.189800] [ema: 0.999792] 
[Epoch 51/78] [Batch 400/646] [D loss: 0.403907] [G loss: 0.180976] [ema: 0.999792] 
[Epoch 51/78] [Batch 500/646] [D loss: 0.365309] [G loss: 0.175844] [ema: 0.999793] 
[Epoch 51/78] [Batch 600/646] [D loss: 0.393623] [G loss: 0.176846] [ema: 0.999793] 
[Epoch 52/78] [Batch 0/646] [D loss: 0.396775] [G loss: 0.175460] [ema: 0.999794] 
[Epoch 52/78] [Batch 100/646] [D loss: 0.416148] [G loss: 0.182748] [ema: 0.999794] 
[Epoch 52/78] [Batch 200/646] [D loss: 0.399859] [G loss: 0.178848] [ema: 0.999795] 
[Epoch 52/78] [Batch 300/646] [D loss: 0.380734] [G loss: 0.202474] [ema: 0.999796] 
[Epoch 52/78] [Batch 400/646] [D loss: 0.408074] [G loss: 0.180877] [ema: 0.999796] 
[Epoch 52/78] [Batch 500/646] [D loss: 0.408687] [G loss: 0.184961] [ema: 0.999797] 
[Epoch 52/78] [Batch 600/646] [D loss: 0.391162] [G loss: 0.198847] [ema: 0.999797] 
[Epoch 53/78] [Batch 0/646] [D loss: 0.376560] [G loss: 0.197644] [ema: 0.999798] 
[Epoch 53/78] [Batch 100/646] [D loss: 0.378221] [G loss: 0.179325] [ema: 0.999798] 
[Epoch 53/78] [Batch 200/646] [D loss: 0.379232] [G loss: 0.184552] [ema: 0.999799] 
[Epoch 53/78] [Batch 300/646] [D loss: 0.390969] [G loss: 0.175930] [ema: 0.999799] 
[Epoch 53/78] [Batch 400/646] [D loss: 0.415304] [G loss: 0.180282] [ema: 0.999800] 
[Epoch 53/78] [Batch 500/646] [D loss: 0.339634] [G loss: 0.181354] [ema: 0.999800] 
[Epoch 53/78] [Batch 600/646] [D loss: 0.415341] [G loss: 0.182243] [ema: 0.999801] 
[Epoch 54/78] [Batch 0/646] [D loss: 0.357316] [G loss: 0.195901] [ema: 0.999801] 
[Epoch 54/78] [Batch 100/646] [D loss: 0.397169] [G loss: 0.175320] [ema: 0.999802] 
[Epoch 54/78] [Batch 200/646] [D loss: 0.364493] [G loss: 0.182153] [ema: 0.999802] 
[Epoch 54/78] [Batch 300/646] [D loss: 0.355693] [G loss: 0.211236] [ema: 0.999803] 
[Epoch 54/78] [Batch 400/646] [D loss: 0.356402] [G loss: 0.190983] [ema: 0.999804] 
[Epoch 54/78] [Batch 500/646] [D loss: 0.378136] [G loss: 0.185627] [ema: 0.999804] 
[Epoch 54/78] [Batch 600/646] [D loss: 0.403302] [G loss: 0.197442] [ema: 0.999805] 
[Epoch 55/78] [Batch 0/646] [D loss: 0.401964] [G loss: 0.188844] [ema: 0.999805] 
[Epoch 55/78] [Batch 100/646] [D loss: 0.383445] [G loss: 0.177453] [ema: 0.999805] 
[Epoch 55/78] [Batch 200/646] [D loss: 0.344902] [G loss: 0.186560] [ema: 0.999806] 
[Epoch 55/78] [Batch 300/646] [D loss: 0.355949] [G loss: 0.186973] [ema: 0.999807] 
[Epoch 55/78] [Batch 400/646] [D loss: 0.400324] [G loss: 0.202464] [ema: 0.999807] 
[Epoch 55/78] [Batch 500/646] [D loss: 0.365160] [G loss: 0.184528] [ema: 0.999808] 
[Epoch 55/78] [Batch 600/646] [D loss: 0.387055] [G loss: 0.171428] [ema: 0.999808] 
[Epoch 56/78] [Batch 0/646] [D loss: 0.362539] [G loss: 0.180480] [ema: 0.999808] 
[Epoch 56/78] [Batch 100/646] [D loss: 0.409113] [G loss: 0.185597] [ema: 0.999809] 
[Epoch 56/78] [Batch 200/646] [D loss: 0.399055] [G loss: 0.183069] [ema: 0.999809] 
[Epoch 56/78] [Batch 300/646] [D loss: 0.380235] [G loss: 0.180062] [ema: 0.999810] 
[Epoch 56/78] [Batch 400/646] [D loss: 0.387282] [G loss: 0.194155] [ema: 0.999811] 
[Epoch 56/78] [Batch 500/646] [D loss: 0.372596] [G loss: 0.189267] [ema: 0.999811] 
[Epoch 56/78] [Batch 600/646] [D loss: 0.390239] [G loss: 0.195805] [ema: 0.999812] 
[Epoch 57/78] [Batch 0/646] [D loss: 0.384445] [G loss: 0.191476] [ema: 0.999812] 
[Epoch 57/78] [Batch 100/646] [D loss: 0.346739] [G loss: 0.184957] [ema: 0.999812] 
[Epoch 57/78] [Batch 200/646] [D loss: 0.440607] [G loss: 0.168740] [ema: 0.999813] 
[Epoch 57/78] [Batch 300/646] [D loss: 0.383545] [G loss: 0.196453] [ema: 0.999813] 
[Epoch 57/78] [Batch 400/646] [D loss: 0.396688] [G loss: 0.182119] [ema: 0.999814] 
[Epoch 57/78] [Batch 500/646] [D loss: 0.459326] [G loss: 0.175252] [ema: 0.999814] 
[Epoch 57/78] [Batch 600/646] [D loss: 0.393448] [G loss: 0.171566] [ema: 0.999815] 
[Epoch 58/78] [Batch 0/646] [D loss: 0.394513] [G loss: 0.171551] [ema: 0.999815] 
[Epoch 58/78] [Batch 100/646] [D loss: 0.445712] [G loss: 0.175563] [ema: 0.999816] 
[Epoch 58/78] [Batch 200/646] [D loss: 0.411214] [G loss: 0.162912] [ema: 0.999816] 
[Epoch 58/78] [Batch 300/646] [D loss: 0.361485] [G loss: 0.179838] [ema: 0.999816] 
[Epoch 58/78] [Batch 400/646] [D loss: 0.373223] [G loss: 0.185383] [ema: 0.999817] 
[Epoch 58/78] [Batch 500/646] [D loss: 0.395256] [G loss: 0.171977] [ema: 0.999817] 
[Epoch 58/78] [Batch 600/646] [D loss: 0.400941] [G loss: 0.173439] [ema: 0.999818] 
[Epoch 59/78] [Batch 0/646] [D loss: 0.418334] [G loss: 0.184455] [ema: 0.999818] 
[Epoch 59/78] [Batch 100/646] [D loss: 0.327289] [G loss: 0.202521] [ema: 0.999819] 
[Epoch 59/78] [Batch 200/646] [D loss: 0.441409] [G loss: 0.184451] [ema: 0.999819] 
[Epoch 59/78] [Batch 300/646] [D loss: 0.393112] [G loss: 0.190062] [ema: 0.999820] 
[Epoch 59/78] [Batch 400/646] [D loss: 0.397224] [G loss: 0.185621] [ema: 0.999820] 
[Epoch 59/78] [Batch 500/646] [D loss: 0.401904] [G loss: 0.171999] [ema: 0.999821] 
[Epoch 59/78] [Batch 600/646] [D loss: 0.390674] [G loss: 0.165221] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_60_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_60_2024_10_25_16_30_38/Model



[Epoch 60/78] [Batch 0/646] [D loss: 0.384406] [G loss: 0.182018] [ema: 0.999821] 
[Epoch 60/78] [Batch 100/646] [D loss: 0.429227] [G loss: 0.180098] [ema: 0.999822] 
[Epoch 60/78] [Batch 200/646] [D loss: 0.419160] [G loss: 0.179212] [ema: 0.999822] 
[Epoch 60/78] [Batch 300/646] [D loss: 0.417095] [G loss: 0.180220] [ema: 0.999823] 
[Epoch 60/78] [Batch 400/646] [D loss: 0.410840] [G loss: 0.187258] [ema: 0.999823] 
[Epoch 60/78] [Batch 500/646] [D loss: 0.396817] [G loss: 0.172360] [ema: 0.999823] 
[Epoch 60/78] [Batch 600/646] [D loss: 0.395665] [G loss: 0.179628] [ema: 0.999824] 
[Epoch 61/78] [Batch 0/646] [D loss: 0.413701] [G loss: 0.181269] [ema: 0.999824] 
[Epoch 61/78] [Batch 100/646] [D loss: 0.398788] [G loss: 0.170996] [ema: 0.999825] 
[Epoch 61/78] [Batch 200/646] [D loss: 0.373661] [G loss: 0.174098] [ema: 0.999825] 
[Epoch 61/78] [Batch 300/646] [D loss: 0.399270] [G loss: 0.160575] [ema: 0.999825] 
[Epoch 61/78] [Batch 400/646] [D loss: 0.403758] [G loss: 0.172409] [ema: 0.999826] 
[Epoch 61/78] [Batch 500/646] [D loss: 0.359760] [G loss: 0.183916] [ema: 0.999826] 
[Epoch 61/78] [Batch 600/646] [D loss: 0.412465] [G loss: 0.179478] [ema: 0.999827] 
[Epoch 62/78] [Batch 0/646] [D loss: 0.420817] [G loss: 0.175804] [ema: 0.999827] 
[Epoch 62/78] [Batch 100/646] [D loss: 0.344504] [G loss: 0.191103] [ema: 0.999827] 
[Epoch 62/78] [Batch 200/646] [D loss: 0.360368] [G loss: 0.186465] [ema: 0.999828] 
[Epoch 62/78] [Batch 300/646] [D loss: 0.391290] [G loss: 0.185909] [ema: 0.999828] 
[Epoch 62/78] [Batch 400/646] [D loss: 0.421822] [G loss: 0.174773] [ema: 0.999829] 
[Epoch 62/78] [Batch 500/646] [D loss: 0.399532] [G loss: 0.176626] [ema: 0.999829] 
[Epoch 62/78] [Batch 600/646] [D loss: 0.405352] [G loss: 0.187812] [ema: 0.999830] 
[Epoch 63/78] [Batch 0/646] [D loss: 0.417670] [G loss: 0.175339] [ema: 0.999830] 
[Epoch 63/78] [Batch 100/646] [D loss: 0.418569] [G loss: 0.183084] [ema: 0.999830] 
[Epoch 63/78] [Batch 200/646] [D loss: 0.402426] [G loss: 0.178643] [ema: 0.999831] 
[Epoch 63/78] [Batch 300/646] [D loss: 0.392145] [G loss: 0.182017] [ema: 0.999831] 
[Epoch 63/78] [Batch 400/646] [D loss: 0.383614] [G loss: 0.190946] [ema: 0.999831] 
[Epoch 63/78] [Batch 500/646] [D loss: 0.405179] [G loss: 0.177002] [ema: 0.999832] 
[Epoch 63/78] [Batch 600/646] [D loss: 0.388783] [G loss: 0.169058] [ema: 0.999832] 
[Epoch 64/78] [Batch 0/646] [D loss: 0.357313] [G loss: 0.187021] [ema: 0.999832] 
[Epoch 64/78] [Batch 100/646] [D loss: 0.412627] [G loss: 0.181685] [ema: 0.999833] 
[Epoch 64/78] [Batch 200/646] [D loss: 0.435714] [G loss: 0.153484] [ema: 0.999833] 
[Epoch 64/78] [Batch 300/646] [D loss: 0.420604] [G loss: 0.166574] [ema: 0.999834] 
[Epoch 64/78] [Batch 400/646] [D loss: 0.397667] [G loss: 0.171770] [ema: 0.999834] 
[Epoch 64/78] [Batch 500/646] [D loss: 0.429258] [G loss: 0.169717] [ema: 0.999834] 
[Epoch 64/78] [Batch 600/646] [D loss: 0.400020] [G loss: 0.184251] [ema: 0.999835] 
[Epoch 65/78] [Batch 0/646] [D loss: 0.434561] [G loss: 0.181733] [ema: 0.999835] 
[Epoch 65/78] [Batch 100/646] [D loss: 0.393595] [G loss: 0.176385] [ema: 0.999835] 
[Epoch 65/78] [Batch 200/646] [D loss: 0.377661] [G loss: 0.183163] [ema: 0.999836] 
[Epoch 65/78] [Batch 300/646] [D loss: 0.415949] [G loss: 0.168938] [ema: 0.999836] 
[Epoch 65/78] [Batch 400/646] [D loss: 0.388646] [G loss: 0.177503] [ema: 0.999836] 
[Epoch 65/78] [Batch 500/646] [D loss: 0.367125] [G loss: 0.182571] [ema: 0.999837] 
[Epoch 65/78] [Batch 600/646] [D loss: 0.384028] [G loss: 0.189347] [ema: 0.999837] 
[Epoch 66/78] [Batch 0/646] [D loss: 0.430536] [G loss: 0.186976] [ema: 0.999837] 
[Epoch 66/78] [Batch 100/646] [D loss: 0.419980] [G loss: 0.181281] [ema: 0.999838] 
[Epoch 66/78] [Batch 200/646] [D loss: 0.402997] [G loss: 0.175023] [ema: 0.999838] 
[Epoch 66/78] [Batch 300/646] [D loss: 0.371976] [G loss: 0.183557] [ema: 0.999839] 
[Epoch 66/78] [Batch 400/646] [D loss: 0.376570] [G loss: 0.186688] [ema: 0.999839] 
[Epoch 66/78] [Batch 500/646] [D loss: 0.439079] [G loss: 0.178002] [ema: 0.999839] 
[Epoch 66/78] [Batch 600/646] [D loss: 0.401095] [G loss: 0.175041] [ema: 0.999840] 
[Epoch 67/78] [Batch 0/646] [D loss: 0.435568] [G loss: 0.177701] [ema: 0.999840] 
[Epoch 67/78] [Batch 100/646] [D loss: 0.399096] [G loss: 0.173902] [ema: 0.999840] 
[Epoch 67/78] [Batch 200/646] [D loss: 0.424174] [G loss: 0.178877] [ema: 0.999841] 
[Epoch 67/78] [Batch 300/646] [D loss: 0.415959] [G loss: 0.175745] [ema: 0.999841] 
[Epoch 67/78] [Batch 400/646] [D loss: 0.403253] [G loss: 0.180374] [ema: 0.999841] 
[Epoch 67/78] [Batch 500/646] [D loss: 0.431889] [G loss: 0.166116] [ema: 0.999842] 
[Epoch 67/78] [Batch 600/646] [D loss: 0.414666] [G loss: 0.185611] [ema: 0.999842] 
[Epoch 68/78] [Batch 0/646] [D loss: 0.463622] [G loss: 0.174827] [ema: 0.999842] 
[Epoch 68/78] [Batch 100/646] [D loss: 0.393468] [G loss: 0.181599] [ema: 0.999843] 
[Epoch 68/78] [Batch 200/646] [D loss: 0.453526] [G loss: 0.176367] [ema: 0.999843] 
[Epoch 68/78] [Batch 300/646] [D loss: 0.375163] [G loss: 0.187454] [ema: 0.999843] 
[Epoch 68/78] [Batch 400/646] [D loss: 0.431805] [G loss: 0.175879] [ema: 0.999844] 
[Epoch 68/78] [Batch 500/646] [D loss: 0.457664] [G loss: 0.172010] [ema: 0.999844] 
[Epoch 68/78] [Batch 600/646] [D loss: 0.454165] [G loss: 0.181103] [ema: 0.999844] 
[Epoch 69/78] [Batch 0/646] [D loss: 0.383902] [G loss: 0.181489] [ema: 0.999845] 
[Epoch 69/78] [Batch 100/646] [D loss: 0.373942] [G loss: 0.161457] [ema: 0.999845] 
[Epoch 69/78] [Batch 200/646] [D loss: 0.458880] [G loss: 0.183381] [ema: 0.999845] 
[Epoch 69/78] [Batch 300/646] [D loss: 0.443699] [G loss: 0.166892] [ema: 0.999846] 
[Epoch 69/78] [Batch 400/646] [D loss: 0.453825] [G loss: 0.175622] [ema: 0.999846] 
[Epoch 69/78] [Batch 500/646] [D loss: 0.427188] [G loss: 0.161923] [ema: 0.999846] 
[Epoch 69/78] [Batch 600/646] [D loss: 0.457406] [G loss: 0.161573] [ema: 0.999847] 
[Epoch 70/78] [Batch 0/646] [D loss: 0.433037] [G loss: 0.174501] [ema: 0.999847] 
[Epoch 70/78] [Batch 100/646] [D loss: 0.453046] [G loss: 0.186026] [ema: 0.999847] 
[Epoch 70/78] [Batch 200/646] [D loss: 0.443234] [G loss: 0.179436] [ema: 0.999847] 
[Epoch 70/78] [Batch 300/646] [D loss: 0.409762] [G loss: 0.162824] [ema: 0.999848] 
[Epoch 70/78] [Batch 400/646] [D loss: 0.396044] [G loss: 0.163635] [ema: 0.999848] 
[Epoch 70/78] [Batch 500/646] [D loss: 0.381075] [G loss: 0.177763] [ema: 0.999848] 
[Epoch 70/78] [Batch 600/646] [D loss: 0.428027] [G loss: 0.191830] [ema: 0.999849] 
[Epoch 71/78] [Batch 0/646] [D loss: 0.390412] [G loss: 0.188228] [ema: 0.999849] 
[Epoch 71/78] [Batch 100/646] [D loss: 0.389111] [G loss: 0.184680] [ema: 0.999849] 
[Epoch 71/78] [Batch 200/646] [D loss: 0.375513] [G loss: 0.183466] [ema: 0.999850] 
[Epoch 71/78] [Batch 300/646] [D loss: 0.398724] [G loss: 0.178838] [ema: 0.999850] 
[Epoch 71/78] [Batch 400/646] [D loss: 0.419203] [G loss: 0.168451] [ema: 0.999850] 
[Epoch 71/78] [Batch 500/646] [D loss: 0.437014] [G loss: 0.178334] [ema: 0.999851] 
[Epoch 71/78] [Batch 600/646] [D loss: 0.422221] [G loss: 0.159278] [ema: 0.999851] 
[Epoch 72/78] [Batch 0/646] [D loss: 0.378835] [G loss: 0.188414] [ema: 0.999851] 
[Epoch 72/78] [Batch 100/646] [D loss: 0.440369] [G loss: 0.180963] [ema: 0.999851] 
[Epoch 72/78] [Batch 200/646] [D loss: 0.417227] [G loss: 0.167069] [ema: 0.999852] 
[Epoch 72/78] [Batch 300/646] [D loss: 0.373241] [G loss: 0.187544] [ema: 0.999852] 
[Epoch 72/78] [Batch 400/646] [D loss: 0.419649] [G loss: 0.181009] [ema: 0.999852] 
[Epoch 72/78] [Batch 500/646] [D loss: 0.420210] [G loss: 0.186337] [ema: 0.999853] 
[Epoch 72/78] [Batch 600/646] [D loss: 0.334079] [G loss: 0.189654] [ema: 0.999853] 
[Epoch 73/78] [Batch 0/646] [D loss: 0.371000] [G loss: 0.187249] [ema: 0.999853] 
[Epoch 73/78] [Batch 100/646] [D loss: 0.436589] [G loss: 0.179629] [ema: 0.999853] 
[Epoch 73/78] [Batch 200/646] [D loss: 0.408346] [G loss: 0.177048] [ema: 0.999854] 
[Epoch 73/78] [Batch 300/646] [D loss: 0.381755] [G loss: 0.190261] [ema: 0.999854] 
[Epoch 73/78] [Batch 400/646] [D loss: 0.411798] [G loss: 0.173784] [ema: 0.999854] 
[Epoch 73/78] [Batch 500/646] [D loss: 0.373606] [G loss: 0.175976] [ema: 0.999855] 
[Epoch 73/78] [Batch 600/646] [D loss: 0.364059] [G loss: 0.184045] [ema: 0.999855] 
[Epoch 74/78] [Batch 0/646] [D loss: 0.349010] [G loss: 0.188850] [ema: 0.999855] 
[Epoch 74/78] [Batch 100/646] [D loss: 0.439506] [G loss: 0.161022] [ema: 0.999855] 
[Epoch 74/78] [Batch 200/646] [D loss: 0.451266] [G loss: 0.167486] [ema: 0.999856] 
[Epoch 74/78] [Batch 300/646] [D loss: 0.373715] [G loss: 0.167367] [ema: 0.999856] 
[Epoch 74/78] [Batch 400/646] [D loss: 0.418651] [G loss: 0.162979] [ema: 0.999856] 
[Epoch 74/78] [Batch 500/646] [D loss: 0.415077] [G loss: 0.183712] [ema: 0.999857] 
[Epoch 74/78] [Batch 600/646] [D loss: 0.396360] [G loss: 0.186358] [ema: 0.999857] 
[Epoch 75/78] [Batch 0/646] [D loss: 0.434672] [G loss: 0.169972] [ema: 0.999857] 
[Epoch 75/78] [Batch 100/646] [D loss: 0.388455] [G loss: 0.173464] [ema: 0.999857] 
[Epoch 75/78] [Batch 200/646] [D loss: 0.418645] [G loss: 0.199015] [ema: 0.999858] 
[Epoch 75/78] [Batch 300/646] [D loss: 0.384326] [G loss: 0.184451] [ema: 0.999858] 
[Epoch 75/78] [Batch 400/646] [D loss: 0.410637] [G loss: 0.175202] [ema: 0.999858] 
[Epoch 75/78] [Batch 500/646] [D loss: 0.417384] [G loss: 0.173116] [ema: 0.999858] 
[Epoch 75/78] [Batch 600/646] [D loss: 0.398688] [G loss: 0.178235] [ema: 0.999859] 
[Epoch 76/78] [Batch 0/646] [D loss: 0.423127] [G loss: 0.172338] [ema: 0.999859] 
[Epoch 76/78] [Batch 100/646] [D loss: 0.416497] [G loss: 0.170605] [ema: 0.999859] 
[Epoch 76/78] [Batch 200/646] [D loss: 0.372836] [G loss: 0.173849] [ema: 0.999859] 
[Epoch 76/78] [Batch 300/646] [D loss: 0.413514] [G loss: 0.184477] [ema: 0.999860] 
[Epoch 76/78] [Batch 400/646] [D loss: 0.385208] [G loss: 0.188336] [ema: 0.999860] 
[Epoch 76/78] [Batch 500/646] [D loss: 0.392793] [G loss: 0.173822] [ema: 0.999860] 
[Epoch 76/78] [Batch 600/646] [D loss: 0.411141] [G loss: 0.173937] [ema: 0.999861] 
[Epoch 77/78] [Batch 0/646] [D loss: 0.416118] [G loss: 0.176212] [ema: 0.999861] 
[Epoch 77/78] [Batch 100/646] [D loss: 0.400699] [G loss: 0.193493] [ema: 0.999861] 
[Epoch 77/78] [Batch 200/646] [D loss: 0.400158] [G loss: 0.168277] [ema: 0.999861] 
[Epoch 77/78] [Batch 300/646] [D loss: 0.409799] [G loss: 0.174549] [ema: 0.999861] 
[Epoch 77/78] [Batch 400/646] [D loss: 0.387776] [G loss: 0.161396] [ema: 0.999862] 
[Epoch 77/78] [Batch 500/646] [D loss: 0.419955] [G loss: 0.180769] [ema: 0.999862] 
[Epoch 77/78] [Batch 600/646] [D loss: 0.417009] [G loss: 0.187733] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
KuHar_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
KuHar_DAGHAR_Multiclass
daghar
return single class data and labels, class is KuHar_DAGHAR_Multiclass
data shape is (1392, 3, 1, 60)
label shape is (1392,)
87
Epochs between checkpoint: 144



Saving checkpoint 1 in logs/daghar_split_dataset_50000_60_100/KuHar_DAGHAR_Multiclass_50000_D_60_2024_10_25_17_04_46/Model



[Epoch 0/575] [Batch 0/87] [D loss: 1.028806] [G loss: 0.474452] [ema: 0.000000] 
[Epoch 1/575] [Batch 0/87] [D loss: 0.547199] [G loss: 0.233403] [ema: 0.923419] 
[Epoch 2/575] [Batch 0/87] [D loss: 0.522456] [G loss: 0.190196] [ema: 0.960947] 
[Epoch 3/575] [Batch 0/87] [D loss: 0.453748] [G loss: 0.168132] [ema: 0.973792] 
[Epoch 4/575] [Batch 0/87] [D loss: 0.395243] [G loss: 0.217969] [ema: 0.980279] 
[Epoch 5/575] [Batch 0/87] [D loss: 0.453374] [G loss: 0.163518] [ema: 0.984192] 
[Epoch 6/575] [Batch 0/87] [D loss: 0.391935] [G loss: 0.213676] [ema: 0.986809] 
[Epoch 7/575] [Batch 0/87] [D loss: 0.382203] [G loss: 0.218321] [ema: 0.988683] 
[Epoch 8/575] [Batch 0/87] [D loss: 0.484214] [G loss: 0.199047] [ema: 0.990090] 
[Epoch 9/575] [Batch 0/87] [D loss: 0.407553] [G loss: 0.180222] [ema: 0.991187] 
[Epoch 10/575] [Batch 0/87] [D loss: 0.441058] [G loss: 0.170682] [ema: 0.992064] 
[Epoch 11/575] [Batch 0/87] [D loss: 0.449752] [G loss: 0.184387] [ema: 0.992783] 
[Epoch 12/575] [Batch 0/87] [D loss: 0.457695] [G loss: 0.205046] [ema: 0.993383] 
[Epoch 13/575] [Batch 0/87] [D loss: 0.432861] [G loss: 0.194603] [ema: 0.993890] 
[Epoch 14/575] [Batch 0/87] [D loss: 0.432741] [G loss: 0.188299] [ema: 0.994325] 
[Epoch 15/575] [Batch 0/87] [D loss: 0.389007] [G loss: 0.189175] [ema: 0.994703] 
[Epoch 16/575] [Batch 0/87] [D loss: 0.383746] [G loss: 0.206378] [ema: 0.995033] 
[Epoch 17/575] [Batch 0/87] [D loss: 0.465186] [G loss: 0.160998] [ema: 0.995324] 
[Epoch 18/575] [Batch 0/87] [D loss: 0.397280] [G loss: 0.187677] [ema: 0.995584] 
[Epoch 19/575] [Batch 0/87] [D loss: 0.370177] [G loss: 0.182778] [ema: 0.995816] 
[Epoch 20/575] [Batch 0/87] [D loss: 0.397681] [G loss: 0.191523] [ema: 0.996024] 
[Epoch 21/575] [Batch 0/87] [D loss: 0.366683] [G loss: 0.176549] [ema: 0.996213] 
[Epoch 22/575] [Batch 0/87] [D loss: 0.425264] [G loss: 0.189464] [ema: 0.996385] 
[Epoch 23/575] [Batch 0/87] [D loss: 0.398596] [G loss: 0.186402] [ema: 0.996542] 
[Epoch 24/575] [Batch 0/87] [D loss: 0.421564] [G loss: 0.200450] [ema: 0.996686] 
[Epoch 25/575] [Batch 0/87] [D loss: 0.415370] [G loss: 0.190575] [ema: 0.996818] 
[Epoch 26/575] [Batch 0/87] [D loss: 0.404779] [G loss: 0.196785] [ema: 0.996940] 
[Epoch 27/575] [Batch 0/87] [D loss: 0.376970] [G loss: 0.196934] [ema: 0.997054] 
[Epoch 28/575] [Batch 0/87] [D loss: 0.377012] [G loss: 0.188670] [ema: 0.997159] 
[Epoch 29/575] [Batch 0/87] [D loss: 0.388027] [G loss: 0.177231] [ema: 0.997256] 
[Epoch 30/575] [Batch 0/87] [D loss: 0.372944] [G loss: 0.203993] [ema: 0.997348] 
[Epoch 31/575] [Batch 0/87] [D loss: 0.411519] [G loss: 0.172128] [ema: 0.997433] 
[Epoch 32/575] [Batch 0/87] [D loss: 0.406624] [G loss: 0.228513] [ema: 0.997513] 
[Epoch 33/575] [Batch 0/87] [D loss: 0.383617] [G loss: 0.168837] [ema: 0.997589] 
[Epoch 34/575] [Batch 0/87] [D loss: 0.409266] [G loss: 0.172163] [ema: 0.997659] 
[Epoch 35/575] [Batch 0/87] [D loss: 0.365868] [G loss: 0.186410] [ema: 0.997726] 
[Epoch 36/575] [Batch 0/87] [D loss: 0.367253] [G loss: 0.203294] [ema: 0.997789] 
[Epoch 37/575] [Batch 0/87] [D loss: 0.366704] [G loss: 0.210862] [ema: 0.997849] 
[Epoch 38/575] [Batch 0/87] [D loss: 0.361564] [G loss: 0.190865] [ema: 0.997906] 
[Epoch 39/575] [Batch 0/87] [D loss: 0.439695] [G loss: 0.199628] [ema: 0.997959] 
[Epoch 40/575] [Batch 0/87] [D loss: 0.351798] [G loss: 0.212178] [ema: 0.998010] 
[Epoch 41/575] [Batch 0/87] [D loss: 0.456986] [G loss: 0.198434] [ema: 0.998059] 
[Epoch 42/575] [Batch 0/87] [D loss: 0.393093] [G loss: 0.194871] [ema: 0.998105] 
[Epoch 43/575] [Batch 0/87] [D loss: 0.391540] [G loss: 0.197845] [ema: 0.998149] 
[Epoch 44/575] [Batch 0/87] [D loss: 0.384614] [G loss: 0.210461] [ema: 0.998191] 
[Epoch 45/575] [Batch 0/87] [D loss: 0.417308] [G loss: 0.197454] [ema: 0.998231] 
[Epoch 46/575] [Batch 0/87] [D loss: 0.454891] [G loss: 0.171931] [ema: 0.998269] 
[Epoch 47/575] [Batch 0/87] [D loss: 0.358895] [G loss: 0.211625] [ema: 0.998306] 
[Epoch 48/575] [Batch 0/87] [D loss: 0.437113] [G loss: 0.189502] [ema: 0.998342] 
[Epoch 49/575] [Batch 0/87] [D loss: 0.392477] [G loss: 0.208648] [ema: 0.998375] 
[Epoch 50/575] [Batch 0/87] [D loss: 0.420027] [G loss: 0.184675] [ema: 0.998408] 
[Epoch 51/575] [Batch 0/87] [D loss: 0.350350] [G loss: 0.192635] [ema: 0.998439] 
[Epoch 52/575] [Batch 0/87] [D loss: 0.382023] [G loss: 0.181921] [ema: 0.998469] 
[Epoch 53/575] [Batch 0/87] [D loss: 0.355252] [G loss: 0.196918] [ema: 0.998498] 
[Epoch 54/575] [Batch 0/87] [D loss: 0.362145] [G loss: 0.179058] [ema: 0.998526] 
[Epoch 55/575] [Batch 0/87] [D loss: 0.453539] [G loss: 0.165316] [ema: 0.998552] 
[Epoch 56/575] [Batch 0/87] [D loss: 0.383432] [G loss: 0.184128] [ema: 0.998578] 
[Epoch 57/575] [Batch 0/87] [D loss: 0.431652] [G loss: 0.188745] [ema: 0.998603] 
[Epoch 58/575] [Batch 0/87] [D loss: 0.409095] [G loss: 0.201181] [ema: 0.998627] 
[Epoch 59/575] [Batch 0/87] [D loss: 0.366738] [G loss: 0.196637] [ema: 0.998651] 
[Epoch 60/575] [Batch 0/87] [D loss: 0.425903] [G loss: 0.200124] [ema: 0.998673] 
[Epoch 61/575] [Batch 0/87] [D loss: 0.358578] [G loss: 0.196598] [ema: 0.998695] 
[Epoch 62/575] [Batch 0/87] [D loss: 0.383126] [G loss: 0.180964] [ema: 0.998716] 
[Epoch 63/575] [Batch 0/87] [D loss: 0.373740] [G loss: 0.188136] [ema: 0.998736] 
[Epoch 64/575] [Batch 0/87] [D loss: 0.383314] [G loss: 0.203093] [ema: 0.998756] 
[Epoch 65/575] [Batch 0/87] [D loss: 0.393550] [G loss: 0.186147] [ema: 0.998775] 
[Epoch 66/575] [Batch 0/87] [D loss: 0.394169] [G loss: 0.196610] [ema: 0.998794] 
[Epoch 67/575] [Batch 0/87] [D loss: 0.401691] [G loss: 0.182928] [ema: 0.998812] 
[Epoch 68/575] [Batch 0/87] [D loss: 0.400180] [G loss: 0.195779] [ema: 0.998829] 
[Epoch 69/575] [Batch 0/87] [D loss: 0.392040] [G loss: 0.184100] [ema: 0.998846] 
[Epoch 70/575] [Batch 0/87] [D loss: 0.399333] [G loss: 0.193522] [ema: 0.998862] 
[Epoch 71/575] [Batch 0/87] [D loss: 0.356008] [G loss: 0.203897] [ema: 0.998878] 
[Epoch 72/575] [Batch 0/87] [D loss: 0.401226] [G loss: 0.183542] [ema: 0.998894] 
[Epoch 73/575] [Batch 0/87] [D loss: 0.356773] [G loss: 0.205543] [ema: 0.998909] 
[Epoch 74/575] [Batch 0/87] [D loss: 0.415465] [G loss: 0.194780] [ema: 0.998924] 
[Epoch 75/575] [Batch 0/87] [D loss: 0.403129] [G loss: 0.175063] [ema: 0.998938] 
[Epoch 76/575] [Batch 0/87] [D loss: 0.397267] [G loss: 0.210371] [ema: 0.998952] 
[Epoch 77/575] [Batch 0/87] [D loss: 0.342827] [G loss: 0.207984] [ema: 0.998966] 
[Epoch 78/575] [Batch 0/87] [D loss: 0.401193] [G loss: 0.171965] [ema: 0.998979] 
[Epoch 79/575] [Batch 0/87] [D loss: 0.343729] [G loss: 0.187301] [ema: 0.998992] 
[Epoch 80/575] [Batch 0/87] [D loss: 0.399267] [G loss: 0.186177] [ema: 0.999005] 
[Epoch 81/575] [Batch 0/87] [D loss: 0.384557] [G loss: 0.184720] [ema: 0.999017] 
[Epoch 82/575] [Batch 0/87] [D loss: 0.382680] [G loss: 0.188890] [ema: 0.999029] 
[Epoch 83/575] [Batch 0/87] [D loss: 0.392374] [G loss: 0.167506] [ema: 0.999041] 
[Epoch 84/575] [Batch 0/87] [D loss: 0.380878] [G loss: 0.183190] [ema: 0.999052] 
[Epoch 85/575] [Batch 0/87] [D loss: 0.345074] [G loss: 0.185634] [ema: 0.999063] 
[Epoch 86/575] [Batch 0/87] [D loss: 0.353680] [G loss: 0.195782] [ema: 0.999074] 
[Epoch 87/575] [Batch 0/87] [D loss: 0.395896] [G loss: 0.191773] [ema: 0.999085] 
[Epoch 88/575] [Batch 0/87] [D loss: 0.414298] [G loss: 0.188325] [ema: 0.999095] 
[Epoch 89/575] [Batch 0/87] [D loss: 0.372769] [G loss: 0.197201] [ema: 0.999105] 
[Epoch 90/575] [Batch 0/87] [D loss: 0.352219] [G loss: 0.198456] [ema: 0.999115] 
[Epoch 91/575] [Batch 0/87] [D loss: 0.382844] [G loss: 0.187384] [ema: 0.999125] 
[Epoch 92/575] [Batch 0/87] [D loss: 0.343688] [G loss: 0.204250] [ema: 0.999134] 
[Epoch 93/575] [Batch 0/87] [D loss: 0.378827] [G loss: 0.200784] [ema: 0.999144] 
[Epoch 94/575] [Batch 0/87] [D loss: 0.363981] [G loss: 0.200847] [ema: 0.999153] 
[Epoch 95/575] [Batch 0/87] [D loss: 0.364042] [G loss: 0.198373] [ema: 0.999162] 
[Epoch 96/575] [Batch 0/87] [D loss: 0.370408] [G loss: 0.188764] [ema: 0.999170] 
[Epoch 97/575] [Batch 0/87] [D loss: 0.412387] [G loss: 0.190974] [ema: 0.999179] 
[Epoch 98/575] [Batch 0/87] [D loss: 0.402471] [G loss: 0.204041] [ema: 0.999187] 
[Epoch 99/575] [Batch 0/87] [D loss: 0.403247] [G loss: 0.181360] [ema: 0.999196] 
[Epoch 100/575] [Batch 0/87] [D loss: 0.361175] [G loss: 0.186506] [ema: 0.999204] 
[Epoch 101/575] [Batch 0/87] [D loss: 0.379173] [G loss: 0.177542] [ema: 0.999211] 
[Epoch 102/575] [Batch 0/87] [D loss: 0.385916] [G loss: 0.179329] [ema: 0.999219] 
[Epoch 103/575] [Batch 0/87] [D loss: 0.421983] [G loss: 0.184141] [ema: 0.999227] 
[Epoch 104/575] [Batch 0/87] [D loss: 0.415463] [G loss: 0.205275] [ema: 0.999234] 
[Epoch 105/575] [Batch 0/87] [D loss: 0.426959] [G loss: 0.171861] [ema: 0.999242] 
[Epoch 106/575] [Batch 0/87] [D loss: 0.353592] [G loss: 0.205696] [ema: 0.999249] 
[Epoch 107/575] [Batch 0/87] [D loss: 0.380339] [G loss: 0.187113] [ema: 0.999256] 
[Epoch 108/575] [Batch 0/87] [D loss: 0.399701] [G loss: 0.184797] [ema: 0.999263] 
[Epoch 109/575] [Batch 0/87] [D loss: 0.368536] [G loss: 0.182207] [ema: 0.999269] 
[Epoch 110/575] [Batch 0/87] [D loss: 0.344358] [G loss: 0.211199] [ema: 0.999276] 
[Epoch 111/575] [Batch 0/87] [D loss: 0.416819] [G loss: 0.184053] [ema: 0.999282] 
[Epoch 112/575] [Batch 0/87] [D loss: 0.344329] [G loss: 0.198921] [ema: 0.999289] 
[Epoch 113/575] [Batch 0/87] [D loss: 0.426167] [G loss: 0.199188] [ema: 0.999295] 
[Epoch 114/575] [Batch 0/87] [D loss: 0.381715] [G loss: 0.172873] [ema: 0.999301] 
[Epoch 115/575] [Batch 0/87] [D loss: 0.435014] [G loss: 0.187319] [ema: 0.999307] 
[Epoch 116/575] [Batch 0/87] [D loss: 0.386266] [G loss: 0.182593] [ema: 0.999313] 
[Epoch 117/575] [Batch 0/87] [D loss: 0.395636] [G loss: 0.192106] [ema: 0.999319] 
[Epoch 118/575] [Batch 0/87] [D loss: 0.415626] [G loss: 0.200985] [ema: 0.999325] 
[Epoch 119/575] [Batch 0/87] [D loss: 0.395944] [G loss: 0.186702] [ema: 0.999331] 
[Epoch 120/575] [Batch 0/87] [D loss: 0.392465] [G loss: 0.182940] [ema: 0.999336] 
[Epoch 121/575] [Batch 0/87] [D loss: 0.468456] [G loss: 0.186092] [ema: 0.999342] 
[Epoch 122/575] [Batch 0/87] [D loss: 0.351936] [G loss: 0.191802] [ema: 0.999347] 
[Epoch 123/575] [Batch 0/87] [D loss: 0.394525] [G loss: 0.190392] [ema: 0.999352] 
[Epoch 124/575] [Batch 0/87] [D loss: 0.441352] [G loss: 0.190688] [ema: 0.999358] 
[Epoch 125/575] [Batch 0/87] [D loss: 0.394266] [G loss: 0.181053] [ema: 0.999363] 
[Epoch 126/575] [Batch 0/87] [D loss: 0.357605] [G loss: 0.200761] [ema: 0.999368] 
[Epoch 127/575] [Batch 0/87] [D loss: 0.400371] [G loss: 0.184156] [ema: 0.999373] 
[Epoch 128/575] [Batch 0/87] [D loss: 0.365973] [G loss: 0.190399] [ema: 0.999378] 
[Epoch 129/575] [Batch 0/87] [D loss: 0.397570] [G loss: 0.187887] [ema: 0.999383] 
[Epoch 130/575] [Batch 0/87] [D loss: 0.378419] [G loss: 0.205073] [ema: 0.999387] 
[Epoch 131/575] [Batch 0/87] [D loss: 0.395068] [G loss: 0.176844] [ema: 0.999392] 
[Epoch 132/575] [Batch 0/87] [D loss: 0.413796] [G loss: 0.195324] [ema: 0.999397] 
[Epoch 133/575] [Batch 0/87] [D loss: 0.388805] [G loss: 0.185569] [ema: 0.999401] 
[Epoch 134/575] [Batch 0/87] [D loss: 0.393021] [G loss: 0.183197] [ema: 0.999406] 
[Epoch 135/575] [Batch 0/87] [D loss: 0.355287] [G loss: 0.195917] [ema: 0.999410] 
[Epoch 136/575] [Batch 0/87] [D loss: 0.407006] [G loss: 0.193125] [ema: 0.999414] 
[Epoch 137/575] [Batch 0/87] [D loss: 0.391037] [G loss: 0.187323] [ema: 0.999419] 
[Epoch 138/575] [Batch 0/87] [D loss: 0.417362] [G loss: 0.182628] [ema: 0.999423] 
[Epoch 139/575] [Batch 0/87] [D loss: 0.424598] [G loss: 0.187412] [ema: 0.999427] 
[Epoch 140/575] [Batch 0/87] [D loss: 0.436846] [G loss: 0.194295] [ema: 0.999431] 
[Epoch 141/575] [Batch 0/87] [D loss: 0.405539] [G loss: 0.195215] [ema: 0.999435] 
[Epoch 142/575] [Batch 0/87] [D loss: 0.381749] [G loss: 0.175866] [ema: 0.999439] 
[Epoch 143/575] [Batch 0/87] [D loss: 0.402002] [G loss: 0.176298] [ema: 0.999443] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_60_100/KuHar_DAGHAR_Multiclass_50000_D_60_2024_10_25_17_04_46/Model



[Epoch 144/575] [Batch 0/87] [D loss: 0.391761] [G loss: 0.188962] [ema: 0.999447] 
[Epoch 145/575] [Batch 0/87] [D loss: 0.350397] [G loss: 0.199848] [ema: 0.999451] 
[Epoch 146/575] [Batch 0/87] [D loss: 0.363152] [G loss: 0.184978] [ema: 0.999454] 
[Epoch 147/575] [Batch 0/87] [D loss: 0.353434] [G loss: 0.191218] [ema: 0.999458] 
[Epoch 148/575] [Batch 0/87] [D loss: 0.374820] [G loss: 0.195178] [ema: 0.999462] 
[Epoch 149/575] [Batch 0/87] [D loss: 0.412930] [G loss: 0.193080] [ema: 0.999465] 
[Epoch 150/575] [Batch 0/87] [D loss: 0.420706] [G loss: 0.197023] [ema: 0.999469] 
[Epoch 151/575] [Batch 0/87] [D loss: 0.393367] [G loss: 0.196775] [ema: 0.999473] 
[Epoch 152/575] [Batch 0/87] [D loss: 0.369874] [G loss: 0.184939] [ema: 0.999476] 
[Epoch 153/575] [Batch 0/87] [D loss: 0.408992] [G loss: 0.182227] [ema: 0.999479] 
[Epoch 154/575] [Batch 0/87] [D loss: 0.365839] [G loss: 0.190876] [ema: 0.999483] 
[Epoch 155/575] [Batch 0/87] [D loss: 0.386183] [G loss: 0.183677] [ema: 0.999486] 
[Epoch 156/575] [Batch 0/87] [D loss: 0.381493] [G loss: 0.192613] [ema: 0.999489] 
[Epoch 157/575] [Batch 0/87] [D loss: 0.388684] [G loss: 0.186474] [ema: 0.999493] 
[Epoch 158/575] [Batch 0/87] [D loss: 0.400481] [G loss: 0.201232] [ema: 0.999496] 
[Epoch 159/575] [Batch 0/87] [D loss: 0.353851] [G loss: 0.193066] [ema: 0.999499] 
[Epoch 160/575] [Batch 0/87] [D loss: 0.389543] [G loss: 0.185157] [ema: 0.999502] 
[Epoch 161/575] [Batch 0/87] [D loss: 0.372218] [G loss: 0.188213] [ema: 0.999505] 
[Epoch 162/575] [Batch 0/87] [D loss: 0.400997] [G loss: 0.196368] [ema: 0.999508] 
[Epoch 163/575] [Batch 0/87] [D loss: 0.416677] [G loss: 0.183386] [ema: 0.999511] 
[Epoch 164/575] [Batch 0/87] [D loss: 0.369579] [G loss: 0.200816] [ema: 0.999514] 
[Epoch 165/575] [Batch 0/87] [D loss: 0.379385] [G loss: 0.184118] [ema: 0.999517] 
[Epoch 166/575] [Batch 0/87] [D loss: 0.370774] [G loss: 0.174918] [ema: 0.999520] 
[Epoch 167/575] [Batch 0/87] [D loss: 0.386811] [G loss: 0.203793] [ema: 0.999523] 
[Epoch 168/575] [Batch 0/87] [D loss: 0.341976] [G loss: 0.192027] [ema: 0.999526] 
[Epoch 169/575] [Batch 0/87] [D loss: 0.436061] [G loss: 0.169357] [ema: 0.999529] 
[Epoch 170/575] [Batch 0/87] [D loss: 0.353125] [G loss: 0.196058] [ema: 0.999531] 
[Epoch 171/575] [Batch 0/87] [D loss: 0.437074] [G loss: 0.176427] [ema: 0.999534] 
[Epoch 172/575] [Batch 0/87] [D loss: 0.414256] [G loss: 0.185327] [ema: 0.999537] 
[Epoch 173/575] [Batch 0/87] [D loss: 0.396663] [G loss: 0.196973] [ema: 0.999540] 
[Epoch 174/575] [Batch 0/87] [D loss: 0.392399] [G loss: 0.193924] [ema: 0.999542] 
[Epoch 175/575] [Batch 0/87] [D loss: 0.397493] [G loss: 0.177420] [ema: 0.999545] 
[Epoch 176/575] [Batch 0/87] [D loss: 0.398208] [G loss: 0.200306] [ema: 0.999547] 
[Epoch 177/575] [Batch 0/87] [D loss: 0.425542] [G loss: 0.183453] [ema: 0.999550] 
[Epoch 178/575] [Batch 0/87] [D loss: 0.397488] [G loss: 0.195039] [ema: 0.999553] 
[Epoch 179/575] [Batch 0/87] [D loss: 0.378478] [G loss: 0.198820] [ema: 0.999555] 
[Epoch 180/575] [Batch 0/87] [D loss: 0.364170] [G loss: 0.189363] [ema: 0.999557] 
[Epoch 181/575] [Batch 0/87] [D loss: 0.405628] [G loss: 0.179400] [ema: 0.999560] 
[Epoch 182/575] [Batch 0/87] [D loss: 0.396670] [G loss: 0.186080] [ema: 0.999562] 
[Epoch 183/575] [Batch 0/87] [D loss: 0.401525] [G loss: 0.199640] [ema: 0.999565] 
[Epoch 184/575] [Batch 0/87] [D loss: 0.439320] [G loss: 0.190580] [ema: 0.999567] 
[Epoch 185/575] [Batch 0/87] [D loss: 0.411828] [G loss: 0.195648] [ema: 0.999569] 
[Epoch 186/575] [Batch 0/87] [D loss: 0.399910] [G loss: 0.188769] [ema: 0.999572] 
[Epoch 187/575] [Batch 0/87] [D loss: 0.432553] [G loss: 0.183290] [ema: 0.999574] 
[Epoch 188/575] [Batch 0/87] [D loss: 0.365545] [G loss: 0.205278] [ema: 0.999576] 
[Epoch 189/575] [Batch 0/87] [D loss: 0.403413] [G loss: 0.180752] [ema: 0.999579] 
[Epoch 190/575] [Batch 0/87] [D loss: 0.386562] [G loss: 0.189094] [ema: 0.999581] 
[Epoch 191/575] [Batch 0/87] [D loss: 0.373318] [G loss: 0.193753] [ema: 0.999583] 
[Epoch 192/575] [Batch 0/87] [D loss: 0.401514] [G loss: 0.192074] [ema: 0.999585] 
[Epoch 193/575] [Batch 0/87] [D loss: 0.384233] [G loss: 0.188994] [ema: 0.999587] 
[Epoch 194/575] [Batch 0/87] [D loss: 0.347327] [G loss: 0.182426] [ema: 0.999589] 
[Epoch 195/575] [Batch 0/87] [D loss: 0.429387] [G loss: 0.176537] [ema: 0.999592] 
[Epoch 196/575] [Batch 0/87] [D loss: 0.357340] [G loss: 0.202107] [ema: 0.999594] 
[Epoch 197/575] [Batch 0/87] [D loss: 0.410808] [G loss: 0.185931] [ema: 0.999596] 
[Epoch 198/575] [Batch 0/87] [D loss: 0.380360] [G loss: 0.195376] [ema: 0.999598] 
[Epoch 199/575] [Batch 0/87] [D loss: 0.344979] [G loss: 0.187837] [ema: 0.999600] 
[Epoch 200/575] [Batch 0/87] [D loss: 0.324464] [G loss: 0.207617] [ema: 0.999602] 
[Epoch 201/575] [Batch 0/87] [D loss: 0.366802] [G loss: 0.187721] [ema: 0.999604] 
[Epoch 202/575] [Batch 0/87] [D loss: 0.363561] [G loss: 0.170359] [ema: 0.999606] 
[Epoch 203/575] [Batch 0/87] [D loss: 0.388291] [G loss: 0.181757] [ema: 0.999608] 
[Epoch 204/575] [Batch 0/87] [D loss: 0.390565] [G loss: 0.195857] [ema: 0.999610] 
[Epoch 205/575] [Batch 0/87] [D loss: 0.382411] [G loss: 0.187941] [ema: 0.999611] 
[Epoch 206/575] [Batch 0/87] [D loss: 0.371897] [G loss: 0.194497] [ema: 0.999613] 
[Epoch 207/575] [Batch 0/87] [D loss: 0.422924] [G loss: 0.181582] [ema: 0.999615] 
[Epoch 208/575] [Batch 0/87] [D loss: 0.358899] [G loss: 0.201580] [ema: 0.999617] 
[Epoch 209/575] [Batch 0/87] [D loss: 0.413923] [G loss: 0.193527] [ema: 0.999619] 
[Epoch 210/575] [Batch 0/87] [D loss: 0.373882] [G loss: 0.193498] [ema: 0.999621] 
[Epoch 211/575] [Batch 0/87] [D loss: 0.375607] [G loss: 0.185976] [ema: 0.999622] 
[Epoch 212/575] [Batch 0/87] [D loss: 0.395415] [G loss: 0.173183] [ema: 0.999624] 
[Epoch 213/575] [Batch 0/87] [D loss: 0.364589] [G loss: 0.198877] [ema: 0.999626] 
[Epoch 214/575] [Batch 0/87] [D loss: 0.414643] [G loss: 0.197917] [ema: 0.999628] 
[Epoch 215/575] [Batch 0/87] [D loss: 0.361827] [G loss: 0.193336] [ema: 0.999630] 
[Epoch 216/575] [Batch 0/87] [D loss: 0.367209] [G loss: 0.199993] [ema: 0.999631] 
[Epoch 217/575] [Batch 0/87] [D loss: 0.383019] [G loss: 0.178270] [ema: 0.999633] 
[Epoch 218/575] [Batch 0/87] [D loss: 0.365228] [G loss: 0.191985] [ema: 0.999635] 
[Epoch 219/575] [Batch 0/87] [D loss: 0.351799] [G loss: 0.186843] [ema: 0.999636] 
[Epoch 220/575] [Batch 0/87] [D loss: 0.374328] [G loss: 0.198290] [ema: 0.999638] 
[Epoch 221/575] [Batch 0/87] [D loss: 0.404731] [G loss: 0.177099] [ema: 0.999640] 
[Epoch 222/575] [Batch 0/87] [D loss: 0.373671] [G loss: 0.183859] [ema: 0.999641] 
[Epoch 223/575] [Batch 0/87] [D loss: 0.372470] [G loss: 0.188841] [ema: 0.999643] 
[Epoch 224/575] [Batch 0/87] [D loss: 0.381311] [G loss: 0.192441] [ema: 0.999644] 
[Epoch 225/575] [Batch 0/87] [D loss: 0.389199] [G loss: 0.201018] [ema: 0.999646] 
[Epoch 226/575] [Batch 0/87] [D loss: 0.351207] [G loss: 0.183370] [ema: 0.999648] 
[Epoch 227/575] [Batch 0/87] [D loss: 0.402345] [G loss: 0.193222] [ema: 0.999649] 
[Epoch 228/575] [Batch 0/87] [D loss: 0.392084] [G loss: 0.178576] [ema: 0.999651] 
[Epoch 229/575] [Batch 0/87] [D loss: 0.426301] [G loss: 0.179456] [ema: 0.999652] 
[Epoch 230/575] [Batch 0/87] [D loss: 0.388329] [G loss: 0.185743] [ema: 0.999654] 
[Epoch 231/575] [Batch 0/87] [D loss: 0.359967] [G loss: 0.192895] [ema: 0.999655] 
[Epoch 232/575] [Batch 0/87] [D loss: 0.387478] [G loss: 0.180510] [ema: 0.999657] 
[Epoch 233/575] [Batch 0/87] [D loss: 0.362747] [G loss: 0.190729] [ema: 0.999658] 
[Epoch 234/575] [Batch 0/87] [D loss: 0.397882] [G loss: 0.188491] [ema: 0.999660] 
[Epoch 235/575] [Batch 0/87] [D loss: 0.361264] [G loss: 0.187953] [ema: 0.999661] 
[Epoch 236/575] [Batch 0/87] [D loss: 0.380489] [G loss: 0.185501] [ema: 0.999662] 
[Epoch 237/575] [Batch 0/87] [D loss: 0.407244] [G loss: 0.192063] [ema: 0.999664] 
[Epoch 238/575] [Batch 0/87] [D loss: 0.389144] [G loss: 0.196646] [ema: 0.999665] 
[Epoch 239/575] [Batch 0/87] [D loss: 0.403472] [G loss: 0.203271] [ema: 0.999667] 
[Epoch 240/575] [Batch 0/87] [D loss: 0.360791] [G loss: 0.184263] [ema: 0.999668] 
[Epoch 241/575] [Batch 0/87] [D loss: 0.398472] [G loss: 0.195647] [ema: 0.999669] 
[Epoch 242/575] [Batch 0/87] [D loss: 0.360981] [G loss: 0.197316] [ema: 0.999671] 
[Epoch 243/575] [Batch 0/87] [D loss: 0.366886] [G loss: 0.186678] [ema: 0.999672] 
[Epoch 244/575] [Batch 0/87] [D loss: 0.379296] [G loss: 0.177459] [ema: 0.999674] 
[Epoch 245/575] [Batch 0/87] [D loss: 0.399124] [G loss: 0.185758] [ema: 0.999675] 
[Epoch 246/575] [Batch 0/87] [D loss: 0.378691] [G loss: 0.200093] [ema: 0.999676] 
[Epoch 247/575] [Batch 0/87] [D loss: 0.392022] [G loss: 0.187143] [ema: 0.999677] 
[Epoch 248/575] [Batch 0/87] [D loss: 0.410857] [G loss: 0.193780] [ema: 0.999679] 
[Epoch 249/575] [Batch 0/87] [D loss: 0.402329] [G loss: 0.178060] [ema: 0.999680] 
[Epoch 250/575] [Batch 0/87] [D loss: 0.379851] [G loss: 0.190536] [ema: 0.999681] 
[Epoch 251/575] [Batch 0/87] [D loss: 0.393950] [G loss: 0.179389] [ema: 0.999683] 
[Epoch 252/575] [Batch 0/87] [D loss: 0.355893] [G loss: 0.200911] [ema: 0.999684] 
[Epoch 253/575] [Batch 0/87] [D loss: 0.396076] [G loss: 0.196000] [ema: 0.999685] 
[Epoch 254/575] [Batch 0/87] [D loss: 0.350435] [G loss: 0.189641] [ema: 0.999686] 
[Epoch 255/575] [Batch 0/87] [D loss: 0.406376] [G loss: 0.184756] [ema: 0.999688] 
[Epoch 256/575] [Batch 0/87] [D loss: 0.398918] [G loss: 0.182924] [ema: 0.999689] 
[Epoch 257/575] [Batch 0/87] [D loss: 0.430239] [G loss: 0.183252] [ema: 0.999690] 
[Epoch 258/575] [Batch 0/87] [D loss: 0.371968] [G loss: 0.178400] [ema: 0.999691] 
[Epoch 259/575] [Batch 0/87] [D loss: 0.433127] [G loss: 0.197157] [ema: 0.999692] 
[Epoch 260/575] [Batch 0/87] [D loss: 0.372417] [G loss: 0.191541] [ema: 0.999694] 
[Epoch 261/575] [Batch 0/87] [D loss: 0.395765] [G loss: 0.189371] [ema: 0.999695] 
[Epoch 262/575] [Batch 0/87] [D loss: 0.389156] [G loss: 0.183387] [ema: 0.999696] 
[Epoch 263/575] [Batch 0/87] [D loss: 0.384850] [G loss: 0.186145] [ema: 0.999697] 
[Epoch 264/575] [Batch 0/87] [D loss: 0.391172] [G loss: 0.179583] [ema: 0.999698] 
[Epoch 265/575] [Batch 0/87] [D loss: 0.374292] [G loss: 0.190508] [ema: 0.999699] 
[Epoch 266/575] [Batch 0/87] [D loss: 0.438382] [G loss: 0.174571] [ema: 0.999701] 
[Epoch 267/575] [Batch 0/87] [D loss: 0.374195] [G loss: 0.194296] [ema: 0.999702] 
[Epoch 268/575] [Batch 0/87] [D loss: 0.367476] [G loss: 0.188654] [ema: 0.999703] 
[Epoch 269/575] [Batch 0/87] [D loss: 0.403893] [G loss: 0.184553] [ema: 0.999704] 
[Epoch 270/575] [Batch 0/87] [D loss: 0.410444] [G loss: 0.177806] [ema: 0.999705] 
[Epoch 271/575] [Batch 0/87] [D loss: 0.376838] [G loss: 0.189624] [ema: 0.999706] 
[Epoch 272/575] [Batch 0/87] [D loss: 0.392866] [G loss: 0.193034] [ema: 0.999707] 
[Epoch 273/575] [Batch 0/87] [D loss: 0.402300] [G loss: 0.188590] [ema: 0.999708] 
[Epoch 274/575] [Batch 0/87] [D loss: 0.395901] [G loss: 0.199105] [ema: 0.999709] 
[Epoch 275/575] [Batch 0/87] [D loss: 0.364756] [G loss: 0.190242] [ema: 0.999710] 
[Epoch 276/575] [Batch 0/87] [D loss: 0.360417] [G loss: 0.202910] [ema: 0.999711] 
[Epoch 277/575] [Batch 0/87] [D loss: 0.428886] [G loss: 0.193097] [ema: 0.999712] 
[Epoch 278/575] [Batch 0/87] [D loss: 0.400264] [G loss: 0.191087] [ema: 0.999713] 
[Epoch 279/575] [Batch 0/87] [D loss: 0.383983] [G loss: 0.181517] [ema: 0.999714] 
[Epoch 280/575] [Batch 0/87] [D loss: 0.389628] [G loss: 0.193507] [ema: 0.999715] 
[Epoch 281/575] [Batch 0/87] [D loss: 0.407268] [G loss: 0.185437] [ema: 0.999717] 
[Epoch 282/575] [Batch 0/87] [D loss: 0.382175] [G loss: 0.193205] [ema: 0.999718] 
[Epoch 283/575] [Batch 0/87] [D loss: 0.391453] [G loss: 0.192673] [ema: 0.999719] 
[Epoch 284/575] [Batch 0/87] [D loss: 0.397767] [G loss: 0.183092] [ema: 0.999720] 
[Epoch 285/575] [Batch 0/87] [D loss: 0.396793] [G loss: 0.187018] [ema: 0.999720] 
[Epoch 286/575] [Batch 0/87] [D loss: 0.366439] [G loss: 0.184482] [ema: 0.999721] 
[Epoch 287/575] [Batch 0/87] [D loss: 0.405419] [G loss: 0.190301] [ema: 0.999722] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_60_100/KuHar_DAGHAR_Multiclass_50000_D_60_2024_10_25_17_04_46/Model



[Epoch 288/575] [Batch 0/87] [D loss: 0.400395] [G loss: 0.171293] [ema: 0.999723] 
[Epoch 289/575] [Batch 0/87] [D loss: 0.404910] [G loss: 0.181571] [ema: 0.999724] 
[Epoch 290/575] [Batch 0/87] [D loss: 0.395449] [G loss: 0.188454] [ema: 0.999725] 
[Epoch 291/575] [Batch 0/87] [D loss: 0.400255] [G loss: 0.189688] [ema: 0.999726] 
[Epoch 292/575] [Batch 0/87] [D loss: 0.393848] [G loss: 0.192931] [ema: 0.999727] 
[Epoch 293/575] [Batch 0/87] [D loss: 0.389187] [G loss: 0.189368] [ema: 0.999728] 
[Epoch 294/575] [Batch 0/87] [D loss: 0.405351] [G loss: 0.187736] [ema: 0.999729] 
[Epoch 295/575] [Batch 0/87] [D loss: 0.362129] [G loss: 0.197422] [ema: 0.999730] 
[Epoch 296/575] [Batch 0/87] [D loss: 0.354707] [G loss: 0.195462] [ema: 0.999731] 
[Epoch 297/575] [Batch 0/87] [D loss: 0.375104] [G loss: 0.200849] [ema: 0.999732] 
[Epoch 298/575] [Batch 0/87] [D loss: 0.376654] [G loss: 0.193074] [ema: 0.999733] 
[Epoch 299/575] [Batch 0/87] [D loss: 0.362599] [G loss: 0.184298] [ema: 0.999734] 
[Epoch 300/575] [Batch 0/87] [D loss: 0.390719] [G loss: 0.194093] [ema: 0.999734] 
[Epoch 301/575] [Batch 0/87] [D loss: 0.395140] [G loss: 0.189470] [ema: 0.999735] 
[Epoch 302/575] [Batch 0/87] [D loss: 0.383492] [G loss: 0.193872] [ema: 0.999736] 
[Epoch 303/575] [Batch 0/87] [D loss: 0.403374] [G loss: 0.197249] [ema: 0.999737] 
[Epoch 304/575] [Batch 0/87] [D loss: 0.345655] [G loss: 0.189384] [ema: 0.999738] 
[Epoch 305/575] [Batch 0/87] [D loss: 0.394000] [G loss: 0.188364] [ema: 0.999739] 
[Epoch 306/575] [Batch 0/87] [D loss: 0.392691] [G loss: 0.185967] [ema: 0.999740] 
[Epoch 307/575] [Batch 0/87] [D loss: 0.388044] [G loss: 0.190112] [ema: 0.999741] 
[Epoch 308/575] [Batch 0/87] [D loss: 0.382096] [G loss: 0.186093] [ema: 0.999741] 
[Epoch 309/575] [Batch 0/87] [D loss: 0.393747] [G loss: 0.191860] [ema: 0.999742] 
[Epoch 310/575] [Batch 0/87] [D loss: 0.446330] [G loss: 0.183543] [ema: 0.999743] 
[Epoch 311/575] [Batch 0/87] [D loss: 0.425460] [G loss: 0.182022] [ema: 0.999744] 
[Epoch 312/575] [Batch 0/87] [D loss: 0.406614] [G loss: 0.185882] [ema: 0.999745] 
[Epoch 313/575] [Batch 0/87] [D loss: 0.355613] [G loss: 0.197269] [ema: 0.999745] 
[Epoch 314/575] [Batch 0/87] [D loss: 0.395784] [G loss: 0.187256] [ema: 0.999746] 
[Epoch 315/575] [Batch 0/87] [D loss: 0.401554] [G loss: 0.193129] [ema: 0.999747] 
[Epoch 316/575] [Batch 0/87] [D loss: 0.346709] [G loss: 0.188958] [ema: 0.999748] 
[Epoch 317/575] [Batch 0/87] [D loss: 0.384868] [G loss: 0.202641] [ema: 0.999749] 
[Epoch 318/575] [Batch 0/87] [D loss: 0.376696] [G loss: 0.182216] [ema: 0.999749] 
[Epoch 319/575] [Batch 0/87] [D loss: 0.391713] [G loss: 0.185562] [ema: 0.999750] 
[Epoch 320/575] [Batch 0/87] [D loss: 0.385653] [G loss: 0.190934] [ema: 0.999751] 
[Epoch 321/575] [Batch 0/87] [D loss: 0.347455] [G loss: 0.189456] [ema: 0.999752] 
[Epoch 322/575] [Batch 0/87] [D loss: 0.378228] [G loss: 0.185007] [ema: 0.999753] 
[Epoch 323/575] [Batch 0/87] [D loss: 0.427954] [G loss: 0.176646] [ema: 0.999753] 
[Epoch 324/575] [Batch 0/87] [D loss: 0.393999] [G loss: 0.184276] [ema: 0.999754] 
[Epoch 325/575] [Batch 0/87] [D loss: 0.346279] [G loss: 0.186483] [ema: 0.999755] 
[Epoch 326/575] [Batch 0/87] [D loss: 0.442890] [G loss: 0.177937] [ema: 0.999756] 
[Epoch 327/575] [Batch 0/87] [D loss: 0.363255] [G loss: 0.188239] [ema: 0.999756] 
[Epoch 328/575] [Batch 0/87] [D loss: 0.371671] [G loss: 0.183909] [ema: 0.999757] 
[Epoch 329/575] [Batch 0/87] [D loss: 0.386857] [G loss: 0.183485] [ema: 0.999758] 
[Epoch 330/575] [Batch 0/87] [D loss: 0.410975] [G loss: 0.181811] [ema: 0.999759] 
[Epoch 331/575] [Batch 0/87] [D loss: 0.361630] [G loss: 0.186853] [ema: 0.999759] 
[Epoch 332/575] [Batch 0/87] [D loss: 0.370947] [G loss: 0.198607] [ema: 0.999760] 
[Epoch 333/575] [Batch 0/87] [D loss: 0.377861] [G loss: 0.202536] [ema: 0.999761] 
[Epoch 334/575] [Batch 0/87] [D loss: 0.381060] [G loss: 0.194661] [ema: 0.999761] 
[Epoch 335/575] [Batch 0/87] [D loss: 0.336557] [G loss: 0.191551] [ema: 0.999762] 
[Epoch 336/575] [Batch 0/87] [D loss: 0.375301] [G loss: 0.179425] [ema: 0.999763] 
[Epoch 337/575] [Batch 0/87] [D loss: 0.390894] [G loss: 0.187168] [ema: 0.999764] 
[Epoch 338/575] [Batch 0/87] [D loss: 0.350371] [G loss: 0.185862] [ema: 0.999764] 
[Epoch 339/575] [Batch 0/87] [D loss: 0.369616] [G loss: 0.190673] [ema: 0.999765] 
[Epoch 340/575] [Batch 0/87] [D loss: 0.349687] [G loss: 0.185376] [ema: 0.999766] 
[Epoch 341/575] [Batch 0/87] [D loss: 0.331478] [G loss: 0.198946] [ema: 0.999766] 
[Epoch 342/575] [Batch 0/87] [D loss: 0.359796] [G loss: 0.193435] [ema: 0.999767] 
[Epoch 343/575] [Batch 0/87] [D loss: 0.384760] [G loss: 0.194652] [ema: 0.999768] 
[Epoch 344/575] [Batch 0/87] [D loss: 0.387591] [G loss: 0.177182] [ema: 0.999768] 
[Epoch 345/575] [Batch 0/87] [D loss: 0.343578] [G loss: 0.182266] [ema: 0.999769] 
[Epoch 346/575] [Batch 0/87] [D loss: 0.349858] [G loss: 0.210816] [ema: 0.999770] 
[Epoch 347/575] [Batch 0/87] [D loss: 0.392256] [G loss: 0.181102] [ema: 0.999770] 
[Epoch 348/575] [Batch 0/87] [D loss: 0.386430] [G loss: 0.180501] [ema: 0.999771] 
[Epoch 349/575] [Batch 0/87] [D loss: 0.359592] [G loss: 0.187232] [ema: 0.999772] 
[Epoch 350/575] [Batch 0/87] [D loss: 0.382991] [G loss: 0.189871] [ema: 0.999772] 
[Epoch 351/575] [Batch 0/87] [D loss: 0.387293] [G loss: 0.185150] [ema: 0.999773] 
[Epoch 352/575] [Batch 0/87] [D loss: 0.379121] [G loss: 0.189526] [ema: 0.999774] 
[Epoch 353/575] [Batch 0/87] [D loss: 0.402458] [G loss: 0.176087] [ema: 0.999774] 
[Epoch 354/575] [Batch 0/87] [D loss: 0.411086] [G loss: 0.184922] [ema: 0.999775] 
[Epoch 355/575] [Batch 0/87] [D loss: 0.357812] [G loss: 0.197416] [ema: 0.999776] 
[Epoch 356/575] [Batch 0/87] [D loss: 0.408261] [G loss: 0.180215] [ema: 0.999776] 
[Epoch 357/575] [Batch 0/87] [D loss: 0.383246] [G loss: 0.194764] [ema: 0.999777] 
[Epoch 358/575] [Batch 0/87] [D loss: 0.391067] [G loss: 0.179360] [ema: 0.999777] 
[Epoch 359/575] [Batch 0/87] [D loss: 0.417793] [G loss: 0.181090] [ema: 0.999778] 
[Epoch 360/575] [Batch 0/87] [D loss: 0.344534] [G loss: 0.187582] [ema: 0.999779] 
[Epoch 361/575] [Batch 0/87] [D loss: 0.381954] [G loss: 0.190601] [ema: 0.999779] 
[Epoch 362/575] [Batch 0/87] [D loss: 0.360241] [G loss: 0.180027] [ema: 0.999780] 
[Epoch 363/575] [Batch 0/87] [D loss: 0.416726] [G loss: 0.182972] [ema: 0.999781] 
[Epoch 364/575] [Batch 0/87] [D loss: 0.415858] [G loss: 0.181085] [ema: 0.999781] 
[Epoch 365/575] [Batch 0/87] [D loss: 0.360015] [G loss: 0.184817] [ema: 0.999782] 
[Epoch 366/575] [Batch 0/87] [D loss: 0.409473] [G loss: 0.180922] [ema: 0.999782] 
[Epoch 367/575] [Batch 0/87] [D loss: 0.351705] [G loss: 0.193751] [ema: 0.999783] 
[Epoch 368/575] [Batch 0/87] [D loss: 0.351919] [G loss: 0.184179] [ema: 0.999784] 
[Epoch 369/575] [Batch 0/87] [D loss: 0.396970] [G loss: 0.187535] [ema: 0.999784] 
[Epoch 370/575] [Batch 0/87] [D loss: 0.376362] [G loss: 0.197934] [ema: 0.999785] 
[Epoch 371/575] [Batch 0/87] [D loss: 0.372929] [G loss: 0.186198] [ema: 0.999785] 
[Epoch 372/575] [Batch 0/87] [D loss: 0.428073] [G loss: 0.181612] [ema: 0.999786] 
[Epoch 373/575] [Batch 0/87] [D loss: 0.351113] [G loss: 0.187444] [ema: 0.999786] 
[Epoch 374/575] [Batch 0/87] [D loss: 0.365474] [G loss: 0.175634] [ema: 0.999787] 
[Epoch 375/575] [Batch 0/87] [D loss: 0.387878] [G loss: 0.194129] [ema: 0.999788] 
[Epoch 376/575] [Batch 0/87] [D loss: 0.371881] [G loss: 0.187458] [ema: 0.999788] 
[Epoch 377/575] [Batch 0/87] [D loss: 0.424670] [G loss: 0.189745] [ema: 0.999789] 
[Epoch 378/575] [Batch 0/87] [D loss: 0.381545] [G loss: 0.186589] [ema: 0.999789] 
[Epoch 379/575] [Batch 0/87] [D loss: 0.393593] [G loss: 0.187266] [ema: 0.999790] 
[Epoch 380/575] [Batch 0/87] [D loss: 0.345722] [G loss: 0.184835] [ema: 0.999790] 
[Epoch 381/575] [Batch 0/87] [D loss: 0.367873] [G loss: 0.185337] [ema: 0.999791] 
[Epoch 382/575] [Batch 0/87] [D loss: 0.362521] [G loss: 0.177948] [ema: 0.999791] 
[Epoch 383/575] [Batch 0/87] [D loss: 0.408691] [G loss: 0.186765] [ema: 0.999792] 
[Epoch 384/575] [Batch 0/87] [D loss: 0.384155] [G loss: 0.189306] [ema: 0.999793] 
[Epoch 385/575] [Batch 0/87] [D loss: 0.397944] [G loss: 0.186305] [ema: 0.999793] 
[Epoch 386/575] [Batch 0/87] [D loss: 0.395961] [G loss: 0.199490] [ema: 0.999794] 
[Epoch 387/575] [Batch 0/87] [D loss: 0.361444] [G loss: 0.180105] [ema: 0.999794] 
[Epoch 388/575] [Batch 0/87] [D loss: 0.417580] [G loss: 0.196263] [ema: 0.999795] 
[Epoch 389/575] [Batch 0/87] [D loss: 0.397073] [G loss: 0.185740] [ema: 0.999795] 
[Epoch 390/575] [Batch 0/87] [D loss: 0.365046] [G loss: 0.183702] [ema: 0.999796] 
[Epoch 391/575] [Batch 0/87] [D loss: 0.401008] [G loss: 0.190663] [ema: 0.999796] 
[Epoch 392/575] [Batch 0/87] [D loss: 0.364865] [G loss: 0.186306] [ema: 0.999797] 
[Epoch 393/575] [Batch 0/87] [D loss: 0.376216] [G loss: 0.184428] [ema: 0.999797] 
[Epoch 394/575] [Batch 0/87] [D loss: 0.433017] [G loss: 0.184468] [ema: 0.999798] 
[Epoch 395/575] [Batch 0/87] [D loss: 0.396514] [G loss: 0.188904] [ema: 0.999798] 
[Epoch 396/575] [Batch 0/87] [D loss: 0.367797] [G loss: 0.190822] [ema: 0.999799] 
[Epoch 397/575] [Batch 0/87] [D loss: 0.348105] [G loss: 0.197539] [ema: 0.999799] 
[Epoch 398/575] [Batch 0/87] [D loss: 0.370039] [G loss: 0.195423] [ema: 0.999800] 
[Epoch 399/575] [Batch 0/87] [D loss: 0.373138] [G loss: 0.192149] [ema: 0.999800] 
[Epoch 400/575] [Batch 0/87] [D loss: 0.391249] [G loss: 0.178087] [ema: 0.999801] 
[Epoch 401/575] [Batch 0/87] [D loss: 0.362171] [G loss: 0.185385] [ema: 0.999801] 
[Epoch 402/575] [Batch 0/87] [D loss: 0.401774] [G loss: 0.199713] [ema: 0.999802] 
[Epoch 403/575] [Batch 0/87] [D loss: 0.431883] [G loss: 0.184835] [ema: 0.999802] 
[Epoch 404/575] [Batch 0/87] [D loss: 0.449581] [G loss: 0.183631] [ema: 0.999803] 
[Epoch 405/575] [Batch 0/87] [D loss: 0.359786] [G loss: 0.186915] [ema: 0.999803] 
[Epoch 406/575] [Batch 0/87] [D loss: 0.426011] [G loss: 0.179056] [ema: 0.999804] 
[Epoch 407/575] [Batch 0/87] [D loss: 0.390789] [G loss: 0.197618] [ema: 0.999804] 
[Epoch 408/575] [Batch 0/87] [D loss: 0.348741] [G loss: 0.197116] [ema: 0.999805] 
[Epoch 409/575] [Batch 0/87] [D loss: 0.391202] [G loss: 0.191975] [ema: 0.999805] 
[Epoch 410/575] [Batch 0/87] [D loss: 0.378996] [G loss: 0.177012] [ema: 0.999806] 
[Epoch 411/575] [Batch 0/87] [D loss: 0.409591] [G loss: 0.177458] [ema: 0.999806] 
[Epoch 412/575] [Batch 0/87] [D loss: 0.375458] [G loss: 0.181229] [ema: 0.999807] 
[Epoch 413/575] [Batch 0/87] [D loss: 0.401838] [G loss: 0.183421] [ema: 0.999807] 
[Epoch 414/575] [Batch 0/87] [D loss: 0.397198] [G loss: 0.188489] [ema: 0.999808] 
[Epoch 415/575] [Batch 0/87] [D loss: 0.387004] [G loss: 0.197106] [ema: 0.999808] 
[Epoch 416/575] [Batch 0/87] [D loss: 0.374353] [G loss: 0.196826] [ema: 0.999808] 
[Epoch 417/575] [Batch 0/87] [D loss: 0.363706] [G loss: 0.190009] [ema: 0.999809] 
[Epoch 418/575] [Batch 0/87] [D loss: 0.394058] [G loss: 0.192335] [ema: 0.999809] 
[Epoch 419/575] [Batch 0/87] [D loss: 0.368895] [G loss: 0.189538] [ema: 0.999810] 
[Epoch 420/575] [Batch 0/87] [D loss: 0.387290] [G loss: 0.199675] [ema: 0.999810] 
[Epoch 421/575] [Batch 0/87] [D loss: 0.408608] [G loss: 0.185785] [ema: 0.999811] 
[Epoch 422/575] [Batch 0/87] [D loss: 0.340506] [G loss: 0.195826] [ema: 0.999811] 
[Epoch 423/575] [Batch 0/87] [D loss: 0.393827] [G loss: 0.178169] [ema: 0.999812] 
[Epoch 424/575] [Batch 0/87] [D loss: 0.425077] [G loss: 0.178592] [ema: 0.999812] 
[Epoch 425/575] [Batch 0/87] [D loss: 0.381558] [G loss: 0.187121] [ema: 0.999813] 
[Epoch 426/575] [Batch 0/87] [D loss: 0.384668] [G loss: 0.180220] [ema: 0.999813] 
[Epoch 427/575] [Batch 0/87] [D loss: 0.384807] [G loss: 0.191500] [ema: 0.999813] 
[Epoch 428/575] [Batch 0/87] [D loss: 0.395749] [G loss: 0.184569] [ema: 0.999814] 
[Epoch 429/575] [Batch 0/87] [D loss: 0.385574] [G loss: 0.190863] [ema: 0.999814] 
[Epoch 430/575] [Batch 0/87] [D loss: 0.374328] [G loss: 0.192955] [ema: 0.999815] 
[Epoch 431/575] [Batch 0/87] [D loss: 0.388602] [G loss: 0.182266] [ema: 0.999815] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_60_100/KuHar_DAGHAR_Multiclass_50000_D_60_2024_10_25_17_04_46/Model



[Epoch 432/575] [Batch 0/87] [D loss: 0.369006] [G loss: 0.193042] [ema: 0.999816] 
[Epoch 433/575] [Batch 0/87] [D loss: 0.398271] [G loss: 0.185697] [ema: 0.999816] 
[Epoch 434/575] [Batch 0/87] [D loss: 0.389357] [G loss: 0.184721] [ema: 0.999816] 
[Epoch 435/575] [Batch 0/87] [D loss: 0.395145] [G loss: 0.193090] [ema: 0.999817] 
[Epoch 436/575] [Batch 0/87] [D loss: 0.395990] [G loss: 0.190261] [ema: 0.999817] 
[Epoch 437/575] [Batch 0/87] [D loss: 0.395136] [G loss: 0.197896] [ema: 0.999818] 
[Epoch 438/575] [Batch 0/87] [D loss: 0.434549] [G loss: 0.179620] [ema: 0.999818] 
[Epoch 439/575] [Batch 0/87] [D loss: 0.386763] [G loss: 0.186108] [ema: 0.999819] 
[Epoch 440/575] [Batch 0/87] [D loss: 0.353857] [G loss: 0.181439] [ema: 0.999819] 
[Epoch 441/575] [Batch 0/87] [D loss: 0.395817] [G loss: 0.181756] [ema: 0.999819] 
[Epoch 442/575] [Batch 0/87] [D loss: 0.362380] [G loss: 0.174467] [ema: 0.999820] 
[Epoch 443/575] [Batch 0/87] [D loss: 0.403840] [G loss: 0.187394] [ema: 0.999820] 
[Epoch 444/575] [Batch 0/87] [D loss: 0.375935] [G loss: 0.184575] [ema: 0.999821] 
[Epoch 445/575] [Batch 0/87] [D loss: 0.420300] [G loss: 0.187916] [ema: 0.999821] 
[Epoch 446/575] [Batch 0/87] [D loss: 0.403531] [G loss: 0.190368] [ema: 0.999821] 
[Epoch 447/575] [Batch 0/87] [D loss: 0.384568] [G loss: 0.190002] [ema: 0.999822] 
[Epoch 448/575] [Batch 0/87] [D loss: 0.430384] [G loss: 0.190639] [ema: 0.999822] 
[Epoch 449/575] [Batch 0/87] [D loss: 0.404607] [G loss: 0.184561] [ema: 0.999823] 
[Epoch 450/575] [Batch 0/87] [D loss: 0.377724] [G loss: 0.200954] [ema: 0.999823] 
[Epoch 451/575] [Batch 0/87] [D loss: 0.381916] [G loss: 0.190672] [ema: 0.999823] 
[Epoch 452/575] [Batch 0/87] [D loss: 0.366501] [G loss: 0.191105] [ema: 0.999824] 
[Epoch 453/575] [Batch 0/87] [D loss: 0.344135] [G loss: 0.185448] [ema: 0.999824] 
[Epoch 454/575] [Batch 0/87] [D loss: 0.400557] [G loss: 0.187832] [ema: 0.999825] 
[Epoch 455/575] [Batch 0/87] [D loss: 0.383536] [G loss: 0.193998] [ema: 0.999825] 
[Epoch 456/575] [Batch 0/87] [D loss: 0.393950] [G loss: 0.177106] [ema: 0.999825] 
[Epoch 457/575] [Batch 0/87] [D loss: 0.352334] [G loss: 0.190351] [ema: 0.999826] 
[Epoch 458/575] [Batch 0/87] [D loss: 0.400072] [G loss: 0.188298] [ema: 0.999826] 
[Epoch 459/575] [Batch 0/87] [D loss: 0.409676] [G loss: 0.195977] [ema: 0.999826] 
[Epoch 460/575] [Batch 0/87] [D loss: 0.390348] [G loss: 0.185489] [ema: 0.999827] 
[Epoch 461/575] [Batch 0/87] [D loss: 0.363976] [G loss: 0.185348] [ema: 0.999827] 
[Epoch 462/575] [Batch 0/87] [D loss: 0.388317] [G loss: 0.181851] [ema: 0.999828] 
[Epoch 463/575] [Batch 0/87] [D loss: 0.369769] [G loss: 0.194301] [ema: 0.999828] 
[Epoch 464/575] [Batch 0/87] [D loss: 0.360256] [G loss: 0.187980] [ema: 0.999828] 
[Epoch 465/575] [Batch 0/87] [D loss: 0.393697] [G loss: 0.189919] [ema: 0.999829] 
[Epoch 466/575] [Batch 0/87] [D loss: 0.373807] [G loss: 0.184500] [ema: 0.999829] 
[Epoch 467/575] [Batch 0/87] [D loss: 0.372358] [G loss: 0.177298] [ema: 0.999829] 
[Epoch 468/575] [Batch 0/87] [D loss: 0.377023] [G loss: 0.186858] [ema: 0.999830] 
[Epoch 469/575] [Batch 0/87] [D loss: 0.367108] [G loss: 0.182676] [ema: 0.999830] 
[Epoch 470/575] [Batch 0/87] [D loss: 0.400036] [G loss: 0.181266] [ema: 0.999830] 
[Epoch 471/575] [Batch 0/87] [D loss: 0.373959] [G loss: 0.180698] [ema: 0.999831] 
[Epoch 472/575] [Batch 0/87] [D loss: 0.347958] [G loss: 0.194329] [ema: 0.999831] 
[Epoch 473/575] [Batch 0/87] [D loss: 0.406817] [G loss: 0.176367] [ema: 0.999832] 
[Epoch 474/575] [Batch 0/87] [D loss: 0.345244] [G loss: 0.180519] [ema: 0.999832] 
[Epoch 475/575] [Batch 0/87] [D loss: 0.384376] [G loss: 0.189269] [ema: 0.999832] 
[Epoch 476/575] [Batch 0/87] [D loss: 0.389976] [G loss: 0.177173] [ema: 0.999833] 
[Epoch 477/575] [Batch 0/87] [D loss: 0.329529] [G loss: 0.203283] [ema: 0.999833] 
[Epoch 478/575] [Batch 0/87] [D loss: 0.364557] [G loss: 0.194071] [ema: 0.999833] 
[Epoch 479/575] [Batch 0/87] [D loss: 0.385331] [G loss: 0.184827] [ema: 0.999834] 
[Epoch 480/575] [Batch 0/87] [D loss: 0.393021] [G loss: 0.188047] [ema: 0.999834] 
[Epoch 481/575] [Batch 0/87] [D loss: 0.383390] [G loss: 0.192113] [ema: 0.999834] 
[Epoch 482/575] [Batch 0/87] [D loss: 0.377061] [G loss: 0.187424] [ema: 0.999835] 
[Epoch 483/575] [Batch 0/87] [D loss: 0.361713] [G loss: 0.189689] [ema: 0.999835] 
[Epoch 484/575] [Batch 0/87] [D loss: 0.364154] [G loss: 0.185152] [ema: 0.999835] 
[Epoch 485/575] [Batch 0/87] [D loss: 0.393630] [G loss: 0.181630] [ema: 0.999836] 
[Epoch 486/575] [Batch 0/87] [D loss: 0.358329] [G loss: 0.178610] [ema: 0.999836] 
[Epoch 487/575] [Batch 0/87] [D loss: 0.375775] [G loss: 0.185995] [ema: 0.999836] 
[Epoch 488/575] [Batch 0/87] [D loss: 0.402952] [G loss: 0.193015] [ema: 0.999837] 
[Epoch 489/575] [Batch 0/87] [D loss: 0.388790] [G loss: 0.194339] [ema: 0.999837] 
[Epoch 490/575] [Batch 0/87] [D loss: 0.379307] [G loss: 0.192520] [ema: 0.999837] 
[Epoch 491/575] [Batch 0/87] [D loss: 0.399113] [G loss: 0.184082] [ema: 0.999838] 
[Epoch 492/575] [Batch 0/87] [D loss: 0.370183] [G loss: 0.187691] [ema: 0.999838] 
[Epoch 493/575] [Batch 0/87] [D loss: 0.358599] [G loss: 0.192353] [ema: 0.999838] 
[Epoch 494/575] [Batch 0/87] [D loss: 0.362128] [G loss: 0.193048] [ema: 0.999839] 
[Epoch 495/575] [Batch 0/87] [D loss: 0.368188] [G loss: 0.186263] [ema: 0.999839] 
[Epoch 496/575] [Batch 0/87] [D loss: 0.384314] [G loss: 0.190198] [ema: 0.999839] 
[Epoch 497/575] [Batch 0/87] [D loss: 0.398355] [G loss: 0.186995] [ema: 0.999840] 
[Epoch 498/575] [Batch 0/87] [D loss: 0.369222] [G loss: 0.188734] [ema: 0.999840] 
[Epoch 499/575] [Batch 0/87] [D loss: 0.363813] [G loss: 0.182427] [ema: 0.999840] 
[Epoch 500/575] [Batch 0/87] [D loss: 0.400740] [G loss: 0.191854] [ema: 0.999841] 
[Epoch 501/575] [Batch 0/87] [D loss: 0.385006] [G loss: 0.191631] [ema: 0.999841] 
[Epoch 502/575] [Batch 0/87] [D loss: 0.414172] [G loss: 0.190985] [ema: 0.999841] 
[Epoch 503/575] [Batch 0/87] [D loss: 0.372832] [G loss: 0.191193] [ema: 0.999842] 
[Epoch 504/575] [Batch 0/87] [D loss: 0.380656] [G loss: 0.193973] [ema: 0.999842] 
[Epoch 505/575] [Batch 0/87] [D loss: 0.373265] [G loss: 0.188575] [ema: 0.999842] 
[Epoch 506/575] [Batch 0/87] [D loss: 0.399410] [G loss: 0.185691] [ema: 0.999843] 
[Epoch 507/575] [Batch 0/87] [D loss: 0.382011] [G loss: 0.193545] [ema: 0.999843] 
[Epoch 508/575] [Batch 0/87] [D loss: 0.385935] [G loss: 0.186055] [ema: 0.999843] 
[Epoch 509/575] [Batch 0/87] [D loss: 0.413322] [G loss: 0.183728] [ema: 0.999843] 
[Epoch 510/575] [Batch 0/87] [D loss: 0.378151] [G loss: 0.185072] [ema: 0.999844] 
[Epoch 511/575] [Batch 0/87] [D loss: 0.358741] [G loss: 0.191977] [ema: 0.999844] 
[Epoch 512/575] [Batch 0/87] [D loss: 0.381132] [G loss: 0.192327] [ema: 0.999844] 
[Epoch 513/575] [Batch 0/87] [D loss: 0.428449] [G loss: 0.182814] [ema: 0.999845] 
[Epoch 514/575] [Batch 0/87] [D loss: 0.423675] [G loss: 0.189282] [ema: 0.999845] 
[Epoch 515/575] [Batch 0/87] [D loss: 0.387926] [G loss: 0.187299] [ema: 0.999845] 
[Epoch 516/575] [Batch 0/87] [D loss: 0.351427] [G loss: 0.188474] [ema: 0.999846] 
[Epoch 517/575] [Batch 0/87] [D loss: 0.393657] [G loss: 0.187632] [ema: 0.999846] 
[Epoch 518/575] [Batch 0/87] [D loss: 0.367469] [G loss: 0.189689] [ema: 0.999846] 
[Epoch 519/575] [Batch 0/87] [D loss: 0.395945] [G loss: 0.185886] [ema: 0.999847] 
[Epoch 520/575] [Batch 0/87] [D loss: 0.376547] [G loss: 0.193226] [ema: 0.999847] 
[Epoch 521/575] [Batch 0/87] [D loss: 0.370494] [G loss: 0.181692] [ema: 0.999847] 
[Epoch 522/575] [Batch 0/87] [D loss: 0.408262] [G loss: 0.182710] [ema: 0.999847] 
[Epoch 523/575] [Batch 0/87] [D loss: 0.370737] [G loss: 0.177531] [ema: 0.999848] 
[Epoch 524/575] [Batch 0/87] [D loss: 0.416428] [G loss: 0.174134] [ema: 0.999848] 
[Epoch 525/575] [Batch 0/87] [D loss: 0.405237] [G loss: 0.190108] [ema: 0.999848] 
[Epoch 526/575] [Batch 0/87] [D loss: 0.394990] [G loss: 0.186957] [ema: 0.999849] 
[Epoch 527/575] [Batch 0/87] [D loss: 0.366613] [G loss: 0.185527] [ema: 0.999849] 
[Epoch 528/575] [Batch 0/87] [D loss: 0.368604] [G loss: 0.187154] [ema: 0.999849] 
[Epoch 529/575] [Batch 0/87] [D loss: 0.362456] [G loss: 0.182788] [ema: 0.999849] 
[Epoch 530/575] [Batch 0/87] [D loss: 0.416184] [G loss: 0.189018] [ema: 0.999850] 
[Epoch 531/575] [Batch 0/87] [D loss: 0.373909] [G loss: 0.179194] [ema: 0.999850] 
[Epoch 532/575] [Batch 0/87] [D loss: 0.388345] [G loss: 0.184867] [ema: 0.999850] 
[Epoch 533/575] [Batch 0/87] [D loss: 0.401688] [G loss: 0.182312] [ema: 0.999851] 
[Epoch 534/575] [Batch 0/87] [D loss: 0.397183] [G loss: 0.186514] [ema: 0.999851] 
[Epoch 535/575] [Batch 0/87] [D loss: 0.410953] [G loss: 0.187125] [ema: 0.999851] 
[Epoch 536/575] [Batch 0/87] [D loss: 0.380797] [G loss: 0.201933] [ema: 0.999851] 
[Epoch 537/575] [Batch 0/87] [D loss: 0.399585] [G loss: 0.182468] [ema: 0.999852] 
[Epoch 538/575] [Batch 0/87] [D loss: 0.372858] [G loss: 0.190646] [ema: 0.999852] 
[Epoch 539/575] [Batch 0/87] [D loss: 0.383016] [G loss: 0.186740] [ema: 0.999852] 
[Epoch 540/575] [Batch 0/87] [D loss: 0.367789] [G loss: 0.187338] [ema: 0.999852] 
[Epoch 541/575] [Batch 0/87] [D loss: 0.405933] [G loss: 0.183372] [ema: 0.999853] 
[Epoch 542/575] [Batch 0/87] [D loss: 0.407213] [G loss: 0.184220] [ema: 0.999853] 
[Epoch 543/575] [Batch 0/87] [D loss: 0.383852] [G loss: 0.201027] [ema: 0.999853] 
[Epoch 544/575] [Batch 0/87] [D loss: 0.464675] [G loss: 0.186492] [ema: 0.999854] 
[Epoch 545/575] [Batch 0/87] [D loss: 0.388542] [G loss: 0.181026] [ema: 0.999854] 
[Epoch 546/575] [Batch 0/87] [D loss: 0.387951] [G loss: 0.179870] [ema: 0.999854] 
[Epoch 547/575] [Batch 0/87] [D loss: 0.379726] [G loss: 0.194488] [ema: 0.999854] 
[Epoch 548/575] [Batch 0/87] [D loss: 0.394837] [G loss: 0.187329] [ema: 0.999855] 
[Epoch 549/575] [Batch 0/87] [D loss: 0.376997] [G loss: 0.180598] [ema: 0.999855] 
[Epoch 550/575] [Batch 0/87] [D loss: 0.382197] [G loss: 0.187911] [ema: 0.999855] 
[Epoch 551/575] [Batch 0/87] [D loss: 0.413216] [G loss: 0.189311] [ema: 0.999855] 
[Epoch 552/575] [Batch 0/87] [D loss: 0.352983] [G loss: 0.184748] [ema: 0.999856] 
[Epoch 553/575] [Batch 0/87] [D loss: 0.389819] [G loss: 0.194232] [ema: 0.999856] 
[Epoch 554/575] [Batch 0/87] [D loss: 0.414053] [G loss: 0.176535] [ema: 0.999856] 
[Epoch 555/575] [Batch 0/87] [D loss: 0.433309] [G loss: 0.187764] [ema: 0.999856] 
[Epoch 556/575] [Batch 0/87] [D loss: 0.382541] [G loss: 0.195222] [ema: 0.999857] 
[Epoch 557/575] [Batch 0/87] [D loss: 0.389161] [G loss: 0.192193] [ema: 0.999857] 
[Epoch 558/575] [Batch 0/87] [D loss: 0.352202] [G loss: 0.198853] [ema: 0.999857] 
[Epoch 559/575] [Batch 0/87] [D loss: 0.365020] [G loss: 0.183979] [ema: 0.999857] 
[Epoch 560/575] [Batch 0/87] [D loss: 0.395716] [G loss: 0.191548] [ema: 0.999858] 
[Epoch 561/575] [Batch 0/87] [D loss: 0.386015] [G loss: 0.175153] [ema: 0.999858] 
[Epoch 562/575] [Batch 0/87] [D loss: 0.375190] [G loss: 0.176637] [ema: 0.999858] 
[Epoch 563/575] [Batch 0/87] [D loss: 0.378915] [G loss: 0.178421] [ema: 0.999858] 
[Epoch 564/575] [Batch 0/87] [D loss: 0.435787] [G loss: 0.180301] [ema: 0.999859] 
[Epoch 565/575] [Batch 0/87] [D loss: 0.387490] [G loss: 0.184946] [ema: 0.999859] 
[Epoch 566/575] [Batch 0/87] [D loss: 0.386064] [G loss: 0.190817] [ema: 0.999859] 
[Epoch 567/575] [Batch 0/87] [D loss: 0.410430] [G loss: 0.185632] [ema: 0.999859] 
[Epoch 568/575] [Batch 0/87] [D loss: 0.400710] [G loss: 0.196760] [ema: 0.999860] 
[Epoch 569/575] [Batch 0/87] [D loss: 0.399473] [G loss: 0.193503] [ema: 0.999860] 
[Epoch 570/575] [Batch 0/87] [D loss: 0.386221] [G loss: 0.186199] [ema: 0.999860] 
[Epoch 571/575] [Batch 0/87] [D loss: 0.411169] [G loss: 0.188038] [ema: 0.999860] 
[Epoch 572/575] [Batch 0/87] [D loss: 0.411338] [G loss: 0.189661] [ema: 0.999861] 
[Epoch 573/575] [Batch 0/87] [D loss: 0.404757] [G loss: 0.190989] [ema: 0.999861] 
[Epoch 574/575] [Batch 0/87] [D loss: 0.357962] [G loss: 0.192363] [ema: 0.999861] 
