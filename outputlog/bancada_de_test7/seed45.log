
 Starting training
Total of classes being trained: 1

['UCI_DAGHAR_Multiclass.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
UCI_DAGHAR_Multiclass training
Data path: ../DAGHAR_split_25_10_all/train/data/UCI_DAGHAR_Multiclass.csv
Label path: ../DAGHAR_split_25_10_all/train/label/UCI_Label_Multiclass.csv
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): RearrangeLayer()
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): ReduceLayer()
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): RearrangeLayer()
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): ReduceLayer()
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
UCI_DAGHAR_Multiclass
daghar
Returning single-class data and labels, class: UCI_DAGHAR_Multiclass
Data shape: (29430, 6, 1, 60)
Label shape: (29430,)
460
Epochs between checkpoint: 75



Saving checkpoint 1 in logs/minerva/UCI_DAGHAR_Multiclass_137694_D_60_6axis_2025_03_18_13_32_31/Model



[Epoch 0/300] [Batch 0/460] [D loss: 0.816038] [G loss: 0.602094] [ema: 0.000000] 
[Epoch 1/300] [Batch 0/460] [D loss: 0.411247] [G loss: 0.210568] [ema: 0.985045] 
[Epoch 2/300] [Batch 0/460] [D loss: 0.467980] [G loss: 0.177718] [ema: 0.992494] 
[Epoch 3/300] [Batch 0/460] [D loss: 0.479793] [G loss: 0.152357] [ema: 0.994990] 
[Epoch 4/300] [Batch 0/460] [D loss: 0.450538] [G loss: 0.167497] [ema: 0.996240] 
[Epoch 5/300] [Batch 0/460] [D loss: 0.406853] [G loss: 0.179196] [ema: 0.996991] 
[Epoch 6/300] [Batch 0/460] [D loss: 0.393280] [G loss: 0.178407] [ema: 0.997492] 
[Epoch 7/300] [Batch 0/460] [D loss: 0.297872] [G loss: 0.243998] [ema: 0.997850] 
[Epoch 8/300] [Batch 0/460] [D loss: 0.324176] [G loss: 0.230765] [ema: 0.998118] 
[Epoch 9/300] [Batch 0/460] [D loss: 0.354064] [G loss: 0.210125] [ema: 0.998327] 
[Epoch 10/300] [Batch 0/460] [D loss: 0.288541] [G loss: 0.242257] [ema: 0.998494] 
[Epoch 11/300] [Batch 0/460] [D loss: 0.329406] [G loss: 0.209306] [ema: 0.998631] 
[Epoch 12/300] [Batch 0/460] [D loss: 0.347373] [G loss: 0.244254] [ema: 0.998745] 
[Epoch 13/300] [Batch 0/460] [D loss: 0.351824] [G loss: 0.216038] [ema: 0.998842] 
[Epoch 14/300] [Batch 0/460] [D loss: 0.310479] [G loss: 0.237770] [ema: 0.998924] 
[Epoch 15/300] [Batch 0/460] [D loss: 0.289954] [G loss: 0.233430] [ema: 0.998996] 
[Epoch 16/300] [Batch 0/460] [D loss: 0.299580] [G loss: 0.230124] [ema: 0.999059] 
[Epoch 17/300] [Batch 0/460] [D loss: 0.286781] [G loss: 0.241079] [ema: 0.999114] 
[Epoch 18/300] [Batch 0/460] [D loss: 0.292479] [G loss: 0.225804] [ema: 0.999163] 
[Epoch 19/300] [Batch 0/460] [D loss: 0.292649] [G loss: 0.240254] [ema: 0.999207] 
[Epoch 20/300] [Batch 0/460] [D loss: 0.295596] [G loss: 0.228549] [ema: 0.999247] 
[Epoch 21/300] [Batch 0/460] [D loss: 0.301644] [G loss: 0.218902] [ema: 0.999283] 
[Epoch 22/300] [Batch 0/460] [D loss: 0.309489] [G loss: 0.248819] [ema: 0.999315] 
[Epoch 23/300] [Batch 0/460] [D loss: 0.292457] [G loss: 0.241327] [ema: 0.999345] 
[Epoch 24/300] [Batch 0/460] [D loss: 0.300345] [G loss: 0.237724] [ema: 0.999372] 
[Epoch 25/300] [Batch 0/460] [D loss: 0.278173] [G loss: 0.249977] [ema: 0.999397] 
[Epoch 26/300] [Batch 0/460] [D loss: 0.287471] [G loss: 0.227456] [ema: 0.999421] 
[Epoch 27/300] [Batch 0/460] [D loss: 0.285252] [G loss: 0.248402] [ema: 0.999442] 
[Epoch 28/300] [Batch 0/460] [D loss: 0.281000] [G loss: 0.234934] [ema: 0.999462] 
[Epoch 29/300] [Batch 0/460] [D loss: 0.277013] [G loss: 0.252234] [ema: 0.999481] 
[Epoch 30/300] [Batch 0/460] [D loss: 0.295318] [G loss: 0.220471] [ema: 0.999498] 
[Epoch 31/300] [Batch 0/460] [D loss: 0.280723] [G loss: 0.230945] [ema: 0.999514] 
[Epoch 32/300] [Batch 0/460] [D loss: 0.288510] [G loss: 0.247688] [ema: 0.999529] 
[Epoch 33/300] [Batch 0/460] [D loss: 0.308771] [G loss: 0.230501] [ema: 0.999543] 
[Epoch 34/300] [Batch 0/460] [D loss: 0.289782] [G loss: 0.244631] [ema: 0.999557] 
[Epoch 35/300] [Batch 0/460] [D loss: 0.278246] [G loss: 0.251443] [ema: 0.999570] 
[Epoch 36/300] [Batch 0/460] [D loss: 0.286953] [G loss: 0.243870] [ema: 0.999582] 
[Epoch 37/300] [Batch 0/460] [D loss: 0.266078] [G loss: 0.235999] [ema: 0.999593] 
[Epoch 38/300] [Batch 0/460] [D loss: 0.290088] [G loss: 0.239462] [ema: 0.999604] 
[Epoch 39/300] [Batch 0/460] [D loss: 0.299771] [G loss: 0.233413] [ema: 0.999614] 
[Epoch 40/300] [Batch 0/460] [D loss: 0.323778] [G loss: 0.244699] [ema: 0.999623] 
[Epoch 41/300] [Batch 0/460] [D loss: 0.298126] [G loss: 0.224994] [ema: 0.999633] 
[Epoch 42/300] [Batch 0/460] [D loss: 0.287509] [G loss: 0.234816] [ema: 0.999641] 
[Epoch 43/300] [Batch 0/460] [D loss: 0.273238] [G loss: 0.242502] [ema: 0.999650] 
[Epoch 44/300] [Batch 0/460] [D loss: 0.300202] [G loss: 0.231587] [ema: 0.999658] 
[Epoch 45/300] [Batch 0/460] [D loss: 0.292296] [G loss: 0.214744] [ema: 0.999665] 
[Epoch 46/300] [Batch 0/460] [D loss: 0.320053] [G loss: 0.227231] [ema: 0.999672] 
[Epoch 47/300] [Batch 0/460] [D loss: 0.301371] [G loss: 0.236977] [ema: 0.999679] 
[Epoch 48/300] [Batch 0/460] [D loss: 0.292455] [G loss: 0.219940] [ema: 0.999686] 
[Epoch 49/300] [Batch 0/460] [D loss: 0.314054] [G loss: 0.229193] [ema: 0.999693] 
[Epoch 50/300] [Batch 0/460] [D loss: 0.311929] [G loss: 0.232567] [ema: 0.999699] 
[Epoch 51/300] [Batch 0/460] [D loss: 0.338637] [G loss: 0.215235] [ema: 0.999705] 
[Epoch 52/300] [Batch 0/460] [D loss: 0.300358] [G loss: 0.225994] [ema: 0.999710] 
[Epoch 53/300] [Batch 0/460] [D loss: 0.282420] [G loss: 0.241715] [ema: 0.999716] 
[Epoch 54/300] [Batch 0/460] [D loss: 0.290058] [G loss: 0.245036] [ema: 0.999721] 
[Epoch 55/300] [Batch 0/460] [D loss: 0.321266] [G loss: 0.227348] [ema: 0.999726] 
[Epoch 56/300] [Batch 0/460] [D loss: 0.308915] [G loss: 0.233345] [ema: 0.999731] 
[Epoch 57/300] [Batch 0/460] [D loss: 0.306552] [G loss: 0.240351] [ema: 0.999736] 
[Epoch 58/300] [Batch 0/460] [D loss: 0.291702] [G loss: 0.231585] [ema: 0.999740] 
[Epoch 59/300] [Batch 0/460] [D loss: 0.310266] [G loss: 0.201997] [ema: 0.999745] 
[Epoch 60/300] [Batch 0/460] [D loss: 0.310162] [G loss: 0.209926] [ema: 0.999749] 
[Epoch 61/300] [Batch 0/460] [D loss: 0.313774] [G loss: 0.215647] [ema: 0.999753] 
[Epoch 62/300] [Batch 0/460] [D loss: 0.291537] [G loss: 0.233185] [ema: 0.999757] 
[Epoch 63/300] [Batch 0/460] [D loss: 0.303412] [G loss: 0.237426] [ema: 0.999761] 
[Epoch 64/300] [Batch 0/460] [D loss: 0.310571] [G loss: 0.221415] [ema: 0.999765] 
[Epoch 65/300] [Batch 0/460] [D loss: 0.312652] [G loss: 0.219305] [ema: 0.999768] 
[Epoch 66/300] [Batch 0/460] [D loss: 0.317553] [G loss: 0.185150] [ema: 0.999772] 
[Epoch 67/300] [Batch 0/460] [D loss: 0.343274] [G loss: 0.242634] [ema: 0.999775] 
[Epoch 68/300] [Batch 0/460] [D loss: 0.287296] [G loss: 0.236381] [ema: 0.999778] 
[Epoch 69/300] [Batch 0/460] [D loss: 0.309489] [G loss: 0.220896] [ema: 0.999782] 
[Epoch 70/300] [Batch 0/460] [D loss: 0.295045] [G loss: 0.228876] [ema: 0.999785] 
[Epoch 71/300] [Batch 0/460] [D loss: 0.269524] [G loss: 0.239943] [ema: 0.999788] 
[Epoch 72/300] [Batch 0/460] [D loss: 0.325023] [G loss: 0.217277] [ema: 0.999791] 
[Epoch 73/300] [Batch 0/460] [D loss: 0.313701] [G loss: 0.224434] [ema: 0.999794] 
[Epoch 74/300] [Batch 0/460] [D loss: 0.308526] [G loss: 0.228330] [ema: 0.999796] 



Saving checkpoint 2 in logs/minerva/UCI_DAGHAR_Multiclass_137694_D_60_6axis_2025_03_18_13_32_31/Model



[Epoch 75/300] [Batch 0/460] [D loss: 0.293475] [G loss: 0.222082] [ema: 0.999799] 
[Epoch 76/300] [Batch 0/460] [D loss: 0.307692] [G loss: 0.259265] [ema: 0.999802] 
[Epoch 77/300] [Batch 0/460] [D loss: 0.280486] [G loss: 0.254956] [ema: 0.999804] 
[Epoch 78/300] [Batch 0/460] [D loss: 0.302523] [G loss: 0.232436] [ema: 0.999807] 
[Epoch 79/300] [Batch 0/460] [D loss: 0.316477] [G loss: 0.219494] [ema: 0.999809] 
[Epoch 80/300] [Batch 0/460] [D loss: 0.298681] [G loss: 0.255116] [ema: 0.999812] 
[Epoch 81/300] [Batch 0/460] [D loss: 0.319185] [G loss: 0.249071] [ema: 0.999814] 
[Epoch 82/300] [Batch 0/460] [D loss: 0.304870] [G loss: 0.229813] [ema: 0.999816] 
[Epoch 83/300] [Batch 0/460] [D loss: 0.323333] [G loss: 0.225179] [ema: 0.999818] 
[Epoch 84/300] [Batch 0/460] [D loss: 0.312692] [G loss: 0.221286] [ema: 0.999821] 
[Epoch 85/300] [Batch 0/460] [D loss: 0.292500] [G loss: 0.250054] [ema: 0.999823] 
[Epoch 86/300] [Batch 0/460] [D loss: 0.316168] [G loss: 0.236396] [ema: 0.999825] 
[Epoch 87/300] [Batch 0/460] [D loss: 0.288443] [G loss: 0.233228] [ema: 0.999827] 
[Epoch 88/300] [Batch 0/460] [D loss: 0.290445] [G loss: 0.217356] [ema: 0.999829] 
[Epoch 89/300] [Batch 0/460] [D loss: 0.291828] [G loss: 0.237866] [ema: 0.999831] 
[Epoch 90/300] [Batch 0/460] [D loss: 0.296022] [G loss: 0.240641] [ema: 0.999833] 
[Epoch 91/300] [Batch 0/460] [D loss: 0.332414] [G loss: 0.201402] [ema: 0.999834] 
[Epoch 92/300] [Batch 0/460] [D loss: 0.314178] [G loss: 0.217821] [ema: 0.999836] 
[Epoch 93/300] [Batch 0/460] [D loss: 0.307592] [G loss: 0.235521] [ema: 0.999838] 
[Epoch 94/300] [Batch 0/460] [D loss: 0.287848] [G loss: 0.233380] [ema: 0.999840] 
[Epoch 95/300] [Batch 0/460] [D loss: 0.323859] [G loss: 0.220052] [ema: 0.999841] 
[Epoch 96/300] [Batch 0/460] [D loss: 0.318715] [G loss: 0.245246] [ema: 0.999843] 
[Epoch 97/300] [Batch 0/460] [D loss: 0.297735] [G loss: 0.246188] [ema: 0.999845] 
[Epoch 98/300] [Batch 0/460] [D loss: 0.321474] [G loss: 0.236284] [ema: 0.999846] 
[Epoch 99/300] [Batch 0/460] [D loss: 0.321906] [G loss: 0.210105] [ema: 0.999848] 
[Epoch 100/300] [Batch 0/460] [D loss: 0.287006] [G loss: 0.224599] [ema: 0.999849] 
[Epoch 101/300] [Batch 0/460] [D loss: 0.276660] [G loss: 0.229705] [ema: 0.999851] 
[Epoch 102/300] [Batch 0/460] [D loss: 0.326847] [G loss: 0.236056] [ema: 0.999852] 
[Epoch 103/300] [Batch 0/460] [D loss: 0.303273] [G loss: 0.241369] [ema: 0.999854] 
[Epoch 104/300] [Batch 0/460] [D loss: 0.300384] [G loss: 0.221541] [ema: 0.999855] 
[Epoch 105/300] [Batch 0/460] [D loss: 0.308328] [G loss: 0.220524] [ema: 0.999857] 
[Epoch 106/300] [Batch 0/460] [D loss: 0.304570] [G loss: 0.208131] [ema: 0.999858] 
[Epoch 107/300] [Batch 0/460] [D loss: 0.318382] [G loss: 0.231036] [ema: 0.999859] 
[Epoch 108/300] [Batch 0/460] [D loss: 0.323155] [G loss: 0.212610] [ema: 0.999860] 
[Epoch 109/300] [Batch 0/460] [D loss: 0.313875] [G loss: 0.256984] [ema: 0.999862] 
[Epoch 110/300] [Batch 0/460] [D loss: 0.307738] [G loss: 0.203859] [ema: 0.999863] 
[Epoch 111/300] [Batch 0/460] [D loss: 0.332377] [G loss: 0.247120] [ema: 0.999864] 
[Epoch 112/300] [Batch 0/460] [D loss: 0.288268] [G loss: 0.255750] [ema: 0.999865] 
[Epoch 113/300] [Batch 0/460] [D loss: 0.310680] [G loss: 0.208522] [ema: 0.999867] 
[Epoch 114/300] [Batch 0/460] [D loss: 0.312429] [G loss: 0.211112] [ema: 0.999868] 
[Epoch 115/300] [Batch 0/460] [D loss: 0.303508] [G loss: 0.228529] [ema: 0.999869] 
[Epoch 116/300] [Batch 0/460] [D loss: 0.292098] [G loss: 0.238611] [ema: 0.999870] 
[Epoch 117/300] [Batch 0/460] [D loss: 0.309908] [G loss: 0.198219] [ema: 0.999871] 
[Epoch 118/300] [Batch 0/460] [D loss: 0.310425] [G loss: 0.217316] [ema: 0.999872] 
[Epoch 119/300] [Batch 0/460] [D loss: 0.308228] [G loss: 0.220403] [ema: 0.999873] 
[Epoch 120/300] [Batch 0/460] [D loss: 0.287570] [G loss: 0.239023] [ema: 0.999874] 
[Epoch 121/300] [Batch 0/460] [D loss: 0.286477] [G loss: 0.240559] [ema: 0.999875] 
[Epoch 122/300] [Batch 0/460] [D loss: 0.300592] [G loss: 0.247906] [ema: 0.999876] 
[Epoch 123/300] [Batch 0/460] [D loss: 0.316273] [G loss: 0.241465] [ema: 0.999878] 
[Epoch 124/300] [Batch 0/460] [D loss: 0.287556] [G loss: 0.235425] [ema: 0.999878] 
[Epoch 125/300] [Batch 0/460] [D loss: 0.293557] [G loss: 0.235428] [ema: 0.999879] 
[Epoch 126/300] [Batch 0/460] [D loss: 0.313004] [G loss: 0.229401] [ema: 0.999880] 
[Epoch 127/300] [Batch 0/460] [D loss: 0.331341] [G loss: 0.239097] [ema: 0.999881] 
[Epoch 128/300] [Batch 0/460] [D loss: 0.295144] [G loss: 0.253158] [ema: 0.999882] 
[Epoch 129/300] [Batch 0/460] [D loss: 0.286053] [G loss: 0.238312] [ema: 0.999883] 
[Epoch 130/300] [Batch 0/460] [D loss: 0.301743] [G loss: 0.242697] [ema: 0.999884] 
[Epoch 131/300] [Batch 0/460] [D loss: 0.322214] [G loss: 0.237936] [ema: 0.999885] 
[Epoch 132/300] [Batch 0/460] [D loss: 0.272847] [G loss: 0.236001] [ema: 0.999886] 
[Epoch 133/300] [Batch 0/460] [D loss: 0.269941] [G loss: 0.238684] [ema: 0.999887] 
[Epoch 134/300] [Batch 0/460] [D loss: 0.316693] [G loss: 0.241289] [ema: 0.999888] 
[Epoch 135/300] [Batch 0/460] [D loss: 0.317412] [G loss: 0.220410] [ema: 0.999888] 
[Epoch 136/300] [Batch 0/460] [D loss: 0.335032] [G loss: 0.247529] [ema: 0.999889] 
[Epoch 137/300] [Batch 0/460] [D loss: 0.307522] [G loss: 0.230275] [ema: 0.999890] 
[Epoch 138/300] [Batch 0/460] [D loss: 0.328333] [G loss: 0.237229] [ema: 0.999891] 
[Epoch 139/300] [Batch 0/460] [D loss: 0.301856] [G loss: 0.239307] [ema: 0.999892] 
[Epoch 140/300] [Batch 0/460] [D loss: 0.327758] [G loss: 0.194749] [ema: 0.999892] 
[Epoch 141/300] [Batch 0/460] [D loss: 0.295786] [G loss: 0.232476] [ema: 0.999893] 
[Epoch 142/300] [Batch 0/460] [D loss: 0.276493] [G loss: 0.264185] [ema: 0.999894] 
[Epoch 143/300] [Batch 0/460] [D loss: 0.298270] [G loss: 0.230359] [ema: 0.999895] 
[Epoch 144/300] [Batch 0/460] [D loss: 0.302111] [G loss: 0.228121] [ema: 0.999895] 
[Epoch 145/300] [Batch 0/460] [D loss: 0.276730] [G loss: 0.238909] [ema: 0.999896] 
[Epoch 146/300] [Batch 0/460] [D loss: 0.291828] [G loss: 0.223248] [ema: 0.999897] 
[Epoch 147/300] [Batch 0/460] [D loss: 0.296103] [G loss: 0.248715] [ema: 0.999897] 
[Epoch 148/300] [Batch 0/460] [D loss: 0.295668] [G loss: 0.213141] [ema: 0.999898] 
[Epoch 149/300] [Batch 0/460] [D loss: 0.291616] [G loss: 0.216191] [ema: 0.999899] 



Saving checkpoint 3 in logs/minerva/UCI_DAGHAR_Multiclass_137694_D_60_6axis_2025_03_18_13_32_31/Model



[Epoch 150/300] [Batch 0/460] [D loss: 0.297555] [G loss: 0.232928] [ema: 0.999900] 
[Epoch 151/300] [Batch 0/460] [D loss: 0.310550] [G loss: 0.238193] [ema: 0.999900] 
[Epoch 152/300] [Batch 0/460] [D loss: 0.310043] [G loss: 0.236877] [ema: 0.999901] 
[Epoch 153/300] [Batch 0/460] [D loss: 0.318616] [G loss: 0.229497] [ema: 0.999902] 
[Epoch 154/300] [Batch 0/460] [D loss: 0.308408] [G loss: 0.213345] [ema: 0.999902] 
[Epoch 155/300] [Batch 0/460] [D loss: 0.282723] [G loss: 0.234467] [ema: 0.999903] 
[Epoch 156/300] [Batch 0/460] [D loss: 0.302577] [G loss: 0.242145] [ema: 0.999903] 
[Epoch 157/300] [Batch 0/460] [D loss: 0.296388] [G loss: 0.256992] [ema: 0.999904] 
[Epoch 158/300] [Batch 0/460] [D loss: 0.296826] [G loss: 0.245150] [ema: 0.999905] 
[Epoch 159/300] [Batch 0/460] [D loss: 0.276838] [G loss: 0.229351] [ema: 0.999905] 
[Epoch 160/300] [Batch 0/460] [D loss: 0.326030] [G loss: 0.236091] [ema: 0.999906] 
[Epoch 161/300] [Batch 0/460] [D loss: 0.318866] [G loss: 0.237724] [ema: 0.999906] 
[Epoch 162/300] [Batch 0/460] [D loss: 0.281348] [G loss: 0.244771] [ema: 0.999907] 
[Epoch 163/300] [Batch 0/460] [D loss: 0.308457] [G loss: 0.237212] [ema: 0.999908] 
[Epoch 164/300] [Batch 0/460] [D loss: 0.292712] [G loss: 0.240404] [ema: 0.999908] 
[Epoch 165/300] [Batch 0/460] [D loss: 0.291283] [G loss: 0.235509] [ema: 0.999909] 
[Epoch 166/300] [Batch 0/460] [D loss: 0.275521] [G loss: 0.260591] [ema: 0.999909] 
[Epoch 167/300] [Batch 0/460] [D loss: 0.305530] [G loss: 0.250571] [ema: 0.999910] 
[Epoch 168/300] [Batch 0/460] [D loss: 0.308756] [G loss: 0.236462] [ema: 0.999910] 
[Epoch 169/300] [Batch 0/460] [D loss: 0.305625] [G loss: 0.225116] [ema: 0.999911] 
[Epoch 170/300] [Batch 0/460] [D loss: 0.293896] [G loss: 0.235798] [ema: 0.999911] 
[Epoch 171/300] [Batch 0/460] [D loss: 0.317122] [G loss: 0.231806] [ema: 0.999912] 
[Epoch 172/300] [Batch 0/460] [D loss: 0.274820] [G loss: 0.236795] [ema: 0.999912] 
[Epoch 173/300] [Batch 0/460] [D loss: 0.303566] [G loss: 0.234826] [ema: 0.999913] 
[Epoch 174/300] [Batch 0/460] [D loss: 0.274867] [G loss: 0.256119] [ema: 0.999913] 
[Epoch 175/300] [Batch 0/460] [D loss: 0.293273] [G loss: 0.220973] [ema: 0.999914] 
[Epoch 176/300] [Batch 0/460] [D loss: 0.282163] [G loss: 0.218798] [ema: 0.999914] 
[Epoch 177/300] [Batch 0/460] [D loss: 0.281506] [G loss: 0.230173] [ema: 0.999915] 
[Epoch 178/300] [Batch 0/460] [D loss: 0.290328] [G loss: 0.256672] [ema: 0.999915] 
[Epoch 179/300] [Batch 0/460] [D loss: 0.281632] [G loss: 0.252065] [ema: 0.999916] 
[Epoch 180/300] [Batch 0/460] [D loss: 0.282616] [G loss: 0.252335] [ema: 0.999916] 
[Epoch 181/300] [Batch 0/460] [D loss: 0.291324] [G loss: 0.228176] [ema: 0.999917] 
[Epoch 182/300] [Batch 0/460] [D loss: 0.281617] [G loss: 0.238542] [ema: 0.999917] 
[Epoch 183/300] [Batch 0/460] [D loss: 0.299197] [G loss: 0.250054] [ema: 0.999918] 
[Epoch 184/300] [Batch 0/460] [D loss: 0.295754] [G loss: 0.243586] [ema: 0.999918] 
[Epoch 185/300] [Batch 0/460] [D loss: 0.287351] [G loss: 0.238644] [ema: 0.999919] 
[Epoch 186/300] [Batch 0/460] [D loss: 0.290004] [G loss: 0.236070] [ema: 0.999919] 
[Epoch 187/300] [Batch 0/460] [D loss: 0.297329] [G loss: 0.248453] [ema: 0.999919] 
[Epoch 188/300] [Batch 0/460] [D loss: 0.282294] [G loss: 0.256376] [ema: 0.999920] 
[Epoch 189/300] [Batch 0/460] [D loss: 0.286145] [G loss: 0.234219] [ema: 0.999920] 
[Epoch 190/300] [Batch 0/460] [D loss: 0.296551] [G loss: 0.215970] [ema: 0.999921] 
[Epoch 191/300] [Batch 0/460] [D loss: 0.313555] [G loss: 0.217644] [ema: 0.999921] 
[Epoch 192/300] [Batch 0/460] [D loss: 0.315870] [G loss: 0.240913] [ema: 0.999922] 
[Epoch 193/300] [Batch 0/460] [D loss: 0.289022] [G loss: 0.242580] [ema: 0.999922] 
[Epoch 194/300] [Batch 0/460] [D loss: 0.280468] [G loss: 0.205502] [ema: 0.999922] 
[Epoch 195/300] [Batch 0/460] [D loss: 0.274737] [G loss: 0.229031] [ema: 0.999923] 
[Epoch 196/300] [Batch 0/460] [D loss: 0.297355] [G loss: 0.233782] [ema: 0.999923] 
[Epoch 197/300] [Batch 0/460] [D loss: 0.287182] [G loss: 0.224272] [ema: 0.999924] 
[Epoch 198/300] [Batch 0/460] [D loss: 0.279012] [G loss: 0.250984] [ema: 0.999924] 
[Epoch 199/300] [Batch 0/460] [D loss: 0.297861] [G loss: 0.245988] [ema: 0.999924] 
[Epoch 200/300] [Batch 0/460] [D loss: 0.312097] [G loss: 0.235126] [ema: 0.999925] 
[Epoch 201/300] [Batch 0/460] [D loss: 0.302476] [G loss: 0.236145] [ema: 0.999925] 
[Epoch 202/300] [Batch 0/460] [D loss: 0.278428] [G loss: 0.241525] [ema: 0.999925] 
[Epoch 203/300] [Batch 0/460] [D loss: 0.312140] [G loss: 0.226496] [ema: 0.999926] 
[Epoch 204/300] [Batch 0/460] [D loss: 0.286979] [G loss: 0.242239] [ema: 0.999926] 
[Epoch 205/300] [Batch 0/460] [D loss: 0.290200] [G loss: 0.240705] [ema: 0.999926] 
[Epoch 206/300] [Batch 0/460] [D loss: 0.285989] [G loss: 0.221842] [ema: 0.999927] 
[Epoch 207/300] [Batch 0/460] [D loss: 0.303913] [G loss: 0.241534] [ema: 0.999927] 
[Epoch 208/300] [Batch 0/460] [D loss: 0.317487] [G loss: 0.228053] [ema: 0.999928] 
[Epoch 209/300] [Batch 0/460] [D loss: 0.301664] [G loss: 0.219183] [ema: 0.999928] 
[Epoch 210/300] [Batch 0/460] [D loss: 0.275241] [G loss: 0.233198] [ema: 0.999928] 
[Epoch 211/300] [Batch 0/460] [D loss: 0.276624] [G loss: 0.247388] [ema: 0.999929] 
[Epoch 212/300] [Batch 0/460] [D loss: 0.299892] [G loss: 0.225890] [ema: 0.999929] 
[Epoch 213/300] [Batch 0/460] [D loss: 0.282494] [G loss: 0.242379] [ema: 0.999929] 
[Epoch 214/300] [Batch 0/460] [D loss: 0.296277] [G loss: 0.242650] [ema: 0.999930] 
[Epoch 215/300] [Batch 0/460] [D loss: 0.284779] [G loss: 0.225108] [ema: 0.999930] 
[Epoch 216/300] [Batch 0/460] [D loss: 0.291688] [G loss: 0.231371] [ema: 0.999930] 
[Epoch 217/300] [Batch 0/460] [D loss: 0.289544] [G loss: 0.239543] [ema: 0.999931] 
[Epoch 218/300] [Batch 0/460] [D loss: 0.283141] [G loss: 0.254307] [ema: 0.999931] 
[Epoch 219/300] [Batch 0/460] [D loss: 0.290550] [G loss: 0.241107] [ema: 0.999931] 
[Epoch 220/300] [Batch 0/460] [D loss: 0.307367] [G loss: 0.230141] [ema: 0.999932] 
[Epoch 221/300] [Batch 0/460] [D loss: 0.297974] [G loss: 0.232214] [ema: 0.999932] 
[Epoch 222/300] [Batch 0/460] [D loss: 0.288330] [G loss: 0.235386] [ema: 0.999932] 
[Epoch 223/300] [Batch 0/460] [D loss: 0.279402] [G loss: 0.253120] [ema: 0.999932] 
[Epoch 224/300] [Batch 0/460] [D loss: 0.282670] [G loss: 0.229721] [ema: 0.999933] 



Saving checkpoint 4 in logs/minerva/UCI_DAGHAR_Multiclass_137694_D_60_6axis_2025_03_18_13_32_31/Model



[Epoch 225/300] [Batch 0/460] [D loss: 0.298251] [G loss: 0.236781] [ema: 0.999933] 
[Epoch 226/300] [Batch 0/460] [D loss: 0.285093] [G loss: 0.234495] [ema: 0.999933] 
[Epoch 227/300] [Batch 0/460] [D loss: 0.292997] [G loss: 0.236488] [ema: 0.999934] 
[Epoch 228/300] [Batch 0/460] [D loss: 0.281797] [G loss: 0.239966] [ema: 0.999934] 
[Epoch 229/300] [Batch 0/460] [D loss: 0.278855] [G loss: 0.220560] [ema: 0.999934] 
[Epoch 230/300] [Batch 0/460] [D loss: 0.302428] [G loss: 0.241276] [ema: 0.999934] 
[Epoch 231/300] [Batch 0/460] [D loss: 0.295908] [G loss: 0.243583] [ema: 0.999935] 
[Epoch 232/300] [Batch 0/460] [D loss: 0.269972] [G loss: 0.244704] [ema: 0.999935] 
[Epoch 233/300] [Batch 0/460] [D loss: 0.288261] [G loss: 0.234073] [ema: 0.999935] 
[Epoch 234/300] [Batch 0/460] [D loss: 0.278190] [G loss: 0.218944] [ema: 0.999936] 
[Epoch 235/300] [Batch 0/460] [D loss: 0.281037] [G loss: 0.227083] [ema: 0.999936] 
[Epoch 236/300] [Batch 0/460] [D loss: 0.282614] [G loss: 0.242683] [ema: 0.999936] 
[Epoch 237/300] [Batch 0/460] [D loss: 0.285603] [G loss: 0.234009] [ema: 0.999936] 
[Epoch 238/300] [Batch 0/460] [D loss: 0.293557] [G loss: 0.242494] [ema: 0.999937] 
[Epoch 239/300] [Batch 0/460] [D loss: 0.277850] [G loss: 0.237123] [ema: 0.999937] 
[Epoch 240/300] [Batch 0/460] [D loss: 0.279670] [G loss: 0.239114] [ema: 0.999937] 
[Epoch 241/300] [Batch 0/460] [D loss: 0.283250] [G loss: 0.236178] [ema: 0.999937] 
[Epoch 242/300] [Batch 0/460] [D loss: 0.281432] [G loss: 0.249678] [ema: 0.999938] 
[Epoch 243/300] [Batch 0/460] [D loss: 0.297090] [G loss: 0.258707] [ema: 0.999938] 
[Epoch 244/300] [Batch 0/460] [D loss: 0.295238] [G loss: 0.222677] [ema: 0.999938] 
[Epoch 245/300] [Batch 0/460] [D loss: 0.300287] [G loss: 0.233955] [ema: 0.999938] 
[Epoch 246/300] [Batch 0/460] [D loss: 0.281713] [G loss: 0.245999] [ema: 0.999939] 
[Epoch 247/300] [Batch 0/460] [D loss: 0.276042] [G loss: 0.233658] [ema: 0.999939] 
[Epoch 248/300] [Batch 0/460] [D loss: 0.278094] [G loss: 0.247361] [ema: 0.999939] 
[Epoch 249/300] [Batch 0/460] [D loss: 0.292851] [G loss: 0.252892] [ema: 0.999939] 
[Epoch 250/300] [Batch 0/460] [D loss: 0.279913] [G loss: 0.239770] [ema: 0.999940] 
[Epoch 251/300] [Batch 0/460] [D loss: 0.271229] [G loss: 0.240909] [ema: 0.999940] 
[Epoch 252/300] [Batch 0/460] [D loss: 0.280480] [G loss: 0.253817] [ema: 0.999940] 
[Epoch 253/300] [Batch 0/460] [D loss: 0.265420] [G loss: 0.253123] [ema: 0.999940] 
[Epoch 254/300] [Batch 0/460] [D loss: 0.272548] [G loss: 0.233144] [ema: 0.999941] 
[Epoch 255/300] [Batch 0/460] [D loss: 0.269239] [G loss: 0.226947] [ema: 0.999941] 
[Epoch 256/300] [Batch 0/460] [D loss: 0.277114] [G loss: 0.244593] [ema: 0.999941] 
[Epoch 257/300] [Batch 0/460] [D loss: 0.297945] [G loss: 0.244766] [ema: 0.999941] 
[Epoch 258/300] [Batch 0/460] [D loss: 0.277433] [G loss: 0.251876] [ema: 0.999942] 
[Epoch 259/300] [Batch 0/460] [D loss: 0.310873] [G loss: 0.240440] [ema: 0.999942] 
[Epoch 260/300] [Batch 0/460] [D loss: 0.274799] [G loss: 0.210478] [ema: 0.999942] 
[Epoch 261/300] [Batch 0/460] [D loss: 0.289546] [G loss: 0.229533] [ema: 0.999942] 
[Epoch 262/300] [Batch 0/460] [D loss: 0.286280] [G loss: 0.233683] [ema: 0.999942] 
[Epoch 263/300] [Batch 0/460] [D loss: 0.286903] [G loss: 0.238238] [ema: 0.999943] 
[Epoch 264/300] [Batch 0/460] [D loss: 0.277513] [G loss: 0.245365] [ema: 0.999943] 
[Epoch 265/300] [Batch 0/460] [D loss: 0.273168] [G loss: 0.231165] [ema: 0.999943] 
[Epoch 266/300] [Batch 0/460] [D loss: 0.294161] [G loss: 0.230561] [ema: 0.999943] 
[Epoch 267/300] [Batch 0/460] [D loss: 0.279782] [G loss: 0.249281] [ema: 0.999944] 
[Epoch 268/300] [Batch 0/460] [D loss: 0.284127] [G loss: 0.241293] [ema: 0.999944] 
[Epoch 269/300] [Batch 0/460] [D loss: 0.276391] [G loss: 0.252319] [ema: 0.999944] 
[Epoch 270/300] [Batch 0/460] [D loss: 0.286833] [G loss: 0.241329] [ema: 0.999944] 
[Epoch 271/300] [Batch 0/460] [D loss: 0.304176] [G loss: 0.250007] [ema: 0.999944] 
[Epoch 272/300] [Batch 0/460] [D loss: 0.299725] [G loss: 0.220404] [ema: 0.999945] 
[Epoch 273/300] [Batch 0/460] [D loss: 0.271802] [G loss: 0.236339] [ema: 0.999945] 
[Epoch 274/300] [Batch 0/460] [D loss: 0.267927] [G loss: 0.240992] [ema: 0.999945] 
[Epoch 275/300] [Batch 0/460] [D loss: 0.276136] [G loss: 0.239022] [ema: 0.999945] 
[Epoch 276/300] [Batch 0/460] [D loss: 0.287719] [G loss: 0.215053] [ema: 0.999945] 
[Epoch 277/300] [Batch 0/460] [D loss: 0.272617] [G loss: 0.228608] [ema: 0.999946] 
[Epoch 278/300] [Batch 0/460] [D loss: 0.266783] [G loss: 0.242805] [ema: 0.999946] 
[Epoch 279/300] [Batch 0/460] [D loss: 0.282821] [G loss: 0.254678] [ema: 0.999946] 
[Epoch 280/300] [Batch 0/460] [D loss: 0.287306] [G loss: 0.244080] [ema: 0.999946] 
[Epoch 281/300] [Batch 0/460] [D loss: 0.284182] [G loss: 0.229926] [ema: 0.999946] 
[Epoch 282/300] [Batch 0/460] [D loss: 0.283359] [G loss: 0.242138] [ema: 0.999947] 
[Epoch 283/300] [Batch 0/460] [D loss: 0.282426] [G loss: 0.230226] [ema: 0.999947] 
[Epoch 284/300] [Batch 0/460] [D loss: 0.280935] [G loss: 0.221883] [ema: 0.999947] 
[Epoch 285/300] [Batch 0/460] [D loss: 0.284385] [G loss: 0.231409] [ema: 0.999947] 
[Epoch 286/300] [Batch 0/460] [D loss: 0.310157] [G loss: 0.242886] [ema: 0.999947] 
[Epoch 287/300] [Batch 0/460] [D loss: 0.308495] [G loss: 0.236372] [ema: 0.999947] 
[Epoch 288/300] [Batch 0/460] [D loss: 0.295330] [G loss: 0.231714] [ema: 0.999948] 
[Epoch 289/300] [Batch 0/460] [D loss: 0.286993] [G loss: 0.244467] [ema: 0.999948] 
[Epoch 290/300] [Batch 0/460] [D loss: 0.287014] [G loss: 0.249535] [ema: 0.999948] 
[Epoch 291/300] [Batch 0/460] [D loss: 0.276729] [G loss: 0.236753] [ema: 0.999948] 
[Epoch 292/300] [Batch 0/460] [D loss: 0.295071] [G loss: 0.240636] [ema: 0.999948] 
[Epoch 293/300] [Batch 0/460] [D loss: 0.287421] [G loss: 0.208279] [ema: 0.999949] 
[Epoch 294/300] [Batch 0/460] [D loss: 0.290142] [G loss: 0.238279] [ema: 0.999949] 
[Epoch 295/300] [Batch 0/460] [D loss: 0.280325] [G loss: 0.226367] [ema: 0.999949] 
[Epoch 296/300] [Batch 0/460] [D loss: 0.268346] [G loss: 0.238506] [ema: 0.999949] 
[Epoch 297/300] [Batch 0/460] [D loss: 0.299516] [G loss: 0.217845] [ema: 0.999949] 
[Epoch 298/300] [Batch 0/460] [D loss: 0.280285] [G loss: 0.241317] [ema: 0.999949] 
[Epoch 299/300] [Batch 0/460] [D loss: 0.260119] [G loss: 0.253668] [ema: 0.999950] 
