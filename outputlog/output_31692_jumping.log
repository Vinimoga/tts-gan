Generator(
  (l1): Linear(in_features=100, out_features=1500, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
x_train shape is (6055, 3, 1, 150), x_test shape is (1524, 3, 1, 150)
y_train shape is (6055,), y_test shape is (1524,)
return single class data and labels, class is Jumping
train_data shape is (600, 3, 1, 150), test_data shape is (146, 3, 1, 150)
train label shape is (600,), test data shape is (146,)
x_train shape is (6055, 3, 1, 150), x_test shape is (1524, 3, 1, 150)
y_train shape is (6055,), y_test shape is (1524,)
return single class data and labels, class is Jumping
train_data shape is (600, 3, 1, 150), test_data shape is (146, 3, 1, 150)
train label shape is (600,), test data shape is (146,)
38
Epochs between ckechpoint: 167




Saving checkpoint 1 in logs/Jumping_test1_2024_10_04_13_34_19/Model




[Epoch 0/834] [Batch 0/38] [D loss: 1.171687] [G loss: 0.745932] [ema: 0.000000] 
[Epoch 1/834] [Batch 0/38] [D loss: 0.347481] [G loss: 0.370370] [ema: 0.833262] 
[Epoch 2/834] [Batch 0/38] [D loss: 0.633231] [G loss: 0.141949] [ema: 0.912832] 
[Epoch 3/834] [Batch 0/38] [D loss: 0.488407] [G loss: 0.138334] [ema: 0.941009] 
[Epoch 4/834] [Batch 0/38] [D loss: 0.397249] [G loss: 0.205023] [ema: 0.955422] 
[Epoch 5/834] [Batch 0/38] [D loss: 0.493774] [G loss: 0.168401] [ema: 0.964176] 
[Epoch 6/834] [Batch 0/38] [D loss: 0.384908] [G loss: 0.232053] [ema: 0.970056] 
[Epoch 7/834] [Batch 0/38] [D loss: 0.388574] [G loss: 0.223005] [ema: 0.974278] 
[Epoch 8/834] [Batch 0/38] [D loss: 0.361521] [G loss: 0.270278] [ema: 0.977457] 
[Epoch 9/834] [Batch 0/38] [D loss: 0.252801] [G loss: 0.233867] [ema: 0.979937] 
[Epoch 10/834] [Batch 0/38] [D loss: 0.311875] [G loss: 0.273648] [ema: 0.981925] 
[Epoch 11/834] [Batch 0/38] [D loss: 0.357811] [G loss: 0.223197] [ema: 0.983554] 
[Epoch 12/834] [Batch 0/38] [D loss: 0.478019] [G loss: 0.170546] [ema: 0.984914] 
[Epoch 13/834] [Batch 0/38] [D loss: 0.290105] [G loss: 0.230024] [ema: 0.986067] 
[Epoch 14/834] [Batch 0/38] [D loss: 0.278203] [G loss: 0.271643] [ema: 0.987055] 
[Epoch 15/834] [Batch 0/38] [D loss: 0.303287] [G loss: 0.260105] [ema: 0.987913] 
[Epoch 16/834] [Batch 0/38] [D loss: 0.304661] [G loss: 0.183699] [ema: 0.988664] 
[Epoch 17/834] [Batch 0/38] [D loss: 0.381871] [G loss: 0.187343] [ema: 0.989328] 
[Epoch 18/834] [Batch 0/38] [D loss: 0.365097] [G loss: 0.231236] [ema: 0.989917] 
[Epoch 19/834] [Batch 0/38] [D loss: 0.334518] [G loss: 0.228222] [ema: 0.990446] 
[Epoch 20/834] [Batch 0/38] [D loss: 0.398253] [G loss: 0.201671] [ema: 0.990921] 
[Epoch 21/834] [Batch 0/38] [D loss: 0.457390] [G loss: 0.162090] [ema: 0.991352] 
[Epoch 22/834] [Batch 0/38] [D loss: 0.340510] [G loss: 0.255229] [ema: 0.991743] 
[Epoch 23/834] [Batch 0/38] [D loss: 0.395965] [G loss: 0.199796] [ema: 0.992101] 
[Epoch 24/834] [Batch 0/38] [D loss: 0.398336] [G loss: 0.186538] [ema: 0.992429] 
[Epoch 25/834] [Batch 0/38] [D loss: 0.342652] [G loss: 0.246275] [ema: 0.992730] 
[Epoch 26/834] [Batch 0/38] [D loss: 0.363180] [G loss: 0.192408] [ema: 0.993009] 
[Epoch 27/834] [Batch 0/38] [D loss: 0.375363] [G loss: 0.232302] [ema: 0.993267] 
[Epoch 28/834] [Batch 0/38] [D loss: 0.369088] [G loss: 0.215036] [ema: 0.993507] 
[Epoch 29/834] [Batch 0/38] [D loss: 0.336410] [G loss: 0.202339] [ema: 0.993730] 
[Epoch 30/834] [Batch 0/38] [D loss: 0.344988] [G loss: 0.232019] [ema: 0.993938] 
[Epoch 31/834] [Batch 0/38] [D loss: 0.393552] [G loss: 0.241479] [ema: 0.994133] 
[Epoch 32/834] [Batch 0/38] [D loss: 0.402575] [G loss: 0.238144] [ema: 0.994316] 
[Epoch 33/834] [Batch 0/38] [D loss: 0.445455] [G loss: 0.204812] [ema: 0.994488] 
[Epoch 34/834] [Batch 0/38] [D loss: 0.381475] [G loss: 0.159225] [ema: 0.994649] 
[Epoch 35/834] [Batch 0/38] [D loss: 0.450809] [G loss: 0.168666] [ema: 0.994802] 
[Epoch 36/834] [Batch 0/38] [D loss: 0.441485] [G loss: 0.151223] [ema: 0.994946] 
[Epoch 37/834] [Batch 0/38] [D loss: 0.410378] [G loss: 0.180306] [ema: 0.995082] 
[Epoch 38/834] [Batch 0/38] [D loss: 0.459620] [G loss: 0.179543] [ema: 0.995211] 
[Epoch 39/834] [Batch 0/38] [D loss: 0.492325] [G loss: 0.205358] [ema: 0.995334] 
[Epoch 40/834] [Batch 0/38] [D loss: 0.495324] [G loss: 0.164059] [ema: 0.995450] 
[Epoch 41/834] [Batch 0/38] [D loss: 0.450360] [G loss: 0.231619] [ema: 0.995561] 
[Epoch 42/834] [Batch 0/38] [D loss: 0.496002] [G loss: 0.175716] [ema: 0.995666] 
[Epoch 43/834] [Batch 0/38] [D loss: 0.393045] [G loss: 0.208899] [ema: 0.995767] 
[Epoch 44/834] [Batch 0/38] [D loss: 0.457650] [G loss: 0.202598] [ema: 0.995863] 
[Epoch 45/834] [Batch 0/38] [D loss: 0.387737] [G loss: 0.198090] [ema: 0.995955] 
[Epoch 46/834] [Batch 0/38] [D loss: 0.542862] [G loss: 0.172148] [ema: 0.996042] 
[Epoch 47/834] [Batch 0/38] [D loss: 0.389340] [G loss: 0.163463] [ema: 0.996127] 
[Epoch 48/834] [Batch 0/38] [D loss: 0.455619] [G loss: 0.182786] [ema: 0.996207] 
[Epoch 49/834] [Batch 0/38] [D loss: 0.393133] [G loss: 0.216747] [ema: 0.996284] 
[Epoch 50/834] [Batch 0/38] [D loss: 0.475973] [G loss: 0.212567] [ema: 0.996359] 
[Epoch 51/834] [Batch 0/38] [D loss: 0.319775] [G loss: 0.249106] [ema: 0.996430] 
[Epoch 52/834] [Batch 0/38] [D loss: 0.422532] [G loss: 0.167453] [ema: 0.996498] 
[Epoch 53/834] [Batch 0/38] [D loss: 0.352223] [G loss: 0.164564] [ema: 0.996564] 
[Epoch 54/834] [Batch 0/38] [D loss: 0.362771] [G loss: 0.226480] [ema: 0.996628] 
[Epoch 55/834] [Batch 0/38] [D loss: 0.445088] [G loss: 0.209011] [ema: 0.996689] 
[Epoch 56/834] [Batch 0/38] [D loss: 0.547505] [G loss: 0.149245] [ema: 0.996748] 
[Epoch 57/834] [Batch 0/38] [D loss: 0.393527] [G loss: 0.226640] [ema: 0.996805] 
[Epoch 58/834] [Batch 0/38] [D loss: 0.397949] [G loss: 0.170911] [ema: 0.996860] 
[Epoch 59/834] [Batch 0/38] [D loss: 0.507110] [G loss: 0.148638] [ema: 0.996913] 
[Epoch 60/834] [Batch 0/38] [D loss: 0.360586] [G loss: 0.231088] [ema: 0.996964] 
[Epoch 61/834] [Batch 0/38] [D loss: 0.378651] [G loss: 0.223321] [ema: 0.997014] 
[Epoch 62/834] [Batch 0/38] [D loss: 0.436624] [G loss: 0.193853] [ema: 0.997062] 
[Epoch 63/834] [Batch 0/38] [D loss: 0.384376] [G loss: 0.235787] [ema: 0.997109] 
[Epoch 64/834] [Batch 0/38] [D loss: 0.418175] [G loss: 0.198416] [ema: 0.997154] 
[Epoch 65/834] [Batch 0/38] [D loss: 0.399810] [G loss: 0.194604] [ema: 0.997198] 
[Epoch 66/834] [Batch 0/38] [D loss: 0.392224] [G loss: 0.157753] [ema: 0.997240] 
[Epoch 67/834] [Batch 0/38] [D loss: 0.326954] [G loss: 0.244704] [ema: 0.997281] 
[Epoch 68/834] [Batch 0/38] [D loss: 0.422458] [G loss: 0.197591] [ema: 0.997321] 
[Epoch 69/834] [Batch 0/38] [D loss: 0.299310] [G loss: 0.211472] [ema: 0.997360] 
[Epoch 70/834] [Batch 0/38] [D loss: 0.418684] [G loss: 0.177680] [ema: 0.997398] 
[Epoch 71/834] [Batch 0/38] [D loss: 0.326550] [G loss: 0.265936] [ema: 0.997434] 
[Epoch 72/834] [Batch 0/38] [D loss: 0.413333] [G loss: 0.259263] [ema: 0.997470] 
[Epoch 73/834] [Batch 0/38] [D loss: 0.374389] [G loss: 0.256554] [ema: 0.997504] 
[Epoch 74/834] [Batch 0/38] [D loss: 0.361516] [G loss: 0.200716] [ema: 0.997538] 
[Epoch 75/834] [Batch 0/38] [D loss: 0.404756] [G loss: 0.224317] [ema: 0.997571] 
[Epoch 76/834] [Batch 0/38] [D loss: 0.330106] [G loss: 0.247829] [ema: 0.997603] 
[Epoch 77/834] [Batch 0/38] [D loss: 0.330490] [G loss: 0.244493] [ema: 0.997634] 
[Epoch 78/834] [Batch 0/38] [D loss: 0.333560] [G loss: 0.213758] [ema: 0.997664] 
[Epoch 79/834] [Batch 0/38] [D loss: 0.361628] [G loss: 0.236093] [ema: 0.997694] 
[Epoch 80/834] [Batch 0/38] [D loss: 0.339858] [G loss: 0.215756] [ema: 0.997723] 
[Epoch 81/834] [Batch 0/38] [D loss: 0.301759] [G loss: 0.243272] [ema: 0.997751] 
[Epoch 82/834] [Batch 0/38] [D loss: 0.352418] [G loss: 0.223097] [ema: 0.997778] 
[Epoch 83/834] [Batch 0/38] [D loss: 0.321499] [G loss: 0.225452] [ema: 0.997805] 
[Epoch 84/834] [Batch 0/38] [D loss: 0.358501] [G loss: 0.233117] [ema: 0.997831] 
[Epoch 85/834] [Batch 0/38] [D loss: 0.335578] [G loss: 0.252079] [ema: 0.997856] 
[Epoch 86/834] [Batch 0/38] [D loss: 0.329232] [G loss: 0.202464] [ema: 0.997881] 
[Epoch 87/834] [Batch 0/38] [D loss: 0.334677] [G loss: 0.202100] [ema: 0.997906] 
[Epoch 88/834] [Batch 0/38] [D loss: 0.354255] [G loss: 0.227887] [ema: 0.997929] 
[Epoch 89/834] [Batch 0/38] [D loss: 0.332490] [G loss: 0.256775] [ema: 0.997953] 
[Epoch 90/834] [Batch 0/38] [D loss: 0.342433] [G loss: 0.245890] [ema: 0.997975] 
[Epoch 91/834] [Batch 0/38] [D loss: 0.301981] [G loss: 0.201493] [ema: 0.997998] 
[Epoch 92/834] [Batch 0/38] [D loss: 0.329059] [G loss: 0.207232] [ema: 0.998019] 
[Epoch 93/834] [Batch 0/38] [D loss: 0.372022] [G loss: 0.240056] [ema: 0.998041] 
[Epoch 94/834] [Batch 0/38] [D loss: 0.372207] [G loss: 0.192337] [ema: 0.998061] 
[Epoch 95/834] [Batch 0/38] [D loss: 0.353963] [G loss: 0.215403] [ema: 0.998082] 
[Epoch 96/834] [Batch 0/38] [D loss: 0.380619] [G loss: 0.235047] [ema: 0.998102] 
[Epoch 97/834] [Batch 0/38] [D loss: 0.324028] [G loss: 0.239364] [ema: 0.998121] 
[Epoch 98/834] [Batch 0/38] [D loss: 0.364861] [G loss: 0.188711] [ema: 0.998140] 
[Epoch 99/834] [Batch 0/38] [D loss: 0.338966] [G loss: 0.221884] [ema: 0.998159] 
[Epoch 100/834] [Batch 0/38] [D loss: 0.361228] [G loss: 0.160579] [ema: 0.998178] 
[Epoch 101/834] [Batch 0/38] [D loss: 0.323121] [G loss: 0.250420] [ema: 0.998196] 
[Epoch 102/834] [Batch 0/38] [D loss: 0.360095] [G loss: 0.245409] [ema: 0.998213] 
[Epoch 103/834] [Batch 0/38] [D loss: 0.353711] [G loss: 0.223318] [ema: 0.998231] 
[Epoch 104/834] [Batch 0/38] [D loss: 0.299394] [G loss: 0.245165] [ema: 0.998248] 
[Epoch 105/834] [Batch 0/38] [D loss: 0.283500] [G loss: 0.241815] [ema: 0.998264] 
[Epoch 106/834] [Batch 0/38] [D loss: 0.369897] [G loss: 0.258905] [ema: 0.998281] 
[Epoch 107/834] [Batch 0/38] [D loss: 0.333977] [G loss: 0.199223] [ema: 0.998297] 
[Epoch 108/834] [Batch 0/38] [D loss: 0.365519] [G loss: 0.266513] [ema: 0.998312] 
[Epoch 109/834] [Batch 0/38] [D loss: 0.322737] [G loss: 0.236227] [ema: 0.998328] 
[Epoch 110/834] [Batch 0/38] [D loss: 0.336588] [G loss: 0.202364] [ema: 0.998343] 
[Epoch 111/834] [Batch 0/38] [D loss: 0.393719] [G loss: 0.166658] [ema: 0.998358] 
[Epoch 112/834] [Batch 0/38] [D loss: 0.324519] [G loss: 0.219149] [ema: 0.998373] 
[Epoch 113/834] [Batch 0/38] [D loss: 0.352379] [G loss: 0.167197] [ema: 0.998387] 
[Epoch 114/834] [Batch 0/38] [D loss: 0.293032] [G loss: 0.218616] [ema: 0.998401] 
[Epoch 115/834] [Batch 0/38] [D loss: 0.308545] [G loss: 0.221108] [ema: 0.998415] 
[Epoch 116/834] [Batch 0/38] [D loss: 0.359726] [G loss: 0.190656] [ema: 0.998429] 
[Epoch 117/834] [Batch 0/38] [D loss: 0.372969] [G loss: 0.223286] [ema: 0.998442] 
[Epoch 118/834] [Batch 0/38] [D loss: 0.357323] [G loss: 0.259343] [ema: 0.998455] 
[Epoch 119/834] [Batch 0/38] [D loss: 0.304300] [G loss: 0.222135] [ema: 0.998468] 
[Epoch 120/834] [Batch 0/38] [D loss: 0.306072] [G loss: 0.224266] [ema: 0.998481] 
[Epoch 121/834] [Batch 0/38] [D loss: 0.348512] [G loss: 0.219066] [ema: 0.998494] 
[Epoch 122/834] [Batch 0/38] [D loss: 0.335639] [G loss: 0.156873] [ema: 0.998506] 
[Epoch 123/834] [Batch 0/38] [D loss: 0.314973] [G loss: 0.192088] [ema: 0.998518] 
[Epoch 124/834] [Batch 0/38] [D loss: 0.380454] [G loss: 0.215897] [ema: 0.998530] 
[Epoch 125/834] [Batch 0/38] [D loss: 0.317578] [G loss: 0.229874] [ema: 0.998542] 
[Epoch 126/834] [Batch 0/38] [D loss: 0.392506] [G loss: 0.174515] [ema: 0.998553] 
[Epoch 127/834] [Batch 0/38] [D loss: 0.340632] [G loss: 0.244788] [ema: 0.998565] 
[Epoch 128/834] [Batch 0/38] [D loss: 0.334911] [G loss: 0.212467] [ema: 0.998576] 
[Epoch 129/834] [Batch 0/38] [D loss: 0.361219] [G loss: 0.205783] [ema: 0.998587] 
[Epoch 130/834] [Batch 0/38] [D loss: 0.305927] [G loss: 0.276768] [ema: 0.998598] 
[Epoch 131/834] [Batch 0/38] [D loss: 0.282817] [G loss: 0.190503] [ema: 0.998609] 
[Epoch 132/834] [Batch 0/38] [D loss: 0.375496] [G loss: 0.189350] [ema: 0.998619] 
[Epoch 133/834] [Batch 0/38] [D loss: 0.383262] [G loss: 0.249956] [ema: 0.998629] 
[Epoch 134/834] [Batch 0/38] [D loss: 0.301332] [G loss: 0.212931] [ema: 0.998640] 
[Epoch 135/834] [Batch 0/38] [D loss: 0.322514] [G loss: 0.220685] [ema: 0.998650] 
[Epoch 136/834] [Batch 0/38] [D loss: 0.297484] [G loss: 0.221217] [ema: 0.998660] 
[Epoch 137/834] [Batch 0/38] [D loss: 0.364343] [G loss: 0.181707] [ema: 0.998669] 
[Epoch 138/834] [Batch 0/38] [D loss: 0.343027] [G loss: 0.201330] [ema: 0.998679] 
[Epoch 139/834] [Batch 0/38] [D loss: 0.340058] [G loss: 0.242956] [ema: 0.998689] 
[Epoch 140/834] [Batch 0/38] [D loss: 0.359466] [G loss: 0.196294] [ema: 0.998698] 
[Epoch 141/834] [Batch 0/38] [D loss: 0.378087] [G loss: 0.226179] [ema: 0.998707] 
[Epoch 142/834] [Batch 0/38] [D loss: 0.364302] [G loss: 0.204352] [ema: 0.998716] 
[Epoch 143/834] [Batch 0/38] [D loss: 0.320473] [G loss: 0.235578] [ema: 0.998725] 
[Epoch 144/834] [Batch 0/38] [D loss: 0.316400] [G loss: 0.222372] [ema: 0.998734] 
[Epoch 145/834] [Batch 0/38] [D loss: 0.296721] [G loss: 0.224190] [ema: 0.998743] 
[Epoch 146/834] [Batch 0/38] [D loss: 0.314597] [G loss: 0.246861] [ema: 0.998751] 
[Epoch 147/834] [Batch 0/38] [D loss: 0.315700] [G loss: 0.215006] [ema: 0.998760] 
[Epoch 148/834] [Batch 0/38] [D loss: 0.354354] [G loss: 0.257890] [ema: 0.998768] 
[Epoch 149/834] [Batch 0/38] [D loss: 0.371769] [G loss: 0.242277] [ema: 0.998777] 
[Epoch 150/834] [Batch 0/38] [D loss: 0.297455] [G loss: 0.211890] [ema: 0.998785] 
[Epoch 151/834] [Batch 0/38] [D loss: 0.340795] [G loss: 0.215031] [ema: 0.998793] 
[Epoch 152/834] [Batch 0/38] [D loss: 0.306403] [G loss: 0.201171] [ema: 0.998801] 
[Epoch 153/834] [Batch 0/38] [D loss: 0.269484] [G loss: 0.226245] [ema: 0.998809] 
[Epoch 154/834] [Batch 0/38] [D loss: 0.348388] [G loss: 0.244446] [ema: 0.998816] 
[Epoch 155/834] [Batch 0/38] [D loss: 0.315925] [G loss: 0.256352] [ema: 0.998824] 
[Epoch 156/834] [Batch 0/38] [D loss: 0.366845] [G loss: 0.234435] [ema: 0.998831] 
[Epoch 157/834] [Batch 0/38] [D loss: 0.325157] [G loss: 0.241706] [ema: 0.998839] 
[Epoch 158/834] [Batch 0/38] [D loss: 0.270885] [G loss: 0.299716] [ema: 0.998846] 
[Epoch 159/834] [Batch 0/38] [D loss: 0.348905] [G loss: 0.239933] [ema: 0.998853] 
[Epoch 160/834] [Batch 0/38] [D loss: 0.336720] [G loss: 0.209257] [ema: 0.998861] 
[Epoch 161/834] [Batch 0/38] [D loss: 0.313333] [G loss: 0.227825] [ema: 0.998868] 
[Epoch 162/834] [Batch 0/38] [D loss: 0.286750] [G loss: 0.233386] [ema: 0.998875] 
[Epoch 163/834] [Batch 0/38] [D loss: 0.312231] [G loss: 0.229866] [ema: 0.998882] 
[Epoch 164/834] [Batch 0/38] [D loss: 0.361277] [G loss: 0.209945] [ema: 0.998888] 
[Epoch 165/834] [Batch 0/38] [D loss: 0.357734] [G loss: 0.203018] [ema: 0.998895] 
[Epoch 166/834] [Batch 0/38] [D loss: 0.285334] [G loss: 0.258171] [ema: 0.998902] 




Saving checkpoint 2 in logs/Jumping_test1_2024_10_04_13_34_19/Model




[Epoch 167/834] [Batch 0/38] [D loss: 0.290283] [G loss: 0.235099] [ema: 0.998908] 
[Epoch 168/834] [Batch 0/38] [D loss: 0.322121] [G loss: 0.240409] [ema: 0.998915] 
[Epoch 169/834] [Batch 0/38] [D loss: 0.377433] [G loss: 0.228905] [ema: 0.998921] 
[Epoch 170/834] [Batch 0/38] [D loss: 0.295493] [G loss: 0.220373] [ema: 0.998928] 
[Epoch 171/834] [Batch 0/38] [D loss: 0.278433] [G loss: 0.250430] [ema: 0.998934] 
[Epoch 172/834] [Batch 0/38] [D loss: 0.304454] [G loss: 0.213809] [ema: 0.998940] 
[Epoch 173/834] [Batch 0/38] [D loss: 0.249516] [G loss: 0.261394] [ema: 0.998946] 
[Epoch 174/834] [Batch 0/38] [D loss: 0.255034] [G loss: 0.244556] [ema: 0.998952] 
[Epoch 175/834] [Batch 0/38] [D loss: 0.254446] [G loss: 0.236077] [ema: 0.998958] 
[Epoch 176/834] [Batch 0/38] [D loss: 0.272862] [G loss: 0.250835] [ema: 0.998964] 
[Epoch 177/834] [Batch 0/38] [D loss: 0.309514] [G loss: 0.209598] [ema: 0.998970] 
[Epoch 178/834] [Batch 0/38] [D loss: 0.259896] [G loss: 0.224452] [ema: 0.998976] 
[Epoch 179/834] [Batch 0/38] [D loss: 0.239036] [G loss: 0.208232] [ema: 0.998981] 
[Epoch 180/834] [Batch 0/38] [D loss: 0.327734] [G loss: 0.232224] [ema: 0.998987] 
[Epoch 181/834] [Batch 0/38] [D loss: 0.295435] [G loss: 0.266183] [ema: 0.998993] 
[Epoch 182/834] [Batch 0/38] [D loss: 0.284863] [G loss: 0.221280] [ema: 0.998998] 
[Epoch 183/834] [Batch 0/38] [D loss: 0.290543] [G loss: 0.250240] [ema: 0.999004] 
[Epoch 184/834] [Batch 0/38] [D loss: 0.302929] [G loss: 0.243582] [ema: 0.999009] 
[Epoch 185/834] [Batch 0/38] [D loss: 0.255714] [G loss: 0.213146] [ema: 0.999015] 
[Epoch 186/834] [Batch 0/38] [D loss: 0.346616] [G loss: 0.226336] [ema: 0.999020] 
[Epoch 187/834] [Batch 0/38] [D loss: 0.268746] [G loss: 0.244758] [ema: 0.999025] 
[Epoch 188/834] [Batch 0/38] [D loss: 0.290926] [G loss: 0.246840] [ema: 0.999030] 
[Epoch 189/834] [Batch 0/38] [D loss: 0.299573] [G loss: 0.227553] [ema: 0.999035] 
[Epoch 190/834] [Batch 0/38] [D loss: 0.261905] [G loss: 0.247842] [ema: 0.999040] 
[Epoch 191/834] [Batch 0/38] [D loss: 0.272879] [G loss: 0.283950] [ema: 0.999045] 
[Epoch 192/834] [Batch 0/38] [D loss: 0.252559] [G loss: 0.274117] [ema: 0.999050] 
[Epoch 193/834] [Batch 0/38] [D loss: 0.312363] [G loss: 0.235058] [ema: 0.999055] 
[Epoch 194/834] [Batch 0/38] [D loss: 0.313784] [G loss: 0.255022] [ema: 0.999060] 
[Epoch 195/834] [Batch 0/38] [D loss: 0.313519] [G loss: 0.207987] [ema: 0.999065] 
[Epoch 196/834] [Batch 0/38] [D loss: 0.270766] [G loss: 0.273865] [ema: 0.999070] 
[Epoch 197/834] [Batch 0/38] [D loss: 0.268629] [G loss: 0.279360] [ema: 0.999075] 
[Epoch 198/834] [Batch 0/38] [D loss: 0.296516] [G loss: 0.234823] [ema: 0.999079] 
[Epoch 199/834] [Batch 0/38] [D loss: 0.287875] [G loss: 0.238498] [ema: 0.999084] 
[Epoch 200/834] [Batch 0/38] [D loss: 0.281155] [G loss: 0.248904] [ema: 0.999088] 
[Epoch 201/834] [Batch 0/38] [D loss: 0.256839] [G loss: 0.233740] [ema: 0.999093] 
[Epoch 202/834] [Batch 0/38] [D loss: 0.302756] [G loss: 0.245108] [ema: 0.999097] 
[Epoch 203/834] [Batch 0/38] [D loss: 0.262806] [G loss: 0.251146] [ema: 0.999102] 
[Epoch 204/834] [Batch 0/38] [D loss: 0.268542] [G loss: 0.259932] [ema: 0.999106] 
[Epoch 205/834] [Batch 0/38] [D loss: 0.253899] [G loss: 0.242211] [ema: 0.999111] 
[Epoch 206/834] [Batch 0/38] [D loss: 0.301088] [G loss: 0.229846] [ema: 0.999115] 
[Epoch 207/834] [Batch 0/38] [D loss: 0.274809] [G loss: 0.231489] [ema: 0.999119] 
[Epoch 208/834] [Batch 0/38] [D loss: 0.295329] [G loss: 0.251024] [ema: 0.999123] 
[Epoch 209/834] [Batch 0/38] [D loss: 0.312180] [G loss: 0.235691] [ema: 0.999128] 
[Epoch 210/834] [Batch 0/38] [D loss: 0.298070] [G loss: 0.243308] [ema: 0.999132] 
[Epoch 211/834] [Batch 0/38] [D loss: 0.263076] [G loss: 0.250772] [ema: 0.999136] 
[Epoch 212/834] [Batch 0/38] [D loss: 0.331624] [G loss: 0.215443] [ema: 0.999140] 
[Epoch 213/834] [Batch 0/38] [D loss: 0.272873] [G loss: 0.251021] [ema: 0.999144] 
[Epoch 214/834] [Batch 0/38] [D loss: 0.293058] [G loss: 0.226552] [ema: 0.999148] 
[Epoch 215/834] [Batch 0/38] [D loss: 0.288394] [G loss: 0.217671] [ema: 0.999152] 
[Epoch 216/834] [Batch 0/38] [D loss: 0.259489] [G loss: 0.268384] [ema: 0.999156] 
[Epoch 217/834] [Batch 0/38] [D loss: 0.274539] [G loss: 0.198030] [ema: 0.999160] 
[Epoch 218/834] [Batch 0/38] [D loss: 0.281257] [G loss: 0.247679] [ema: 0.999164] 
[Epoch 219/834] [Batch 0/38] [D loss: 0.280192] [G loss: 0.246828] [ema: 0.999167] 
[Epoch 220/834] [Batch 0/38] [D loss: 0.271888] [G loss: 0.243353] [ema: 0.999171] 
[Epoch 221/834] [Batch 0/38] [D loss: 0.275248] [G loss: 0.253866] [ema: 0.999175] 
[Epoch 222/834] [Batch 0/38] [D loss: 0.262261] [G loss: 0.254075] [ema: 0.999179] 
[Epoch 223/834] [Batch 0/38] [D loss: 0.292501] [G loss: 0.234190] [ema: 0.999182] 
[Epoch 224/834] [Batch 0/38] [D loss: 0.295970] [G loss: 0.208713] [ema: 0.999186] 
[Epoch 225/834] [Batch 0/38] [D loss: 0.271204] [G loss: 0.250854] [ema: 0.999190] 
[Epoch 226/834] [Batch 0/38] [D loss: 0.272269] [G loss: 0.245472] [ema: 0.999193] 
[Epoch 227/834] [Batch 0/38] [D loss: 0.277907] [G loss: 0.279754] [ema: 0.999197] 
[Epoch 228/834] [Batch 0/38] [D loss: 0.290732] [G loss: 0.236218] [ema: 0.999200] 
[Epoch 229/834] [Batch 0/38] [D loss: 0.266734] [G loss: 0.239190] [ema: 0.999204] 
[Epoch 230/834] [Batch 0/38] [D loss: 0.258146] [G loss: 0.241325] [ema: 0.999207] 
[Epoch 231/834] [Batch 0/38] [D loss: 0.265030] [G loss: 0.253239] [ema: 0.999211] 
[Epoch 232/834] [Batch 0/38] [D loss: 0.282681] [G loss: 0.254597] [ema: 0.999214] 
[Epoch 233/834] [Batch 0/38] [D loss: 0.276654] [G loss: 0.211132] [ema: 0.999217] 
[Epoch 234/834] [Batch 0/38] [D loss: 0.273206] [G loss: 0.239227] [ema: 0.999221] 
[Epoch 235/834] [Batch 0/38] [D loss: 0.262514] [G loss: 0.231738] [ema: 0.999224] 
[Epoch 236/834] [Batch 0/38] [D loss: 0.286789] [G loss: 0.231589] [ema: 0.999227] 
[Epoch 237/834] [Batch 0/38] [D loss: 0.305832] [G loss: 0.245700] [ema: 0.999231] 
[Epoch 238/834] [Batch 0/38] [D loss: 0.285607] [G loss: 0.255159] [ema: 0.999234] 
[Epoch 239/834] [Batch 0/38] [D loss: 0.305225] [G loss: 0.235125] [ema: 0.999237] 
[Epoch 240/834] [Batch 0/38] [D loss: 0.290836] [G loss: 0.252800] [ema: 0.999240] 
[Epoch 241/834] [Batch 0/38] [D loss: 0.280340] [G loss: 0.263416] [ema: 0.999243] 
[Epoch 242/834] [Batch 0/38] [D loss: 0.273152] [G loss: 0.270131] [ema: 0.999247] 
[Epoch 243/834] [Batch 0/38] [D loss: 0.286175] [G loss: 0.231982] [ema: 0.999250] 
[Epoch 244/834] [Batch 0/38] [D loss: 0.290655] [G loss: 0.222840] [ema: 0.999253] 
[Epoch 245/834] [Batch 0/38] [D loss: 0.227441] [G loss: 0.260922] [ema: 0.999256] 
[Epoch 246/834] [Batch 0/38] [D loss: 0.270338] [G loss: 0.244952] [ema: 0.999259] 
[Epoch 247/834] [Batch 0/38] [D loss: 0.303811] [G loss: 0.239287] [ema: 0.999262] 
[Epoch 248/834] [Batch 0/38] [D loss: 0.303199] [G loss: 0.214359] [ema: 0.999265] 
[Epoch 249/834] [Batch 0/38] [D loss: 0.316452] [G loss: 0.205717] [ema: 0.999268] 
[Epoch 250/834] [Batch 0/38] [D loss: 0.283527] [G loss: 0.230610] [ema: 0.999271] 
[Epoch 251/834] [Batch 0/38] [D loss: 0.273715] [G loss: 0.272054] [ema: 0.999274] 
[Epoch 252/834] [Batch 0/38] [D loss: 0.268272] [G loss: 0.242210] [ema: 0.999276] 
[Epoch 253/834] [Batch 0/38] [D loss: 0.248108] [G loss: 0.228499] [ema: 0.999279] 
[Epoch 254/834] [Batch 0/38] [D loss: 0.286906] [G loss: 0.237723] [ema: 0.999282] 
[Epoch 255/834] [Batch 0/38] [D loss: 0.284488] [G loss: 0.221864] [ema: 0.999285] 
[Epoch 256/834] [Batch 0/38] [D loss: 0.282369] [G loss: 0.254850] [ema: 0.999288] 
[Epoch 257/834] [Batch 0/38] [D loss: 0.270694] [G loss: 0.250051] [ema: 0.999290] 
[Epoch 258/834] [Batch 0/38] [D loss: 0.273739] [G loss: 0.257747] [ema: 0.999293] 
[Epoch 259/834] [Batch 0/38] [D loss: 0.295447] [G loss: 0.241319] [ema: 0.999296] 
[Epoch 260/834] [Batch 0/38] [D loss: 0.295125] [G loss: 0.254187] [ema: 0.999299] 
[Epoch 261/834] [Batch 0/38] [D loss: 0.241048] [G loss: 0.272947] [ema: 0.999301] 
[Epoch 262/834] [Batch 0/38] [D loss: 0.254684] [G loss: 0.247613] [ema: 0.999304] 
[Epoch 263/834] [Batch 0/38] [D loss: 0.282622] [G loss: 0.226112] [ema: 0.999307] 
[Epoch 264/834] [Batch 0/38] [D loss: 0.289037] [G loss: 0.247130] [ema: 0.999309] 
[Epoch 265/834] [Batch 0/38] [D loss: 0.253273] [G loss: 0.260941] [ema: 0.999312] 
[Epoch 266/834] [Batch 0/38] [D loss: 0.276552] [G loss: 0.241880] [ema: 0.999314] 
[Epoch 267/834] [Batch 0/38] [D loss: 0.329726] [G loss: 0.254131] [ema: 0.999317] 
[Epoch 268/834] [Batch 0/38] [D loss: 0.236325] [G loss: 0.255728] [ema: 0.999320] 
[Epoch 269/834] [Batch 0/38] [D loss: 0.285263] [G loss: 0.233074] [ema: 0.999322] 
[Epoch 270/834] [Batch 0/38] [D loss: 0.273539] [G loss: 0.210924] [ema: 0.999325] 
[Epoch 271/834] [Batch 0/38] [D loss: 0.239125] [G loss: 0.270209] [ema: 0.999327] 
[Epoch 272/834] [Batch 0/38] [D loss: 0.275068] [G loss: 0.230327] [ema: 0.999330] 
[Epoch 273/834] [Batch 0/38] [D loss: 0.271694] [G loss: 0.208762] [ema: 0.999332] 
[Epoch 274/834] [Batch 0/38] [D loss: 0.274104] [G loss: 0.257650] [ema: 0.999335] 
[Epoch 275/834] [Batch 0/38] [D loss: 0.247295] [G loss: 0.255008] [ema: 0.999337] 
[Epoch 276/834] [Batch 0/38] [D loss: 0.328395] [G loss: 0.283824] [ema: 0.999339] 
[Epoch 277/834] [Batch 0/38] [D loss: 0.314328] [G loss: 0.246308] [ema: 0.999342] 
[Epoch 278/834] [Batch 0/38] [D loss: 0.253247] [G loss: 0.273400] [ema: 0.999344] 
[Epoch 279/834] [Batch 0/38] [D loss: 0.278291] [G loss: 0.248721] [ema: 0.999346] 
[Epoch 280/834] [Batch 0/38] [D loss: 0.287062] [G loss: 0.251876] [ema: 0.999349] 
[Epoch 281/834] [Batch 0/38] [D loss: 0.257397] [G loss: 0.253803] [ema: 0.999351] 
[Epoch 282/834] [Batch 0/38] [D loss: 0.295301] [G loss: 0.230968] [ema: 0.999353] 
[Epoch 283/834] [Batch 0/38] [D loss: 0.321037] [G loss: 0.226502] [ema: 0.999356] 
[Epoch 284/834] [Batch 0/38] [D loss: 0.260464] [G loss: 0.247702] [ema: 0.999358] 
[Epoch 285/834] [Batch 0/38] [D loss: 0.248764] [G loss: 0.269713] [ema: 0.999360] 
[Epoch 286/834] [Batch 0/38] [D loss: 0.282195] [G loss: 0.232472] [ema: 0.999362] 
[Epoch 287/834] [Batch 0/38] [D loss: 0.310134] [G loss: 0.218235] [ema: 0.999365] 
[Epoch 288/834] [Batch 0/38] [D loss: 0.288609] [G loss: 0.256918] [ema: 0.999367] 
[Epoch 289/834] [Batch 0/38] [D loss: 0.253493] [G loss: 0.261543] [ema: 0.999369] 
[Epoch 290/834] [Batch 0/38] [D loss: 0.257515] [G loss: 0.240542] [ema: 0.999371] 
[Epoch 291/834] [Batch 0/38] [D loss: 0.276901] [G loss: 0.270932] [ema: 0.999373] 
[Epoch 292/834] [Batch 0/38] [D loss: 0.317853] [G loss: 0.220866] [ema: 0.999376] 
[Epoch 293/834] [Batch 0/38] [D loss: 0.236462] [G loss: 0.239941] [ema: 0.999378] 
[Epoch 294/834] [Batch 0/38] [D loss: 0.250088] [G loss: 0.251040] [ema: 0.999380] 
[Epoch 295/834] [Batch 0/38] [D loss: 0.292980] [G loss: 0.228565] [ema: 0.999382] 
[Epoch 296/834] [Batch 0/38] [D loss: 0.312416] [G loss: 0.222931] [ema: 0.999384] 
[Epoch 297/834] [Batch 0/38] [D loss: 0.283088] [G loss: 0.245814] [ema: 0.999386] 
[Epoch 298/834] [Batch 0/38] [D loss: 0.294162] [G loss: 0.231528] [ema: 0.999388] 
[Epoch 299/834] [Batch 0/38] [D loss: 0.248776] [G loss: 0.245364] [ema: 0.999390] 
[Epoch 300/834] [Batch 0/38] [D loss: 0.303979] [G loss: 0.217266] [ema: 0.999392] 
[Epoch 301/834] [Batch 0/38] [D loss: 0.337826] [G loss: 0.217036] [ema: 0.999394] 
[Epoch 302/834] [Batch 0/38] [D loss: 0.333538] [G loss: 0.228713] [ema: 0.999396] 
[Epoch 303/834] [Batch 0/38] [D loss: 0.260478] [G loss: 0.240444] [ema: 0.999398] 
[Epoch 304/834] [Batch 0/38] [D loss: 0.248180] [G loss: 0.273505] [ema: 0.999400] 
[Epoch 305/834] [Batch 0/38] [D loss: 0.309785] [G loss: 0.249444] [ema: 0.999402] 
[Epoch 306/834] [Batch 0/38] [D loss: 0.280797] [G loss: 0.250031] [ema: 0.999404] 
[Epoch 307/834] [Batch 0/38] [D loss: 0.264334] [G loss: 0.250015] [ema: 0.999406] 
[Epoch 308/834] [Batch 0/38] [D loss: 0.291561] [G loss: 0.240641] [ema: 0.999408] 
[Epoch 309/834] [Batch 0/38] [D loss: 0.305634] [G loss: 0.249025] [ema: 0.999410] 
[Epoch 310/834] [Batch 0/38] [D loss: 0.282506] [G loss: 0.237031] [ema: 0.999412] 
[Epoch 311/834] [Batch 0/38] [D loss: 0.253730] [G loss: 0.252007] [ema: 0.999414] 
[Epoch 312/834] [Batch 0/38] [D loss: 0.256830] [G loss: 0.244933] [ema: 0.999416] 
[Epoch 313/834] [Batch 0/38] [D loss: 0.296262] [G loss: 0.223251] [ema: 0.999417] 
[Epoch 314/834] [Batch 0/38] [D loss: 0.353481] [G loss: 0.243050] [ema: 0.999419] 
[Epoch 315/834] [Batch 0/38] [D loss: 0.277761] [G loss: 0.232554] [ema: 0.999421] 
[Epoch 316/834] [Batch 0/38] [D loss: 0.294829] [G loss: 0.224502] [ema: 0.999423] 
[Epoch 317/834] [Batch 0/38] [D loss: 0.270593] [G loss: 0.254777] [ema: 0.999425] 
[Epoch 318/834] [Batch 0/38] [D loss: 0.299664] [G loss: 0.227485] [ema: 0.999427] 
[Epoch 319/834] [Batch 0/38] [D loss: 0.284740] [G loss: 0.247647] [ema: 0.999428] 
[Epoch 320/834] [Batch 0/38] [D loss: 0.285635] [G loss: 0.217349] [ema: 0.999430] 
[Epoch 321/834] [Batch 0/38] [D loss: 0.280847] [G loss: 0.242162] [ema: 0.999432] 
[Epoch 322/834] [Batch 0/38] [D loss: 0.296955] [G loss: 0.246016] [ema: 0.999434] 
[Epoch 323/834] [Batch 0/38] [D loss: 0.350833] [G loss: 0.231168] [ema: 0.999435] 
[Epoch 324/834] [Batch 0/38] [D loss: 0.282816] [G loss: 0.223397] [ema: 0.999437] 
[Epoch 325/834] [Batch 0/38] [D loss: 0.324323] [G loss: 0.252446] [ema: 0.999439] 
[Epoch 326/834] [Batch 0/38] [D loss: 0.263307] [G loss: 0.238138] [ema: 0.999441] 
[Epoch 327/834] [Batch 0/38] [D loss: 0.279188] [G loss: 0.228468] [ema: 0.999442] 
[Epoch 328/834] [Batch 0/38] [D loss: 0.306231] [G loss: 0.244534] [ema: 0.999444] 
[Epoch 329/834] [Batch 0/38] [D loss: 0.272383] [G loss: 0.255853] [ema: 0.999446] 
[Epoch 330/834] [Batch 0/38] [D loss: 0.311904] [G loss: 0.248430] [ema: 0.999447] 
[Epoch 331/834] [Batch 0/38] [D loss: 0.282203] [G loss: 0.230738] [ema: 0.999449] 
[Epoch 332/834] [Batch 0/38] [D loss: 0.295702] [G loss: 0.206202] [ema: 0.999451] 
[Epoch 333/834] [Batch 0/38] [D loss: 0.268746] [G loss: 0.247202] [ema: 0.999452] 




Saving checkpoint 3 in logs/Jumping_test1_2024_10_04_13_34_19/Model




[Epoch 334/834] [Batch 0/38] [D loss: 0.287573] [G loss: 0.225091] [ema: 0.999454] 
[Epoch 335/834] [Batch 0/38] [D loss: 0.290607] [G loss: 0.244111] [ema: 0.999456] 
[Epoch 336/834] [Batch 0/38] [D loss: 0.344122] [G loss: 0.200504] [ema: 0.999457] 
[Epoch 337/834] [Batch 0/38] [D loss: 0.346614] [G loss: 0.228708] [ema: 0.999459] 
[Epoch 338/834] [Batch 0/38] [D loss: 0.253209] [G loss: 0.267666] [ema: 0.999460] 
[Epoch 339/834] [Batch 0/38] [D loss: 0.255635] [G loss: 0.245971] [ema: 0.999462] 
[Epoch 340/834] [Batch 0/38] [D loss: 0.295985] [G loss: 0.233835] [ema: 0.999464] 
[Epoch 341/834] [Batch 0/38] [D loss: 0.295169] [G loss: 0.254848] [ema: 0.999465] 
[Epoch 342/834] [Batch 0/38] [D loss: 0.304256] [G loss: 0.224747] [ema: 0.999467] 
[Epoch 343/834] [Batch 0/38] [D loss: 0.254015] [G loss: 0.243292] [ema: 0.999468] 
[Epoch 344/834] [Batch 0/38] [D loss: 0.311964] [G loss: 0.230104] [ema: 0.999470] 
[Epoch 345/834] [Batch 0/38] [D loss: 0.252265] [G loss: 0.218666] [ema: 0.999471] 
[Epoch 346/834] [Batch 0/38] [D loss: 0.332657] [G loss: 0.206166] [ema: 0.999473] 
[Epoch 347/834] [Batch 0/38] [D loss: 0.275541] [G loss: 0.218314] [ema: 0.999474] 
[Epoch 348/834] [Batch 0/38] [D loss: 0.280554] [G loss: 0.237340] [ema: 0.999476] 
[Epoch 349/834] [Batch 0/38] [D loss: 0.260971] [G loss: 0.229029] [ema: 0.999477] 
[Epoch 350/834] [Batch 0/38] [D loss: 0.325834] [G loss: 0.258126] [ema: 0.999479] 
[Epoch 351/834] [Batch 0/38] [D loss: 0.260002] [G loss: 0.246864] [ema: 0.999480] 
[Epoch 352/834] [Batch 0/38] [D loss: 0.270346] [G loss: 0.217666] [ema: 0.999482] 
[Epoch 353/834] [Batch 0/38] [D loss: 0.286299] [G loss: 0.215374] [ema: 0.999483] 
[Epoch 354/834] [Batch 0/38] [D loss: 0.288383] [G loss: 0.250835] [ema: 0.999485] 
[Epoch 355/834] [Batch 0/38] [D loss: 0.284959] [G loss: 0.214545] [ema: 0.999486] 
[Epoch 356/834] [Batch 0/38] [D loss: 0.271970] [G loss: 0.251687] [ema: 0.999488] 
[Epoch 357/834] [Batch 0/38] [D loss: 0.259280] [G loss: 0.258875] [ema: 0.999489] 
[Epoch 358/834] [Batch 0/38] [D loss: 0.289833] [G loss: 0.262802] [ema: 0.999491] 
[Epoch 359/834] [Batch 0/38] [D loss: 0.275743] [G loss: 0.239902] [ema: 0.999492] 
[Epoch 360/834] [Batch 0/38] [D loss: 0.239738] [G loss: 0.219743] [ema: 0.999493] 
[Epoch 361/834] [Batch 0/38] [D loss: 0.317077] [G loss: 0.235298] [ema: 0.999495] 
[Epoch 362/834] [Batch 0/38] [D loss: 0.308992] [G loss: 0.255538] [ema: 0.999496] 
[Epoch 363/834] [Batch 0/38] [D loss: 0.265636] [G loss: 0.235946] [ema: 0.999498] 
[Epoch 364/834] [Batch 0/38] [D loss: 0.285911] [G loss: 0.249990] [ema: 0.999499] 
[Epoch 365/834] [Batch 0/38] [D loss: 0.284959] [G loss: 0.203531] [ema: 0.999500] 
[Epoch 366/834] [Batch 0/38] [D loss: 0.319330] [G loss: 0.224900] [ema: 0.999502] 
[Epoch 367/834] [Batch 0/38] [D loss: 0.336274] [G loss: 0.237460] [ema: 0.999503] 
[Epoch 368/834] [Batch 0/38] [D loss: 0.276224] [G loss: 0.230861] [ema: 0.999504] 
[Epoch 369/834] [Batch 0/38] [D loss: 0.285741] [G loss: 0.252883] [ema: 0.999506] 
[Epoch 370/834] [Batch 0/38] [D loss: 0.271672] [G loss: 0.249560] [ema: 0.999507] 
[Epoch 371/834] [Batch 0/38] [D loss: 0.276024] [G loss: 0.245081] [ema: 0.999508] 
[Epoch 372/834] [Batch 0/38] [D loss: 0.299494] [G loss: 0.244954] [ema: 0.999510] 
[Epoch 373/834] [Batch 0/38] [D loss: 0.303135] [G loss: 0.234452] [ema: 0.999511] 
[Epoch 374/834] [Batch 0/38] [D loss: 0.322140] [G loss: 0.254328] [ema: 0.999512] 
[Epoch 375/834] [Batch 0/38] [D loss: 0.287626] [G loss: 0.226505] [ema: 0.999514] 
[Epoch 376/834] [Batch 0/38] [D loss: 0.275929] [G loss: 0.238105] [ema: 0.999515] 
[Epoch 377/834] [Batch 0/38] [D loss: 0.293651] [G loss: 0.227120] [ema: 0.999516] 
[Epoch 378/834] [Batch 0/38] [D loss: 0.335249] [G loss: 0.249632] [ema: 0.999518] 
[Epoch 379/834] [Batch 0/38] [D loss: 0.289892] [G loss: 0.245506] [ema: 0.999519] 
[Epoch 380/834] [Batch 0/38] [D loss: 0.246858] [G loss: 0.271424] [ema: 0.999520] 
[Epoch 381/834] [Batch 0/38] [D loss: 0.274196] [G loss: 0.231927] [ema: 0.999521] 
[Epoch 382/834] [Batch 0/38] [D loss: 0.292358] [G loss: 0.254893] [ema: 0.999523] 
[Epoch 383/834] [Batch 0/38] [D loss: 0.261621] [G loss: 0.233817] [ema: 0.999524] 
[Epoch 384/834] [Batch 0/38] [D loss: 0.300946] [G loss: 0.227092] [ema: 0.999525] 
[Epoch 385/834] [Batch 0/38] [D loss: 0.303817] [G loss: 0.219929] [ema: 0.999526] 
[Epoch 386/834] [Batch 0/38] [D loss: 0.301220] [G loss: 0.247852] [ema: 0.999528] 
[Epoch 387/834] [Batch 0/38] [D loss: 0.278113] [G loss: 0.220927] [ema: 0.999529] 
[Epoch 388/834] [Batch 0/38] [D loss: 0.280486] [G loss: 0.230939] [ema: 0.999530] 
[Epoch 389/834] [Batch 0/38] [D loss: 0.251483] [G loss: 0.228702] [ema: 0.999531] 
[Epoch 390/834] [Batch 0/38] [D loss: 0.298606] [G loss: 0.216900] [ema: 0.999532] 
[Epoch 391/834] [Batch 0/38] [D loss: 0.222133] [G loss: 0.261769] [ema: 0.999534] 
[Epoch 392/834] [Batch 0/38] [D loss: 0.309216] [G loss: 0.212678] [ema: 0.999535] 
[Epoch 393/834] [Batch 0/38] [D loss: 0.270457] [G loss: 0.245806] [ema: 0.999536] 
[Epoch 394/834] [Batch 0/38] [D loss: 0.265169] [G loss: 0.214105] [ema: 0.999537] 
[Epoch 395/834] [Batch 0/38] [D loss: 0.315739] [G loss: 0.221308] [ema: 0.999538] 
[Epoch 396/834] [Batch 0/38] [D loss: 0.283530] [G loss: 0.214932] [ema: 0.999539] 
[Epoch 397/834] [Batch 0/38] [D loss: 0.307811] [G loss: 0.240599] [ema: 0.999541] 
[Epoch 398/834] [Batch 0/38] [D loss: 0.269885] [G loss: 0.250037] [ema: 0.999542] 
[Epoch 399/834] [Batch 0/38] [D loss: 0.270779] [G loss: 0.235224] [ema: 0.999543] 
[Epoch 400/834] [Batch 0/38] [D loss: 0.267332] [G loss: 0.216664] [ema: 0.999544] 
[Epoch 401/834] [Batch 0/38] [D loss: 0.294439] [G loss: 0.227300] [ema: 0.999545] 
[Epoch 402/834] [Batch 0/38] [D loss: 0.281814] [G loss: 0.230484] [ema: 0.999546] 
[Epoch 403/834] [Batch 0/38] [D loss: 0.288442] [G loss: 0.207289] [ema: 0.999547] 
[Epoch 404/834] [Batch 0/38] [D loss: 0.258922] [G loss: 0.209255] [ema: 0.999549] 
[Epoch 405/834] [Batch 0/38] [D loss: 0.338207] [G loss: 0.213256] [ema: 0.999550] 
[Epoch 406/834] [Batch 0/38] [D loss: 0.284358] [G loss: 0.221490] [ema: 0.999551] 
[Epoch 407/834] [Batch 0/38] [D loss: 0.293073] [G loss: 0.246004] [ema: 0.999552] 
[Epoch 408/834] [Batch 0/38] [D loss: 0.291010] [G loss: 0.208213] [ema: 0.999553] 
[Epoch 409/834] [Batch 0/38] [D loss: 0.266333] [G loss: 0.255182] [ema: 0.999554] 
[Epoch 410/834] [Batch 0/38] [D loss: 0.281551] [G loss: 0.224578] [ema: 0.999555] 
[Epoch 411/834] [Batch 0/38] [D loss: 0.267768] [G loss: 0.236860] [ema: 0.999556] 
[Epoch 412/834] [Batch 0/38] [D loss: 0.301726] [G loss: 0.247976] [ema: 0.999557] 
[Epoch 413/834] [Batch 0/38] [D loss: 0.254599] [G loss: 0.257469] [ema: 0.999558] 
[Epoch 414/834] [Batch 0/38] [D loss: 0.302476] [G loss: 0.234783] [ema: 0.999560] 
[Epoch 415/834] [Batch 0/38] [D loss: 0.269143] [G loss: 0.227069] [ema: 0.999561] 
[Epoch 416/834] [Batch 0/38] [D loss: 0.277474] [G loss: 0.229811] [ema: 0.999562] 
[Epoch 417/834] [Batch 0/38] [D loss: 0.267942] [G loss: 0.262259] [ema: 0.999563] 
[Epoch 418/834] [Batch 0/38] [D loss: 0.253689] [G loss: 0.244852] [ema: 0.999564] 
[Epoch 419/834] [Batch 0/38] [D loss: 0.252268] [G loss: 0.264205] [ema: 0.999565] 
[Epoch 420/834] [Batch 0/38] [D loss: 0.288599] [G loss: 0.243535] [ema: 0.999566] 
[Epoch 421/834] [Batch 0/38] [D loss: 0.287083] [G loss: 0.235776] [ema: 0.999567] 
[Epoch 422/834] [Batch 0/38] [D loss: 0.294258] [G loss: 0.237631] [ema: 0.999568] 
[Epoch 423/834] [Batch 0/38] [D loss: 0.301132] [G loss: 0.234153] [ema: 0.999569] 
[Epoch 424/834] [Batch 0/38] [D loss: 0.254524] [G loss: 0.281397] [ema: 0.999570] 
[Epoch 425/834] [Batch 0/38] [D loss: 0.253003] [G loss: 0.251887] [ema: 0.999571] 
[Epoch 426/834] [Batch 0/38] [D loss: 0.289190] [G loss: 0.231719] [ema: 0.999572] 
[Epoch 427/834] [Batch 0/38] [D loss: 0.249641] [G loss: 0.238618] [ema: 0.999573] 
[Epoch 428/834] [Batch 0/38] [D loss: 0.267665] [G loss: 0.180988] [ema: 0.999574] 
[Epoch 429/834] [Batch 0/38] [D loss: 0.259997] [G loss: 0.257890] [ema: 0.999575] 
[Epoch 430/834] [Batch 0/38] [D loss: 0.312922] [G loss: 0.223057] [ema: 0.999576] 
[Epoch 431/834] [Batch 0/38] [D loss: 0.257869] [G loss: 0.234089] [ema: 0.999577] 
[Epoch 432/834] [Batch 0/38] [D loss: 0.288870] [G loss: 0.195620] [ema: 0.999578] 
[Epoch 433/834] [Batch 0/38] [D loss: 0.282298] [G loss: 0.250568] [ema: 0.999579] 
[Epoch 434/834] [Batch 0/38] [D loss: 0.255638] [G loss: 0.245534] [ema: 0.999580] 
[Epoch 435/834] [Batch 0/38] [D loss: 0.267470] [G loss: 0.230441] [ema: 0.999581] 
[Epoch 436/834] [Batch 0/38] [D loss: 0.299960] [G loss: 0.231491] [ema: 0.999582] 
[Epoch 437/834] [Batch 0/38] [D loss: 0.285572] [G loss: 0.253263] [ema: 0.999583] 
[Epoch 438/834] [Batch 0/38] [D loss: 0.256767] [G loss: 0.243576] [ema: 0.999584] 
[Epoch 439/834] [Batch 0/38] [D loss: 0.281008] [G loss: 0.253529] [ema: 0.999585] 
[Epoch 440/834] [Batch 0/38] [D loss: 0.293250] [G loss: 0.245594] [ema: 0.999586] 
[Epoch 441/834] [Batch 0/38] [D loss: 0.264020] [G loss: 0.257369] [ema: 0.999586] 
[Epoch 442/834] [Batch 0/38] [D loss: 0.255190] [G loss: 0.236329] [ema: 0.999587] 
[Epoch 443/834] [Batch 0/38] [D loss: 0.233247] [G loss: 0.249263] [ema: 0.999588] 
[Epoch 444/834] [Batch 0/38] [D loss: 0.273368] [G loss: 0.233986] [ema: 0.999589] 
[Epoch 445/834] [Batch 0/38] [D loss: 0.294192] [G loss: 0.259129] [ema: 0.999590] 
[Epoch 446/834] [Batch 0/38] [D loss: 0.236274] [G loss: 0.254779] [ema: 0.999591] 
[Epoch 447/834] [Batch 0/38] [D loss: 0.289467] [G loss: 0.254713] [ema: 0.999592] 
[Epoch 448/834] [Batch 0/38] [D loss: 0.268781] [G loss: 0.245578] [ema: 0.999593] 
[Epoch 449/834] [Batch 0/38] [D loss: 0.265392] [G loss: 0.244119] [ema: 0.999594] 
[Epoch 450/834] [Batch 0/38] [D loss: 0.258735] [G loss: 0.250754] [ema: 0.999595] 
[Epoch 451/834] [Batch 0/38] [D loss: 0.317270] [G loss: 0.241905] [ema: 0.999596] 
[Epoch 452/834] [Batch 0/38] [D loss: 0.304371] [G loss: 0.230271] [ema: 0.999597] 
[Epoch 453/834] [Batch 0/38] [D loss: 0.293575] [G loss: 0.220598] [ema: 0.999597] 
[Epoch 454/834] [Batch 0/38] [D loss: 0.269899] [G loss: 0.210130] [ema: 0.999598] 
[Epoch 455/834] [Batch 0/38] [D loss: 0.351523] [G loss: 0.237840] [ema: 0.999599] 
[Epoch 456/834] [Batch 0/38] [D loss: 0.289343] [G loss: 0.242758] [ema: 0.999600] 
[Epoch 457/834] [Batch 0/38] [D loss: 0.338099] [G loss: 0.233679] [ema: 0.999601] 
[Epoch 458/834] [Batch 0/38] [D loss: 0.290303] [G loss: 0.233463] [ema: 0.999602] 
[Epoch 459/834] [Batch 0/38] [D loss: 0.263196] [G loss: 0.242030] [ema: 0.999603] 
[Epoch 460/834] [Batch 0/38] [D loss: 0.261224] [G loss: 0.241808] [ema: 0.999604] 
[Epoch 461/834] [Batch 0/38] [D loss: 0.290803] [G loss: 0.250982] [ema: 0.999604] 
[Epoch 462/834] [Batch 0/38] [D loss: 0.296110] [G loss: 0.242960] [ema: 0.999605] 
[Epoch 463/834] [Batch 0/38] [D loss: 0.309925] [G loss: 0.226032] [ema: 0.999606] 
[Epoch 464/834] [Batch 0/38] [D loss: 0.392148] [G loss: 0.212778] [ema: 0.999607] 
[Epoch 465/834] [Batch 0/38] [D loss: 0.269419] [G loss: 0.237340] [ema: 0.999608] 
[Epoch 466/834] [Batch 0/38] [D loss: 0.272636] [G loss: 0.243234] [ema: 0.999609] 
[Epoch 467/834] [Batch 0/38] [D loss: 0.288826] [G loss: 0.237582] [ema: 0.999609] 
[Epoch 468/834] [Batch 0/38] [D loss: 0.268323] [G loss: 0.247268] [ema: 0.999610] 
[Epoch 469/834] [Batch 0/38] [D loss: 0.298451] [G loss: 0.264552] [ema: 0.999611] 
[Epoch 470/834] [Batch 0/38] [D loss: 0.310890] [G loss: 0.252220] [ema: 0.999612] 
[Epoch 471/834] [Batch 0/38] [D loss: 0.250840] [G loss: 0.272573] [ema: 0.999613] 
[Epoch 472/834] [Batch 0/38] [D loss: 0.301218] [G loss: 0.245399] [ema: 0.999614] 
[Epoch 473/834] [Batch 0/38] [D loss: 0.290434] [G loss: 0.231111] [ema: 0.999614] 
[Epoch 474/834] [Batch 0/38] [D loss: 0.275316] [G loss: 0.235805] [ema: 0.999615] 
[Epoch 475/834] [Batch 0/38] [D loss: 0.277349] [G loss: 0.206409] [ema: 0.999616] 
[Epoch 476/834] [Batch 0/38] [D loss: 0.291556] [G loss: 0.225045] [ema: 0.999617] 
[Epoch 477/834] [Batch 0/38] [D loss: 0.268487] [G loss: 0.244772] [ema: 0.999618] 
[Epoch 478/834] [Batch 0/38] [D loss: 0.288561] [G loss: 0.249889] [ema: 0.999618] 
[Epoch 479/834] [Batch 0/38] [D loss: 0.244561] [G loss: 0.236871] [ema: 0.999619] 
[Epoch 480/834] [Batch 0/38] [D loss: 0.271303] [G loss: 0.227866] [ema: 0.999620] 
[Epoch 481/834] [Batch 0/38] [D loss: 0.322947] [G loss: 0.245084] [ema: 0.999621] 
[Epoch 482/834] [Batch 0/38] [D loss: 0.280333] [G loss: 0.235533] [ema: 0.999622] 
[Epoch 483/834] [Batch 0/38] [D loss: 0.237646] [G loss: 0.246967] [ema: 0.999622] 
[Epoch 484/834] [Batch 0/38] [D loss: 0.275232] [G loss: 0.189905] [ema: 0.999623] 
[Epoch 485/834] [Batch 0/38] [D loss: 0.299111] [G loss: 0.225886] [ema: 0.999624] 
[Epoch 486/834] [Batch 0/38] [D loss: 0.307256] [G loss: 0.241952] [ema: 0.999625] 
[Epoch 487/834] [Batch 0/38] [D loss: 0.257978] [G loss: 0.245268] [ema: 0.999626] 
[Epoch 488/834] [Batch 0/38] [D loss: 0.268125] [G loss: 0.240763] [ema: 0.999626] 
[Epoch 489/834] [Batch 0/38] [D loss: 0.264923] [G loss: 0.251684] [ema: 0.999627] 
[Epoch 490/834] [Batch 0/38] [D loss: 0.306509] [G loss: 0.232038] [ema: 0.999628] 
[Epoch 491/834] [Batch 0/38] [D loss: 0.276871] [G loss: 0.248488] [ema: 0.999629] 
[Epoch 492/834] [Batch 0/38] [D loss: 0.276455] [G loss: 0.230397] [ema: 0.999629] 
[Epoch 493/834] [Batch 0/38] [D loss: 0.267810] [G loss: 0.228639] [ema: 0.999630] 
[Epoch 494/834] [Batch 0/38] [D loss: 0.267128] [G loss: 0.262151] [ema: 0.999631] 
[Epoch 495/834] [Batch 0/38] [D loss: 0.264673] [G loss: 0.249978] [ema: 0.999632] 
[Epoch 496/834] [Batch 0/38] [D loss: 0.290966] [G loss: 0.210134] [ema: 0.999632] 
[Epoch 497/834] [Batch 0/38] [D loss: 0.287061] [G loss: 0.227650] [ema: 0.999633] 
[Epoch 498/834] [Batch 0/38] [D loss: 0.307678] [G loss: 0.248108] [ema: 0.999634] 
[Epoch 499/834] [Batch 0/38] [D loss: 0.295076] [G loss: 0.239848] [ema: 0.999635] 
[Epoch 500/834] [Batch 0/38] [D loss: 0.299888] [G loss: 0.234286] [ema: 0.999635] 




Saving checkpoint 4 in logs/Jumping_test1_2024_10_04_13_34_19/Model




[Epoch 501/834] [Batch 0/38] [D loss: 0.308960] [G loss: 0.247132] [ema: 0.999636] 
[Epoch 502/834] [Batch 0/38] [D loss: 0.256052] [G loss: 0.249798] [ema: 0.999637] 
[Epoch 503/834] [Batch 0/38] [D loss: 0.269572] [G loss: 0.244285] [ema: 0.999637] 
[Epoch 504/834] [Batch 0/38] [D loss: 0.263099] [G loss: 0.234773] [ema: 0.999638] 
[Epoch 505/834] [Batch 0/38] [D loss: 0.252578] [G loss: 0.228446] [ema: 0.999639] 
[Epoch 506/834] [Batch 0/38] [D loss: 0.246515] [G loss: 0.254250] [ema: 0.999640] 
[Epoch 507/834] [Batch 0/38] [D loss: 0.298309] [G loss: 0.246440] [ema: 0.999640] 
[Epoch 508/834] [Batch 0/38] [D loss: 0.245726] [G loss: 0.225954] [ema: 0.999641] 
[Epoch 509/834] [Batch 0/38] [D loss: 0.284147] [G loss: 0.249335] [ema: 0.999642] 
[Epoch 510/834] [Batch 0/38] [D loss: 0.277061] [G loss: 0.251518] [ema: 0.999642] 
[Epoch 511/834] [Batch 0/38] [D loss: 0.287534] [G loss: 0.242003] [ema: 0.999643] 
[Epoch 512/834] [Batch 0/38] [D loss: 0.307435] [G loss: 0.204617] [ema: 0.999644] 
[Epoch 513/834] [Batch 0/38] [D loss: 0.290516] [G loss: 0.233274] [ema: 0.999644] 
[Epoch 514/834] [Batch 0/38] [D loss: 0.301074] [G loss: 0.227644] [ema: 0.999645] 
[Epoch 515/834] [Batch 0/38] [D loss: 0.233515] [G loss: 0.271092] [ema: 0.999646] 
[Epoch 516/834] [Batch 0/38] [D loss: 0.265461] [G loss: 0.252764] [ema: 0.999647] 
[Epoch 517/834] [Batch 0/38] [D loss: 0.258847] [G loss: 0.238301] [ema: 0.999647] 
[Epoch 518/834] [Batch 0/38] [D loss: 0.273013] [G loss: 0.257337] [ema: 0.999648] 
[Epoch 519/834] [Batch 0/38] [D loss: 0.244663] [G loss: 0.240825] [ema: 0.999649] 
[Epoch 520/834] [Batch 0/38] [D loss: 0.248733] [G loss: 0.262546] [ema: 0.999649] 
[Epoch 521/834] [Batch 0/38] [D loss: 0.256443] [G loss: 0.244756] [ema: 0.999650] 
[Epoch 522/834] [Batch 0/38] [D loss: 0.259155] [G loss: 0.243341] [ema: 0.999651] 
[Epoch 523/834] [Batch 0/38] [D loss: 0.308231] [G loss: 0.229410] [ema: 0.999651] 
[Epoch 524/834] [Batch 0/38] [D loss: 0.306329] [G loss: 0.232475] [ema: 0.999652] 
[Epoch 525/834] [Batch 0/38] [D loss: 0.275488] [G loss: 0.245502] [ema: 0.999653] 
[Epoch 526/834] [Batch 0/38] [D loss: 0.278660] [G loss: 0.235980] [ema: 0.999653] 
[Epoch 527/834] [Batch 0/38] [D loss: 0.299511] [G loss: 0.234184] [ema: 0.999654] 
[Epoch 528/834] [Batch 0/38] [D loss: 0.348824] [G loss: 0.249398] [ema: 0.999655] 
[Epoch 529/834] [Batch 0/38] [D loss: 0.292311] [G loss: 0.229203] [ema: 0.999655] 
[Epoch 530/834] [Batch 0/38] [D loss: 0.260759] [G loss: 0.244221] [ema: 0.999656] 
[Epoch 531/834] [Batch 0/38] [D loss: 0.300035] [G loss: 0.245178] [ema: 0.999657] 
[Epoch 532/834] [Batch 0/38] [D loss: 0.239637] [G loss: 0.255297] [ema: 0.999657] 
[Epoch 533/834] [Batch 0/38] [D loss: 0.280206] [G loss: 0.227613] [ema: 0.999658] 
[Epoch 534/834] [Batch 0/38] [D loss: 0.249405] [G loss: 0.258114] [ema: 0.999658] 
[Epoch 535/834] [Batch 0/38] [D loss: 0.279042] [G loss: 0.240671] [ema: 0.999659] 
[Epoch 536/834] [Batch 0/38] [D loss: 0.296667] [G loss: 0.234052] [ema: 0.999660] 
[Epoch 537/834] [Batch 0/38] [D loss: 0.270344] [G loss: 0.241925] [ema: 0.999660] 
[Epoch 538/834] [Batch 0/38] [D loss: 0.262109] [G loss: 0.235503] [ema: 0.999661] 
[Epoch 539/834] [Batch 0/38] [D loss: 0.303867] [G loss: 0.226436] [ema: 0.999662] 
[Epoch 540/834] [Batch 0/38] [D loss: 0.277351] [G loss: 0.222627] [ema: 0.999662] 
[Epoch 541/834] [Batch 0/38] [D loss: 0.292069] [G loss: 0.233689] [ema: 0.999663] 
[Epoch 542/834] [Batch 0/38] [D loss: 0.285315] [G loss: 0.245205] [ema: 0.999664] 
[Epoch 543/834] [Batch 0/38] [D loss: 0.254886] [G loss: 0.238710] [ema: 0.999664] 
[Epoch 544/834] [Batch 0/38] [D loss: 0.322241] [G loss: 0.247239] [ema: 0.999665] 
[Epoch 545/834] [Batch 0/38] [D loss: 0.263004] [G loss: 0.250342] [ema: 0.999665] 
[Epoch 546/834] [Batch 0/38] [D loss: 0.338212] [G loss: 0.224484] [ema: 0.999666] 
[Epoch 547/834] [Batch 0/38] [D loss: 0.272247] [G loss: 0.245271] [ema: 0.999667] 
[Epoch 548/834] [Batch 0/38] [D loss: 0.276443] [G loss: 0.256644] [ema: 0.999667] 
[Epoch 549/834] [Batch 0/38] [D loss: 0.277050] [G loss: 0.246983] [ema: 0.999668] 
[Epoch 550/834] [Batch 0/38] [D loss: 0.283712] [G loss: 0.239666] [ema: 0.999668] 
[Epoch 551/834] [Batch 0/38] [D loss: 0.264766] [G loss: 0.221513] [ema: 0.999669] 
[Epoch 552/834] [Batch 0/38] [D loss: 0.259056] [G loss: 0.222641] [ema: 0.999670] 
[Epoch 553/834] [Batch 0/38] [D loss: 0.298803] [G loss: 0.247773] [ema: 0.999670] 
[Epoch 554/834] [Batch 0/38] [D loss: 0.273418] [G loss: 0.240192] [ema: 0.999671] 
[Epoch 555/834] [Batch 0/38] [D loss: 0.274462] [G loss: 0.237906] [ema: 0.999671] 
[Epoch 556/834] [Batch 0/38] [D loss: 0.268010] [G loss: 0.227110] [ema: 0.999672] 
[Epoch 557/834] [Batch 0/38] [D loss: 0.280040] [G loss: 0.250036] [ema: 0.999673] 
[Epoch 558/834] [Batch 0/38] [D loss: 0.294029] [G loss: 0.252040] [ema: 0.999673] 
[Epoch 559/834] [Batch 0/38] [D loss: 0.253222] [G loss: 0.265624] [ema: 0.999674] 
[Epoch 560/834] [Batch 0/38] [D loss: 0.247467] [G loss: 0.225575] [ema: 0.999674] 
[Epoch 561/834] [Batch 0/38] [D loss: 0.263269] [G loss: 0.248575] [ema: 0.999675] 
[Epoch 562/834] [Batch 0/38] [D loss: 0.251960] [G loss: 0.231096] [ema: 0.999675] 
[Epoch 563/834] [Batch 0/38] [D loss: 0.258340] [G loss: 0.238786] [ema: 0.999676] 
[Epoch 564/834] [Batch 0/38] [D loss: 0.259682] [G loss: 0.221899] [ema: 0.999677] 
[Epoch 565/834] [Batch 0/38] [D loss: 0.288767] [G loss: 0.238944] [ema: 0.999677] 
[Epoch 566/834] [Batch 0/38] [D loss: 0.296542] [G loss: 0.221062] [ema: 0.999678] 
[Epoch 567/834] [Batch 0/38] [D loss: 0.293656] [G loss: 0.237214] [ema: 0.999678] 
[Epoch 568/834] [Batch 0/38] [D loss: 0.268593] [G loss: 0.235665] [ema: 0.999679] 
[Epoch 569/834] [Batch 0/38] [D loss: 0.288595] [G loss: 0.252153] [ema: 0.999679] 
[Epoch 570/834] [Batch 0/38] [D loss: 0.271608] [G loss: 0.239299] [ema: 0.999680] 
[Epoch 571/834] [Batch 0/38] [D loss: 0.268681] [G loss: 0.254098] [ema: 0.999681] 
[Epoch 572/834] [Batch 0/38] [D loss: 0.278827] [G loss: 0.238335] [ema: 0.999681] 
[Epoch 573/834] [Batch 0/38] [D loss: 0.262638] [G loss: 0.233166] [ema: 0.999682] 
[Epoch 574/834] [Batch 0/38] [D loss: 0.262637] [G loss: 0.253614] [ema: 0.999682] 
[Epoch 575/834] [Batch 0/38] [D loss: 0.298090] [G loss: 0.265733] [ema: 0.999683] 
[Epoch 576/834] [Batch 0/38] [D loss: 0.251797] [G loss: 0.255147] [ema: 0.999683] 
[Epoch 577/834] [Batch 0/38] [D loss: 0.262983] [G loss: 0.249069] [ema: 0.999684] 
[Epoch 578/834] [Batch 0/38] [D loss: 0.274253] [G loss: 0.236461] [ema: 0.999684] 
[Epoch 579/834] [Batch 0/38] [D loss: 0.280038] [G loss: 0.236029] [ema: 0.999685] 
[Epoch 580/834] [Batch 0/38] [D loss: 0.326655] [G loss: 0.242713] [ema: 0.999686] 
[Epoch 581/834] [Batch 0/38] [D loss: 0.274506] [G loss: 0.237263] [ema: 0.999686] 
[Epoch 582/834] [Batch 0/38] [D loss: 0.274219] [G loss: 0.253238] [ema: 0.999687] 
[Epoch 583/834] [Batch 0/38] [D loss: 0.280414] [G loss: 0.235017] [ema: 0.999687] 
[Epoch 584/834] [Batch 0/38] [D loss: 0.288202] [G loss: 0.254435] [ema: 0.999688] 
[Epoch 585/834] [Batch 0/38] [D loss: 0.321714] [G loss: 0.230917] [ema: 0.999688] 
[Epoch 586/834] [Batch 0/38] [D loss: 0.283949] [G loss: 0.261724] [ema: 0.999689] 
[Epoch 587/834] [Batch 0/38] [D loss: 0.286236] [G loss: 0.245926] [ema: 0.999689] 
[Epoch 588/834] [Batch 0/38] [D loss: 0.252921] [G loss: 0.243112] [ema: 0.999690] 
[Epoch 589/834] [Batch 0/38] [D loss: 0.269529] [G loss: 0.245541] [ema: 0.999690] 
[Epoch 590/834] [Batch 0/38] [D loss: 0.328276] [G loss: 0.244560] [ema: 0.999691] 
[Epoch 591/834] [Batch 0/38] [D loss: 0.278930] [G loss: 0.243169] [ema: 0.999691] 
[Epoch 592/834] [Batch 0/38] [D loss: 0.311966] [G loss: 0.242052] [ema: 0.999692] 
[Epoch 593/834] [Batch 0/38] [D loss: 0.295267] [G loss: 0.245357] [ema: 0.999692] 
[Epoch 594/834] [Batch 0/38] [D loss: 0.265975] [G loss: 0.246988] [ema: 0.999693] 
[Epoch 595/834] [Batch 0/38] [D loss: 0.262119] [G loss: 0.233203] [ema: 0.999693] 
[Epoch 596/834] [Batch 0/38] [D loss: 0.257118] [G loss: 0.233722] [ema: 0.999694] 
[Epoch 597/834] [Batch 0/38] [D loss: 0.253714] [G loss: 0.258443] [ema: 0.999695] 
[Epoch 598/834] [Batch 0/38] [D loss: 0.257775] [G loss: 0.250134] [ema: 0.999695] 
[Epoch 599/834] [Batch 0/38] [D loss: 0.256121] [G loss: 0.260302] [ema: 0.999696] 
[Epoch 600/834] [Batch 0/38] [D loss: 0.312970] [G loss: 0.233120] [ema: 0.999696] 
[Epoch 601/834] [Batch 0/38] [D loss: 0.247326] [G loss: 0.261330] [ema: 0.999697] 
[Epoch 602/834] [Batch 0/38] [D loss: 0.256210] [G loss: 0.241590] [ema: 0.999697] 
[Epoch 603/834] [Batch 0/38] [D loss: 0.246581] [G loss: 0.256590] [ema: 0.999698] 
[Epoch 604/834] [Batch 0/38] [D loss: 0.267919] [G loss: 0.252255] [ema: 0.999698] 
[Epoch 605/834] [Batch 0/38] [D loss: 0.297584] [G loss: 0.221845] [ema: 0.999699] 
[Epoch 606/834] [Batch 0/38] [D loss: 0.259602] [G loss: 0.235458] [ema: 0.999699] 
[Epoch 607/834] [Batch 0/38] [D loss: 0.325101] [G loss: 0.239140] [ema: 0.999700] 
[Epoch 608/834] [Batch 0/38] [D loss: 0.265835] [G loss: 0.218371] [ema: 0.999700] 
[Epoch 609/834] [Batch 0/38] [D loss: 0.255725] [G loss: 0.248586] [ema: 0.999701] 
[Epoch 610/834] [Batch 0/38] [D loss: 0.259008] [G loss: 0.244468] [ema: 0.999701] 
[Epoch 611/834] [Batch 0/38] [D loss: 0.255716] [G loss: 0.254264] [ema: 0.999702] 
[Epoch 612/834] [Batch 0/38] [D loss: 0.256106] [G loss: 0.230935] [ema: 0.999702] 
[Epoch 613/834] [Batch 0/38] [D loss: 0.261964] [G loss: 0.230498] [ema: 0.999702] 
[Epoch 614/834] [Batch 0/38] [D loss: 0.277727] [G loss: 0.244575] [ema: 0.999703] 
[Epoch 615/834] [Batch 0/38] [D loss: 0.254676] [G loss: 0.247706] [ema: 0.999703] 
[Epoch 616/834] [Batch 0/38] [D loss: 0.263341] [G loss: 0.232146] [ema: 0.999704] 
[Epoch 617/834] [Batch 0/38] [D loss: 0.249313] [G loss: 0.256974] [ema: 0.999704] 
[Epoch 618/834] [Batch 0/38] [D loss: 0.280939] [G loss: 0.240316] [ema: 0.999705] 
[Epoch 619/834] [Batch 0/38] [D loss: 0.299083] [G loss: 0.252188] [ema: 0.999705] 
[Epoch 620/834] [Batch 0/38] [D loss: 0.263051] [G loss: 0.247237] [ema: 0.999706] 
[Epoch 621/834] [Batch 0/38] [D loss: 0.257707] [G loss: 0.241501] [ema: 0.999706] 
[Epoch 622/834] [Batch 0/38] [D loss: 0.311036] [G loss: 0.229435] [ema: 0.999707] 
[Epoch 623/834] [Batch 0/38] [D loss: 0.321100] [G loss: 0.215082] [ema: 0.999707] 
[Epoch 624/834] [Batch 0/38] [D loss: 0.280736] [G loss: 0.254943] [ema: 0.999708] 
[Epoch 625/834] [Batch 0/38] [D loss: 0.275057] [G loss: 0.232745] [ema: 0.999708] 
[Epoch 626/834] [Batch 0/38] [D loss: 0.273857] [G loss: 0.241982] [ema: 0.999709] 
[Epoch 627/834] [Batch 0/38] [D loss: 0.278322] [G loss: 0.240078] [ema: 0.999709] 
[Epoch 628/834] [Batch 0/38] [D loss: 0.269849] [G loss: 0.248056] [ema: 0.999710] 
[Epoch 629/834] [Batch 0/38] [D loss: 0.297828] [G loss: 0.254264] [ema: 0.999710] 
[Epoch 630/834] [Batch 0/38] [D loss: 0.279325] [G loss: 0.240794] [ema: 0.999711] 
[Epoch 631/834] [Batch 0/38] [D loss: 0.267184] [G loss: 0.246845] [ema: 0.999711] 
[Epoch 632/834] [Batch 0/38] [D loss: 0.262941] [G loss: 0.254498] [ema: 0.999711] 
[Epoch 633/834] [Batch 0/38] [D loss: 0.311977] [G loss: 0.218019] [ema: 0.999712] 
[Epoch 634/834] [Batch 0/38] [D loss: 0.265720] [G loss: 0.251112] [ema: 0.999712] 
[Epoch 635/834] [Batch 0/38] [D loss: 0.250245] [G loss: 0.244237] [ema: 0.999713] 
[Epoch 636/834] [Batch 0/38] [D loss: 0.285218] [G loss: 0.253296] [ema: 0.999713] 
[Epoch 637/834] [Batch 0/38] [D loss: 0.266457] [G loss: 0.256874] [ema: 0.999714] 
[Epoch 638/834] [Batch 0/38] [D loss: 0.275596] [G loss: 0.256586] [ema: 0.999714] 
[Epoch 639/834] [Batch 0/38] [D loss: 0.287992] [G loss: 0.233475] [ema: 0.999715] 
[Epoch 640/834] [Batch 0/38] [D loss: 0.261567] [G loss: 0.227665] [ema: 0.999715] 
[Epoch 641/834] [Batch 0/38] [D loss: 0.249466] [G loss: 0.262079] [ema: 0.999715] 
[Epoch 642/834] [Batch 0/38] [D loss: 0.244808] [G loss: 0.256627] [ema: 0.999716] 
[Epoch 643/834] [Batch 0/38] [D loss: 0.275268] [G loss: 0.231964] [ema: 0.999716] 
[Epoch 644/834] [Batch 0/38] [D loss: 0.299055] [G loss: 0.246911] [ema: 0.999717] 
[Epoch 645/834] [Batch 0/38] [D loss: 0.256129] [G loss: 0.239937] [ema: 0.999717] 
[Epoch 646/834] [Batch 0/38] [D loss: 0.245787] [G loss: 0.273156] [ema: 0.999718] 
[Epoch 647/834] [Batch 0/38] [D loss: 0.264172] [G loss: 0.246501] [ema: 0.999718] 
[Epoch 648/834] [Batch 0/38] [D loss: 0.252055] [G loss: 0.261077] [ema: 0.999719] 
[Epoch 649/834] [Batch 0/38] [D loss: 0.260799] [G loss: 0.229683] [ema: 0.999719] 
[Epoch 650/834] [Batch 0/38] [D loss: 0.284750] [G loss: 0.240559] [ema: 0.999719] 
[Epoch 651/834] [Batch 0/38] [D loss: 0.271992] [G loss: 0.242034] [ema: 0.999720] 
[Epoch 652/834] [Batch 0/38] [D loss: 0.260919] [G loss: 0.229239] [ema: 0.999720] 
[Epoch 653/834] [Batch 0/38] [D loss: 0.312815] [G loss: 0.241013] [ema: 0.999721] 
[Epoch 654/834] [Batch 0/38] [D loss: 0.297383] [G loss: 0.239640] [ema: 0.999721] 
[Epoch 655/834] [Batch 0/38] [D loss: 0.275173] [G loss: 0.237433] [ema: 0.999722] 
[Epoch 656/834] [Batch 0/38] [D loss: 0.270597] [G loss: 0.232762] [ema: 0.999722] 
[Epoch 657/834] [Batch 0/38] [D loss: 0.265499] [G loss: 0.247213] [ema: 0.999722] 
[Epoch 658/834] [Batch 0/38] [D loss: 0.265631] [G loss: 0.217829] [ema: 0.999723] 
[Epoch 659/834] [Batch 0/38] [D loss: 0.268412] [G loss: 0.224111] [ema: 0.999723] 
[Epoch 660/834] [Batch 0/38] [D loss: 0.308999] [G loss: 0.248547] [ema: 0.999724] 
[Epoch 661/834] [Batch 0/38] [D loss: 0.267033] [G loss: 0.239912] [ema: 0.999724] 
[Epoch 662/834] [Batch 0/38] [D loss: 0.251517] [G loss: 0.250111] [ema: 0.999724] 
[Epoch 663/834] [Batch 0/38] [D loss: 0.257801] [G loss: 0.241003] [ema: 0.999725] 
[Epoch 664/834] [Batch 0/38] [D loss: 0.254119] [G loss: 0.253091] [ema: 0.999725] 
[Epoch 665/834] [Batch 0/38] [D loss: 0.251831] [G loss: 0.242895] [ema: 0.999726] 
[Epoch 666/834] [Batch 0/38] [D loss: 0.269898] [G loss: 0.235675] [ema: 0.999726] 
[Epoch 667/834] [Batch 0/38] [D loss: 0.270379] [G loss: 0.263570] [ema: 0.999727] 




Saving checkpoint 5 in logs/Jumping_test1_2024_10_04_13_34_19/Model




[Epoch 668/834] [Batch 0/38] [D loss: 0.266513] [G loss: 0.250760] [ema: 0.999727] 
[Epoch 669/834] [Batch 0/38] [D loss: 0.295317] [G loss: 0.237580] [ema: 0.999727] 
[Epoch 670/834] [Batch 0/38] [D loss: 0.260346] [G loss: 0.240152] [ema: 0.999728] 
[Epoch 671/834] [Batch 0/38] [D loss: 0.249806] [G loss: 0.249868] [ema: 0.999728] 
[Epoch 672/834] [Batch 0/38] [D loss: 0.277082] [G loss: 0.238807] [ema: 0.999729] 
[Epoch 673/834] [Batch 0/38] [D loss: 0.291750] [G loss: 0.243706] [ema: 0.999729] 
[Epoch 674/834] [Batch 0/38] [D loss: 0.265144] [G loss: 0.247882] [ema: 0.999729] 
[Epoch 675/834] [Batch 0/38] [D loss: 0.242084] [G loss: 0.224835] [ema: 0.999730] 
[Epoch 676/834] [Batch 0/38] [D loss: 0.319332] [G loss: 0.213669] [ema: 0.999730] 
[Epoch 677/834] [Batch 0/38] [D loss: 0.284599] [G loss: 0.221353] [ema: 0.999731] 
[Epoch 678/834] [Batch 0/38] [D loss: 0.258451] [G loss: 0.242350] [ema: 0.999731] 
[Epoch 679/834] [Batch 0/38] [D loss: 0.269534] [G loss: 0.244492] [ema: 0.999731] 
[Epoch 680/834] [Batch 0/38] [D loss: 0.287293] [G loss: 0.234972] [ema: 0.999732] 
[Epoch 681/834] [Batch 0/38] [D loss: 0.264007] [G loss: 0.241215] [ema: 0.999732] 
[Epoch 682/834] [Batch 0/38] [D loss: 0.274951] [G loss: 0.238283] [ema: 0.999733] 
[Epoch 683/834] [Batch 0/38] [D loss: 0.274689] [G loss: 0.253423] [ema: 0.999733] 
[Epoch 684/834] [Batch 0/38] [D loss: 0.261868] [G loss: 0.245701] [ema: 0.999733] 
[Epoch 685/834] [Batch 0/38] [D loss: 0.258632] [G loss: 0.246416] [ema: 0.999734] 
[Epoch 686/834] [Batch 0/38] [D loss: 0.253468] [G loss: 0.245398] [ema: 0.999734] 
[Epoch 687/834] [Batch 0/38] [D loss: 0.309709] [G loss: 0.243215] [ema: 0.999735] 
[Epoch 688/834] [Batch 0/38] [D loss: 0.259119] [G loss: 0.246404] [ema: 0.999735] 
[Epoch 689/834] [Batch 0/38] [D loss: 0.283236] [G loss: 0.238610] [ema: 0.999735] 
[Epoch 690/834] [Batch 0/38] [D loss: 0.295655] [G loss: 0.239715] [ema: 0.999736] 
[Epoch 691/834] [Batch 0/38] [D loss: 0.267772] [G loss: 0.240367] [ema: 0.999736] 
[Epoch 692/834] [Batch 0/38] [D loss: 0.248863] [G loss: 0.265124] [ema: 0.999736] 
[Epoch 693/834] [Batch 0/38] [D loss: 0.228231] [G loss: 0.253009] [ema: 0.999737] 
[Epoch 694/834] [Batch 0/38] [D loss: 0.279711] [G loss: 0.240703] [ema: 0.999737] 
[Epoch 695/834] [Batch 0/38] [D loss: 0.260292] [G loss: 0.239292] [ema: 0.999738] 
[Epoch 696/834] [Batch 0/38] [D loss: 0.271384] [G loss: 0.247241] [ema: 0.999738] 
[Epoch 697/834] [Batch 0/38] [D loss: 0.247378] [G loss: 0.246659] [ema: 0.999738] 
[Epoch 698/834] [Batch 0/38] [D loss: 0.277509] [G loss: 0.244293] [ema: 0.999739] 
[Epoch 699/834] [Batch 0/38] [D loss: 0.274962] [G loss: 0.259750] [ema: 0.999739] 
[Epoch 700/834] [Batch 0/38] [D loss: 0.268646] [G loss: 0.221177] [ema: 0.999739] 
[Epoch 701/834] [Batch 0/38] [D loss: 0.258714] [G loss: 0.247749] [ema: 0.999740] 
[Epoch 702/834] [Batch 0/38] [D loss: 0.279860] [G loss: 0.267483] [ema: 0.999740] 
[Epoch 703/834] [Batch 0/38] [D loss: 0.269020] [G loss: 0.246768] [ema: 0.999741] 
[Epoch 704/834] [Batch 0/38] [D loss: 0.265637] [G loss: 0.240476] [ema: 0.999741] 
[Epoch 705/834] [Batch 0/38] [D loss: 0.250613] [G loss: 0.239435] [ema: 0.999741] 
[Epoch 706/834] [Batch 0/38] [D loss: 0.253725] [G loss: 0.248648] [ema: 0.999742] 
[Epoch 707/834] [Batch 0/38] [D loss: 0.240123] [G loss: 0.247869] [ema: 0.999742] 
[Epoch 708/834] [Batch 0/38] [D loss: 0.248500] [G loss: 0.246902] [ema: 0.999742] 
[Epoch 709/834] [Batch 0/38] [D loss: 0.342369] [G loss: 0.238829] [ema: 0.999743] 
[Epoch 710/834] [Batch 0/38] [D loss: 0.262477] [G loss: 0.266671] [ema: 0.999743] 
[Epoch 711/834] [Batch 0/38] [D loss: 0.279094] [G loss: 0.214967] [ema: 0.999743] 
[Epoch 712/834] [Batch 0/38] [D loss: 0.254484] [G loss: 0.248369] [ema: 0.999744] 
[Epoch 713/834] [Batch 0/38] [D loss: 0.277717] [G loss: 0.238983] [ema: 0.999744] 
[Epoch 714/834] [Batch 0/38] [D loss: 0.249878] [G loss: 0.255380] [ema: 0.999745] 
[Epoch 715/834] [Batch 0/38] [D loss: 0.286865] [G loss: 0.217044] [ema: 0.999745] 
[Epoch 716/834] [Batch 0/38] [D loss: 0.256011] [G loss: 0.210540] [ema: 0.999745] 
[Epoch 717/834] [Batch 0/38] [D loss: 0.273729] [G loss: 0.235510] [ema: 0.999746] 
[Epoch 718/834] [Batch 0/38] [D loss: 0.250838] [G loss: 0.260611] [ema: 0.999746] 
[Epoch 719/834] [Batch 0/38] [D loss: 0.271781] [G loss: 0.215523] [ema: 0.999746] 
[Epoch 720/834] [Batch 0/38] [D loss: 0.258405] [G loss: 0.249495] [ema: 0.999747] 
[Epoch 721/834] [Batch 0/38] [D loss: 0.264077] [G loss: 0.240123] [ema: 0.999747] 
[Epoch 722/834] [Batch 0/38] [D loss: 0.279854] [G loss: 0.257046] [ema: 0.999747] 
[Epoch 723/834] [Batch 0/38] [D loss: 0.250349] [G loss: 0.251117] [ema: 0.999748] 
[Epoch 724/834] [Batch 0/38] [D loss: 0.280514] [G loss: 0.236871] [ema: 0.999748] 
[Epoch 725/834] [Batch 0/38] [D loss: 0.283013] [G loss: 0.238210] [ema: 0.999748] 
[Epoch 726/834] [Batch 0/38] [D loss: 0.268329] [G loss: 0.262562] [ema: 0.999749] 
[Epoch 727/834] [Batch 0/38] [D loss: 0.265529] [G loss: 0.238178] [ema: 0.999749] 
[Epoch 728/834] [Batch 0/38] [D loss: 0.278622] [G loss: 0.243448] [ema: 0.999749] 
[Epoch 729/834] [Batch 0/38] [D loss: 0.291933] [G loss: 0.237425] [ema: 0.999750] 
[Epoch 730/834] [Batch 0/38] [D loss: 0.253065] [G loss: 0.245527] [ema: 0.999750] 
[Epoch 731/834] [Batch 0/38] [D loss: 0.266151] [G loss: 0.233470] [ema: 0.999751] 
[Epoch 732/834] [Batch 0/38] [D loss: 0.269694] [G loss: 0.235690] [ema: 0.999751] 
[Epoch 733/834] [Batch 0/38] [D loss: 0.285227] [G loss: 0.236037] [ema: 0.999751] 
[Epoch 734/834] [Batch 0/38] [D loss: 0.264365] [G loss: 0.240391] [ema: 0.999752] 
[Epoch 735/834] [Batch 0/38] [D loss: 0.276274] [G loss: 0.232707] [ema: 0.999752] 
[Epoch 736/834] [Batch 0/38] [D loss: 0.296106] [G loss: 0.251215] [ema: 0.999752] 
[Epoch 737/834] [Batch 0/38] [D loss: 0.245222] [G loss: 0.250346] [ema: 0.999753] 
[Epoch 738/834] [Batch 0/38] [D loss: 0.289848] [G loss: 0.235170] [ema: 0.999753] 
[Epoch 739/834] [Batch 0/38] [D loss: 0.314731] [G loss: 0.240021] [ema: 0.999753] 
[Epoch 740/834] [Batch 0/38] [D loss: 0.280794] [G loss: 0.252838] [ema: 0.999754] 
[Epoch 741/834] [Batch 0/38] [D loss: 0.271844] [G loss: 0.234836] [ema: 0.999754] 
[Epoch 742/834] [Batch 0/38] [D loss: 0.278740] [G loss: 0.247643] [ema: 0.999754] 
[Epoch 743/834] [Batch 0/38] [D loss: 0.290972] [G loss: 0.230844] [ema: 0.999755] 
[Epoch 744/834] [Batch 0/38] [D loss: 0.247624] [G loss: 0.233523] [ema: 0.999755] 
[Epoch 745/834] [Batch 0/38] [D loss: 0.268172] [G loss: 0.238396] [ema: 0.999755] 
[Epoch 746/834] [Batch 0/38] [D loss: 0.263980] [G loss: 0.266165] [ema: 0.999756] 
[Epoch 747/834] [Batch 0/38] [D loss: 0.257130] [G loss: 0.241715] [ema: 0.999756] 
[Epoch 748/834] [Batch 0/38] [D loss: 0.263719] [G loss: 0.245450] [ema: 0.999756] 
[Epoch 749/834] [Batch 0/38] [D loss: 0.280123] [G loss: 0.231596] [ema: 0.999756] 
[Epoch 750/834] [Batch 0/38] [D loss: 0.259450] [G loss: 0.248786] [ema: 0.999757] 
[Epoch 751/834] [Batch 0/38] [D loss: 0.250597] [G loss: 0.257137] [ema: 0.999757] 
[Epoch 752/834] [Batch 0/38] [D loss: 0.308283] [G loss: 0.243511] [ema: 0.999757] 
[Epoch 753/834] [Batch 0/38] [D loss: 0.276191] [G loss: 0.257653] [ema: 0.999758] 
[Epoch 754/834] [Batch 0/38] [D loss: 0.274960] [G loss: 0.235410] [ema: 0.999758] 
[Epoch 755/834] [Batch 0/38] [D loss: 0.270296] [G loss: 0.242728] [ema: 0.999758] 
[Epoch 756/834] [Batch 0/38] [D loss: 0.239053] [G loss: 0.232218] [ema: 0.999759] 
[Epoch 757/834] [Batch 0/38] [D loss: 0.343898] [G loss: 0.228857] [ema: 0.999759] 
[Epoch 758/834] [Batch 0/38] [D loss: 0.243391] [G loss: 0.243608] [ema: 0.999759] 
[Epoch 759/834] [Batch 0/38] [D loss: 0.270763] [G loss: 0.239384] [ema: 0.999760] 
[Epoch 760/834] [Batch 0/38] [D loss: 0.258153] [G loss: 0.260269] [ema: 0.999760] 
[Epoch 761/834] [Batch 0/38] [D loss: 0.265230] [G loss: 0.240434] [ema: 0.999760] 
[Epoch 762/834] [Batch 0/38] [D loss: 0.260632] [G loss: 0.251279] [ema: 0.999761] 
[Epoch 763/834] [Batch 0/38] [D loss: 0.250434] [G loss: 0.263752] [ema: 0.999761] 
[Epoch 764/834] [Batch 0/38] [D loss: 0.296792] [G loss: 0.242858] [ema: 0.999761] 
[Epoch 765/834] [Batch 0/38] [D loss: 0.264696] [G loss: 0.241936] [ema: 0.999762] 
[Epoch 766/834] [Batch 0/38] [D loss: 0.293199] [G loss: 0.252672] [ema: 0.999762] 
[Epoch 767/834] [Batch 0/38] [D loss: 0.254948] [G loss: 0.248739] [ema: 0.999762] 
[Epoch 768/834] [Batch 0/38] [D loss: 0.339928] [G loss: 0.256662] [ema: 0.999763] 
[Epoch 769/834] [Batch 0/38] [D loss: 0.289305] [G loss: 0.261934] [ema: 0.999763] 
[Epoch 770/834] [Batch 0/38] [D loss: 0.254100] [G loss: 0.243374] [ema: 0.999763] 
[Epoch 771/834] [Batch 0/38] [D loss: 0.259327] [G loss: 0.256741] [ema: 0.999763] 
[Epoch 772/834] [Batch 0/38] [D loss: 0.251033] [G loss: 0.257200] [ema: 0.999764] 
[Epoch 773/834] [Batch 0/38] [D loss: 0.262590] [G loss: 0.245293] [ema: 0.999764] 
[Epoch 774/834] [Batch 0/38] [D loss: 0.262834] [G loss: 0.246398] [ema: 0.999764] 
[Epoch 775/834] [Batch 0/38] [D loss: 0.335598] [G loss: 0.241468] [ema: 0.999765] 
[Epoch 776/834] [Batch 0/38] [D loss: 0.235802] [G loss: 0.274265] [ema: 0.999765] 
[Epoch 777/834] [Batch 0/38] [D loss: 0.282962] [G loss: 0.235044] [ema: 0.999765] 
[Epoch 778/834] [Batch 0/38] [D loss: 0.263186] [G loss: 0.255695] [ema: 0.999766] 
[Epoch 779/834] [Batch 0/38] [D loss: 0.265253] [G loss: 0.247889] [ema: 0.999766] 
[Epoch 780/834] [Batch 0/38] [D loss: 0.244985] [G loss: 0.264555] [ema: 0.999766] 
[Epoch 781/834] [Batch 0/38] [D loss: 0.258488] [G loss: 0.241788] [ema: 0.999766] 
[Epoch 782/834] [Batch 0/38] [D loss: 0.296817] [G loss: 0.225243] [ema: 0.999767] 
[Epoch 783/834] [Batch 0/38] [D loss: 0.267109] [G loss: 0.247066] [ema: 0.999767] 
[Epoch 784/834] [Batch 0/38] [D loss: 0.282443] [G loss: 0.243210] [ema: 0.999767] 
[Epoch 785/834] [Batch 0/38] [D loss: 0.270371] [G loss: 0.246990] [ema: 0.999768] 
[Epoch 786/834] [Batch 0/38] [D loss: 0.265066] [G loss: 0.242928] [ema: 0.999768] 
[Epoch 787/834] [Batch 0/38] [D loss: 0.271890] [G loss: 0.246424] [ema: 0.999768] 
[Epoch 788/834] [Batch 0/38] [D loss: 0.265670] [G loss: 0.241860] [ema: 0.999769] 
[Epoch 789/834] [Batch 0/38] [D loss: 0.257547] [G loss: 0.245317] [ema: 0.999769] 
[Epoch 790/834] [Batch 0/38] [D loss: 0.283677] [G loss: 0.246568] [ema: 0.999769] 
[Epoch 791/834] [Batch 0/38] [D loss: 0.267663] [G loss: 0.250025] [ema: 0.999769] 
[Epoch 792/834] [Batch 0/38] [D loss: 0.294154] [G loss: 0.239211] [ema: 0.999770] 
[Epoch 793/834] [Batch 0/38] [D loss: 0.262214] [G loss: 0.232988] [ema: 0.999770] 
[Epoch 794/834] [Batch 0/38] [D loss: 0.278593] [G loss: 0.244980] [ema: 0.999770] 
[Epoch 795/834] [Batch 0/38] [D loss: 0.320966] [G loss: 0.257240] [ema: 0.999771] 
[Epoch 796/834] [Batch 0/38] [D loss: 0.310523] [G loss: 0.251023] [ema: 0.999771] 
[Epoch 797/834] [Batch 0/38] [D loss: 0.256459] [G loss: 0.249700] [ema: 0.999771] 
[Epoch 798/834] [Batch 0/38] [D loss: 0.259604] [G loss: 0.242042] [ema: 0.999771] 
[Epoch 799/834] [Batch 0/38] [D loss: 0.276049] [G loss: 0.252928] [ema: 0.999772] 
[Epoch 800/834] [Batch 0/38] [D loss: 0.268939] [G loss: 0.261327] [ema: 0.999772] 
[Epoch 801/834] [Batch 0/38] [D loss: 0.294430] [G loss: 0.223945] [ema: 0.999772] 
[Epoch 802/834] [Batch 0/38] [D loss: 0.265350] [G loss: 0.251900] [ema: 0.999773] 
[Epoch 803/834] [Batch 0/38] [D loss: 0.245302] [G loss: 0.242494] [ema: 0.999773] 
[Epoch 804/834] [Batch 0/38] [D loss: 0.247702] [G loss: 0.252509] [ema: 0.999773] 
[Epoch 805/834] [Batch 0/38] [D loss: 0.287345] [G loss: 0.225719] [ema: 0.999773] 
[Epoch 806/834] [Batch 0/38] [D loss: 0.323304] [G loss: 0.246541] [ema: 0.999774] 
[Epoch 807/834] [Batch 0/38] [D loss: 0.260608] [G loss: 0.262687] [ema: 0.999774] 
[Epoch 808/834] [Batch 0/38] [D loss: 0.250633] [G loss: 0.251288] [ema: 0.999774] 
[Epoch 809/834] [Batch 0/38] [D loss: 0.264934] [G loss: 0.245700] [ema: 0.999775] 
[Epoch 810/834] [Batch 0/38] [D loss: 0.246801] [G loss: 0.237299] [ema: 0.999775] 
[Epoch 811/834] [Batch 0/38] [D loss: 0.280311] [G loss: 0.243982] [ema: 0.999775] 
[Epoch 812/834] [Batch 0/38] [D loss: 0.251351] [G loss: 0.241042] [ema: 0.999775] 
[Epoch 813/834] [Batch 0/38] [D loss: 0.280006] [G loss: 0.233658] [ema: 0.999776] 
[Epoch 814/834] [Batch 0/38] [D loss: 0.254017] [G loss: 0.238010] [ema: 0.999776] 
[Epoch 815/834] [Batch 0/38] [D loss: 0.266827] [G loss: 0.232734] [ema: 0.999776] 
[Epoch 816/834] [Batch 0/38] [D loss: 0.263120] [G loss: 0.248440] [ema: 0.999776] 
[Epoch 817/834] [Batch 0/38] [D loss: 0.250782] [G loss: 0.254614] [ema: 0.999777] 
[Epoch 818/834] [Batch 0/38] [D loss: 0.284717] [G loss: 0.247751] [ema: 0.999777] 
[Epoch 819/834] [Batch 0/38] [D loss: 0.266851] [G loss: 0.226653] [ema: 0.999777] 
[Epoch 820/834] [Batch 0/38] [D loss: 0.247213] [G loss: 0.249734] [ema: 0.999778] 
[Epoch 821/834] [Batch 0/38] [D loss: 0.354522] [G loss: 0.250538] [ema: 0.999778] 
[Epoch 822/834] [Batch 0/38] [D loss: 0.262365] [G loss: 0.252488] [ema: 0.999778] 
[Epoch 823/834] [Batch 0/38] [D loss: 0.252646] [G loss: 0.246382] [ema: 0.999778] 
[Epoch 824/834] [Batch 0/38] [D loss: 0.252584] [G loss: 0.252456] [ema: 0.999779] 
[Epoch 825/834] [Batch 0/38] [D loss: 0.260283] [G loss: 0.258148] [ema: 0.999779] 
[Epoch 826/834] [Batch 0/38] [D loss: 0.269249] [G loss: 0.232060] [ema: 0.999779] 
[Epoch 827/834] [Batch 0/38] [D loss: 0.284316] [G loss: 0.253783] [ema: 0.999779] 
[Epoch 828/834] [Batch 0/38] [D loss: 0.266673] [G loss: 0.243579] [ema: 0.999780] 
[Epoch 829/834] [Batch 0/38] [D loss: 0.269998] [G loss: 0.240260] [ema: 0.999780] 
[Epoch 830/834] [Batch 0/38] [D loss: 0.289186] [G loss: 0.240739] [ema: 0.999780] 
[Epoch 831/834] [Batch 0/38] [D loss: 0.256889] [G loss: 0.243765] [ema: 0.999781] 
[Epoch 832/834] [Batch 0/38] [D loss: 0.244417] [G loss: 0.244114] [ema: 0.999781] 
[Epoch 833/834] [Batch 0/38] [D loss: 0.300682] [G loss: 0.233516] [ema: 0.999781] 
