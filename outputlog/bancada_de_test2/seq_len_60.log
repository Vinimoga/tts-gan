
 Starting training
Total of classes being trained: 6

['upstairs.csv', 'run.csv', 'walk.csv', 'downstairs.csv', 'sit.csv', 'stand.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
upstairs training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
upstairs
daghar
return single class data and labels, class is upstairs
data shape is (6427, 3, 1, 60)
label shape is (6427,)
402
Epochs between checkpoint: 32



Saving checkpoint 1 in logs/daghar_50000_60_100/upstairs_50000_D_60_2024_10_23_17_18_20/Model



[Epoch 0/125] [Batch 0/402] [D loss: 1.137400] [G loss: 0.379290] [ema: 0.000000] 
[Epoch 0/125] [Batch 100/402] [D loss: 0.387056] [G loss: 0.245731] [ema: 0.933033] 
[Epoch 0/125] [Batch 200/402] [D loss: 0.445673] [G loss: 0.196556] [ema: 0.965936] 
[Epoch 0/125] [Batch 300/402] [D loss: 0.329980] [G loss: 0.201797] [ema: 0.977160] 
[Epoch 0/125] [Batch 400/402] [D loss: 0.391089] [G loss: 0.183558] [ema: 0.982821] 
[Epoch 1/125] [Batch 0/402] [D loss: 0.347718] [G loss: 0.208128] [ema: 0.982905] 
[Epoch 1/125] [Batch 100/402] [D loss: 0.386020] [G loss: 0.201921] [ema: 0.986287] 
[Epoch 1/125] [Batch 200/402] [D loss: 0.347986] [G loss: 0.201707] [ema: 0.988552] 
[Epoch 1/125] [Batch 300/402] [D loss: 0.367111] [G loss: 0.181042] [ema: 0.990175] 
[Epoch 1/125] [Batch 400/402] [D loss: 0.342485] [G loss: 0.188676] [ema: 0.991395] 
[Epoch 2/125] [Batch 0/402] [D loss: 0.380810] [G loss: 0.193698] [ema: 0.991416] 
[Epoch 2/125] [Batch 100/402] [D loss: 0.382329] [G loss: 0.217151] [ema: 0.992362] 
[Epoch 2/125] [Batch 200/402] [D loss: 0.416325] [G loss: 0.212661] [ema: 0.993120] 
[Epoch 2/125] [Batch 300/402] [D loss: 0.372957] [G loss: 0.224993] [ema: 0.993741] 
[Epoch 2/125] [Batch 400/402] [D loss: 0.326573] [G loss: 0.245488] [ema: 0.994260] 
[Epoch 3/125] [Batch 0/402] [D loss: 0.341122] [G loss: 0.245643] [ema: 0.994269] 
[Epoch 3/125] [Batch 100/402] [D loss: 0.352785] [G loss: 0.181255] [ema: 0.994707] 
[Epoch 3/125] [Batch 200/402] [D loss: 0.356525] [G loss: 0.171374] [ema: 0.995082] 
[Epoch 3/125] [Batch 300/402] [D loss: 0.479471] [G loss: 0.168760] [ema: 0.995408] 
[Epoch 3/125] [Batch 400/402] [D loss: 0.493725] [G loss: 0.118258] [ema: 0.995693] 
[Epoch 4/125] [Batch 0/402] [D loss: 0.464509] [G loss: 0.184891] [ema: 0.995699] 
[Epoch 4/125] [Batch 100/402] [D loss: 0.501011] [G loss: 0.140900] [ema: 0.995950] 
[Epoch 4/125] [Batch 200/402] [D loss: 0.493448] [G loss: 0.157130] [ema: 0.996174] 
[Epoch 4/125] [Batch 300/402] [D loss: 0.463408] [G loss: 0.149039] [ema: 0.996374] 
[Epoch 4/125] [Batch 400/402] [D loss: 0.332374] [G loss: 0.187649] [ema: 0.996554] 
[Epoch 5/125] [Batch 0/402] [D loss: 0.410111] [G loss: 0.195319] [ema: 0.996557] 
[Epoch 5/125] [Batch 100/402] [D loss: 0.469583] [G loss: 0.152784] [ema: 0.996720] 
[Epoch 5/125] [Batch 200/402] [D loss: 0.482484] [G loss: 0.144746] [ema: 0.996869] 
[Epoch 5/125] [Batch 300/402] [D loss: 0.440110] [G loss: 0.145070] [ema: 0.997004] 
[Epoch 5/125] [Batch 400/402] [D loss: 0.493393] [G loss: 0.133017] [ema: 0.997128] 
[Epoch 6/125] [Batch 0/402] [D loss: 0.539912] [G loss: 0.108120] [ema: 0.997130] 
[Epoch 6/125] [Batch 100/402] [D loss: 0.503757] [G loss: 0.097000] [ema: 0.997244] 
[Epoch 6/125] [Batch 200/402] [D loss: 0.409858] [G loss: 0.120909] [ema: 0.997350] 
[Epoch 6/125] [Batch 300/402] [D loss: 0.544123] [G loss: 0.125823] [ema: 0.997447] 
[Epoch 6/125] [Batch 400/402] [D loss: 0.518184] [G loss: 0.170036] [ema: 0.997538] 
[Epoch 7/125] [Batch 0/402] [D loss: 0.518722] [G loss: 0.142512] [ema: 0.997540] 
[Epoch 7/125] [Batch 100/402] [D loss: 0.507808] [G loss: 0.157636] [ema: 0.997624] 
[Epoch 7/125] [Batch 200/402] [D loss: 0.451954] [G loss: 0.142600] [ema: 0.997703] 
[Epoch 7/125] [Batch 300/402] [D loss: 0.486407] [G loss: 0.140578] [ema: 0.997777] 
[Epoch 7/125] [Batch 400/402] [D loss: 0.463662] [G loss: 0.135610] [ema: 0.997846] 
[Epoch 8/125] [Batch 0/402] [D loss: 0.523701] [G loss: 0.165890] [ema: 0.997847] 
[Epoch 8/125] [Batch 100/402] [D loss: 0.525790] [G loss: 0.141609] [ema: 0.997912] 
[Epoch 8/125] [Batch 200/402] [D loss: 0.508894] [G loss: 0.149355] [ema: 0.997973] 
[Epoch 8/125] [Batch 300/402] [D loss: 0.495946] [G loss: 0.132981] [ema: 0.998031] 
[Epoch 8/125] [Batch 400/402] [D loss: 0.508941] [G loss: 0.149764] [ema: 0.998085] 
[Epoch 9/125] [Batch 0/402] [D loss: 0.481233] [G loss: 0.163004] [ema: 0.998086] 
[Epoch 9/125] [Batch 100/402] [D loss: 0.544084] [G loss: 0.166446] [ema: 0.998137] 
[Epoch 9/125] [Batch 200/402] [D loss: 0.518496] [G loss: 0.137493] [ema: 0.998186] 
[Epoch 9/125] [Batch 300/402] [D loss: 0.460528] [G loss: 0.125705] [ema: 0.998232] 
[Epoch 9/125] [Batch 400/402] [D loss: 0.526474] [G loss: 0.102108] [ema: 0.998276] 
[Epoch 10/125] [Batch 0/402] [D loss: 0.500338] [G loss: 0.106017] [ema: 0.998277] 
[Epoch 10/125] [Batch 100/402] [D loss: 0.570375] [G loss: 0.153344] [ema: 0.998319] 
[Epoch 10/125] [Batch 200/402] [D loss: 0.512747] [G loss: 0.130162] [ema: 0.998359] 
[Epoch 10/125] [Batch 300/402] [D loss: 0.466522] [G loss: 0.138821] [ema: 0.998397] 
[Epoch 10/125] [Batch 400/402] [D loss: 0.506276] [G loss: 0.155358] [ema: 0.998433] 
[Epoch 11/125] [Batch 0/402] [D loss: 0.466175] [G loss: 0.161081] [ema: 0.998434] 
[Epoch 11/125] [Batch 100/402] [D loss: 0.486036] [G loss: 0.136290] [ema: 0.998468] 
[Epoch 11/125] [Batch 200/402] [D loss: 0.480479] [G loss: 0.117297] [ema: 0.998501] 
[Epoch 11/125] [Batch 300/402] [D loss: 0.472047] [G loss: 0.138582] [ema: 0.998533] 
[Epoch 11/125] [Batch 400/402] [D loss: 0.493000] [G loss: 0.152218] [ema: 0.998564] 
[Epoch 12/125] [Batch 0/402] [D loss: 0.469297] [G loss: 0.150635] [ema: 0.998564] 
[Epoch 12/125] [Batch 100/402] [D loss: 0.463068] [G loss: 0.143248] [ema: 0.998593] 
[Epoch 12/125] [Batch 200/402] [D loss: 0.509991] [G loss: 0.151577] [ema: 0.998621] 
[Epoch 12/125] [Batch 300/402] [D loss: 0.418313] [G loss: 0.199372] [ema: 0.998648] 
[Epoch 12/125] [Batch 400/402] [D loss: 0.436127] [G loss: 0.178981] [ema: 0.998674] 
[Epoch 13/125] [Batch 0/402] [D loss: 0.409488] [G loss: 0.196753] [ema: 0.998675] 
[Epoch 13/125] [Batch 100/402] [D loss: 0.489232] [G loss: 0.119796] [ema: 0.998699] 
[Epoch 13/125] [Batch 200/402] [D loss: 0.455690] [G loss: 0.178890] [ema: 0.998723] 
[Epoch 13/125] [Batch 300/402] [D loss: 0.443007] [G loss: 0.154224] [ema: 0.998746] 
[Epoch 13/125] [Batch 400/402] [D loss: 0.436897] [G loss: 0.195981] [ema: 0.998769] 
[Epoch 14/125] [Batch 0/402] [D loss: 0.460648] [G loss: 0.181112] [ema: 0.998769] 
[Epoch 14/125] [Batch 100/402] [D loss: 0.490397] [G loss: 0.166754] [ema: 0.998791] 
[Epoch 14/125] [Batch 200/402] [D loss: 0.452792] [G loss: 0.200440] [ema: 0.998811] 
[Epoch 14/125] [Batch 300/402] [D loss: 0.394648] [G loss: 0.164694] [ema: 0.998831] 
[Epoch 14/125] [Batch 400/402] [D loss: 0.414483] [G loss: 0.182144] [ema: 0.998851] 
[Epoch 15/125] [Batch 0/402] [D loss: 0.449837] [G loss: 0.184695] [ema: 0.998851] 
[Epoch 15/125] [Batch 100/402] [D loss: 0.442643] [G loss: 0.155094] [ema: 0.998870] 
[Epoch 15/125] [Batch 200/402] [D loss: 0.422078] [G loss: 0.170800] [ema: 0.998888] 
[Epoch 15/125] [Batch 300/402] [D loss: 0.467525] [G loss: 0.179045] [ema: 0.998906] 
[Epoch 15/125] [Batch 400/402] [D loss: 0.432606] [G loss: 0.196437] [ema: 0.998923] 
[Epoch 16/125] [Batch 0/402] [D loss: 0.459555] [G loss: 0.183619] [ema: 0.998923] 
[Epoch 16/125] [Batch 100/402] [D loss: 0.448877] [G loss: 0.173107] [ema: 0.998939] 
[Epoch 16/125] [Batch 200/402] [D loss: 0.392567] [G loss: 0.176621] [ema: 0.998955] 
[Epoch 16/125] [Batch 300/402] [D loss: 0.433254] [G loss: 0.154624] [ema: 0.998971] 
[Epoch 16/125] [Batch 400/402] [D loss: 0.449425] [G loss: 0.181130] [ema: 0.998986] 
[Epoch 17/125] [Batch 0/402] [D loss: 0.368915] [G loss: 0.174298] [ema: 0.998986] 
[Epoch 17/125] [Batch 100/402] [D loss: 0.397512] [G loss: 0.163477] [ema: 0.999001] 
[Epoch 17/125] [Batch 200/402] [D loss: 0.440012] [G loss: 0.224206] [ema: 0.999015] 
[Epoch 17/125] [Batch 300/402] [D loss: 0.405502] [G loss: 0.188484] [ema: 0.999029] 
[Epoch 17/125] [Batch 400/402] [D loss: 0.419847] [G loss: 0.173723] [ema: 0.999042] 
[Epoch 18/125] [Batch 0/402] [D loss: 0.440982] [G loss: 0.187049] [ema: 0.999043] 
[Epoch 18/125] [Batch 100/402] [D loss: 0.423513] [G loss: 0.158430] [ema: 0.999056] 
[Epoch 18/125] [Batch 200/402] [D loss: 0.445680] [G loss: 0.148352] [ema: 0.999068] 
[Epoch 18/125] [Batch 300/402] [D loss: 0.411371] [G loss: 0.196739] [ema: 0.999081] 
[Epoch 18/125] [Batch 400/402] [D loss: 0.424207] [G loss: 0.187511] [ema: 0.999093] 
[Epoch 19/125] [Batch 0/402] [D loss: 0.381909] [G loss: 0.207219] [ema: 0.999093] 
[Epoch 19/125] [Batch 100/402] [D loss: 0.472344] [G loss: 0.165014] [ema: 0.999105] 
[Epoch 19/125] [Batch 200/402] [D loss: 0.418360] [G loss: 0.176857] [ema: 0.999116] 
[Epoch 19/125] [Batch 300/402] [D loss: 0.399004] [G loss: 0.142075] [ema: 0.999127] 
[Epoch 19/125] [Batch 400/402] [D loss: 0.478816] [G loss: 0.164905] [ema: 0.999138] 
[Epoch 20/125] [Batch 0/402] [D loss: 0.478521] [G loss: 0.176627] [ema: 0.999138] 
[Epoch 20/125] [Batch 100/402] [D loss: 0.464260] [G loss: 0.177114] [ema: 0.999149] 
[Epoch 20/125] [Batch 200/402] [D loss: 0.387822] [G loss: 0.177722] [ema: 0.999159] 
[Epoch 20/125] [Batch 300/402] [D loss: 0.444853] [G loss: 0.198049] [ema: 0.999169] 
[Epoch 20/125] [Batch 400/402] [D loss: 0.412961] [G loss: 0.156911] [ema: 0.999179] 
[Epoch 21/125] [Batch 0/402] [D loss: 0.439087] [G loss: 0.140073] [ema: 0.999179] 
[Epoch 21/125] [Batch 100/402] [D loss: 0.372272] [G loss: 0.202410] [ema: 0.999189] 
[Epoch 21/125] [Batch 200/402] [D loss: 0.387586] [G loss: 0.166129] [ema: 0.999198] 
[Epoch 21/125] [Batch 300/402] [D loss: 0.397897] [G loss: 0.165912] [ema: 0.999207] 
[Epoch 21/125] [Batch 400/402] [D loss: 0.357664] [G loss: 0.172699] [ema: 0.999216] 
[Epoch 22/125] [Batch 0/402] [D loss: 0.468334] [G loss: 0.192331] [ema: 0.999217] 
[Epoch 22/125] [Batch 100/402] [D loss: 0.402298] [G loss: 0.153651] [ema: 0.999225] 
[Epoch 22/125] [Batch 200/402] [D loss: 0.474731] [G loss: 0.136045] [ema: 0.999234] 
[Epoch 22/125] [Batch 300/402] [D loss: 0.372650] [G loss: 0.174824] [ema: 0.999242] 
[Epoch 22/125] [Batch 400/402] [D loss: 0.459398] [G loss: 0.152549] [ema: 0.999250] 
[Epoch 23/125] [Batch 0/402] [D loss: 0.401459] [G loss: 0.178871] [ema: 0.999251] 
[Epoch 23/125] [Batch 100/402] [D loss: 0.364931] [G loss: 0.197033] [ema: 0.999259] 
[Epoch 23/125] [Batch 200/402] [D loss: 0.469195] [G loss: 0.151409] [ema: 0.999266] 
[Epoch 23/125] [Batch 300/402] [D loss: 0.353732] [G loss: 0.168721] [ema: 0.999274] 
[Epoch 23/125] [Batch 400/402] [D loss: 0.416995] [G loss: 0.193878] [ema: 0.999282] 
[Epoch 24/125] [Batch 0/402] [D loss: 0.400645] [G loss: 0.223453] [ema: 0.999282] 
[Epoch 24/125] [Batch 100/402] [D loss: 0.395887] [G loss: 0.196116] [ema: 0.999289] 
[Epoch 24/125] [Batch 200/402] [D loss: 0.386940] [G loss: 0.174213] [ema: 0.999296] 
[Epoch 24/125] [Batch 300/402] [D loss: 0.395610] [G loss: 0.164145] [ema: 0.999303] 
[Epoch 24/125] [Batch 400/402] [D loss: 0.397583] [G loss: 0.203332] [ema: 0.999310] 
[Epoch 25/125] [Batch 0/402] [D loss: 0.322855] [G loss: 0.212619] [ema: 0.999311] 
[Epoch 25/125] [Batch 100/402] [D loss: 0.361593] [G loss: 0.193416] [ema: 0.999317] 
[Epoch 25/125] [Batch 200/402] [D loss: 0.341337] [G loss: 0.178651] [ema: 0.999324] 
[Epoch 25/125] [Batch 300/402] [D loss: 0.365901] [G loss: 0.200645] [ema: 0.999331] 
[Epoch 25/125] [Batch 400/402] [D loss: 0.367233] [G loss: 0.192862] [ema: 0.999337] 
[Epoch 26/125] [Batch 0/402] [D loss: 0.373086] [G loss: 0.210987] [ema: 0.999337] 
[Epoch 26/125] [Batch 100/402] [D loss: 0.408819] [G loss: 0.157252] [ema: 0.999343] 
[Epoch 26/125] [Batch 200/402] [D loss: 0.408594] [G loss: 0.181632] [ema: 0.999349] 
[Epoch 26/125] [Batch 300/402] [D loss: 0.351577] [G loss: 0.181072] [ema: 0.999356] 
[Epoch 26/125] [Batch 400/402] [D loss: 0.399054] [G loss: 0.229920] [ema: 0.999361] 
[Epoch 27/125] [Batch 0/402] [D loss: 0.352648] [G loss: 0.191258] [ema: 0.999362] 
[Epoch 27/125] [Batch 100/402] [D loss: 0.464951] [G loss: 0.152304] [ema: 0.999367] 
[Epoch 27/125] [Batch 200/402] [D loss: 0.397924] [G loss: 0.190753] [ema: 0.999373] 
[Epoch 27/125] [Batch 300/402] [D loss: 0.411829] [G loss: 0.161129] [ema: 0.999379] 
[Epoch 27/125] [Batch 400/402] [D loss: 0.428572] [G loss: 0.161949] [ema: 0.999384] 
[Epoch 28/125] [Batch 0/402] [D loss: 0.387976] [G loss: 0.187035] [ema: 0.999384] 
[Epoch 28/125] [Batch 100/402] [D loss: 0.367470] [G loss: 0.219970] [ema: 0.999390] 
[Epoch 28/125] [Batch 200/402] [D loss: 0.411410] [G loss: 0.159423] [ema: 0.999395] 
[Epoch 28/125] [Batch 300/402] [D loss: 0.422432] [G loss: 0.159686] [ema: 0.999400] 
[Epoch 28/125] [Batch 400/402] [D loss: 0.423120] [G loss: 0.158074] [ema: 0.999406] 
[Epoch 29/125] [Batch 0/402] [D loss: 0.390763] [G loss: 0.188113] [ema: 0.999406] 
[Epoch 29/125] [Batch 100/402] [D loss: 0.396369] [G loss: 0.159295] [ema: 0.999411] 
[Epoch 29/125] [Batch 200/402] [D loss: 0.391199] [G loss: 0.208257] [ema: 0.999416] 
[Epoch 29/125] [Batch 300/402] [D loss: 0.374457] [G loss: 0.198051] [ema: 0.999421] 
[Epoch 29/125] [Batch 400/402] [D loss: 0.389016] [G loss: 0.172701] [ema: 0.999425] 
[Epoch 30/125] [Batch 0/402] [D loss: 0.453617] [G loss: 0.186896] [ema: 0.999425] 
[Epoch 30/125] [Batch 100/402] [D loss: 0.403343] [G loss: 0.194098] [ema: 0.999430] 
[Epoch 30/125] [Batch 200/402] [D loss: 0.391157] [G loss: 0.191784] [ema: 0.999435] 
[Epoch 30/125] [Batch 300/402] [D loss: 0.492092] [G loss: 0.190016] [ema: 0.999439] 
[Epoch 30/125] [Batch 400/402] [D loss: 0.418772] [G loss: 0.187326] [ema: 0.999444] 
[Epoch 31/125] [Batch 0/402] [D loss: 0.436349] [G loss: 0.176216] [ema: 0.999444] 
[Epoch 31/125] [Batch 100/402] [D loss: 0.387895] [G loss: 0.207001] [ema: 0.999448] 
[Epoch 31/125] [Batch 200/402] [D loss: 0.433293] [G loss: 0.189806] [ema: 0.999453] 
[Epoch 31/125] [Batch 300/402] [D loss: 0.316922] [G loss: 0.209397] [ema: 0.999457] 
[Epoch 31/125] [Batch 400/402] [D loss: 0.446527] [G loss: 0.188536] [ema: 0.999461] 



Saving checkpoint 2 in logs/daghar_50000_60_100/upstairs_50000_D_60_2024_10_23_17_18_20/Model



[Epoch 32/125] [Batch 0/402] [D loss: 0.330400] [G loss: 0.190269] [ema: 0.999461] 
[Epoch 32/125] [Batch 100/402] [D loss: 0.345973] [G loss: 0.195751] [ema: 0.999465] 
[Epoch 32/125] [Batch 200/402] [D loss: 0.387929] [G loss: 0.191378] [ema: 0.999470] 
[Epoch 32/125] [Batch 300/402] [D loss: 0.389628] [G loss: 0.176155] [ema: 0.999474] 
[Epoch 32/125] [Batch 400/402] [D loss: 0.417221] [G loss: 0.207283] [ema: 0.999478] 
[Epoch 33/125] [Batch 0/402] [D loss: 0.363937] [G loss: 0.233394] [ema: 0.999478] 
[Epoch 33/125] [Batch 100/402] [D loss: 0.302584] [G loss: 0.189147] [ema: 0.999482] 
[Epoch 33/125] [Batch 200/402] [D loss: 0.317828] [G loss: 0.194152] [ema: 0.999485] 
[Epoch 33/125] [Batch 300/402] [D loss: 0.317905] [G loss: 0.189003] [ema: 0.999489] 
[Epoch 33/125] [Batch 400/402] [D loss: 0.417122] [G loss: 0.202720] [ema: 0.999493] 
[Epoch 34/125] [Batch 0/402] [D loss: 0.370807] [G loss: 0.224748] [ema: 0.999493] 
[Epoch 34/125] [Batch 100/402] [D loss: 0.404984] [G loss: 0.193191] [ema: 0.999497] 
[Epoch 34/125] [Batch 200/402] [D loss: 0.363399] [G loss: 0.191178] [ema: 0.999500] 
[Epoch 34/125] [Batch 300/402] [D loss: 0.381663] [G loss: 0.197515] [ema: 0.999504] 
[Epoch 34/125] [Batch 400/402] [D loss: 0.420717] [G loss: 0.242618] [ema: 0.999507] 
[Epoch 35/125] [Batch 0/402] [D loss: 0.347382] [G loss: 0.212099] [ema: 0.999507] 
[Epoch 35/125] [Batch 100/402] [D loss: 0.343928] [G loss: 0.184289] [ema: 0.999511] 
[Epoch 35/125] [Batch 200/402] [D loss: 0.453602] [G loss: 0.167605] [ema: 0.999514] 
[Epoch 35/125] [Batch 300/402] [D loss: 0.375922] [G loss: 0.215845] [ema: 0.999518] 
[Epoch 35/125] [Batch 400/402] [D loss: 0.344274] [G loss: 0.163470] [ema: 0.999521] 
[Epoch 36/125] [Batch 0/402] [D loss: 0.327043] [G loss: 0.175979] [ema: 0.999521] 
[Epoch 36/125] [Batch 100/402] [D loss: 0.368867] [G loss: 0.194655] [ema: 0.999524] 
[Epoch 36/125] [Batch 200/402] [D loss: 0.339016] [G loss: 0.176482] [ema: 0.999528] 
[Epoch 36/125] [Batch 300/402] [D loss: 0.405304] [G loss: 0.187352] [ema: 0.999531] 
[Epoch 36/125] [Batch 400/402] [D loss: 0.341324] [G loss: 0.201338] [ema: 0.999534] 
[Epoch 37/125] [Batch 0/402] [D loss: 0.360178] [G loss: 0.207825] [ema: 0.999534] 
[Epoch 37/125] [Batch 100/402] [D loss: 0.349043] [G loss: 0.209260] [ema: 0.999537] 
[Epoch 37/125] [Batch 200/402] [D loss: 0.362160] [G loss: 0.202834] [ema: 0.999540] 
[Epoch 37/125] [Batch 300/402] [D loss: 0.388979] [G loss: 0.174804] [ema: 0.999543] 
[Epoch 37/125] [Batch 400/402] [D loss: 0.365156] [G loss: 0.152780] [ema: 0.999546] 
[Epoch 38/125] [Batch 0/402] [D loss: 0.320041] [G loss: 0.236990] [ema: 0.999546] 
[Epoch 38/125] [Batch 100/402] [D loss: 0.324920] [G loss: 0.173452] [ema: 0.999549] 
[Epoch 38/125] [Batch 200/402] [D loss: 0.427152] [G loss: 0.202072] [ema: 0.999552] 
[Epoch 38/125] [Batch 300/402] [D loss: 0.423623] [G loss: 0.169928] [ema: 0.999555] 
[Epoch 38/125] [Batch 400/402] [D loss: 0.420311] [G loss: 0.186363] [ema: 0.999558] 
[Epoch 39/125] [Batch 0/402] [D loss: 0.355717] [G loss: 0.179552] [ema: 0.999558] 
[Epoch 39/125] [Batch 100/402] [D loss: 0.407611] [G loss: 0.191916] [ema: 0.999561] 
[Epoch 39/125] [Batch 200/402] [D loss: 0.354972] [G loss: 0.175796] [ema: 0.999564] 
[Epoch 39/125] [Batch 300/402] [D loss: 0.426373] [G loss: 0.180487] [ema: 0.999566] 
[Epoch 39/125] [Batch 400/402] [D loss: 0.346563] [G loss: 0.175013] [ema: 0.999569] 
[Epoch 40/125] [Batch 0/402] [D loss: 0.382268] [G loss: 0.199679] [ema: 0.999569] 
[Epoch 40/125] [Batch 100/402] [D loss: 0.400867] [G loss: 0.225084] [ema: 0.999572] 
[Epoch 40/125] [Batch 200/402] [D loss: 0.346987] [G loss: 0.200831] [ema: 0.999574] 
[Epoch 40/125] [Batch 300/402] [D loss: 0.381607] [G loss: 0.216033] [ema: 0.999577] 
[Epoch 40/125] [Batch 400/402] [D loss: 0.387377] [G loss: 0.175389] [ema: 0.999579] 
[Epoch 41/125] [Batch 0/402] [D loss: 0.378147] [G loss: 0.244394] [ema: 0.999580] 
[Epoch 41/125] [Batch 100/402] [D loss: 0.375995] [G loss: 0.215433] [ema: 0.999582] 
[Epoch 41/125] [Batch 200/402] [D loss: 0.374312] [G loss: 0.208753] [ema: 0.999585] 
[Epoch 41/125] [Batch 300/402] [D loss: 0.359438] [G loss: 0.179060] [ema: 0.999587] 
[Epoch 41/125] [Batch 400/402] [D loss: 0.374157] [G loss: 0.181118] [ema: 0.999590] 
[Epoch 42/125] [Batch 0/402] [D loss: 0.295413] [G loss: 0.218001] [ema: 0.999590] 
[Epoch 42/125] [Batch 100/402] [D loss: 0.356442] [G loss: 0.219155] [ema: 0.999592] 
[Epoch 42/125] [Batch 200/402] [D loss: 0.417753] [G loss: 0.186128] [ema: 0.999594] 
[Epoch 42/125] [Batch 300/402] [D loss: 0.321526] [G loss: 0.185557] [ema: 0.999597] 
[Epoch 42/125] [Batch 400/402] [D loss: 0.354763] [G loss: 0.193254] [ema: 0.999599] 
[Epoch 43/125] [Batch 0/402] [D loss: 0.384101] [G loss: 0.224641] [ema: 0.999599] 
[Epoch 43/125] [Batch 100/402] [D loss: 0.354548] [G loss: 0.182987] [ema: 0.999601] 
[Epoch 43/125] [Batch 200/402] [D loss: 0.389275] [G loss: 0.186535] [ema: 0.999604] 
[Epoch 43/125] [Batch 300/402] [D loss: 0.420833] [G loss: 0.226067] [ema: 0.999606] 
[Epoch 43/125] [Batch 400/402] [D loss: 0.310954] [G loss: 0.203511] [ema: 0.999608] 
[Epoch 44/125] [Batch 0/402] [D loss: 0.374534] [G loss: 0.237572] [ema: 0.999608] 
[Epoch 44/125] [Batch 100/402] [D loss: 0.315747] [G loss: 0.216239] [ema: 0.999610] 
[Epoch 44/125] [Batch 200/402] [D loss: 0.395911] [G loss: 0.223312] [ema: 0.999613] 
[Epoch 44/125] [Batch 300/402] [D loss: 0.380499] [G loss: 0.241964] [ema: 0.999615] 
[Epoch 44/125] [Batch 400/402] [D loss: 0.320704] [G loss: 0.169582] [ema: 0.999617] 
[Epoch 45/125] [Batch 0/402] [D loss: 0.437040] [G loss: 0.229067] [ema: 0.999617] 
[Epoch 45/125] [Batch 100/402] [D loss: 0.351975] [G loss: 0.243182] [ema: 0.999619] 
[Epoch 45/125] [Batch 200/402] [D loss: 0.370050] [G loss: 0.191973] [ema: 0.999621] 
[Epoch 45/125] [Batch 300/402] [D loss: 0.408390] [G loss: 0.170880] [ema: 0.999623] 
[Epoch 45/125] [Batch 400/402] [D loss: 0.328494] [G loss: 0.213091] [ema: 0.999625] 
[Epoch 46/125] [Batch 0/402] [D loss: 0.370279] [G loss: 0.203553] [ema: 0.999625] 
[Epoch 46/125] [Batch 100/402] [D loss: 0.388464] [G loss: 0.205495] [ema: 0.999627] 
[Epoch 46/125] [Batch 200/402] [D loss: 0.388419] [G loss: 0.200777] [ema: 0.999629] 
[Epoch 46/125] [Batch 300/402] [D loss: 0.333616] [G loss: 0.231057] [ema: 0.999631] 
[Epoch 46/125] [Batch 400/402] [D loss: 0.367313] [G loss: 0.195471] [ema: 0.999633] 
[Epoch 47/125] [Batch 0/402] [D loss: 0.366776] [G loss: 0.218497] [ema: 0.999633] 
[Epoch 47/125] [Batch 100/402] [D loss: 0.328465] [G loss: 0.161691] [ema: 0.999635] 
[Epoch 47/125] [Batch 200/402] [D loss: 0.366364] [G loss: 0.200326] [ema: 0.999637] 
[Epoch 47/125] [Batch 300/402] [D loss: 0.328549] [G loss: 0.189251] [ema: 0.999639] 
[Epoch 47/125] [Batch 400/402] [D loss: 0.341660] [G loss: 0.206829] [ema: 0.999641] 
[Epoch 48/125] [Batch 0/402] [D loss: 0.375136] [G loss: 0.210143] [ema: 0.999641] 
[Epoch 48/125] [Batch 100/402] [D loss: 0.363066] [G loss: 0.239463] [ema: 0.999643] 
[Epoch 48/125] [Batch 200/402] [D loss: 0.390420] [G loss: 0.176362] [ema: 0.999645] 
[Epoch 48/125] [Batch 300/402] [D loss: 0.364737] [G loss: 0.187012] [ema: 0.999646] 
[Epoch 48/125] [Batch 400/402] [D loss: 0.346335] [G loss: 0.199326] [ema: 0.999648] 
[Epoch 49/125] [Batch 0/402] [D loss: 0.318398] [G loss: 0.226818] [ema: 0.999648] 
[Epoch 49/125] [Batch 100/402] [D loss: 0.403277] [G loss: 0.222948] [ema: 0.999650] 
[Epoch 49/125] [Batch 200/402] [D loss: 0.332732] [G loss: 0.188370] [ema: 0.999652] 
[Epoch 49/125] [Batch 300/402] [D loss: 0.412529] [G loss: 0.236311] [ema: 0.999653] 
[Epoch 49/125] [Batch 400/402] [D loss: 0.267572] [G loss: 0.211489] [ema: 0.999655] 
[Epoch 50/125] [Batch 0/402] [D loss: 0.301144] [G loss: 0.258602] [ema: 0.999655] 
[Epoch 50/125] [Batch 100/402] [D loss: 0.322762] [G loss: 0.189598] [ema: 0.999657] 
[Epoch 50/125] [Batch 200/402] [D loss: 0.405745] [G loss: 0.212779] [ema: 0.999659] 
[Epoch 50/125] [Batch 300/402] [D loss: 0.407357] [G loss: 0.191171] [ema: 0.999660] 
[Epoch 50/125] [Batch 400/402] [D loss: 0.358418] [G loss: 0.224321] [ema: 0.999662] 
[Epoch 51/125] [Batch 0/402] [D loss: 0.322160] [G loss: 0.189823] [ema: 0.999662] 
[Epoch 51/125] [Batch 100/402] [D loss: 0.326120] [G loss: 0.212019] [ema: 0.999664] 
[Epoch 51/125] [Batch 200/402] [D loss: 0.350707] [G loss: 0.201409] [ema: 0.999665] 
[Epoch 51/125] [Batch 300/402] [D loss: 0.309393] [G loss: 0.217623] [ema: 0.999667] 
[Epoch 51/125] [Batch 400/402] [D loss: 0.336499] [G loss: 0.213467] [ema: 0.999668] 
[Epoch 52/125] [Batch 0/402] [D loss: 0.334491] [G loss: 0.217676] [ema: 0.999668] 
[Epoch 52/125] [Batch 100/402] [D loss: 0.357293] [G loss: 0.228506] [ema: 0.999670] 
[Epoch 52/125] [Batch 200/402] [D loss: 0.446550] [G loss: 0.200513] [ema: 0.999672] 
[Epoch 52/125] [Batch 300/402] [D loss: 0.371736] [G loss: 0.190850] [ema: 0.999673] 
[Epoch 52/125] [Batch 400/402] [D loss: 0.350737] [G loss: 0.197435] [ema: 0.999675] 
[Epoch 53/125] [Batch 0/402] [D loss: 0.345886] [G loss: 0.185364] [ema: 0.999675] 
[Epoch 53/125] [Batch 100/402] [D loss: 0.402508] [G loss: 0.174268] [ema: 0.999676] 
[Epoch 53/125] [Batch 200/402] [D loss: 0.342119] [G loss: 0.202556] [ema: 0.999678] 
[Epoch 53/125] [Batch 300/402] [D loss: 0.388012] [G loss: 0.256678] [ema: 0.999679] 
[Epoch 53/125] [Batch 400/402] [D loss: 0.339684] [G loss: 0.216434] [ema: 0.999681] 
[Epoch 54/125] [Batch 0/402] [D loss: 0.330734] [G loss: 0.196834] [ema: 0.999681] 
[Epoch 54/125] [Batch 100/402] [D loss: 0.301576] [G loss: 0.223100] [ema: 0.999682] 
[Epoch 54/125] [Batch 200/402] [D loss: 0.324509] [G loss: 0.228383] [ema: 0.999684] 
[Epoch 54/125] [Batch 300/402] [D loss: 0.297117] [G loss: 0.252771] [ema: 0.999685] 
[Epoch 54/125] [Batch 400/402] [D loss: 0.401324] [G loss: 0.205605] [ema: 0.999687] 
[Epoch 55/125] [Batch 0/402] [D loss: 0.318304] [G loss: 0.147310] [ema: 0.999687] 
[Epoch 55/125] [Batch 100/402] [D loss: 0.327040] [G loss: 0.217352] [ema: 0.999688] 
[Epoch 55/125] [Batch 200/402] [D loss: 0.327195] [G loss: 0.235854] [ema: 0.999689] 
[Epoch 55/125] [Batch 300/402] [D loss: 0.339012] [G loss: 0.201948] [ema: 0.999691] 
[Epoch 55/125] [Batch 400/402] [D loss: 0.393710] [G loss: 0.208710] [ema: 0.999692] 
[Epoch 56/125] [Batch 0/402] [D loss: 0.323815] [G loss: 0.187429] [ema: 0.999692] 
[Epoch 56/125] [Batch 100/402] [D loss: 0.356448] [G loss: 0.202417] [ema: 0.999694] 
[Epoch 56/125] [Batch 200/402] [D loss: 0.370547] [G loss: 0.215538] [ema: 0.999695] 
[Epoch 56/125] [Batch 300/402] [D loss: 0.376716] [G loss: 0.229566] [ema: 0.999696] 
[Epoch 56/125] [Batch 400/402] [D loss: 0.368700] [G loss: 0.179168] [ema: 0.999698] 
[Epoch 57/125] [Batch 0/402] [D loss: 0.351145] [G loss: 0.192339] [ema: 0.999698] 
[Epoch 57/125] [Batch 100/402] [D loss: 0.302932] [G loss: 0.211428] [ema: 0.999699] 
[Epoch 57/125] [Batch 200/402] [D loss: 0.356159] [G loss: 0.230388] [ema: 0.999700] 
[Epoch 57/125] [Batch 300/402] [D loss: 0.323144] [G loss: 0.224105] [ema: 0.999701] 
[Epoch 57/125] [Batch 400/402] [D loss: 0.303414] [G loss: 0.181025] [ema: 0.999703] 
[Epoch 58/125] [Batch 0/402] [D loss: 0.366849] [G loss: 0.223851] [ema: 0.999703] 
[Epoch 58/125] [Batch 100/402] [D loss: 0.339044] [G loss: 0.191488] [ema: 0.999704] 
[Epoch 58/125] [Batch 200/402] [D loss: 0.312796] [G loss: 0.191854] [ema: 0.999705] 
[Epoch 58/125] [Batch 300/402] [D loss: 0.364856] [G loss: 0.219699] [ema: 0.999707] 
[Epoch 58/125] [Batch 400/402] [D loss: 0.336051] [G loss: 0.190192] [ema: 0.999708] 
[Epoch 59/125] [Batch 0/402] [D loss: 0.339535] [G loss: 0.244739] [ema: 0.999708] 
[Epoch 59/125] [Batch 100/402] [D loss: 0.314038] [G loss: 0.193409] [ema: 0.999709] 
[Epoch 59/125] [Batch 200/402] [D loss: 0.318742] [G loss: 0.244817] [ema: 0.999710] 
[Epoch 59/125] [Batch 300/402] [D loss: 0.333227] [G loss: 0.207281] [ema: 0.999711] 
[Epoch 59/125] [Batch 400/402] [D loss: 0.328024] [G loss: 0.228029] [ema: 0.999713] 
[Epoch 60/125] [Batch 0/402] [D loss: 0.362602] [G loss: 0.245871] [ema: 0.999713] 
[Epoch 60/125] [Batch 100/402] [D loss: 0.318813] [G loss: 0.194141] [ema: 0.999714] 
[Epoch 60/125] [Batch 200/402] [D loss: 0.321737] [G loss: 0.203906] [ema: 0.999715] 
[Epoch 60/125] [Batch 300/402] [D loss: 0.401919] [G loss: 0.181893] [ema: 0.999716] 
[Epoch 60/125] [Batch 400/402] [D loss: 0.330345] [G loss: 0.210344] [ema: 0.999717] 
[Epoch 61/125] [Batch 0/402] [D loss: 0.340540] [G loss: 0.216944] [ema: 0.999717] 
[Epoch 61/125] [Batch 100/402] [D loss: 0.380571] [G loss: 0.228526] [ema: 0.999719] 
[Epoch 61/125] [Batch 200/402] [D loss: 0.351451] [G loss: 0.197885] [ema: 0.999720] 
[Epoch 61/125] [Batch 300/402] [D loss: 0.343018] [G loss: 0.200771] [ema: 0.999721] 
[Epoch 61/125] [Batch 400/402] [D loss: 0.353319] [G loss: 0.235471] [ema: 0.999722] 
[Epoch 62/125] [Batch 0/402] [D loss: 0.406036] [G loss: 0.192509] [ema: 0.999722] 
[Epoch 62/125] [Batch 100/402] [D loss: 0.411468] [G loss: 0.220514] [ema: 0.999723] 
[Epoch 62/125] [Batch 200/402] [D loss: 0.379750] [G loss: 0.206243] [ema: 0.999724] 
[Epoch 62/125] [Batch 300/402] [D loss: 0.381538] [G loss: 0.203461] [ema: 0.999725] 
[Epoch 62/125] [Batch 400/402] [D loss: 0.302734] [G loss: 0.199722] [ema: 0.999726] 
[Epoch 63/125] [Batch 0/402] [D loss: 0.347560] [G loss: 0.222591] [ema: 0.999726] 
[Epoch 63/125] [Batch 100/402] [D loss: 0.337179] [G loss: 0.236341] [ema: 0.999727] 
[Epoch 63/125] [Batch 200/402] [D loss: 0.376405] [G loss: 0.225417] [ema: 0.999728] 
[Epoch 63/125] [Batch 300/402] [D loss: 0.297955] [G loss: 0.205244] [ema: 0.999730] 
[Epoch 63/125] [Batch 400/402] [D loss: 0.336251] [G loss: 0.219415] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_50000_60_100/upstairs_50000_D_60_2024_10_23_17_18_20/Model



[Epoch 64/125] [Batch 0/402] [D loss: 0.295786] [G loss: 0.207166] [ema: 0.999731] 
[Epoch 64/125] [Batch 100/402] [D loss: 0.335610] [G loss: 0.195299] [ema: 0.999732] 
[Epoch 64/125] [Batch 200/402] [D loss: 0.356732] [G loss: 0.220852] [ema: 0.999733] 
[Epoch 64/125] [Batch 300/402] [D loss: 0.367158] [G loss: 0.218700] [ema: 0.999734] 
[Epoch 64/125] [Batch 400/402] [D loss: 0.305291] [G loss: 0.229945] [ema: 0.999735] 
[Epoch 65/125] [Batch 0/402] [D loss: 0.367623] [G loss: 0.229588] [ema: 0.999735] 
[Epoch 65/125] [Batch 100/402] [D loss: 0.330660] [G loss: 0.229066] [ema: 0.999736] 
[Epoch 65/125] [Batch 200/402] [D loss: 0.335274] [G loss: 0.248697] [ema: 0.999737] 
[Epoch 65/125] [Batch 300/402] [D loss: 0.302763] [G loss: 0.222295] [ema: 0.999738] 
[Epoch 65/125] [Batch 400/402] [D loss: 0.379934] [G loss: 0.221340] [ema: 0.999739] 
[Epoch 66/125] [Batch 0/402] [D loss: 0.444952] [G loss: 0.255308] [ema: 0.999739] 
[Epoch 66/125] [Batch 100/402] [D loss: 0.380040] [G loss: 0.213684] [ema: 0.999740] 
[Epoch 66/125] [Batch 200/402] [D loss: 0.361729] [G loss: 0.217607] [ema: 0.999741] 
[Epoch 66/125] [Batch 300/402] [D loss: 0.314360] [G loss: 0.249598] [ema: 0.999742] 
[Epoch 66/125] [Batch 400/402] [D loss: 0.316179] [G loss: 0.232727] [ema: 0.999743] 
[Epoch 67/125] [Batch 0/402] [D loss: 0.301068] [G loss: 0.246884] [ema: 0.999743] 
[Epoch 67/125] [Batch 100/402] [D loss: 0.281946] [G loss: 0.227376] [ema: 0.999744] 
[Epoch 67/125] [Batch 200/402] [D loss: 0.307961] [G loss: 0.218934] [ema: 0.999745] 
[Epoch 67/125] [Batch 300/402] [D loss: 0.310767] [G loss: 0.202918] [ema: 0.999746] 
[Epoch 67/125] [Batch 400/402] [D loss: 0.284736] [G loss: 0.204737] [ema: 0.999746] 
[Epoch 68/125] [Batch 0/402] [D loss: 0.331012] [G loss: 0.218312] [ema: 0.999746] 
[Epoch 68/125] [Batch 100/402] [D loss: 0.316958] [G loss: 0.223447] [ema: 0.999747] 
[Epoch 68/125] [Batch 200/402] [D loss: 0.325293] [G loss: 0.223753] [ema: 0.999748] 
[Epoch 68/125] [Batch 300/402] [D loss: 0.336048] [G loss: 0.231010] [ema: 0.999749] 
[Epoch 68/125] [Batch 400/402] [D loss: 0.365878] [G loss: 0.232511] [ema: 0.999750] 
[Epoch 69/125] [Batch 0/402] [D loss: 0.315022] [G loss: 0.218125] [ema: 0.999750] 
[Epoch 69/125] [Batch 100/402] [D loss: 0.346172] [G loss: 0.194073] [ema: 0.999751] 
[Epoch 69/125] [Batch 200/402] [D loss: 0.349639] [G loss: 0.213567] [ema: 0.999752] 
[Epoch 69/125] [Batch 300/402] [D loss: 0.344422] [G loss: 0.224192] [ema: 0.999753] 
[Epoch 69/125] [Batch 400/402] [D loss: 0.370110] [G loss: 0.224332] [ema: 0.999754] 
[Epoch 70/125] [Batch 0/402] [D loss: 0.347644] [G loss: 0.210275] [ema: 0.999754] 
[Epoch 70/125] [Batch 100/402] [D loss: 0.384251] [G loss: 0.193923] [ema: 0.999755] 
[Epoch 70/125] [Batch 200/402] [D loss: 0.289433] [G loss: 0.210423] [ema: 0.999755] 
[Epoch 70/125] [Batch 300/402] [D loss: 0.342010] [G loss: 0.227381] [ema: 0.999756] 
[Epoch 70/125] [Batch 400/402] [D loss: 0.347894] [G loss: 0.210379] [ema: 0.999757] 
[Epoch 71/125] [Batch 0/402] [D loss: 0.337920] [G loss: 0.224344] [ema: 0.999757] 
[Epoch 71/125] [Batch 100/402] [D loss: 0.287649] [G loss: 0.203489] [ema: 0.999758] 
[Epoch 71/125] [Batch 200/402] [D loss: 0.296661] [G loss: 0.226915] [ema: 0.999759] 
[Epoch 71/125] [Batch 300/402] [D loss: 0.345653] [G loss: 0.205351] [ema: 0.999760] 
[Epoch 71/125] [Batch 400/402] [D loss: 0.307230] [G loss: 0.224029] [ema: 0.999761] 
[Epoch 72/125] [Batch 0/402] [D loss: 0.379740] [G loss: 0.213460] [ema: 0.999761] 
[Epoch 72/125] [Batch 100/402] [D loss: 0.330582] [G loss: 0.209292] [ema: 0.999761] 
[Epoch 72/125] [Batch 200/402] [D loss: 0.320476] [G loss: 0.228075] [ema: 0.999762] 
[Epoch 72/125] [Batch 300/402] [D loss: 0.331117] [G loss: 0.228886] [ema: 0.999763] 
[Epoch 72/125] [Batch 400/402] [D loss: 0.327757] [G loss: 0.220109] [ema: 0.999764] 
[Epoch 73/125] [Batch 0/402] [D loss: 0.344325] [G loss: 0.203276] [ema: 0.999764] 
[Epoch 73/125] [Batch 100/402] [D loss: 0.289973] [G loss: 0.188075] [ema: 0.999765] 
[Epoch 73/125] [Batch 200/402] [D loss: 0.330892] [G loss: 0.210480] [ema: 0.999765] 
[Epoch 73/125] [Batch 300/402] [D loss: 0.322074] [G loss: 0.199827] [ema: 0.999766] 
[Epoch 73/125] [Batch 400/402] [D loss: 0.332285] [G loss: 0.211009] [ema: 0.999767] 
[Epoch 74/125] [Batch 0/402] [D loss: 0.341343] [G loss: 0.224555] [ema: 0.999767] 
[Epoch 74/125] [Batch 100/402] [D loss: 0.322837] [G loss: 0.212457] [ema: 0.999768] 
[Epoch 74/125] [Batch 200/402] [D loss: 0.333610] [G loss: 0.193528] [ema: 0.999769] 
[Epoch 74/125] [Batch 300/402] [D loss: 0.313933] [G loss: 0.213028] [ema: 0.999769] 
[Epoch 74/125] [Batch 400/402] [D loss: 0.323918] [G loss: 0.238737] [ema: 0.999770] 
[Epoch 75/125] [Batch 0/402] [D loss: 0.353531] [G loss: 0.214600] [ema: 0.999770] 
[Epoch 75/125] [Batch 100/402] [D loss: 0.276411] [G loss: 0.261244] [ema: 0.999771] 
[Epoch 75/125] [Batch 200/402] [D loss: 0.351847] [G loss: 0.181750] [ema: 0.999772] 
[Epoch 75/125] [Batch 300/402] [D loss: 0.345346] [G loss: 0.246010] [ema: 0.999772] 
[Epoch 75/125] [Batch 400/402] [D loss: 0.286425] [G loss: 0.195560] [ema: 0.999773] 
[Epoch 76/125] [Batch 0/402] [D loss: 0.351982] [G loss: 0.275895] [ema: 0.999773] 
[Epoch 76/125] [Batch 100/402] [D loss: 0.361840] [G loss: 0.198256] [ema: 0.999774] 
[Epoch 76/125] [Batch 200/402] [D loss: 0.370986] [G loss: 0.182114] [ema: 0.999775] 
[Epoch 76/125] [Batch 300/402] [D loss: 0.387532] [G loss: 0.191108] [ema: 0.999775] 
[Epoch 76/125] [Batch 400/402] [D loss: 0.373152] [G loss: 0.233802] [ema: 0.999776] 
[Epoch 77/125] [Batch 0/402] [D loss: 0.315699] [G loss: 0.190542] [ema: 0.999776] 
[Epoch 77/125] [Batch 100/402] [D loss: 0.288718] [G loss: 0.192224] [ema: 0.999777] 
[Epoch 77/125] [Batch 200/402] [D loss: 0.302182] [G loss: 0.205803] [ema: 0.999778] 
[Epoch 77/125] [Batch 300/402] [D loss: 0.348507] [G loss: 0.210148] [ema: 0.999778] 
[Epoch 77/125] [Batch 400/402] [D loss: 0.322210] [G loss: 0.205833] [ema: 0.999779] 
[Epoch 78/125] [Batch 0/402] [D loss: 0.309844] [G loss: 0.200847] [ema: 0.999779] 
[Epoch 78/125] [Batch 100/402] [D loss: 0.317458] [G loss: 0.194537] [ema: 0.999780] 
[Epoch 78/125] [Batch 200/402] [D loss: 0.339980] [G loss: 0.221010] [ema: 0.999780] 
[Epoch 78/125] [Batch 300/402] [D loss: 0.313308] [G loss: 0.218544] [ema: 0.999781] 
[Epoch 78/125] [Batch 400/402] [D loss: 0.283869] [G loss: 0.210309] [ema: 0.999782] 
[Epoch 79/125] [Batch 0/402] [D loss: 0.343080] [G loss: 0.235974] [ema: 0.999782] 
[Epoch 79/125] [Batch 100/402] [D loss: 0.367105] [G loss: 0.228528] [ema: 0.999782] 
[Epoch 79/125] [Batch 200/402] [D loss: 0.320657] [G loss: 0.218020] [ema: 0.999783] 
[Epoch 79/125] [Batch 300/402] [D loss: 0.365046] [G loss: 0.189634] [ema: 0.999784] 
[Epoch 79/125] [Batch 400/402] [D loss: 0.324855] [G loss: 0.236465] [ema: 0.999784] 
[Epoch 80/125] [Batch 0/402] [D loss: 0.351239] [G loss: 0.185567] [ema: 0.999784] 
[Epoch 80/125] [Batch 100/402] [D loss: 0.296710] [G loss: 0.190658] [ema: 0.999785] 
[Epoch 80/125] [Batch 200/402] [D loss: 0.374464] [G loss: 0.218414] [ema: 0.999786] 
[Epoch 80/125] [Batch 300/402] [D loss: 0.301647] [G loss: 0.189874] [ema: 0.999786] 
[Epoch 80/125] [Batch 400/402] [D loss: 0.354208] [G loss: 0.222580] [ema: 0.999787] 
[Epoch 81/125] [Batch 0/402] [D loss: 0.311354] [G loss: 0.210521] [ema: 0.999787] 
[Epoch 81/125] [Batch 100/402] [D loss: 0.331760] [G loss: 0.232551] [ema: 0.999788] 
[Epoch 81/125] [Batch 200/402] [D loss: 0.352257] [G loss: 0.215565] [ema: 0.999788] 
[Epoch 81/125] [Batch 300/402] [D loss: 0.331471] [G loss: 0.207791] [ema: 0.999789] 
[Epoch 81/125] [Batch 400/402] [D loss: 0.307551] [G loss: 0.198788] [ema: 0.999790] 
[Epoch 82/125] [Batch 0/402] [D loss: 0.300701] [G loss: 0.221170] [ema: 0.999790] 
[Epoch 82/125] [Batch 100/402] [D loss: 0.347512] [G loss: 0.215854] [ema: 0.999790] 
[Epoch 82/125] [Batch 200/402] [D loss: 0.332415] [G loss: 0.227192] [ema: 0.999791] 
[Epoch 82/125] [Batch 300/402] [D loss: 0.343769] [G loss: 0.227893] [ema: 0.999792] 
[Epoch 82/125] [Batch 400/402] [D loss: 0.316525] [G loss: 0.229843] [ema: 0.999792] 
[Epoch 83/125] [Batch 0/402] [D loss: 0.312206] [G loss: 0.239946] [ema: 0.999792] 
[Epoch 83/125] [Batch 100/402] [D loss: 0.367393] [G loss: 0.232375] [ema: 0.999793] 
[Epoch 83/125] [Batch 200/402] [D loss: 0.394836] [G loss: 0.218097] [ema: 0.999794] 
[Epoch 83/125] [Batch 300/402] [D loss: 0.350863] [G loss: 0.220295] [ema: 0.999794] 
[Epoch 83/125] [Batch 400/402] [D loss: 0.347296] [G loss: 0.220004] [ema: 0.999795] 
[Epoch 84/125] [Batch 0/402] [D loss: 0.320226] [G loss: 0.245412] [ema: 0.999795] 
[Epoch 84/125] [Batch 100/402] [D loss: 0.382378] [G loss: 0.242690] [ema: 0.999795] 
[Epoch 84/125] [Batch 200/402] [D loss: 0.323191] [G loss: 0.204295] [ema: 0.999796] 
[Epoch 84/125] [Batch 300/402] [D loss: 0.376560] [G loss: 0.213106] [ema: 0.999797] 
[Epoch 84/125] [Batch 400/402] [D loss: 0.327113] [G loss: 0.205820] [ema: 0.999797] 
[Epoch 85/125] [Batch 0/402] [D loss: 0.349048] [G loss: 0.169230] [ema: 0.999797] 
[Epoch 85/125] [Batch 100/402] [D loss: 0.349422] [G loss: 0.215526] [ema: 0.999798] 
[Epoch 85/125] [Batch 200/402] [D loss: 0.289199] [G loss: 0.212992] [ema: 0.999798] 
[Epoch 85/125] [Batch 300/402] [D loss: 0.333800] [G loss: 0.188968] [ema: 0.999799] 
[Epoch 85/125] [Batch 400/402] [D loss: 0.360088] [G loss: 0.213796] [ema: 0.999800] 
[Epoch 86/125] [Batch 0/402] [D loss: 0.289555] [G loss: 0.235573] [ema: 0.999800] 
[Epoch 86/125] [Batch 100/402] [D loss: 0.299553] [G loss: 0.236655] [ema: 0.999800] 
[Epoch 86/125] [Batch 200/402] [D loss: 0.342965] [G loss: 0.228030] [ema: 0.999801] 
[Epoch 86/125] [Batch 300/402] [D loss: 0.313027] [G loss: 0.197586] [ema: 0.999801] 
[Epoch 86/125] [Batch 400/402] [D loss: 0.334615] [G loss: 0.215391] [ema: 0.999802] 
[Epoch 87/125] [Batch 0/402] [D loss: 0.397112] [G loss: 0.215460] [ema: 0.999802] 
[Epoch 87/125] [Batch 100/402] [D loss: 0.368138] [G loss: 0.232193] [ema: 0.999802] 
[Epoch 87/125] [Batch 200/402] [D loss: 0.399979] [G loss: 0.164077] [ema: 0.999803] 
[Epoch 87/125] [Batch 300/402] [D loss: 0.422456] [G loss: 0.235530] [ema: 0.999804] 
[Epoch 87/125] [Batch 400/402] [D loss: 0.354286] [G loss: 0.193614] [ema: 0.999804] 
[Epoch 88/125] [Batch 0/402] [D loss: 0.303752] [G loss: 0.202785] [ema: 0.999804] 
[Epoch 88/125] [Batch 100/402] [D loss: 0.307166] [G loss: 0.214571] [ema: 0.999805] 
[Epoch 88/125] [Batch 200/402] [D loss: 0.362132] [G loss: 0.218084] [ema: 0.999805] 
[Epoch 88/125] [Batch 300/402] [D loss: 0.363068] [G loss: 0.225680] [ema: 0.999806] 
[Epoch 88/125] [Batch 400/402] [D loss: 0.344697] [G loss: 0.201345] [ema: 0.999806] 
[Epoch 89/125] [Batch 0/402] [D loss: 0.352118] [G loss: 0.199115] [ema: 0.999806] 
[Epoch 89/125] [Batch 100/402] [D loss: 0.311394] [G loss: 0.200691] [ema: 0.999807] 
[Epoch 89/125] [Batch 200/402] [D loss: 0.325649] [G loss: 0.200858] [ema: 0.999807] 
[Epoch 89/125] [Batch 300/402] [D loss: 0.295854] [G loss: 0.236220] [ema: 0.999808] 
[Epoch 89/125] [Batch 400/402] [D loss: 0.392964] [G loss: 0.216142] [ema: 0.999808] 
[Epoch 90/125] [Batch 0/402] [D loss: 0.301861] [G loss: 0.246187] [ema: 0.999808] 
[Epoch 90/125] [Batch 100/402] [D loss: 0.315070] [G loss: 0.249345] [ema: 0.999809] 
[Epoch 90/125] [Batch 200/402] [D loss: 0.351361] [G loss: 0.212834] [ema: 0.999809] 
[Epoch 90/125] [Batch 300/402] [D loss: 0.291625] [G loss: 0.215467] [ema: 0.999810] 
[Epoch 90/125] [Batch 400/402] [D loss: 0.349590] [G loss: 0.192088] [ema: 0.999811] 
[Epoch 91/125] [Batch 0/402] [D loss: 0.339637] [G loss: 0.227296] [ema: 0.999811] 
[Epoch 91/125] [Batch 100/402] [D loss: 0.349290] [G loss: 0.198870] [ema: 0.999811] 
[Epoch 91/125] [Batch 200/402] [D loss: 0.312662] [G loss: 0.227146] [ema: 0.999812] 
[Epoch 91/125] [Batch 300/402] [D loss: 0.283824] [G loss: 0.232349] [ema: 0.999812] 
[Epoch 91/125] [Batch 400/402] [D loss: 0.296091] [G loss: 0.206732] [ema: 0.999813] 
[Epoch 92/125] [Batch 0/402] [D loss: 0.373929] [G loss: 0.204145] [ema: 0.999813] 
[Epoch 92/125] [Batch 100/402] [D loss: 0.328765] [G loss: 0.201819] [ema: 0.999813] 
[Epoch 92/125] [Batch 200/402] [D loss: 0.332571] [G loss: 0.245908] [ema: 0.999814] 
[Epoch 92/125] [Batch 300/402] [D loss: 0.372479] [G loss: 0.213185] [ema: 0.999814] 
[Epoch 92/125] [Batch 400/402] [D loss: 0.336098] [G loss: 0.234913] [ema: 0.999815] 
[Epoch 93/125] [Batch 0/402] [D loss: 0.328835] [G loss: 0.227406] [ema: 0.999815] 
[Epoch 93/125] [Batch 100/402] [D loss: 0.346299] [G loss: 0.201001] [ema: 0.999815] 
[Epoch 93/125] [Batch 200/402] [D loss: 0.306628] [G loss: 0.197661] [ema: 0.999816] 
[Epoch 93/125] [Batch 300/402] [D loss: 0.330588] [G loss: 0.199012] [ema: 0.999816] 
[Epoch 93/125] [Batch 400/402] [D loss: 0.336224] [G loss: 0.205039] [ema: 0.999817] 
[Epoch 94/125] [Batch 0/402] [D loss: 0.397036] [G loss: 0.230044] [ema: 0.999817] 
[Epoch 94/125] [Batch 100/402] [D loss: 0.337442] [G loss: 0.220747] [ema: 0.999817] 
[Epoch 94/125] [Batch 200/402] [D loss: 0.329505] [G loss: 0.217188] [ema: 0.999818] 
[Epoch 94/125] [Batch 300/402] [D loss: 0.356241] [G loss: 0.180087] [ema: 0.999818] 
[Epoch 94/125] [Batch 400/402] [D loss: 0.309577] [G loss: 0.224396] [ema: 0.999819] 
[Epoch 95/125] [Batch 0/402] [D loss: 0.324149] [G loss: 0.207047] [ema: 0.999819] 
[Epoch 95/125] [Batch 100/402] [D loss: 0.354845] [G loss: 0.207402] [ema: 0.999819] 
[Epoch 95/125] [Batch 200/402] [D loss: 0.288155] [G loss: 0.234845] [ema: 0.999819] 
[Epoch 95/125] [Batch 300/402] [D loss: 0.377105] [G loss: 0.179394] [ema: 0.999820] 
[Epoch 95/125] [Batch 400/402] [D loss: 0.318747] [G loss: 0.210898] [ema: 0.999820] 



Saving checkpoint 4 in logs/daghar_50000_60_100/upstairs_50000_D_60_2024_10_23_17_18_20/Model



[Epoch 96/125] [Batch 0/402] [D loss: 0.383333] [G loss: 0.232599] [ema: 0.999820] 
[Epoch 96/125] [Batch 100/402] [D loss: 0.317889] [G loss: 0.205790] [ema: 0.999821] 
[Epoch 96/125] [Batch 200/402] [D loss: 0.284986] [G loss: 0.232401] [ema: 0.999821] 
[Epoch 96/125] [Batch 300/402] [D loss: 0.333256] [G loss: 0.227152] [ema: 0.999822] 
[Epoch 96/125] [Batch 400/402] [D loss: 0.301544] [G loss: 0.202214] [ema: 0.999822] 
[Epoch 97/125] [Batch 0/402] [D loss: 0.401756] [G loss: 0.185241] [ema: 0.999822] 
[Epoch 97/125] [Batch 100/402] [D loss: 0.343377] [G loss: 0.217144] [ema: 0.999823] 
[Epoch 97/125] [Batch 200/402] [D loss: 0.317965] [G loss: 0.202420] [ema: 0.999823] 
[Epoch 97/125] [Batch 300/402] [D loss: 0.307313] [G loss: 0.181968] [ema: 0.999824] 
[Epoch 97/125] [Batch 400/402] [D loss: 0.328728] [G loss: 0.217780] [ema: 0.999824] 
[Epoch 98/125] [Batch 0/402] [D loss: 0.328561] [G loss: 0.204932] [ema: 0.999824] 
[Epoch 98/125] [Batch 100/402] [D loss: 0.356915] [G loss: 0.219136] [ema: 0.999825] 
[Epoch 98/125] [Batch 200/402] [D loss: 0.346995] [G loss: 0.231841] [ema: 0.999825] 
[Epoch 98/125] [Batch 300/402] [D loss: 0.388265] [G loss: 0.201568] [ema: 0.999825] 
[Epoch 98/125] [Batch 400/402] [D loss: 0.284681] [G loss: 0.221916] [ema: 0.999826] 
[Epoch 99/125] [Batch 0/402] [D loss: 0.298399] [G loss: 0.204163] [ema: 0.999826] 
[Epoch 99/125] [Batch 100/402] [D loss: 0.362071] [G loss: 0.217322] [ema: 0.999826] 
[Epoch 99/125] [Batch 200/402] [D loss: 0.386386] [G loss: 0.231045] [ema: 0.999827] 
[Epoch 99/125] [Batch 300/402] [D loss: 0.355112] [G loss: 0.209066] [ema: 0.999827] 
[Epoch 99/125] [Batch 400/402] [D loss: 0.343714] [G loss: 0.232374] [ema: 0.999828] 
[Epoch 100/125] [Batch 0/402] [D loss: 0.309185] [G loss: 0.223767] [ema: 0.999828] 
[Epoch 100/125] [Batch 100/402] [D loss: 0.333885] [G loss: 0.223282] [ema: 0.999828] 
[Epoch 100/125] [Batch 200/402] [D loss: 0.332736] [G loss: 0.226275] [ema: 0.999828] 
[Epoch 100/125] [Batch 300/402] [D loss: 0.298523] [G loss: 0.196117] [ema: 0.999829] 
[Epoch 100/125] [Batch 400/402] [D loss: 0.294115] [G loss: 0.221461] [ema: 0.999829] 
[Epoch 101/125] [Batch 0/402] [D loss: 0.370670] [G loss: 0.227981] [ema: 0.999829] 
[Epoch 101/125] [Batch 100/402] [D loss: 0.330566] [G loss: 0.229137] [ema: 0.999830] 
[Epoch 101/125] [Batch 200/402] [D loss: 0.320123] [G loss: 0.196149] [ema: 0.999830] 
[Epoch 101/125] [Batch 300/402] [D loss: 0.365893] [G loss: 0.208320] [ema: 0.999831] 
[Epoch 101/125] [Batch 400/402] [D loss: 0.355084] [G loss: 0.219384] [ema: 0.999831] 
[Epoch 102/125] [Batch 0/402] [D loss: 0.317787] [G loss: 0.224703] [ema: 0.999831] 
[Epoch 102/125] [Batch 100/402] [D loss: 0.314666] [G loss: 0.255725] [ema: 0.999831] 
[Epoch 102/125] [Batch 200/402] [D loss: 0.296779] [G loss: 0.193223] [ema: 0.999832] 
[Epoch 102/125] [Batch 300/402] [D loss: 0.332196] [G loss: 0.237514] [ema: 0.999832] 
[Epoch 102/125] [Batch 400/402] [D loss: 0.334427] [G loss: 0.200447] [ema: 0.999833] 
[Epoch 103/125] [Batch 0/402] [D loss: 0.324747] [G loss: 0.226995] [ema: 0.999833] 
[Epoch 103/125] [Batch 100/402] [D loss: 0.358183] [G loss: 0.213982] [ema: 0.999833] 
[Epoch 103/125] [Batch 200/402] [D loss: 0.323259] [G loss: 0.224002] [ema: 0.999833] 
[Epoch 103/125] [Batch 300/402] [D loss: 0.282926] [G loss: 0.199586] [ema: 0.999834] 
[Epoch 103/125] [Batch 400/402] [D loss: 0.344190] [G loss: 0.232683] [ema: 0.999834] 
[Epoch 104/125] [Batch 0/402] [D loss: 0.370948] [G loss: 0.196449] [ema: 0.999834] 
[Epoch 104/125] [Batch 100/402] [D loss: 0.311710] [G loss: 0.220277] [ema: 0.999835] 
[Epoch 104/125] [Batch 200/402] [D loss: 0.308240] [G loss: 0.232312] [ema: 0.999835] 
[Epoch 104/125] [Batch 300/402] [D loss: 0.346439] [G loss: 0.221939] [ema: 0.999835] 
[Epoch 104/125] [Batch 400/402] [D loss: 0.352713] [G loss: 0.206180] [ema: 0.999836] 
[Epoch 105/125] [Batch 0/402] [D loss: 0.349518] [G loss: 0.209053] [ema: 0.999836] 
[Epoch 105/125] [Batch 100/402] [D loss: 0.338207] [G loss: 0.213416] [ema: 0.999836] 
[Epoch 105/125] [Batch 200/402] [D loss: 0.297001] [G loss: 0.239108] [ema: 0.999837] 
[Epoch 105/125] [Batch 300/402] [D loss: 0.289816] [G loss: 0.217014] [ema: 0.999837] 
[Epoch 105/125] [Batch 400/402] [D loss: 0.306705] [G loss: 0.198013] [ema: 0.999837] 
[Epoch 106/125] [Batch 0/402] [D loss: 0.326748] [G loss: 0.245419] [ema: 0.999837] 
[Epoch 106/125] [Batch 100/402] [D loss: 0.278541] [G loss: 0.199205] [ema: 0.999838] 
[Epoch 106/125] [Batch 200/402] [D loss: 0.358587] [G loss: 0.213132] [ema: 0.999838] 
[Epoch 106/125] [Batch 300/402] [D loss: 0.321671] [G loss: 0.231021] [ema: 0.999838] 
[Epoch 106/125] [Batch 400/402] [D loss: 0.269610] [G loss: 0.248469] [ema: 0.999839] 
[Epoch 107/125] [Batch 0/402] [D loss: 0.336995] [G loss: 0.239155] [ema: 0.999839] 
[Epoch 107/125] [Batch 100/402] [D loss: 0.301060] [G loss: 0.228269] [ema: 0.999839] 
[Epoch 107/125] [Batch 200/402] [D loss: 0.321150] [G loss: 0.189730] [ema: 0.999840] 
[Epoch 107/125] [Batch 300/402] [D loss: 0.380765] [G loss: 0.234309] [ema: 0.999840] 
[Epoch 107/125] [Batch 400/402] [D loss: 0.321662] [G loss: 0.196699] [ema: 0.999840] 
[Epoch 108/125] [Batch 0/402] [D loss: 0.349375] [G loss: 0.237617] [ema: 0.999840] 
[Epoch 108/125] [Batch 100/402] [D loss: 0.368622] [G loss: 0.224898] [ema: 0.999841] 
[Epoch 108/125] [Batch 200/402] [D loss: 0.342116] [G loss: 0.210578] [ema: 0.999841] 
[Epoch 108/125] [Batch 300/402] [D loss: 0.335988] [G loss: 0.235945] [ema: 0.999841] 
[Epoch 108/125] [Batch 400/402] [D loss: 0.362660] [G loss: 0.239559] [ema: 0.999842] 
[Epoch 109/125] [Batch 0/402] [D loss: 0.285268] [G loss: 0.262676] [ema: 0.999842] 
[Epoch 109/125] [Batch 100/402] [D loss: 0.286029] [G loss: 0.204641] [ema: 0.999842] 
[Epoch 109/125] [Batch 200/402] [D loss: 0.300838] [G loss: 0.194578] [ema: 0.999843] 
[Epoch 109/125] [Batch 300/402] [D loss: 0.330221] [G loss: 0.241465] [ema: 0.999843] 
[Epoch 109/125] [Batch 400/402] [D loss: 0.322858] [G loss: 0.219978] [ema: 0.999843] 
[Epoch 110/125] [Batch 0/402] [D loss: 0.336512] [G loss: 0.219560] [ema: 0.999843] 
[Epoch 110/125] [Batch 100/402] [D loss: 0.336216] [G loss: 0.232655] [ema: 0.999844] 
[Epoch 110/125] [Batch 200/402] [D loss: 0.324366] [G loss: 0.251680] [ema: 0.999844] 
[Epoch 110/125] [Batch 300/402] [D loss: 0.373950] [G loss: 0.244618] [ema: 0.999844] 
[Epoch 110/125] [Batch 400/402] [D loss: 0.314485] [G loss: 0.209829] [ema: 0.999845] 
[Epoch 111/125] [Batch 0/402] [D loss: 0.347185] [G loss: 0.218416] [ema: 0.999845] 
[Epoch 111/125] [Batch 100/402] [D loss: 0.365871] [G loss: 0.183212] [ema: 0.999845] 
[Epoch 111/125] [Batch 200/402] [D loss: 0.343179] [G loss: 0.204688] [ema: 0.999845] 
[Epoch 111/125] [Batch 300/402] [D loss: 0.273500] [G loss: 0.235196] [ema: 0.999846] 
[Epoch 111/125] [Batch 400/402] [D loss: 0.393964] [G loss: 0.197574] [ema: 0.999846] 
[Epoch 112/125] [Batch 0/402] [D loss: 0.342217] [G loss: 0.205193] [ema: 0.999846] 
[Epoch 112/125] [Batch 100/402] [D loss: 0.267265] [G loss: 0.210457] [ema: 0.999846] 
[Epoch 112/125] [Batch 200/402] [D loss: 0.327978] [G loss: 0.246137] [ema: 0.999847] 
[Epoch 112/125] [Batch 300/402] [D loss: 0.335146] [G loss: 0.245088] [ema: 0.999847] 
[Epoch 112/125] [Batch 400/402] [D loss: 0.370687] [G loss: 0.227696] [ema: 0.999847] 
[Epoch 113/125] [Batch 0/402] [D loss: 0.290113] [G loss: 0.196251] [ema: 0.999847] 
[Epoch 113/125] [Batch 100/402] [D loss: 0.400755] [G loss: 0.207635] [ema: 0.999848] 
[Epoch 113/125] [Batch 200/402] [D loss: 0.313600] [G loss: 0.226536] [ema: 0.999848] 
[Epoch 113/125] [Batch 300/402] [D loss: 0.318264] [G loss: 0.174957] [ema: 0.999848] 
[Epoch 113/125] [Batch 400/402] [D loss: 0.311519] [G loss: 0.219435] [ema: 0.999849] 
[Epoch 114/125] [Batch 0/402] [D loss: 0.291160] [G loss: 0.207366] [ema: 0.999849] 
[Epoch 114/125] [Batch 100/402] [D loss: 0.337357] [G loss: 0.225290] [ema: 0.999849] 
[Epoch 114/125] [Batch 200/402] [D loss: 0.397057] [G loss: 0.207468] [ema: 0.999849] 
[Epoch 114/125] [Batch 300/402] [D loss: 0.318400] [G loss: 0.204531] [ema: 0.999850] 
[Epoch 114/125] [Batch 400/402] [D loss: 0.451433] [G loss: 0.207878] [ema: 0.999850] 
[Epoch 115/125] [Batch 0/402] [D loss: 0.328863] [G loss: 0.204801] [ema: 0.999850] 
[Epoch 115/125] [Batch 100/402] [D loss: 0.323184] [G loss: 0.206329] [ema: 0.999850] 
[Epoch 115/125] [Batch 200/402] [D loss: 0.323825] [G loss: 0.213627] [ema: 0.999851] 
[Epoch 115/125] [Batch 300/402] [D loss: 0.374255] [G loss: 0.201391] [ema: 0.999851] 
[Epoch 115/125] [Batch 400/402] [D loss: 0.334489] [G loss: 0.213048] [ema: 0.999851] 
[Epoch 116/125] [Batch 0/402] [D loss: 0.412965] [G loss: 0.251342] [ema: 0.999851] 
[Epoch 116/125] [Batch 100/402] [D loss: 0.329316] [G loss: 0.190369] [ema: 0.999852] 
[Epoch 116/125] [Batch 200/402] [D loss: 0.333485] [G loss: 0.246884] [ema: 0.999852] 
[Epoch 116/125] [Batch 300/402] [D loss: 0.316110] [G loss: 0.189739] [ema: 0.999852] 
[Epoch 116/125] [Batch 400/402] [D loss: 0.332945] [G loss: 0.179220] [ema: 0.999853] 
[Epoch 117/125] [Batch 0/402] [D loss: 0.311573] [G loss: 0.212603] [ema: 0.999853] 
[Epoch 117/125] [Batch 100/402] [D loss: 0.354381] [G loss: 0.206837] [ema: 0.999853] 
[Epoch 117/125] [Batch 200/402] [D loss: 0.348319] [G loss: 0.224135] [ema: 0.999853] 
[Epoch 117/125] [Batch 300/402] [D loss: 0.314646] [G loss: 0.220398] [ema: 0.999854] 
[Epoch 117/125] [Batch 400/402] [D loss: 0.343285] [G loss: 0.227577] [ema: 0.999854] 
[Epoch 118/125] [Batch 0/402] [D loss: 0.327717] [G loss: 0.223629] [ema: 0.999854] 
[Epoch 118/125] [Batch 100/402] [D loss: 0.341225] [G loss: 0.237187] [ema: 0.999854] 
[Epoch 118/125] [Batch 200/402] [D loss: 0.276408] [G loss: 0.213033] [ema: 0.999855] 
[Epoch 118/125] [Batch 300/402] [D loss: 0.324209] [G loss: 0.181925] [ema: 0.999855] 
[Epoch 118/125] [Batch 400/402] [D loss: 0.285373] [G loss: 0.236861] [ema: 0.999855] 
[Epoch 119/125] [Batch 0/402] [D loss: 0.288116] [G loss: 0.228892] [ema: 0.999855] 
[Epoch 119/125] [Batch 100/402] [D loss: 0.379485] [G loss: 0.188414] [ema: 0.999855] 
[Epoch 119/125] [Batch 200/402] [D loss: 0.320595] [G loss: 0.208580] [ema: 0.999856] 
[Epoch 119/125] [Batch 300/402] [D loss: 0.349855] [G loss: 0.227631] [ema: 0.999856] 
[Epoch 119/125] [Batch 400/402] [D loss: 0.389536] [G loss: 0.202848] [ema: 0.999856] 
[Epoch 120/125] [Batch 0/402] [D loss: 0.302393] [G loss: 0.232157] [ema: 0.999856] 
[Epoch 120/125] [Batch 100/402] [D loss: 0.351838] [G loss: 0.235397] [ema: 0.999857] 
[Epoch 120/125] [Batch 200/402] [D loss: 0.304695] [G loss: 0.210644] [ema: 0.999857] 
[Epoch 120/125] [Batch 300/402] [D loss: 0.438263] [G loss: 0.228048] [ema: 0.999857] 
[Epoch 120/125] [Batch 400/402] [D loss: 0.347060] [G loss: 0.201890] [ema: 0.999858] 
[Epoch 121/125] [Batch 0/402] [D loss: 0.282484] [G loss: 0.214352] [ema: 0.999858] 
[Epoch 121/125] [Batch 100/402] [D loss: 0.377836] [G loss: 0.211309] [ema: 0.999858] 
[Epoch 121/125] [Batch 200/402] [D loss: 0.361214] [G loss: 0.244489] [ema: 0.999858] 
[Epoch 121/125] [Batch 300/402] [D loss: 0.270498] [G loss: 0.213431] [ema: 0.999858] 
[Epoch 121/125] [Batch 400/402] [D loss: 0.353644] [G loss: 0.205884] [ema: 0.999859] 
[Epoch 122/125] [Batch 0/402] [D loss: 0.361255] [G loss: 0.219314] [ema: 0.999859] 
[Epoch 122/125] [Batch 100/402] [D loss: 0.292012] [G loss: 0.250577] [ema: 0.999859] 
[Epoch 122/125] [Batch 200/402] [D loss: 0.330628] [G loss: 0.193836] [ema: 0.999859] 
[Epoch 122/125] [Batch 300/402] [D loss: 0.330438] [G loss: 0.204005] [ema: 0.999860] 
[Epoch 122/125] [Batch 400/402] [D loss: 0.328391] [G loss: 0.228431] [ema: 0.999860] 
[Epoch 123/125] [Batch 0/402] [D loss: 0.339887] [G loss: 0.198684] [ema: 0.999860] 
[Epoch 123/125] [Batch 100/402] [D loss: 0.359950] [G loss: 0.228498] [ema: 0.999860] 
[Epoch 123/125] [Batch 200/402] [D loss: 0.293171] [G loss: 0.233415] [ema: 0.999860] 
[Epoch 123/125] [Batch 300/402] [D loss: 0.314863] [G loss: 0.192429] [ema: 0.999861] 
[Epoch 123/125] [Batch 400/402] [D loss: 0.360269] [G loss: 0.187120] [ema: 0.999861] 
[Epoch 124/125] [Batch 0/402] [D loss: 0.338963] [G loss: 0.204281] [ema: 0.999861] 
[Epoch 124/125] [Batch 100/402] [D loss: 0.331877] [G loss: 0.186011] [ema: 0.999861] 
[Epoch 124/125] [Batch 200/402] [D loss: 0.330005] [G loss: 0.206091] [ema: 0.999862] 
[Epoch 124/125] [Batch 300/402] [D loss: 0.331256] [G loss: 0.217823] [ema: 0.999862] 
[Epoch 124/125] [Batch 400/402] [D loss: 0.369426] [G loss: 0.220782] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
run training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
run
daghar
return single class data and labels, class is run
data shape is (8130, 3, 1, 60)
label shape is (8130,)
509
Epochs between checkpoint: 25



Saving checkpoint 1 in logs/daghar_50000_60_100/run_50000_D_60_2024_10_23_17_53_34/Model



[Epoch 0/99] [Batch 0/509] [D loss: 1.190125] [G loss: 0.519553] [ema: 0.000000] 
[Epoch 0/99] [Batch 100/509] [D loss: 0.495277] [G loss: 0.198377] [ema: 0.933033] 
[Epoch 0/99] [Batch 200/509] [D loss: 0.429797] [G loss: 0.201655] [ema: 0.965936] 
[Epoch 0/99] [Batch 300/509] [D loss: 0.353766] [G loss: 0.203613] [ema: 0.977160] 
[Epoch 0/99] [Batch 400/509] [D loss: 0.336299] [G loss: 0.227201] [ema: 0.982821] 
[Epoch 0/99] [Batch 500/509] [D loss: 0.301741] [G loss: 0.224767] [ema: 0.986233] 
[Epoch 1/99] [Batch 0/509] [D loss: 0.323923] [G loss: 0.210272] [ema: 0.986474] 
[Epoch 1/99] [Batch 100/509] [D loss: 0.370709] [G loss: 0.197170] [ema: 0.988683] 
[Epoch 1/99] [Batch 200/509] [D loss: 0.321605] [G loss: 0.256438] [ema: 0.990271] 
[Epoch 1/99] [Batch 300/509] [D loss: 0.379437] [G loss: 0.224968] [ema: 0.991469] 
[Epoch 1/99] [Batch 400/509] [D loss: 0.312443] [G loss: 0.222762] [ema: 0.992404] 
[Epoch 1/99] [Batch 500/509] [D loss: 0.401318] [G loss: 0.227008] [ema: 0.993154] 
[Epoch 2/99] [Batch 0/509] [D loss: 0.305164] [G loss: 0.232675] [ema: 0.993214] 
[Epoch 2/99] [Batch 100/509] [D loss: 0.385206] [G loss: 0.213901] [ema: 0.993819] 
[Epoch 2/99] [Batch 200/509] [D loss: 0.365443] [G loss: 0.225124] [ema: 0.994325] 
[Epoch 2/99] [Batch 300/509] [D loss: 0.398411] [G loss: 0.159057] [ema: 0.994755] 
[Epoch 2/99] [Batch 400/509] [D loss: 0.384136] [G loss: 0.199757] [ema: 0.995124] 
[Epoch 2/99] [Batch 500/509] [D loss: 0.437307] [G loss: 0.201027] [ema: 0.995444] 
[Epoch 3/99] [Batch 0/509] [D loss: 0.405160] [G loss: 0.204290] [ema: 0.995471] 
[Epoch 3/99] [Batch 100/509] [D loss: 0.397590] [G loss: 0.198714] [ema: 0.995749] 
[Epoch 3/99] [Batch 200/509] [D loss: 0.327483] [G loss: 0.222414] [ema: 0.995994] 
[Epoch 3/99] [Batch 300/509] [D loss: 0.430235] [G loss: 0.195764] [ema: 0.996213] 
[Epoch 3/99] [Batch 400/509] [D loss: 0.457672] [G loss: 0.193375] [ema: 0.996409] 
[Epoch 3/99] [Batch 500/509] [D loss: 0.431608] [G loss: 0.177958] [ema: 0.996586] 
[Epoch 4/99] [Batch 0/509] [D loss: 0.452675] [G loss: 0.187629] [ema: 0.996601] 
[Epoch 4/99] [Batch 100/509] [D loss: 0.419599] [G loss: 0.212995] [ema: 0.996760] 
[Epoch 4/99] [Batch 200/509] [D loss: 0.433967] [G loss: 0.164374] [ema: 0.996905] 
[Epoch 4/99] [Batch 300/509] [D loss: 0.350886] [G loss: 0.183295] [ema: 0.997037] 
[Epoch 4/99] [Batch 400/509] [D loss: 0.395018] [G loss: 0.192685] [ema: 0.997159] 
[Epoch 4/99] [Batch 500/509] [D loss: 0.323480] [G loss: 0.153415] [ema: 0.997271] 
[Epoch 5/99] [Batch 0/509] [D loss: 0.324330] [G loss: 0.261982] [ema: 0.997280] 
[Epoch 5/99] [Batch 100/509] [D loss: 0.375019] [G loss: 0.187629] [ema: 0.997383] 
[Epoch 5/99] [Batch 200/509] [D loss: 0.372213] [G loss: 0.166743] [ema: 0.997478] 
[Epoch 5/99] [Batch 300/509] [D loss: 0.408675] [G loss: 0.166768] [ema: 0.997567] 
[Epoch 5/99] [Batch 400/509] [D loss: 0.324163] [G loss: 0.223729] [ema: 0.997649] 
[Epoch 5/99] [Batch 500/509] [D loss: 0.350717] [G loss: 0.173663] [ema: 0.997726] 
[Epoch 6/99] [Batch 0/509] [D loss: 0.389438] [G loss: 0.181491] [ema: 0.997733] 
[Epoch 6/99] [Batch 100/509] [D loss: 0.338454] [G loss: 0.209349] [ema: 0.997805] 
[Epoch 6/99] [Batch 200/509] [D loss: 0.370571] [G loss: 0.162023] [ema: 0.997872] 
[Epoch 6/99] [Batch 300/509] [D loss: 0.465392] [G loss: 0.159415] [ema: 0.997936] 
[Epoch 6/99] [Batch 400/509] [D loss: 0.432845] [G loss: 0.205434] [ema: 0.997995] 
[Epoch 6/99] [Batch 500/509] [D loss: 0.380298] [G loss: 0.234728] [ema: 0.998052] 
[Epoch 7/99] [Batch 0/509] [D loss: 0.390972] [G loss: 0.164455] [ema: 0.998056] 
[Epoch 7/99] [Batch 100/509] [D loss: 0.392936] [G loss: 0.202579] [ema: 0.998109] 
[Epoch 7/99] [Batch 200/509] [D loss: 0.411794] [G loss: 0.197901] [ema: 0.998160] 
[Epoch 7/99] [Batch 300/509] [D loss: 0.379896] [G loss: 0.174288] [ema: 0.998207] 
[Epoch 7/99] [Batch 400/509] [D loss: 0.383169] [G loss: 0.193510] [ema: 0.998252] 
[Epoch 7/99] [Batch 500/509] [D loss: 0.380318] [G loss: 0.179293] [ema: 0.998295] 
[Epoch 8/99] [Batch 0/509] [D loss: 0.394076] [G loss: 0.222097] [ema: 0.998299] 
[Epoch 8/99] [Batch 100/509] [D loss: 0.369840] [G loss: 0.170881] [ema: 0.998340] 
[Epoch 8/99] [Batch 200/509] [D loss: 0.498396] [G loss: 0.209515] [ema: 0.998379] 
[Epoch 8/99] [Batch 300/509] [D loss: 0.442811] [G loss: 0.198080] [ema: 0.998416] 
[Epoch 8/99] [Batch 400/509] [D loss: 0.395220] [G loss: 0.206617] [ema: 0.998451] 
[Epoch 8/99] [Batch 500/509] [D loss: 0.493696] [G loss: 0.192263] [ema: 0.998485] 
[Epoch 9/99] [Batch 0/509] [D loss: 0.459379] [G loss: 0.143595] [ema: 0.998488] 
[Epoch 9/99] [Batch 100/509] [D loss: 0.388177] [G loss: 0.186875] [ema: 0.998520] 
[Epoch 9/99] [Batch 200/509] [D loss: 0.393464] [G loss: 0.169776] [ema: 0.998551] 
[Epoch 9/99] [Batch 300/509] [D loss: 0.435165] [G loss: 0.196038] [ema: 0.998581] 
[Epoch 9/99] [Batch 400/509] [D loss: 0.422545] [G loss: 0.196736] [ema: 0.998609] 
[Epoch 9/99] [Batch 500/509] [D loss: 0.474579] [G loss: 0.205515] [ema: 0.998637] 
[Epoch 10/99] [Batch 0/509] [D loss: 0.435948] [G loss: 0.157824] [ema: 0.998639] 
[Epoch 10/99] [Batch 100/509] [D loss: 0.432464] [G loss: 0.183295] [ema: 0.998665] 
[Epoch 10/99] [Batch 200/509] [D loss: 0.373185] [G loss: 0.201996] [ema: 0.998691] 
[Epoch 10/99] [Batch 300/509] [D loss: 0.372474] [G loss: 0.215506] [ema: 0.998715] 
[Epoch 10/99] [Batch 400/509] [D loss: 0.329816] [G loss: 0.207061] [ema: 0.998738] 
[Epoch 10/99] [Batch 500/509] [D loss: 0.431741] [G loss: 0.183683] [ema: 0.998761] 
[Epoch 11/99] [Batch 0/509] [D loss: 0.451266] [G loss: 0.190022] [ema: 0.998763] 
[Epoch 11/99] [Batch 100/509] [D loss: 0.430695] [G loss: 0.175005] [ema: 0.998784] 
[Epoch 11/99] [Batch 200/509] [D loss: 0.507183] [G loss: 0.137924] [ema: 0.998805] 
[Epoch 11/99] [Batch 300/509] [D loss: 0.391840] [G loss: 0.154710] [ema: 0.998826] 
[Epoch 11/99] [Batch 400/509] [D loss: 0.456806] [G loss: 0.181349] [ema: 0.998845] 
[Epoch 11/99] [Batch 500/509] [D loss: 0.387256] [G loss: 0.175275] [ema: 0.998864] 
[Epoch 12/99] [Batch 0/509] [D loss: 0.514764] [G loss: 0.183110] [ema: 0.998866] 
[Epoch 12/99] [Batch 100/509] [D loss: 0.411467] [G loss: 0.145987] [ema: 0.998884] 
[Epoch 12/99] [Batch 200/509] [D loss: 0.363104] [G loss: 0.207679] [ema: 0.998902] 
[Epoch 12/99] [Batch 300/509] [D loss: 0.328074] [G loss: 0.186356] [ema: 0.998919] 
[Epoch 12/99] [Batch 400/509] [D loss: 0.396643] [G loss: 0.188429] [ema: 0.998935] 
[Epoch 12/99] [Batch 500/509] [D loss: 0.449713] [G loss: 0.165823] [ema: 0.998952] 
[Epoch 13/99] [Batch 0/509] [D loss: 0.472010] [G loss: 0.169860] [ema: 0.998953] 
[Epoch 13/99] [Batch 100/509] [D loss: 0.359416] [G loss: 0.186173] [ema: 0.998969] 
[Epoch 13/99] [Batch 200/509] [D loss: 0.413350] [G loss: 0.234632] [ema: 0.998984] 
[Epoch 13/99] [Batch 300/509] [D loss: 0.477213] [G loss: 0.175578] [ema: 0.998998] 
[Epoch 13/99] [Batch 400/509] [D loss: 0.382320] [G loss: 0.172842] [ema: 0.999013] 
[Epoch 13/99] [Batch 500/509] [D loss: 0.367655] [G loss: 0.183809] [ema: 0.999027] 
[Epoch 14/99] [Batch 0/509] [D loss: 0.342571] [G loss: 0.216181] [ema: 0.999028] 
[Epoch 14/99] [Batch 100/509] [D loss: 0.337209] [G loss: 0.160288] [ema: 0.999041] 
[Epoch 14/99] [Batch 200/509] [D loss: 0.407648] [G loss: 0.204106] [ema: 0.999054] 
[Epoch 14/99] [Batch 300/509] [D loss: 0.474701] [G loss: 0.196473] [ema: 0.999067] 
[Epoch 14/99] [Batch 400/509] [D loss: 0.421840] [G loss: 0.191967] [ema: 0.999079] 
[Epoch 14/99] [Batch 500/509] [D loss: 0.366757] [G loss: 0.187751] [ema: 0.999091] 
[Epoch 15/99] [Batch 0/509] [D loss: 0.368077] [G loss: 0.221511] [ema: 0.999093] 
[Epoch 15/99] [Batch 100/509] [D loss: 0.368740] [G loss: 0.204137] [ema: 0.999104] 
[Epoch 15/99] [Batch 200/509] [D loss: 0.355568] [G loss: 0.203906] [ema: 0.999116] 
[Epoch 15/99] [Batch 300/509] [D loss: 0.448934] [G loss: 0.173899] [ema: 0.999127] 
[Epoch 15/99] [Batch 400/509] [D loss: 0.299338] [G loss: 0.200783] [ema: 0.999138] 
[Epoch 15/99] [Batch 500/509] [D loss: 0.433587] [G loss: 0.171315] [ema: 0.999148] 
[Epoch 16/99] [Batch 0/509] [D loss: 0.377517] [G loss: 0.183704] [ema: 0.999149] 
[Epoch 16/99] [Batch 100/509] [D loss: 0.372213] [G loss: 0.185284] [ema: 0.999160] 
[Epoch 16/99] [Batch 200/509] [D loss: 0.397109] [G loss: 0.203838] [ema: 0.999170] 
[Epoch 16/99] [Batch 300/509] [D loss: 0.435846] [G loss: 0.187624] [ema: 0.999179] 
[Epoch 16/99] [Batch 400/509] [D loss: 0.379863] [G loss: 0.187754] [ema: 0.999189] 
[Epoch 16/99] [Batch 500/509] [D loss: 0.374744] [G loss: 0.199239] [ema: 0.999198] 
[Epoch 17/99] [Batch 0/509] [D loss: 0.338541] [G loss: 0.197891] [ema: 0.999199] 
[Epoch 17/99] [Batch 100/509] [D loss: 0.352756] [G loss: 0.172289] [ema: 0.999208] 
[Epoch 17/99] [Batch 200/509] [D loss: 0.388602] [G loss: 0.186663] [ema: 0.999217] 
[Epoch 17/99] [Batch 300/509] [D loss: 0.384462] [G loss: 0.217609] [ema: 0.999226] 
[Epoch 17/99] [Batch 400/509] [D loss: 0.365510] [G loss: 0.161185] [ema: 0.999235] 
[Epoch 17/99] [Batch 500/509] [D loss: 0.423499] [G loss: 0.209087] [ema: 0.999243] 
[Epoch 18/99] [Batch 0/509] [D loss: 0.416124] [G loss: 0.182787] [ema: 0.999244] 
[Epoch 18/99] [Batch 100/509] [D loss: 0.325759] [G loss: 0.222127] [ema: 0.999252] 
[Epoch 18/99] [Batch 200/509] [D loss: 0.342756] [G loss: 0.208139] [ema: 0.999260] 
[Epoch 18/99] [Batch 300/509] [D loss: 0.372565] [G loss: 0.182467] [ema: 0.999268] 
[Epoch 18/99] [Batch 400/509] [D loss: 0.344140] [G loss: 0.199877] [ema: 0.999275] 
[Epoch 18/99] [Batch 500/509] [D loss: 0.322351] [G loss: 0.211000] [ema: 0.999283] 
[Epoch 19/99] [Batch 0/509] [D loss: 0.347948] [G loss: 0.165200] [ema: 0.999284] 
[Epoch 19/99] [Batch 100/509] [D loss: 0.305477] [G loss: 0.220847] [ema: 0.999291] 
[Epoch 19/99] [Batch 200/509] [D loss: 0.408057] [G loss: 0.188577] [ema: 0.999298] 
[Epoch 19/99] [Batch 300/509] [D loss: 0.387246] [G loss: 0.189828] [ema: 0.999305] 
[Epoch 19/99] [Batch 400/509] [D loss: 0.445770] [G loss: 0.167161] [ema: 0.999312] 
[Epoch 19/99] [Batch 500/509] [D loss: 0.339313] [G loss: 0.200517] [ema: 0.999319] 
[Epoch 20/99] [Batch 0/509] [D loss: 0.412158] [G loss: 0.246438] [ema: 0.999319] 
[Epoch 20/99] [Batch 100/509] [D loss: 0.347749] [G loss: 0.191458] [ema: 0.999326] 
[Epoch 20/99] [Batch 200/509] [D loss: 0.382529] [G loss: 0.243140] [ema: 0.999332] 
[Epoch 20/99] [Batch 300/509] [D loss: 0.380889] [G loss: 0.230254] [ema: 0.999339] 
[Epoch 20/99] [Batch 400/509] [D loss: 0.445271] [G loss: 0.218378] [ema: 0.999345] 
[Epoch 20/99] [Batch 500/509] [D loss: 0.326873] [G loss: 0.242044] [ema: 0.999351] 
[Epoch 21/99] [Batch 0/509] [D loss: 0.351229] [G loss: 0.218187] [ema: 0.999352] 
[Epoch 21/99] [Batch 100/509] [D loss: 0.367644] [G loss: 0.211321] [ema: 0.999358] 
[Epoch 21/99] [Batch 200/509] [D loss: 0.438074] [G loss: 0.173038] [ema: 0.999364] 
[Epoch 21/99] [Batch 300/509] [D loss: 0.301381] [G loss: 0.209417] [ema: 0.999369] 
[Epoch 21/99] [Batch 400/509] [D loss: 0.360771] [G loss: 0.171610] [ema: 0.999375] 
[Epoch 21/99] [Batch 500/509] [D loss: 0.438209] [G loss: 0.205394] [ema: 0.999381] 
[Epoch 22/99] [Batch 0/509] [D loss: 0.378478] [G loss: 0.217247] [ema: 0.999381] 
[Epoch 22/99] [Batch 100/509] [D loss: 0.353946] [G loss: 0.187248] [ema: 0.999387] 
[Epoch 22/99] [Batch 200/509] [D loss: 0.313621] [G loss: 0.209340] [ema: 0.999392] 
[Epoch 22/99] [Batch 300/509] [D loss: 0.320436] [G loss: 0.214126] [ema: 0.999397] 
[Epoch 22/99] [Batch 400/509] [D loss: 0.396799] [G loss: 0.208141] [ema: 0.999403] 
[Epoch 22/99] [Batch 500/509] [D loss: 0.343212] [G loss: 0.187967] [ema: 0.999408] 
[Epoch 23/99] [Batch 0/509] [D loss: 0.351617] [G loss: 0.201911] [ema: 0.999408] 
[Epoch 23/99] [Batch 100/509] [D loss: 0.371469] [G loss: 0.195750] [ema: 0.999413] 
[Epoch 23/99] [Batch 200/509] [D loss: 0.305032] [G loss: 0.220642] [ema: 0.999418] 
[Epoch 23/99] [Batch 300/509] [D loss: 0.385960] [G loss: 0.163503] [ema: 0.999423] 
[Epoch 23/99] [Batch 400/509] [D loss: 0.359813] [G loss: 0.195923] [ema: 0.999428] 
[Epoch 23/99] [Batch 500/509] [D loss: 0.374884] [G loss: 0.211754] [ema: 0.999432] 
[Epoch 24/99] [Batch 0/509] [D loss: 0.295051] [G loss: 0.218302] [ema: 0.999433] 
[Epoch 24/99] [Batch 100/509] [D loss: 0.337734] [G loss: 0.223873] [ema: 0.999437] 
[Epoch 24/99] [Batch 200/509] [D loss: 0.352232] [G loss: 0.230789] [ema: 0.999442] 
[Epoch 24/99] [Batch 300/509] [D loss: 0.327507] [G loss: 0.227370] [ema: 0.999446] 
[Epoch 24/99] [Batch 400/509] [D loss: 0.320864] [G loss: 0.206312] [ema: 0.999451] 
[Epoch 24/99] [Batch 500/509] [D loss: 0.358460] [G loss: 0.214539] [ema: 0.999455] 



Saving checkpoint 2 in logs/daghar_50000_60_100/run_50000_D_60_2024_10_23_17_53_34/Model



[Epoch 25/99] [Batch 0/509] [D loss: 0.328019] [G loss: 0.215541] [ema: 0.999455] 
[Epoch 25/99] [Batch 100/509] [D loss: 0.356907] [G loss: 0.203227] [ema: 0.999460] 
[Epoch 25/99] [Batch 200/509] [D loss: 0.366393] [G loss: 0.183879] [ema: 0.999464] 
[Epoch 25/99] [Batch 300/509] [D loss: 0.359054] [G loss: 0.212907] [ema: 0.999468] 
[Epoch 25/99] [Batch 400/509] [D loss: 0.370166] [G loss: 0.202287] [ema: 0.999472] 
[Epoch 25/99] [Batch 500/509] [D loss: 0.364648] [G loss: 0.194120] [ema: 0.999476] 
[Epoch 26/99] [Batch 0/509] [D loss: 0.348062] [G loss: 0.192367] [ema: 0.999476] 
[Epoch 26/99] [Batch 100/509] [D loss: 0.333634] [G loss: 0.216068] [ema: 0.999480] 
[Epoch 26/99] [Batch 200/509] [D loss: 0.335739] [G loss: 0.202470] [ema: 0.999484] 
[Epoch 26/99] [Batch 300/509] [D loss: 0.322247] [G loss: 0.207760] [ema: 0.999488] 
[Epoch 26/99] [Batch 400/509] [D loss: 0.375976] [G loss: 0.216480] [ema: 0.999492] 
[Epoch 26/99] [Batch 500/509] [D loss: 0.335464] [G loss: 0.213197] [ema: 0.999495] 
[Epoch 27/99] [Batch 0/509] [D loss: 0.377615] [G loss: 0.194708] [ema: 0.999496] 
[Epoch 27/99] [Batch 100/509] [D loss: 0.375366] [G loss: 0.184811] [ema: 0.999499] 
[Epoch 27/99] [Batch 200/509] [D loss: 0.369059] [G loss: 0.207582] [ema: 0.999503] 
[Epoch 27/99] [Batch 300/509] [D loss: 0.316921] [G loss: 0.213693] [ema: 0.999507] 
[Epoch 27/99] [Batch 400/509] [D loss: 0.354062] [G loss: 0.205690] [ema: 0.999510] 
[Epoch 27/99] [Batch 500/509] [D loss: 0.308373] [G loss: 0.224430] [ema: 0.999513] 
[Epoch 28/99] [Batch 0/509] [D loss: 0.456743] [G loss: 0.226795] [ema: 0.999514] 
[Epoch 28/99] [Batch 100/509] [D loss: 0.335340] [G loss: 0.226099] [ema: 0.999517] 
[Epoch 28/99] [Batch 200/509] [D loss: 0.312337] [G loss: 0.221798] [ema: 0.999520] 
[Epoch 28/99] [Batch 300/509] [D loss: 0.428837] [G loss: 0.189695] [ema: 0.999524] 
[Epoch 28/99] [Batch 400/509] [D loss: 0.325358] [G loss: 0.236273] [ema: 0.999527] 
[Epoch 28/99] [Batch 500/509] [D loss: 0.359725] [G loss: 0.239037] [ema: 0.999530] 
[Epoch 29/99] [Batch 0/509] [D loss: 0.311975] [G loss: 0.205594] [ema: 0.999531] 
[Epoch 29/99] [Batch 100/509] [D loss: 0.387763] [G loss: 0.196955] [ema: 0.999534] 
[Epoch 29/99] [Batch 200/509] [D loss: 0.369915] [G loss: 0.203179] [ema: 0.999537] 
[Epoch 29/99] [Batch 300/509] [D loss: 0.319869] [G loss: 0.215831] [ema: 0.999540] 
[Epoch 29/99] [Batch 400/509] [D loss: 0.342148] [G loss: 0.219683] [ema: 0.999543] 
[Epoch 29/99] [Batch 500/509] [D loss: 0.360536] [G loss: 0.195181] [ema: 0.999546] 
[Epoch 30/99] [Batch 0/509] [D loss: 0.330472] [G loss: 0.220341] [ema: 0.999546] 
[Epoch 30/99] [Batch 100/509] [D loss: 0.376624] [G loss: 0.211979] [ema: 0.999549] 
[Epoch 30/99] [Batch 200/509] [D loss: 0.349059] [G loss: 0.231259] [ema: 0.999552] 
[Epoch 30/99] [Batch 300/509] [D loss: 0.352589] [G loss: 0.219899] [ema: 0.999555] 
[Epoch 30/99] [Batch 400/509] [D loss: 0.363192] [G loss: 0.238513] [ema: 0.999558] 
[Epoch 30/99] [Batch 500/509] [D loss: 0.311676] [G loss: 0.222024] [ema: 0.999561] 
[Epoch 31/99] [Batch 0/509] [D loss: 0.367468] [G loss: 0.206715] [ema: 0.999561] 
[Epoch 31/99] [Batch 100/509] [D loss: 0.357443] [G loss: 0.219826] [ema: 0.999564] 
[Epoch 31/99] [Batch 200/509] [D loss: 0.356706] [G loss: 0.189325] [ema: 0.999566] 
[Epoch 31/99] [Batch 300/509] [D loss: 0.413219] [G loss: 0.166742] [ema: 0.999569] 
[Epoch 31/99] [Batch 400/509] [D loss: 0.320892] [G loss: 0.225016] [ema: 0.999572] 
[Epoch 31/99] [Batch 500/509] [D loss: 0.334891] [G loss: 0.210873] [ema: 0.999574] 
[Epoch 32/99] [Batch 0/509] [D loss: 0.334938] [G loss: 0.201227] [ema: 0.999575] 
[Epoch 32/99] [Batch 100/509] [D loss: 0.311566] [G loss: 0.220858] [ema: 0.999577] 
[Epoch 32/99] [Batch 200/509] [D loss: 0.288844] [G loss: 0.218767] [ema: 0.999580] 
[Epoch 32/99] [Batch 300/509] [D loss: 0.333560] [G loss: 0.183766] [ema: 0.999582] 
[Epoch 32/99] [Batch 400/509] [D loss: 0.402273] [G loss: 0.200495] [ema: 0.999585] 
[Epoch 32/99] [Batch 500/509] [D loss: 0.409267] [G loss: 0.197382] [ema: 0.999587] 
[Epoch 33/99] [Batch 0/509] [D loss: 0.427383] [G loss: 0.210011] [ema: 0.999587] 
[Epoch 33/99] [Batch 100/509] [D loss: 0.452756] [G loss: 0.205341] [ema: 0.999590] 
[Epoch 33/99] [Batch 200/509] [D loss: 0.320334] [G loss: 0.206142] [ema: 0.999592] 
[Epoch 33/99] [Batch 300/509] [D loss: 0.364204] [G loss: 0.213592] [ema: 0.999595] 
[Epoch 33/99] [Batch 400/509] [D loss: 0.382392] [G loss: 0.179594] [ema: 0.999597] 
[Epoch 33/99] [Batch 500/509] [D loss: 0.308491] [G loss: 0.210270] [ema: 0.999599] 
[Epoch 34/99] [Batch 0/509] [D loss: 0.340114] [G loss: 0.186386] [ema: 0.999600] 
[Epoch 34/99] [Batch 100/509] [D loss: 0.370527] [G loss: 0.195142] [ema: 0.999602] 
[Epoch 34/99] [Batch 200/509] [D loss: 0.365243] [G loss: 0.226236] [ema: 0.999604] 
[Epoch 34/99] [Batch 300/509] [D loss: 0.349083] [G loss: 0.188028] [ema: 0.999606] 
[Epoch 34/99] [Batch 400/509] [D loss: 0.309921] [G loss: 0.206341] [ema: 0.999609] 
[Epoch 34/99] [Batch 500/509] [D loss: 0.352868] [G loss: 0.238709] [ema: 0.999611] 
[Epoch 35/99] [Batch 0/509] [D loss: 0.333194] [G loss: 0.199111] [ema: 0.999611] 
[Epoch 35/99] [Batch 100/509] [D loss: 0.317852] [G loss: 0.215334] [ema: 0.999613] 
[Epoch 35/99] [Batch 200/509] [D loss: 0.307072] [G loss: 0.202511] [ema: 0.999615] 
[Epoch 35/99] [Batch 300/509] [D loss: 0.314794] [G loss: 0.209629] [ema: 0.999617] 
[Epoch 35/99] [Batch 400/509] [D loss: 0.337563] [G loss: 0.233366] [ema: 0.999620] 
[Epoch 35/99] [Batch 500/509] [D loss: 0.299844] [G loss: 0.224875] [ema: 0.999622] 
[Epoch 36/99] [Batch 0/509] [D loss: 0.299674] [G loss: 0.257931] [ema: 0.999622] 
[Epoch 36/99] [Batch 100/509] [D loss: 0.344996] [G loss: 0.204622] [ema: 0.999624] 
[Epoch 36/99] [Batch 200/509] [D loss: 0.330481] [G loss: 0.194233] [ema: 0.999626] 
[Epoch 36/99] [Batch 300/509] [D loss: 0.324391] [G loss: 0.184700] [ema: 0.999628] 
[Epoch 36/99] [Batch 400/509] [D loss: 0.390404] [G loss: 0.179253] [ema: 0.999630] 
[Epoch 36/99] [Batch 500/509] [D loss: 0.333916] [G loss: 0.200267] [ema: 0.999632] 
[Epoch 37/99] [Batch 0/509] [D loss: 0.325613] [G loss: 0.188984] [ema: 0.999632] 
[Epoch 37/99] [Batch 100/509] [D loss: 0.351983] [G loss: 0.207143] [ema: 0.999634] 
[Epoch 37/99] [Batch 200/509] [D loss: 0.320806] [G loss: 0.158846] [ema: 0.999636] 
[Epoch 37/99] [Batch 300/509] [D loss: 0.314947] [G loss: 0.224602] [ema: 0.999638] 
[Epoch 37/99] [Batch 400/509] [D loss: 0.308523] [G loss: 0.216424] [ema: 0.999640] 
[Epoch 37/99] [Batch 500/509] [D loss: 0.307284] [G loss: 0.215127] [ema: 0.999642] 
[Epoch 38/99] [Batch 0/509] [D loss: 0.349667] [G loss: 0.177534] [ema: 0.999642] 
[Epoch 38/99] [Batch 100/509] [D loss: 0.338318] [G loss: 0.213892] [ema: 0.999644] 
[Epoch 38/99] [Batch 200/509] [D loss: 0.343481] [G loss: 0.200512] [ema: 0.999645] 
[Epoch 38/99] [Batch 300/509] [D loss: 0.290293] [G loss: 0.215095] [ema: 0.999647] 
[Epoch 38/99] [Batch 400/509] [D loss: 0.305821] [G loss: 0.211310] [ema: 0.999649] 
[Epoch 38/99] [Batch 500/509] [D loss: 0.312897] [G loss: 0.220706] [ema: 0.999651] 
[Epoch 39/99] [Batch 0/509] [D loss: 0.279158] [G loss: 0.207842] [ema: 0.999651] 
[Epoch 39/99] [Batch 100/509] [D loss: 0.320279] [G loss: 0.234692] [ema: 0.999653] 
[Epoch 39/99] [Batch 200/509] [D loss: 0.383102] [G loss: 0.205672] [ema: 0.999654] 
[Epoch 39/99] [Batch 300/509] [D loss: 0.348862] [G loss: 0.195898] [ema: 0.999656] 
[Epoch 39/99] [Batch 400/509] [D loss: 0.379272] [G loss: 0.198113] [ema: 0.999658] 
[Epoch 39/99] [Batch 500/509] [D loss: 0.346840] [G loss: 0.169496] [ema: 0.999659] 
[Epoch 40/99] [Batch 0/509] [D loss: 0.360286] [G loss: 0.214622] [ema: 0.999660] 
[Epoch 40/99] [Batch 100/509] [D loss: 0.347432] [G loss: 0.194898] [ema: 0.999661] 
[Epoch 40/99] [Batch 200/509] [D loss: 0.318323] [G loss: 0.222836] [ema: 0.999663] 
[Epoch 40/99] [Batch 300/509] [D loss: 0.340668] [G loss: 0.204044] [ema: 0.999665] 
[Epoch 40/99] [Batch 400/509] [D loss: 0.409097] [G loss: 0.201346] [ema: 0.999666] 
[Epoch 40/99] [Batch 500/509] [D loss: 0.341281] [G loss: 0.184228] [ema: 0.999668] 
[Epoch 41/99] [Batch 0/509] [D loss: 0.338706] [G loss: 0.211138] [ema: 0.999668] 
[Epoch 41/99] [Batch 100/509] [D loss: 0.337884] [G loss: 0.212706] [ema: 0.999669] 
[Epoch 41/99] [Batch 200/509] [D loss: 0.338966] [G loss: 0.198974] [ema: 0.999671] 
[Epoch 41/99] [Batch 300/509] [D loss: 0.340043] [G loss: 0.149782] [ema: 0.999673] 
[Epoch 41/99] [Batch 400/509] [D loss: 0.298161] [G loss: 0.241220] [ema: 0.999674] 
[Epoch 41/99] [Batch 500/509] [D loss: 0.397246] [G loss: 0.201326] [ema: 0.999676] 
[Epoch 42/99] [Batch 0/509] [D loss: 0.354225] [G loss: 0.212229] [ema: 0.999676] 
[Epoch 42/99] [Batch 100/509] [D loss: 0.377934] [G loss: 0.217685] [ema: 0.999677] 
[Epoch 42/99] [Batch 200/509] [D loss: 0.326728] [G loss: 0.215761] [ema: 0.999679] 
[Epoch 42/99] [Batch 300/509] [D loss: 0.348996] [G loss: 0.182306] [ema: 0.999680] 
[Epoch 42/99] [Batch 400/509] [D loss: 0.314251] [G loss: 0.236322] [ema: 0.999682] 
[Epoch 42/99] [Batch 500/509] [D loss: 0.308504] [G loss: 0.225734] [ema: 0.999683] 
[Epoch 43/99] [Batch 0/509] [D loss: 0.338760] [G loss: 0.216498] [ema: 0.999683] 
[Epoch 43/99] [Batch 100/509] [D loss: 0.319892] [G loss: 0.225247] [ema: 0.999685] 
[Epoch 43/99] [Batch 200/509] [D loss: 0.333975] [G loss: 0.201359] [ema: 0.999686] 
[Epoch 43/99] [Batch 300/509] [D loss: 0.380188] [G loss: 0.203796] [ema: 0.999688] 
[Epoch 43/99] [Batch 400/509] [D loss: 0.335526] [G loss: 0.204806] [ema: 0.999689] 
[Epoch 43/99] [Batch 500/509] [D loss: 0.351269] [G loss: 0.172544] [ema: 0.999690] 
[Epoch 44/99] [Batch 0/509] [D loss: 0.429579] [G loss: 0.196443] [ema: 0.999691] 
[Epoch 44/99] [Batch 100/509] [D loss: 0.388104] [G loss: 0.233090] [ema: 0.999692] 
[Epoch 44/99] [Batch 200/509] [D loss: 0.329538] [G loss: 0.189869] [ema: 0.999693] 
[Epoch 44/99] [Batch 300/509] [D loss: 0.385917] [G loss: 0.158151] [ema: 0.999695] 
[Epoch 44/99] [Batch 400/509] [D loss: 0.348934] [G loss: 0.207170] [ema: 0.999696] 
[Epoch 44/99] [Batch 500/509] [D loss: 0.420759] [G loss: 0.180796] [ema: 0.999697] 
[Epoch 45/99] [Batch 0/509] [D loss: 0.382368] [G loss: 0.198803] [ema: 0.999697] 
[Epoch 45/99] [Batch 100/509] [D loss: 0.314349] [G loss: 0.197735] [ema: 0.999699] 
[Epoch 45/99] [Batch 200/509] [D loss: 0.339688] [G loss: 0.206509] [ema: 0.999700] 
[Epoch 45/99] [Batch 300/509] [D loss: 0.360550] [G loss: 0.210162] [ema: 0.999701] 
[Epoch 45/99] [Batch 400/509] [D loss: 0.364156] [G loss: 0.190616] [ema: 0.999703] 
[Epoch 45/99] [Batch 500/509] [D loss: 0.433612] [G loss: 0.180699] [ema: 0.999704] 
[Epoch 46/99] [Batch 0/509] [D loss: 0.407774] [G loss: 0.251991] [ema: 0.999704] 
[Epoch 46/99] [Batch 100/509] [D loss: 0.332085] [G loss: 0.209009] [ema: 0.999705] 
[Epoch 46/99] [Batch 200/509] [D loss: 0.329877] [G loss: 0.217453] [ema: 0.999707] 
[Epoch 46/99] [Batch 300/509] [D loss: 0.400071] [G loss: 0.178242] [ema: 0.999708] 
[Epoch 46/99] [Batch 400/509] [D loss: 0.327788] [G loss: 0.222420] [ema: 0.999709] 
[Epoch 46/99] [Batch 500/509] [D loss: 0.301371] [G loss: 0.214234] [ema: 0.999710] 
[Epoch 47/99] [Batch 0/509] [D loss: 0.312263] [G loss: 0.217586] [ema: 0.999710] 
[Epoch 47/99] [Batch 100/509] [D loss: 0.359125] [G loss: 0.188361] [ema: 0.999712] 
[Epoch 47/99] [Batch 200/509] [D loss: 0.322491] [G loss: 0.212461] [ema: 0.999713] 
[Epoch 47/99] [Batch 300/509] [D loss: 0.335341] [G loss: 0.217133] [ema: 0.999714] 
[Epoch 47/99] [Batch 400/509] [D loss: 0.335664] [G loss: 0.204034] [ema: 0.999715] 
[Epoch 47/99] [Batch 500/509] [D loss: 0.308002] [G loss: 0.197357] [ema: 0.999716] 
[Epoch 48/99] [Batch 0/509] [D loss: 0.299275] [G loss: 0.225632] [ema: 0.999716] 
[Epoch 48/99] [Batch 100/509] [D loss: 0.367096] [G loss: 0.174074] [ema: 0.999717] 
[Epoch 48/99] [Batch 200/509] [D loss: 0.402921] [G loss: 0.165874] [ema: 0.999719] 
[Epoch 48/99] [Batch 300/509] [D loss: 0.383669] [G loss: 0.186416] [ema: 0.999720] 
[Epoch 48/99] [Batch 400/509] [D loss: 0.405383] [G loss: 0.168705] [ema: 0.999721] 
[Epoch 48/99] [Batch 500/509] [D loss: 0.384428] [G loss: 0.216910] [ema: 0.999722] 
[Epoch 49/99] [Batch 0/509] [D loss: 0.374044] [G loss: 0.210142] [ema: 0.999722] 
[Epoch 49/99] [Batch 100/509] [D loss: 0.389418] [G loss: 0.197728] [ema: 0.999723] 
[Epoch 49/99] [Batch 200/509] [D loss: 0.383070] [G loss: 0.182901] [ema: 0.999724] 
[Epoch 49/99] [Batch 300/509] [D loss: 0.347441] [G loss: 0.214410] [ema: 0.999725] 
[Epoch 49/99] [Batch 400/509] [D loss: 0.333365] [G loss: 0.185325] [ema: 0.999727] 
[Epoch 49/99] [Batch 500/509] [D loss: 0.327005] [G loss: 0.207487] [ema: 0.999728] 



Saving checkpoint 3 in logs/daghar_50000_60_100/run_50000_D_60_2024_10_23_17_53_34/Model



[Epoch 50/99] [Batch 0/509] [D loss: 0.371100] [G loss: 0.210048] [ema: 0.999728] 
[Epoch 50/99] [Batch 100/509] [D loss: 0.345601] [G loss: 0.197181] [ema: 0.999729] 
[Epoch 50/99] [Batch 200/509] [D loss: 0.301739] [G loss: 0.206306] [ema: 0.999730] 
[Epoch 50/99] [Batch 300/509] [D loss: 0.367359] [G loss: 0.186872] [ema: 0.999731] 
[Epoch 50/99] [Batch 400/509] [D loss: 0.384010] [G loss: 0.181210] [ema: 0.999732] 
[Epoch 50/99] [Batch 500/509] [D loss: 0.319504] [G loss: 0.182877] [ema: 0.999733] 
[Epoch 51/99] [Batch 0/509] [D loss: 0.340581] [G loss: 0.202976] [ema: 0.999733] 
[Epoch 51/99] [Batch 100/509] [D loss: 0.330932] [G loss: 0.228434] [ema: 0.999734] 
[Epoch 51/99] [Batch 200/509] [D loss: 0.350714] [G loss: 0.238537] [ema: 0.999735] 
[Epoch 51/99] [Batch 300/509] [D loss: 0.387676] [G loss: 0.206157] [ema: 0.999736] 
[Epoch 51/99] [Batch 400/509] [D loss: 0.361536] [G loss: 0.211657] [ema: 0.999737] 
[Epoch 51/99] [Batch 500/509] [D loss: 0.329145] [G loss: 0.200364] [ema: 0.999738] 
[Epoch 52/99] [Batch 0/509] [D loss: 0.380073] [G loss: 0.209533] [ema: 0.999738] 
[Epoch 52/99] [Batch 100/509] [D loss: 0.349399] [G loss: 0.153535] [ema: 0.999739] 
[Epoch 52/99] [Batch 200/509] [D loss: 0.369874] [G loss: 0.175007] [ema: 0.999740] 
[Epoch 52/99] [Batch 300/509] [D loss: 0.369652] [G loss: 0.197447] [ema: 0.999741] 
[Epoch 52/99] [Batch 400/509] [D loss: 0.331833] [G loss: 0.227223] [ema: 0.999742] 
[Epoch 52/99] [Batch 500/509] [D loss: 0.364981] [G loss: 0.186461] [ema: 0.999743] 
[Epoch 53/99] [Batch 0/509] [D loss: 0.316225] [G loss: 0.214436] [ema: 0.999743] 
[Epoch 53/99] [Batch 100/509] [D loss: 0.350674] [G loss: 0.213085] [ema: 0.999744] 
[Epoch 53/99] [Batch 200/509] [D loss: 0.317819] [G loss: 0.202523] [ema: 0.999745] 
[Epoch 53/99] [Batch 300/509] [D loss: 0.380093] [G loss: 0.200398] [ema: 0.999746] 
[Epoch 53/99] [Batch 400/509] [D loss: 0.331727] [G loss: 0.202701] [ema: 0.999747] 
[Epoch 53/99] [Batch 500/509] [D loss: 0.348277] [G loss: 0.202103] [ema: 0.999748] 
[Epoch 54/99] [Batch 0/509] [D loss: 0.327745] [G loss: 0.233023] [ema: 0.999748] 
[Epoch 54/99] [Batch 100/509] [D loss: 0.309866] [G loss: 0.197851] [ema: 0.999749] 
[Epoch 54/99] [Batch 200/509] [D loss: 0.322075] [G loss: 0.214968] [ema: 0.999750] 
[Epoch 54/99] [Batch 300/509] [D loss: 0.344804] [G loss: 0.198355] [ema: 0.999751] 
[Epoch 54/99] [Batch 400/509] [D loss: 0.354532] [G loss: 0.192892] [ema: 0.999751] 
[Epoch 54/99] [Batch 500/509] [D loss: 0.401824] [G loss: 0.192736] [ema: 0.999752] 
[Epoch 55/99] [Batch 0/509] [D loss: 0.353225] [G loss: 0.174624] [ema: 0.999752] 
[Epoch 55/99] [Batch 100/509] [D loss: 0.338838] [G loss: 0.195602] [ema: 0.999753] 
[Epoch 55/99] [Batch 200/509] [D loss: 0.456468] [G loss: 0.188369] [ema: 0.999754] 
[Epoch 55/99] [Batch 300/509] [D loss: 0.409025] [G loss: 0.188801] [ema: 0.999755] 
[Epoch 55/99] [Batch 400/509] [D loss: 0.313897] [G loss: 0.214134] [ema: 0.999756] 
[Epoch 55/99] [Batch 500/509] [D loss: 0.341656] [G loss: 0.182578] [ema: 0.999757] 
[Epoch 56/99] [Batch 0/509] [D loss: 0.395947] [G loss: 0.201691] [ema: 0.999757] 
[Epoch 56/99] [Batch 100/509] [D loss: 0.361943] [G loss: 0.182863] [ema: 0.999758] 
[Epoch 56/99] [Batch 200/509] [D loss: 0.389195] [G loss: 0.180785] [ema: 0.999759] 
[Epoch 56/99] [Batch 300/509] [D loss: 0.431310] [G loss: 0.216609] [ema: 0.999759] 
[Epoch 56/99] [Batch 400/509] [D loss: 0.412329] [G loss: 0.195517] [ema: 0.999760] 
[Epoch 56/99] [Batch 500/509] [D loss: 0.400726] [G loss: 0.212157] [ema: 0.999761] 
[Epoch 57/99] [Batch 0/509] [D loss: 0.355579] [G loss: 0.195045] [ema: 0.999761] 
[Epoch 57/99] [Batch 100/509] [D loss: 0.371012] [G loss: 0.194162] [ema: 0.999762] 
[Epoch 57/99] [Batch 200/509] [D loss: 0.307908] [G loss: 0.175611] [ema: 0.999763] 
[Epoch 57/99] [Batch 300/509] [D loss: 0.355875] [G loss: 0.186789] [ema: 0.999764] 
[Epoch 57/99] [Batch 400/509] [D loss: 0.439882] [G loss: 0.176471] [ema: 0.999764] 
[Epoch 57/99] [Batch 500/509] [D loss: 0.437169] [G loss: 0.181550] [ema: 0.999765] 
[Epoch 58/99] [Batch 0/509] [D loss: 0.361656] [G loss: 0.191097] [ema: 0.999765] 
[Epoch 58/99] [Batch 100/509] [D loss: 0.351716] [G loss: 0.200961] [ema: 0.999766] 
[Epoch 58/99] [Batch 200/509] [D loss: 0.445352] [G loss: 0.222284] [ema: 0.999767] 
[Epoch 58/99] [Batch 300/509] [D loss: 0.398523] [G loss: 0.172383] [ema: 0.999768] 
[Epoch 58/99] [Batch 400/509] [D loss: 0.340329] [G loss: 0.212779] [ema: 0.999768] 
[Epoch 58/99] [Batch 500/509] [D loss: 0.349000] [G loss: 0.188571] [ema: 0.999769] 
[Epoch 59/99] [Batch 0/509] [D loss: 0.355809] [G loss: 0.212570] [ema: 0.999769] 
[Epoch 59/99] [Batch 100/509] [D loss: 0.411266] [G loss: 0.157163] [ema: 0.999770] 
[Epoch 59/99] [Batch 200/509] [D loss: 0.407723] [G loss: 0.178377] [ema: 0.999771] 
[Epoch 59/99] [Batch 300/509] [D loss: 0.303999] [G loss: 0.211342] [ema: 0.999771] 
[Epoch 59/99] [Batch 400/509] [D loss: 0.334985] [G loss: 0.189035] [ema: 0.999772] 
[Epoch 59/99] [Batch 500/509] [D loss: 0.359531] [G loss: 0.196365] [ema: 0.999773] 
[Epoch 60/99] [Batch 0/509] [D loss: 0.361878] [G loss: 0.186984] [ema: 0.999773] 
[Epoch 60/99] [Batch 100/509] [D loss: 0.352770] [G loss: 0.184253] [ema: 0.999774] 
[Epoch 60/99] [Batch 200/509] [D loss: 0.395576] [G loss: 0.202103] [ema: 0.999775] 
[Epoch 60/99] [Batch 300/509] [D loss: 0.430855] [G loss: 0.197231] [ema: 0.999775] 
[Epoch 60/99] [Batch 400/509] [D loss: 0.392153] [G loss: 0.164578] [ema: 0.999776] 
[Epoch 60/99] [Batch 500/509] [D loss: 0.399553] [G loss: 0.172636] [ema: 0.999777] 
[Epoch 61/99] [Batch 0/509] [D loss: 0.353456] [G loss: 0.213218] [ema: 0.999777] 
[Epoch 61/99] [Batch 100/509] [D loss: 0.407542] [G loss: 0.180438] [ema: 0.999777] 
[Epoch 61/99] [Batch 200/509] [D loss: 0.488293] [G loss: 0.191932] [ema: 0.999778] 
[Epoch 61/99] [Batch 300/509] [D loss: 0.335648] [G loss: 0.175917] [ema: 0.999779] 
[Epoch 61/99] [Batch 400/509] [D loss: 0.356515] [G loss: 0.186569] [ema: 0.999780] 
[Epoch 61/99] [Batch 500/509] [D loss: 0.389098] [G loss: 0.206617] [ema: 0.999780] 
[Epoch 62/99] [Batch 0/509] [D loss: 0.415282] [G loss: 0.179056] [ema: 0.999780] 
[Epoch 62/99] [Batch 100/509] [D loss: 0.386966] [G loss: 0.188138] [ema: 0.999781] 
[Epoch 62/99] [Batch 200/509] [D loss: 0.383875] [G loss: 0.169675] [ema: 0.999782] 
[Epoch 62/99] [Batch 300/509] [D loss: 0.396795] [G loss: 0.179770] [ema: 0.999782] 
[Epoch 62/99] [Batch 400/509] [D loss: 0.367292] [G loss: 0.178102] [ema: 0.999783] 
[Epoch 62/99] [Batch 500/509] [D loss: 0.380332] [G loss: 0.170316] [ema: 0.999784] 
[Epoch 63/99] [Batch 0/509] [D loss: 0.470291] [G loss: 0.162102] [ema: 0.999784] 
[Epoch 63/99] [Batch 100/509] [D loss: 0.377675] [G loss: 0.193633] [ema: 0.999785] 
[Epoch 63/99] [Batch 200/509] [D loss: 0.473207] [G loss: 0.176079] [ema: 0.999785] 
[Epoch 63/99] [Batch 300/509] [D loss: 0.357038] [G loss: 0.173698] [ema: 0.999786] 
[Epoch 63/99] [Batch 400/509] [D loss: 0.320508] [G loss: 0.219317] [ema: 0.999787] 
[Epoch 63/99] [Batch 500/509] [D loss: 0.394481] [G loss: 0.205831] [ema: 0.999787] 
[Epoch 64/99] [Batch 0/509] [D loss: 0.404109] [G loss: 0.193608] [ema: 0.999787] 
[Epoch 64/99] [Batch 100/509] [D loss: 0.417754] [G loss: 0.214799] [ema: 0.999788] 
[Epoch 64/99] [Batch 200/509] [D loss: 0.360795] [G loss: 0.206105] [ema: 0.999789] 
[Epoch 64/99] [Batch 300/509] [D loss: 0.333699] [G loss: 0.164003] [ema: 0.999789] 
[Epoch 64/99] [Batch 400/509] [D loss: 0.351871] [G loss: 0.192693] [ema: 0.999790] 
[Epoch 64/99] [Batch 500/509] [D loss: 0.319882] [G loss: 0.207632] [ema: 0.999790] 
[Epoch 65/99] [Batch 0/509] [D loss: 0.379587] [G loss: 0.190030] [ema: 0.999791] 
[Epoch 65/99] [Batch 100/509] [D loss: 0.397966] [G loss: 0.194905] [ema: 0.999791] 
[Epoch 65/99] [Batch 200/509] [D loss: 0.363843] [G loss: 0.176408] [ema: 0.999792] 
[Epoch 65/99] [Batch 300/509] [D loss: 0.354205] [G loss: 0.189823] [ema: 0.999792] 
[Epoch 65/99] [Batch 400/509] [D loss: 0.323931] [G loss: 0.181015] [ema: 0.999793] 
[Epoch 65/99] [Batch 500/509] [D loss: 0.420647] [G loss: 0.190112] [ema: 0.999794] 
[Epoch 66/99] [Batch 0/509] [D loss: 0.396423] [G loss: 0.184747] [ema: 0.999794] 
[Epoch 66/99] [Batch 100/509] [D loss: 0.397575] [G loss: 0.222254] [ema: 0.999794] 
[Epoch 66/99] [Batch 200/509] [D loss: 0.406194] [G loss: 0.193832] [ema: 0.999795] 
[Epoch 66/99] [Batch 300/509] [D loss: 0.329615] [G loss: 0.186279] [ema: 0.999796] 
[Epoch 66/99] [Batch 400/509] [D loss: 0.369625] [G loss: 0.199551] [ema: 0.999796] 
[Epoch 66/99] [Batch 500/509] [D loss: 0.347081] [G loss: 0.197339] [ema: 0.999797] 
[Epoch 67/99] [Batch 0/509] [D loss: 0.379555] [G loss: 0.183647] [ema: 0.999797] 
[Epoch 67/99] [Batch 100/509] [D loss: 0.391781] [G loss: 0.187478] [ema: 0.999797] 
[Epoch 67/99] [Batch 200/509] [D loss: 0.359919] [G loss: 0.197647] [ema: 0.999798] 
[Epoch 67/99] [Batch 300/509] [D loss: 0.339629] [G loss: 0.185546] [ema: 0.999799] 
[Epoch 67/99] [Batch 400/509] [D loss: 0.373074] [G loss: 0.199980] [ema: 0.999799] 
[Epoch 67/99] [Batch 500/509] [D loss: 0.336175] [G loss: 0.206668] [ema: 0.999800] 
[Epoch 68/99] [Batch 0/509] [D loss: 0.384062] [G loss: 0.180339] [ema: 0.999800] 
[Epoch 68/99] [Batch 100/509] [D loss: 0.458015] [G loss: 0.194386] [ema: 0.999800] 
[Epoch 68/99] [Batch 200/509] [D loss: 0.361932] [G loss: 0.208836] [ema: 0.999801] 
[Epoch 68/99] [Batch 300/509] [D loss: 0.383195] [G loss: 0.199468] [ema: 0.999801] 
[Epoch 68/99] [Batch 400/509] [D loss: 0.340685] [G loss: 0.230077] [ema: 0.999802] 
[Epoch 68/99] [Batch 500/509] [D loss: 0.358282] [G loss: 0.216661] [ema: 0.999803] 
[Epoch 69/99] [Batch 0/509] [D loss: 0.432771] [G loss: 0.199463] [ema: 0.999803] 
[Epoch 69/99] [Batch 100/509] [D loss: 0.364536] [G loss: 0.210487] [ema: 0.999803] 
[Epoch 69/99] [Batch 200/509] [D loss: 0.365370] [G loss: 0.222566] [ema: 0.999804] 
[Epoch 69/99] [Batch 300/509] [D loss: 0.444686] [G loss: 0.199167] [ema: 0.999804] 
[Epoch 69/99] [Batch 400/509] [D loss: 0.369022] [G loss: 0.191134] [ema: 0.999805] 
[Epoch 69/99] [Batch 500/509] [D loss: 0.408749] [G loss: 0.195094] [ema: 0.999805] 
[Epoch 70/99] [Batch 0/509] [D loss: 0.360245] [G loss: 0.204262] [ema: 0.999805] 
[Epoch 70/99] [Batch 100/509] [D loss: 0.336940] [G loss: 0.192529] [ema: 0.999806] 
[Epoch 70/99] [Batch 200/509] [D loss: 0.364727] [G loss: 0.145766] [ema: 0.999807] 
[Epoch 70/99] [Batch 300/509] [D loss: 0.358773] [G loss: 0.197077] [ema: 0.999807] 
[Epoch 70/99] [Batch 400/509] [D loss: 0.342920] [G loss: 0.197226] [ema: 0.999808] 
[Epoch 70/99] [Batch 500/509] [D loss: 0.383442] [G loss: 0.196289] [ema: 0.999808] 
[Epoch 71/99] [Batch 0/509] [D loss: 0.347239] [G loss: 0.190021] [ema: 0.999808] 
[Epoch 71/99] [Batch 100/509] [D loss: 0.345452] [G loss: 0.195090] [ema: 0.999809] 
[Epoch 71/99] [Batch 200/509] [D loss: 0.393069] [G loss: 0.173506] [ema: 0.999809] 
[Epoch 71/99] [Batch 300/509] [D loss: 0.344254] [G loss: 0.188558] [ema: 0.999810] 
[Epoch 71/99] [Batch 400/509] [D loss: 0.337070] [G loss: 0.173536] [ema: 0.999810] 
[Epoch 71/99] [Batch 500/509] [D loss: 0.407412] [G loss: 0.194986] [ema: 0.999811] 
[Epoch 72/99] [Batch 0/509] [D loss: 0.365147] [G loss: 0.208432] [ema: 0.999811] 
[Epoch 72/99] [Batch 100/509] [D loss: 0.398140] [G loss: 0.205713] [ema: 0.999811] 
[Epoch 72/99] [Batch 200/509] [D loss: 0.344394] [G loss: 0.197023] [ema: 0.999812] 
[Epoch 72/99] [Batch 300/509] [D loss: 0.371504] [G loss: 0.203591] [ema: 0.999812] 
[Epoch 72/99] [Batch 400/509] [D loss: 0.405299] [G loss: 0.196757] [ema: 0.999813] 
[Epoch 72/99] [Batch 500/509] [D loss: 0.365495] [G loss: 0.189452] [ema: 0.999813] 
[Epoch 73/99] [Batch 0/509] [D loss: 0.346935] [G loss: 0.203071] [ema: 0.999813] 
[Epoch 73/99] [Batch 100/509] [D loss: 0.382967] [G loss: 0.208801] [ema: 0.999814] 
[Epoch 73/99] [Batch 200/509] [D loss: 0.366712] [G loss: 0.221071] [ema: 0.999814] 
[Epoch 73/99] [Batch 300/509] [D loss: 0.346145] [G loss: 0.207870] [ema: 0.999815] 
[Epoch 73/99] [Batch 400/509] [D loss: 0.337905] [G loss: 0.171438] [ema: 0.999815] 
[Epoch 73/99] [Batch 500/509] [D loss: 0.349083] [G loss: 0.204684] [ema: 0.999816] 
[Epoch 74/99] [Batch 0/509] [D loss: 0.347609] [G loss: 0.217161] [ema: 0.999816] 
[Epoch 74/99] [Batch 100/509] [D loss: 0.359840] [G loss: 0.216298] [ema: 0.999816] 
[Epoch 74/99] [Batch 200/509] [D loss: 0.405661] [G loss: 0.198977] [ema: 0.999817] 
[Epoch 74/99] [Batch 300/509] [D loss: 0.382305] [G loss: 0.188431] [ema: 0.999817] 
[Epoch 74/99] [Batch 400/509] [D loss: 0.392904] [G loss: 0.187034] [ema: 0.999818] 
[Epoch 74/99] [Batch 500/509] [D loss: 0.385196] [G loss: 0.165392] [ema: 0.999818] 



Saving checkpoint 4 in logs/daghar_50000_60_100/run_50000_D_60_2024_10_23_17_53_34/Model



[Epoch 75/99] [Batch 0/509] [D loss: 0.341152] [G loss: 0.198181] [ema: 0.999818] 
[Epoch 75/99] [Batch 100/509] [D loss: 0.321612] [G loss: 0.217495] [ema: 0.999819] 
[Epoch 75/99] [Batch 200/509] [D loss: 0.334344] [G loss: 0.216313] [ema: 0.999819] 
[Epoch 75/99] [Batch 300/509] [D loss: 0.323911] [G loss: 0.204620] [ema: 0.999820] 
[Epoch 75/99] [Batch 400/509] [D loss: 0.345192] [G loss: 0.175260] [ema: 0.999820] 
[Epoch 75/99] [Batch 500/509] [D loss: 0.354979] [G loss: 0.181254] [ema: 0.999821] 
[Epoch 76/99] [Batch 0/509] [D loss: 0.296661] [G loss: 0.197698] [ema: 0.999821] 
[Epoch 76/99] [Batch 100/509] [D loss: 0.345328] [G loss: 0.199393] [ema: 0.999821] 
[Epoch 76/99] [Batch 200/509] [D loss: 0.402821] [G loss: 0.187590] [ema: 0.999822] 
[Epoch 76/99] [Batch 300/509] [D loss: 0.361059] [G loss: 0.204383] [ema: 0.999822] 
[Epoch 76/99] [Batch 400/509] [D loss: 0.424339] [G loss: 0.226470] [ema: 0.999823] 
[Epoch 76/99] [Batch 500/509] [D loss: 0.352222] [G loss: 0.222901] [ema: 0.999823] 
[Epoch 77/99] [Batch 0/509] [D loss: 0.414774] [G loss: 0.197113] [ema: 0.999823] 
[Epoch 77/99] [Batch 100/509] [D loss: 0.360244] [G loss: 0.212058] [ema: 0.999824] 
[Epoch 77/99] [Batch 200/509] [D loss: 0.346682] [G loss: 0.201249] [ema: 0.999824] 
[Epoch 77/99] [Batch 300/509] [D loss: 0.363146] [G loss: 0.203605] [ema: 0.999825] 
[Epoch 77/99] [Batch 400/509] [D loss: 0.315626] [G loss: 0.165320] [ema: 0.999825] 
[Epoch 77/99] [Batch 500/509] [D loss: 0.314091] [G loss: 0.182122] [ema: 0.999825] 
[Epoch 78/99] [Batch 0/509] [D loss: 0.384404] [G loss: 0.196645] [ema: 0.999825] 
[Epoch 78/99] [Batch 100/509] [D loss: 0.420134] [G loss: 0.180595] [ema: 0.999826] 
[Epoch 78/99] [Batch 200/509] [D loss: 0.363695] [G loss: 0.217079] [ema: 0.999826] 
[Epoch 78/99] [Batch 300/509] [D loss: 0.389962] [G loss: 0.198209] [ema: 0.999827] 
[Epoch 78/99] [Batch 400/509] [D loss: 0.334402] [G loss: 0.213143] [ema: 0.999827] 
[Epoch 78/99] [Batch 500/509] [D loss: 0.406684] [G loss: 0.199729] [ema: 0.999828] 
[Epoch 79/99] [Batch 0/509] [D loss: 0.365137] [G loss: 0.201916] [ema: 0.999828] 
[Epoch 79/99] [Batch 100/509] [D loss: 0.326462] [G loss: 0.200347] [ema: 0.999828] 
[Epoch 79/99] [Batch 200/509] [D loss: 0.325058] [G loss: 0.191507] [ema: 0.999828] 
[Epoch 79/99] [Batch 300/509] [D loss: 0.387590] [G loss: 0.205121] [ema: 0.999829] 
[Epoch 79/99] [Batch 400/509] [D loss: 0.312005] [G loss: 0.183284] [ema: 0.999829] 
[Epoch 79/99] [Batch 500/509] [D loss: 0.360346] [G loss: 0.188859] [ema: 0.999830] 
[Epoch 80/99] [Batch 0/509] [D loss: 0.355810] [G loss: 0.196259] [ema: 0.999830] 
[Epoch 80/99] [Batch 100/509] [D loss: 0.348663] [G loss: 0.205050] [ema: 0.999830] 
[Epoch 80/99] [Batch 200/509] [D loss: 0.371619] [G loss: 0.196078] [ema: 0.999831] 
[Epoch 80/99] [Batch 300/509] [D loss: 0.399984] [G loss: 0.198228] [ema: 0.999831] 
[Epoch 80/99] [Batch 400/509] [D loss: 0.334375] [G loss: 0.208210] [ema: 0.999831] 
[Epoch 80/99] [Batch 500/509] [D loss: 0.375587] [G loss: 0.169037] [ema: 0.999832] 
[Epoch 81/99] [Batch 0/509] [D loss: 0.332660] [G loss: 0.201404] [ema: 0.999832] 
[Epoch 81/99] [Batch 100/509] [D loss: 0.378086] [G loss: 0.205818] [ema: 0.999832] 
[Epoch 81/99] [Batch 200/509] [D loss: 0.360389] [G loss: 0.181215] [ema: 0.999833] 
[Epoch 81/99] [Batch 300/509] [D loss: 0.397893] [G loss: 0.188604] [ema: 0.999833] 
[Epoch 81/99] [Batch 400/509] [D loss: 0.323501] [G loss: 0.207077] [ema: 0.999834] 
[Epoch 81/99] [Batch 500/509] [D loss: 0.379015] [G loss: 0.181341] [ema: 0.999834] 
[Epoch 82/99] [Batch 0/509] [D loss: 0.364334] [G loss: 0.202023] [ema: 0.999834] 
[Epoch 82/99] [Batch 100/509] [D loss: 0.323756] [G loss: 0.190643] [ema: 0.999834] 
[Epoch 82/99] [Batch 200/509] [D loss: 0.390615] [G loss: 0.176747] [ema: 0.999835] 
[Epoch 82/99] [Batch 300/509] [D loss: 0.354874] [G loss: 0.213128] [ema: 0.999835] 
[Epoch 82/99] [Batch 400/509] [D loss: 0.343734] [G loss: 0.193037] [ema: 0.999836] 
[Epoch 82/99] [Batch 500/509] [D loss: 0.301104] [G loss: 0.192559] [ema: 0.999836] 
[Epoch 83/99] [Batch 0/509] [D loss: 0.354076] [G loss: 0.226840] [ema: 0.999836] 
[Epoch 83/99] [Batch 100/509] [D loss: 0.369951] [G loss: 0.208481] [ema: 0.999836] 
[Epoch 83/99] [Batch 200/509] [D loss: 0.366434] [G loss: 0.222106] [ema: 0.999837] 
[Epoch 83/99] [Batch 300/509] [D loss: 0.335334] [G loss: 0.197141] [ema: 0.999837] 
[Epoch 83/99] [Batch 400/509] [D loss: 0.372676] [G loss: 0.207417] [ema: 0.999837] 
[Epoch 83/99] [Batch 500/509] [D loss: 0.383343] [G loss: 0.199850] [ema: 0.999838] 
[Epoch 84/99] [Batch 0/509] [D loss: 0.317026] [G loss: 0.216067] [ema: 0.999838] 
[Epoch 84/99] [Batch 100/509] [D loss: 0.339788] [G loss: 0.214833] [ema: 0.999838] 
[Epoch 84/99] [Batch 200/509] [D loss: 0.319638] [G loss: 0.155393] [ema: 0.999839] 
[Epoch 84/99] [Batch 300/509] [D loss: 0.315763] [G loss: 0.206189] [ema: 0.999839] 
[Epoch 84/99] [Batch 400/509] [D loss: 0.351617] [G loss: 0.197662] [ema: 0.999839] 
[Epoch 84/99] [Batch 500/509] [D loss: 0.368510] [G loss: 0.179191] [ema: 0.999840] 
[Epoch 85/99] [Batch 0/509] [D loss: 0.345195] [G loss: 0.215388] [ema: 0.999840] 
[Epoch 85/99] [Batch 100/509] [D loss: 0.332139] [G loss: 0.191922] [ema: 0.999840] 
[Epoch 85/99] [Batch 200/509] [D loss: 0.326865] [G loss: 0.212360] [ema: 0.999841] 
[Epoch 85/99] [Batch 300/509] [D loss: 0.349168] [G loss: 0.190375] [ema: 0.999841] 
[Epoch 85/99] [Batch 400/509] [D loss: 0.425720] [G loss: 0.184511] [ema: 0.999841] 
[Epoch 85/99] [Batch 500/509] [D loss: 0.330347] [G loss: 0.201035] [ema: 0.999842] 
[Epoch 86/99] [Batch 0/509] [D loss: 0.348505] [G loss: 0.214515] [ema: 0.999842] 
[Epoch 86/99] [Batch 100/509] [D loss: 0.350016] [G loss: 0.193505] [ema: 0.999842] 
[Epoch 86/99] [Batch 200/509] [D loss: 0.391980] [G loss: 0.205530] [ema: 0.999842] 
[Epoch 86/99] [Batch 300/509] [D loss: 0.346800] [G loss: 0.196589] [ema: 0.999843] 
[Epoch 86/99] [Batch 400/509] [D loss: 0.335375] [G loss: 0.203559] [ema: 0.999843] 
[Epoch 86/99] [Batch 500/509] [D loss: 0.365024] [G loss: 0.187872] [ema: 0.999843] 
[Epoch 87/99] [Batch 0/509] [D loss: 0.403863] [G loss: 0.188309] [ema: 0.999843] 
[Epoch 87/99] [Batch 100/509] [D loss: 0.361797] [G loss: 0.205573] [ema: 0.999844] 
[Epoch 87/99] [Batch 200/509] [D loss: 0.368921] [G loss: 0.189066] [ema: 0.999844] 
[Epoch 87/99] [Batch 300/509] [D loss: 0.355809] [G loss: 0.205344] [ema: 0.999845] 
[Epoch 87/99] [Batch 400/509] [D loss: 0.354927] [G loss: 0.155687] [ema: 0.999845] 
[Epoch 87/99] [Batch 500/509] [D loss: 0.408098] [G loss: 0.194311] [ema: 0.999845] 
[Epoch 88/99] [Batch 0/509] [D loss: 0.338685] [G loss: 0.206808] [ema: 0.999845] 
[Epoch 88/99] [Batch 100/509] [D loss: 0.347373] [G loss: 0.201320] [ema: 0.999846] 
[Epoch 88/99] [Batch 200/509] [D loss: 0.438695] [G loss: 0.160817] [ema: 0.999846] 
[Epoch 88/99] [Batch 300/509] [D loss: 0.347638] [G loss: 0.205198] [ema: 0.999846] 
[Epoch 88/99] [Batch 400/509] [D loss: 0.404556] [G loss: 0.202827] [ema: 0.999847] 
[Epoch 88/99] [Batch 500/509] [D loss: 0.320210] [G loss: 0.175924] [ema: 0.999847] 
[Epoch 89/99] [Batch 0/509] [D loss: 0.385480] [G loss: 0.185146] [ema: 0.999847] 
[Epoch 89/99] [Batch 100/509] [D loss: 0.455990] [G loss: 0.208757] [ema: 0.999847] 
[Epoch 89/99] [Batch 200/509] [D loss: 0.359962] [G loss: 0.192846] [ema: 0.999848] 
[Epoch 89/99] [Batch 300/509] [D loss: 0.334720] [G loss: 0.206522] [ema: 0.999848] 
[Epoch 89/99] [Batch 400/509] [D loss: 0.378209] [G loss: 0.176149] [ema: 0.999848] 
[Epoch 89/99] [Batch 500/509] [D loss: 0.410607] [G loss: 0.206975] [ema: 0.999849] 
[Epoch 90/99] [Batch 0/509] [D loss: 0.367359] [G loss: 0.225266] [ema: 0.999849] 
[Epoch 90/99] [Batch 100/509] [D loss: 0.336496] [G loss: 0.194905] [ema: 0.999849] 
[Epoch 90/99] [Batch 200/509] [D loss: 0.386113] [G loss: 0.202101] [ema: 0.999849] 
[Epoch 90/99] [Batch 300/509] [D loss: 0.377087] [G loss: 0.202568] [ema: 0.999850] 
[Epoch 90/99] [Batch 400/509] [D loss: 0.319977] [G loss: 0.223226] [ema: 0.999850] 
[Epoch 90/99] [Batch 500/509] [D loss: 0.413072] [G loss: 0.188063] [ema: 0.999850] 
[Epoch 91/99] [Batch 0/509] [D loss: 0.366272] [G loss: 0.196622] [ema: 0.999850] 
[Epoch 91/99] [Batch 100/509] [D loss: 0.399898] [G loss: 0.195451] [ema: 0.999851] 
[Epoch 91/99] [Batch 200/509] [D loss: 0.346443] [G loss: 0.217822] [ema: 0.999851] 
[Epoch 91/99] [Batch 300/509] [D loss: 0.380405] [G loss: 0.205111] [ema: 0.999851] 
[Epoch 91/99] [Batch 400/509] [D loss: 0.361333] [G loss: 0.193312] [ema: 0.999852] 
[Epoch 91/99] [Batch 500/509] [D loss: 0.371064] [G loss: 0.206768] [ema: 0.999852] 
[Epoch 92/99] [Batch 0/509] [D loss: 0.330092] [G loss: 0.229462] [ema: 0.999852] 
[Epoch 92/99] [Batch 100/509] [D loss: 0.377452] [G loss: 0.200048] [ema: 0.999852] 
[Epoch 92/99] [Batch 200/509] [D loss: 0.342755] [G loss: 0.186000] [ema: 0.999853] 
[Epoch 92/99] [Batch 300/509] [D loss: 0.367664] [G loss: 0.185470] [ema: 0.999853] 
[Epoch 92/99] [Batch 400/509] [D loss: 0.377038] [G loss: 0.199781] [ema: 0.999853] 
[Epoch 92/99] [Batch 500/509] [D loss: 0.339207] [G loss: 0.212659] [ema: 0.999854] 
[Epoch 93/99] [Batch 0/509] [D loss: 0.382683] [G loss: 0.193955] [ema: 0.999854] 
[Epoch 93/99] [Batch 100/509] [D loss: 0.371431] [G loss: 0.183548] [ema: 0.999854] 
[Epoch 93/99] [Batch 200/509] [D loss: 0.343770] [G loss: 0.221530] [ema: 0.999854] 
[Epoch 93/99] [Batch 300/509] [D loss: 0.397249] [G loss: 0.174463] [ema: 0.999855] 
[Epoch 93/99] [Batch 400/509] [D loss: 0.320047] [G loss: 0.194909] [ema: 0.999855] 
[Epoch 93/99] [Batch 500/509] [D loss: 0.375841] [G loss: 0.201684] [ema: 0.999855] 
[Epoch 94/99] [Batch 0/509] [D loss: 0.428777] [G loss: 0.198449] [ema: 0.999855] 
[Epoch 94/99] [Batch 100/509] [D loss: 0.371787] [G loss: 0.210375] [ema: 0.999855] 
[Epoch 94/99] [Batch 200/509] [D loss: 0.382737] [G loss: 0.194831] [ema: 0.999856] 
[Epoch 94/99] [Batch 300/509] [D loss: 0.389084] [G loss: 0.177795] [ema: 0.999856] 
[Epoch 94/99] [Batch 400/509] [D loss: 0.380712] [G loss: 0.192929] [ema: 0.999856] 
[Epoch 94/99] [Batch 500/509] [D loss: 0.387156] [G loss: 0.209892] [ema: 0.999857] 
[Epoch 95/99] [Batch 0/509] [D loss: 0.318724] [G loss: 0.202602] [ema: 0.999857] 
[Epoch 95/99] [Batch 100/509] [D loss: 0.376445] [G loss: 0.189688] [ema: 0.999857] 
[Epoch 95/99] [Batch 200/509] [D loss: 0.380259] [G loss: 0.202117] [ema: 0.999857] 
[Epoch 95/99] [Batch 300/509] [D loss: 0.380884] [G loss: 0.202597] [ema: 0.999858] 
[Epoch 95/99] [Batch 400/509] [D loss: 0.334028] [G loss: 0.220159] [ema: 0.999858] 
[Epoch 95/99] [Batch 500/509] [D loss: 0.388625] [G loss: 0.186170] [ema: 0.999858] 
[Epoch 96/99] [Batch 0/509] [D loss: 0.348772] [G loss: 0.175643] [ema: 0.999858] 
[Epoch 96/99] [Batch 100/509] [D loss: 0.372525] [G loss: 0.195046] [ema: 0.999858] 
[Epoch 96/99] [Batch 200/509] [D loss: 0.350145] [G loss: 0.220060] [ema: 0.999859] 
[Epoch 96/99] [Batch 300/509] [D loss: 0.297558] [G loss: 0.203350] [ema: 0.999859] 
[Epoch 96/99] [Batch 400/509] [D loss: 0.412805] [G loss: 0.174863] [ema: 0.999859] 
[Epoch 96/99] [Batch 500/509] [D loss: 0.352763] [G loss: 0.216930] [ema: 0.999860] 
[Epoch 97/99] [Batch 0/509] [D loss: 0.313824] [G loss: 0.197482] [ema: 0.999860] 
[Epoch 97/99] [Batch 100/509] [D loss: 0.357977] [G loss: 0.200261] [ema: 0.999860] 
[Epoch 97/99] [Batch 200/509] [D loss: 0.397231] [G loss: 0.198151] [ema: 0.999860] 
[Epoch 97/99] [Batch 300/509] [D loss: 0.335063] [G loss: 0.212655] [ema: 0.999860] 
[Epoch 97/99] [Batch 400/509] [D loss: 0.440548] [G loss: 0.198047] [ema: 0.999861] 
[Epoch 97/99] [Batch 500/509] [D loss: 0.418315] [G loss: 0.196819] [ema: 0.999861] 
[Epoch 98/99] [Batch 0/509] [D loss: 0.343367] [G loss: 0.205083] [ema: 0.999861] 
[Epoch 98/99] [Batch 100/509] [D loss: 0.372357] [G loss: 0.198617] [ema: 0.999861] 
[Epoch 98/99] [Batch 200/509] [D loss: 0.383972] [G loss: 0.198065] [ema: 0.999862] 
[Epoch 98/99] [Batch 300/509] [D loss: 0.348409] [G loss: 0.193251] [ema: 0.999862] 
[Epoch 98/99] [Batch 400/509] [D loss: 0.365697] [G loss: 0.191158] [ema: 0.999862] 
[Epoch 98/99] [Batch 500/509] [D loss: 0.355881] [G loss: 0.186105] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
walk training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
walk
daghar
return single class data and labels, class is walk
data shape is (8614, 3, 1, 60)
label shape is (8614,)
539
Epochs between checkpoint: 24



Saving checkpoint 1 in logs/daghar_50000_60_100/walk_50000_D_60_2024_10_23_18_27_33/Model



[Epoch 0/93] [Batch 0/539] [D loss: 0.993082] [G loss: 0.560338] [ema: 0.000000] 
[Epoch 0/93] [Batch 100/539] [D loss: 0.367588] [G loss: 0.237731] [ema: 0.933033] 
[Epoch 0/93] [Batch 200/539] [D loss: 0.403112] [G loss: 0.197361] [ema: 0.965936] 
[Epoch 0/93] [Batch 300/539] [D loss: 0.332184] [G loss: 0.234830] [ema: 0.977160] 
[Epoch 0/93] [Batch 400/539] [D loss: 0.337199] [G loss: 0.199707] [ema: 0.982821] 
[Epoch 0/93] [Batch 500/539] [D loss: 0.328371] [G loss: 0.228542] [ema: 0.986233] 
[Epoch 1/93] [Batch 0/539] [D loss: 0.319254] [G loss: 0.243355] [ema: 0.987222] 
[Epoch 1/93] [Batch 100/539] [D loss: 0.343316] [G loss: 0.276833] [ema: 0.989211] 
[Epoch 1/93] [Batch 200/539] [D loss: 0.310034] [G loss: 0.188243] [ema: 0.990664] 
[Epoch 1/93] [Batch 300/539] [D loss: 0.424682] [G loss: 0.167815] [ema: 0.991772] 
[Epoch 1/93] [Batch 400/539] [D loss: 0.328548] [G loss: 0.203644] [ema: 0.992645] 
[Epoch 1/93] [Batch 500/539] [D loss: 0.384841] [G loss: 0.150995] [ema: 0.993351] 
[Epoch 2/93] [Batch 0/539] [D loss: 0.434494] [G loss: 0.177736] [ema: 0.993591] 
[Epoch 2/93] [Batch 100/539] [D loss: 0.397146] [G loss: 0.193347] [ema: 0.994133] 
[Epoch 2/93] [Batch 200/539] [D loss: 0.358141] [G loss: 0.240086] [ema: 0.994591] 
[Epoch 2/93] [Batch 300/539] [D loss: 0.345476] [G loss: 0.213599] [ema: 0.994983] 
[Epoch 2/93] [Batch 400/539] [D loss: 0.395774] [G loss: 0.184345] [ema: 0.995321] 
[Epoch 2/93] [Batch 500/539] [D loss: 0.455018] [G loss: 0.148445] [ema: 0.995617] 
[Epoch 3/93] [Batch 0/539] [D loss: 0.446469] [G loss: 0.183595] [ema: 0.995723] 
[Epoch 3/93] [Batch 100/539] [D loss: 0.451095] [G loss: 0.182216] [ema: 0.995971] 
[Epoch 3/93] [Batch 200/539] [D loss: 0.553852] [G loss: 0.143571] [ema: 0.996192] 
[Epoch 3/93] [Batch 300/539] [D loss: 0.416244] [G loss: 0.163154] [ema: 0.996391] 
[Epoch 3/93] [Batch 400/539] [D loss: 0.466671] [G loss: 0.155202] [ema: 0.996569] 
[Epoch 3/93] [Batch 500/539] [D loss: 0.459210] [G loss: 0.157910] [ema: 0.996731] 
[Epoch 4/93] [Batch 0/539] [D loss: 0.484583] [G loss: 0.143105] [ema: 0.996790] 
[Epoch 4/93] [Batch 100/539] [D loss: 0.567548] [G loss: 0.133272] [ema: 0.996932] 
[Epoch 4/93] [Batch 200/539] [D loss: 0.469634] [G loss: 0.146274] [ema: 0.997062] 
[Epoch 4/93] [Batch 300/539] [D loss: 0.481488] [G loss: 0.175535] [ema: 0.997182] 
[Epoch 4/93] [Batch 400/539] [D loss: 0.495921] [G loss: 0.130514] [ema: 0.997292] 
[Epoch 4/93] [Batch 500/539] [D loss: 0.508079] [G loss: 0.147934] [ema: 0.997394] 
[Epoch 5/93] [Batch 0/539] [D loss: 0.525354] [G loss: 0.150247] [ema: 0.997431] 
[Epoch 5/93] [Batch 100/539] [D loss: 0.466444] [G loss: 0.140756] [ema: 0.997523] 
[Epoch 5/93] [Batch 200/539] [D loss: 0.513505] [G loss: 0.144107] [ema: 0.997609] 
[Epoch 5/93] [Batch 300/539] [D loss: 0.510368] [G loss: 0.138980] [ema: 0.997688] 
[Epoch 5/93] [Batch 400/539] [D loss: 0.456231] [G loss: 0.120111] [ema: 0.997763] 
[Epoch 5/93] [Batch 500/539] [D loss: 0.495311] [G loss: 0.133872] [ema: 0.997833] 
[Epoch 6/93] [Batch 0/539] [D loss: 0.492935] [G loss: 0.126801] [ema: 0.997859] 
[Epoch 6/93] [Batch 100/539] [D loss: 0.557368] [G loss: 0.162422] [ema: 0.997923] 
[Epoch 6/93] [Batch 200/539] [D loss: 0.526226] [G loss: 0.110321] [ema: 0.997984] 
[Epoch 6/93] [Batch 300/539] [D loss: 0.521034] [G loss: 0.127490] [ema: 0.998041] 
[Epoch 6/93] [Batch 400/539] [D loss: 0.487405] [G loss: 0.104469] [ema: 0.998094] 
[Epoch 6/93] [Batch 500/539] [D loss: 0.558271] [G loss: 0.131136] [ema: 0.998145] 
[Epoch 7/93] [Batch 0/539] [D loss: 0.463612] [G loss: 0.156442] [ema: 0.998165] 
[Epoch 7/93] [Batch 100/539] [D loss: 0.444322] [G loss: 0.147522] [ema: 0.998212] 
[Epoch 7/93] [Batch 200/539] [D loss: 0.404542] [G loss: 0.137995] [ema: 0.998257] 
[Epoch 7/93] [Batch 300/539] [D loss: 0.496210] [G loss: 0.150669] [ema: 0.998300] 
[Epoch 7/93] [Batch 400/539] [D loss: 0.499568] [G loss: 0.153169] [ema: 0.998340] 
[Epoch 7/93] [Batch 500/539] [D loss: 0.530660] [G loss: 0.141105] [ema: 0.998379] 
[Epoch 8/93] [Batch 0/539] [D loss: 0.464303] [G loss: 0.139104] [ema: 0.998394] 
[Epoch 8/93] [Batch 100/539] [D loss: 0.494241] [G loss: 0.153873] [ema: 0.998430] 
[Epoch 8/93] [Batch 200/539] [D loss: 0.499964] [G loss: 0.122149] [ema: 0.998465] 
[Epoch 8/93] [Batch 300/539] [D loss: 0.488693] [G loss: 0.209537] [ema: 0.998498] 
[Epoch 8/93] [Batch 400/539] [D loss: 0.450041] [G loss: 0.126427] [ema: 0.998530] 
[Epoch 8/93] [Batch 500/539] [D loss: 0.437703] [G loss: 0.139773] [ema: 0.998561] 
[Epoch 9/93] [Batch 0/539] [D loss: 0.407232] [G loss: 0.225374] [ema: 0.998572] 
[Epoch 9/93] [Batch 100/539] [D loss: 0.462389] [G loss: 0.141004] [ema: 0.998601] 
[Epoch 9/93] [Batch 200/539] [D loss: 0.423937] [G loss: 0.159940] [ema: 0.998629] 
[Epoch 9/93] [Batch 300/539] [D loss: 0.468187] [G loss: 0.174392] [ema: 0.998655] 
[Epoch 9/93] [Batch 400/539] [D loss: 0.409447] [G loss: 0.168317] [ema: 0.998681] 
[Epoch 9/93] [Batch 500/539] [D loss: 0.462080] [G loss: 0.187150] [ema: 0.998705] 
[Epoch 10/93] [Batch 0/539] [D loss: 0.418261] [G loss: 0.201180] [ema: 0.998715] 
[Epoch 10/93] [Batch 100/539] [D loss: 0.400973] [G loss: 0.150151] [ema: 0.998738] 
[Epoch 10/93] [Batch 200/539] [D loss: 0.386250] [G loss: 0.187729] [ema: 0.998761] 
[Epoch 10/93] [Batch 300/539] [D loss: 0.418185] [G loss: 0.172199] [ema: 0.998783] 
[Epoch 10/93] [Batch 400/539] [D loss: 0.417698] [G loss: 0.192837] [ema: 0.998804] 
[Epoch 10/93] [Batch 500/539] [D loss: 0.356877] [G loss: 0.144833] [ema: 0.998824] 
[Epoch 11/93] [Batch 0/539] [D loss: 0.349087] [G loss: 0.200006] [ema: 0.998832] 
[Epoch 11/93] [Batch 100/539] [D loss: 0.464294] [G loss: 0.187629] [ema: 0.998851] 
[Epoch 11/93] [Batch 200/539] [D loss: 0.360384] [G loss: 0.189164] [ema: 0.998870] 
[Epoch 11/93] [Batch 300/539] [D loss: 0.370907] [G loss: 0.188622] [ema: 0.998888] 
[Epoch 11/93] [Batch 400/539] [D loss: 0.323667] [G loss: 0.206142] [ema: 0.998905] 
[Epoch 11/93] [Batch 500/539] [D loss: 0.365438] [G loss: 0.170451] [ema: 0.998922] 
[Epoch 12/93] [Batch 0/539] [D loss: 0.409110] [G loss: 0.171231] [ema: 0.998929] 
[Epoch 12/93] [Batch 100/539] [D loss: 0.453194] [G loss: 0.191249] [ema: 0.998945] 
[Epoch 12/93] [Batch 200/539] [D loss: 0.394343] [G loss: 0.191983] [ema: 0.998961] 
[Epoch 12/93] [Batch 300/539] [D loss: 0.379703] [G loss: 0.189777] [ema: 0.998976] 
[Epoch 12/93] [Batch 400/539] [D loss: 0.352041] [G loss: 0.217886] [ema: 0.998991] 
[Epoch 12/93] [Batch 500/539] [D loss: 0.329810] [G loss: 0.161668] [ema: 0.999006] 
[Epoch 13/93] [Batch 0/539] [D loss: 0.430705] [G loss: 0.178108] [ema: 0.999011] 
[Epoch 13/93] [Batch 100/539] [D loss: 0.380461] [G loss: 0.191544] [ema: 0.999025] 
[Epoch 13/93] [Batch 200/539] [D loss: 0.331764] [G loss: 0.224146] [ema: 0.999039] 
[Epoch 13/93] [Batch 300/539] [D loss: 0.383477] [G loss: 0.188912] [ema: 0.999052] 
[Epoch 13/93] [Batch 400/539] [D loss: 0.344224] [G loss: 0.181704] [ema: 0.999065] 
[Epoch 13/93] [Batch 500/539] [D loss: 0.371934] [G loss: 0.218301] [ema: 0.999077] 
[Epoch 14/93] [Batch 0/539] [D loss: 0.385492] [G loss: 0.199287] [ema: 0.999082] 
[Epoch 14/93] [Batch 100/539] [D loss: 0.397197] [G loss: 0.253645] [ema: 0.999094] 
[Epoch 14/93] [Batch 200/539] [D loss: 0.388471] [G loss: 0.226387] [ema: 0.999106] 
[Epoch 14/93] [Batch 300/539] [D loss: 0.425365] [G loss: 0.193675] [ema: 0.999117] 
[Epoch 14/93] [Batch 400/539] [D loss: 0.332255] [G loss: 0.207648] [ema: 0.999128] 
[Epoch 14/93] [Batch 500/539] [D loss: 0.372066] [G loss: 0.183034] [ema: 0.999139] 
[Epoch 15/93] [Batch 0/539] [D loss: 0.417891] [G loss: 0.190836] [ema: 0.999143] 
[Epoch 15/93] [Batch 100/539] [D loss: 0.386350] [G loss: 0.197566] [ema: 0.999154] 
[Epoch 15/93] [Batch 200/539] [D loss: 0.329494] [G loss: 0.183349] [ema: 0.999164] 
[Epoch 15/93] [Batch 300/539] [D loss: 0.387989] [G loss: 0.203023] [ema: 0.999174] 
[Epoch 15/93] [Batch 400/539] [D loss: 0.401421] [G loss: 0.205685] [ema: 0.999183] 
[Epoch 15/93] [Batch 500/539] [D loss: 0.424703] [G loss: 0.195210] [ema: 0.999193] 
[Epoch 16/93] [Batch 0/539] [D loss: 0.409083] [G loss: 0.198203] [ema: 0.999197] 
[Epoch 16/93] [Batch 100/539] [D loss: 0.369755] [G loss: 0.181911] [ema: 0.999206] 
[Epoch 16/93] [Batch 200/539] [D loss: 0.422914] [G loss: 0.157786] [ema: 0.999215] 
[Epoch 16/93] [Batch 300/539] [D loss: 0.398826] [G loss: 0.176236] [ema: 0.999224] 
[Epoch 16/93] [Batch 400/539] [D loss: 0.333746] [G loss: 0.197022] [ema: 0.999232] 
[Epoch 16/93] [Batch 500/539] [D loss: 0.345496] [G loss: 0.193054] [ema: 0.999241] 
[Epoch 17/93] [Batch 0/539] [D loss: 0.340373] [G loss: 0.216553] [ema: 0.999244] 
[Epoch 17/93] [Batch 100/539] [D loss: 0.365632] [G loss: 0.218083] [ema: 0.999252] 
[Epoch 17/93] [Batch 200/539] [D loss: 0.348494] [G loss: 0.193247] [ema: 0.999260] 
[Epoch 17/93] [Batch 300/539] [D loss: 0.330212] [G loss: 0.209643] [ema: 0.999268] 
[Epoch 17/93] [Batch 400/539] [D loss: 0.343556] [G loss: 0.187350] [ema: 0.999275] 
[Epoch 17/93] [Batch 500/539] [D loss: 0.415393] [G loss: 0.221585] [ema: 0.999283] 
[Epoch 18/93] [Batch 0/539] [D loss: 0.338012] [G loss: 0.192047] [ema: 0.999286] 
[Epoch 18/93] [Batch 100/539] [D loss: 0.305833] [G loss: 0.243501] [ema: 0.999293] 
[Epoch 18/93] [Batch 200/539] [D loss: 0.310571] [G loss: 0.233544] [ema: 0.999300] 
[Epoch 18/93] [Batch 300/539] [D loss: 0.339091] [G loss: 0.209052] [ema: 0.999307] 
[Epoch 18/93] [Batch 400/539] [D loss: 0.308067] [G loss: 0.185249] [ema: 0.999314] 
[Epoch 18/93] [Batch 500/539] [D loss: 0.307333] [G loss: 0.194785] [ema: 0.999321] 
[Epoch 19/93] [Batch 0/539] [D loss: 0.335394] [G loss: 0.212608] [ema: 0.999323] 
[Epoch 19/93] [Batch 100/539] [D loss: 0.315354] [G loss: 0.176696] [ema: 0.999330] 
[Epoch 19/93] [Batch 200/539] [D loss: 0.319390] [G loss: 0.175642] [ema: 0.999336] 
[Epoch 19/93] [Batch 300/539] [D loss: 0.380706] [G loss: 0.198381] [ema: 0.999343] 
[Epoch 19/93] [Batch 400/539] [D loss: 0.301338] [G loss: 0.203350] [ema: 0.999349] 
[Epoch 19/93] [Batch 500/539] [D loss: 0.297673] [G loss: 0.198802] [ema: 0.999355] 
[Epoch 20/93] [Batch 0/539] [D loss: 0.373282] [G loss: 0.221382] [ema: 0.999357] 
[Epoch 20/93] [Batch 100/539] [D loss: 0.313132] [G loss: 0.213897] [ema: 0.999363] 
[Epoch 20/93] [Batch 200/539] [D loss: 0.347345] [G loss: 0.216879] [ema: 0.999369] 
[Epoch 20/93] [Batch 300/539] [D loss: 0.350209] [G loss: 0.219038] [ema: 0.999375] 
[Epoch 20/93] [Batch 400/539] [D loss: 0.342351] [G loss: 0.207534] [ema: 0.999380] 
[Epoch 20/93] [Batch 500/539] [D loss: 0.337786] [G loss: 0.216985] [ema: 0.999386] 
[Epoch 21/93] [Batch 0/539] [D loss: 0.331253] [G loss: 0.197433] [ema: 0.999388] 
[Epoch 21/93] [Batch 100/539] [D loss: 0.305497] [G loss: 0.211749] [ema: 0.999393] 
[Epoch 21/93] [Batch 200/539] [D loss: 0.304902] [G loss: 0.197585] [ema: 0.999398] 
[Epoch 21/93] [Batch 300/539] [D loss: 0.305964] [G loss: 0.199190] [ema: 0.999404] 
[Epoch 21/93] [Batch 400/539] [D loss: 0.276137] [G loss: 0.224912] [ema: 0.999409] 
[Epoch 21/93] [Batch 500/539] [D loss: 0.313174] [G loss: 0.228867] [ema: 0.999414] 
[Epoch 22/93] [Batch 0/539] [D loss: 0.279373] [G loss: 0.209830] [ema: 0.999416] 
[Epoch 22/93] [Batch 100/539] [D loss: 0.291443] [G loss: 0.235764] [ema: 0.999421] 
[Epoch 22/93] [Batch 200/539] [D loss: 0.335994] [G loss: 0.210382] [ema: 0.999425] 
[Epoch 22/93] [Batch 300/539] [D loss: 0.278731] [G loss: 0.206689] [ema: 0.999430] 
[Epoch 22/93] [Batch 400/539] [D loss: 0.354716] [G loss: 0.187462] [ema: 0.999435] 
[Epoch 22/93] [Batch 500/539] [D loss: 0.299547] [G loss: 0.216984] [ema: 0.999439] 
[Epoch 23/93] [Batch 0/539] [D loss: 0.391420] [G loss: 0.218283] [ema: 0.999441] 
[Epoch 23/93] [Batch 100/539] [D loss: 0.364166] [G loss: 0.212883] [ema: 0.999446] 
[Epoch 23/93] [Batch 200/539] [D loss: 0.378534] [G loss: 0.218841] [ema: 0.999450] 
[Epoch 23/93] [Batch 300/539] [D loss: 0.351593] [G loss: 0.193243] [ema: 0.999454] 
[Epoch 23/93] [Batch 400/539] [D loss: 0.369258] [G loss: 0.209291] [ema: 0.999458] 
[Epoch 23/93] [Batch 500/539] [D loss: 0.329678] [G loss: 0.238160] [ema: 0.999463] 



Saving checkpoint 2 in logs/daghar_50000_60_100/walk_50000_D_60_2024_10_23_18_27_33/Model



[Epoch 24/93] [Batch 0/539] [D loss: 0.385225] [G loss: 0.186604] [ema: 0.999464] 
[Epoch 24/93] [Batch 100/539] [D loss: 0.320223] [G loss: 0.218873] [ema: 0.999468] 
[Epoch 24/93] [Batch 200/539] [D loss: 0.347165] [G loss: 0.198069] [ema: 0.999472] 
[Epoch 24/93] [Batch 300/539] [D loss: 0.347475] [G loss: 0.178185] [ema: 0.999476] 
[Epoch 24/93] [Batch 400/539] [D loss: 0.331788] [G loss: 0.171772] [ema: 0.999480] 
[Epoch 24/93] [Batch 500/539] [D loss: 0.353989] [G loss: 0.204138] [ema: 0.999484] 
[Epoch 25/93] [Batch 0/539] [D loss: 0.372864] [G loss: 0.206310] [ema: 0.999486] 
[Epoch 25/93] [Batch 100/539] [D loss: 0.391029] [G loss: 0.186692] [ema: 0.999490] 
[Epoch 25/93] [Batch 200/539] [D loss: 0.370064] [G loss: 0.180164] [ema: 0.999493] 
[Epoch 25/93] [Batch 300/539] [D loss: 0.454360] [G loss: 0.154028] [ema: 0.999497] 
[Epoch 25/93] [Batch 400/539] [D loss: 0.381162] [G loss: 0.207313] [ema: 0.999501] 
[Epoch 25/93] [Batch 500/539] [D loss: 0.331268] [G loss: 0.210910] [ema: 0.999504] 
[Epoch 26/93] [Batch 0/539] [D loss: 0.479806] [G loss: 0.183377] [ema: 0.999506] 
[Epoch 26/93] [Batch 100/539] [D loss: 0.395727] [G loss: 0.192226] [ema: 0.999509] 
[Epoch 26/93] [Batch 200/539] [D loss: 0.314406] [G loss: 0.175060] [ema: 0.999512] 
[Epoch 26/93] [Batch 300/539] [D loss: 0.404380] [G loss: 0.184656] [ema: 0.999516] 
[Epoch 26/93] [Batch 400/539] [D loss: 0.433934] [G loss: 0.156565] [ema: 0.999519] 
[Epoch 26/93] [Batch 500/539] [D loss: 0.432318] [G loss: 0.194225] [ema: 0.999523] 
[Epoch 27/93] [Batch 0/539] [D loss: 0.461835] [G loss: 0.185503] [ema: 0.999524] 
[Epoch 27/93] [Batch 100/539] [D loss: 0.360726] [G loss: 0.184145] [ema: 0.999527] 
[Epoch 27/93] [Batch 200/539] [D loss: 0.401220] [G loss: 0.207275] [ema: 0.999530] 
[Epoch 27/93] [Batch 300/539] [D loss: 0.453519] [G loss: 0.147751] [ema: 0.999533] 
[Epoch 27/93] [Batch 400/539] [D loss: 0.397851] [G loss: 0.157230] [ema: 0.999537] 
[Epoch 27/93] [Batch 500/539] [D loss: 0.391028] [G loss: 0.178475] [ema: 0.999540] 
[Epoch 28/93] [Batch 0/539] [D loss: 0.446193] [G loss: 0.184803] [ema: 0.999541] 
[Epoch 28/93] [Batch 100/539] [D loss: 0.411251] [G loss: 0.165695] [ema: 0.999544] 
[Epoch 28/93] [Batch 200/539] [D loss: 0.414601] [G loss: 0.173444] [ema: 0.999547] 
[Epoch 28/93] [Batch 300/539] [D loss: 0.481112] [G loss: 0.153800] [ema: 0.999550] 
[Epoch 28/93] [Batch 400/539] [D loss: 0.396295] [G loss: 0.185177] [ema: 0.999553] 
[Epoch 28/93] [Batch 500/539] [D loss: 0.486715] [G loss: 0.137936] [ema: 0.999556] 
[Epoch 29/93] [Batch 0/539] [D loss: 0.417654] [G loss: 0.153307] [ema: 0.999557] 
[Epoch 29/93] [Batch 100/539] [D loss: 0.382559] [G loss: 0.195533] [ema: 0.999559] 
[Epoch 29/93] [Batch 200/539] [D loss: 0.390029] [G loss: 0.157298] [ema: 0.999562] 
[Epoch 29/93] [Batch 300/539] [D loss: 0.420098] [G loss: 0.166285] [ema: 0.999565] 
[Epoch 29/93] [Batch 400/539] [D loss: 0.490314] [G loss: 0.156135] [ema: 0.999568] 
[Epoch 29/93] [Batch 500/539] [D loss: 0.372449] [G loss: 0.200069] [ema: 0.999570] 
[Epoch 30/93] [Batch 0/539] [D loss: 0.396789] [G loss: 0.169451] [ema: 0.999571] 
[Epoch 30/93] [Batch 100/539] [D loss: 0.446694] [G loss: 0.149470] [ema: 0.999574] 
[Epoch 30/93] [Batch 200/539] [D loss: 0.452402] [G loss: 0.185633] [ema: 0.999577] 
[Epoch 30/93] [Batch 300/539] [D loss: 0.487275] [G loss: 0.181584] [ema: 0.999579] 
[Epoch 30/93] [Batch 400/539] [D loss: 0.464990] [G loss: 0.168470] [ema: 0.999582] 
[Epoch 30/93] [Batch 500/539] [D loss: 0.457330] [G loss: 0.179977] [ema: 0.999584] 
[Epoch 31/93] [Batch 0/539] [D loss: 0.423471] [G loss: 0.209884] [ema: 0.999585] 
[Epoch 31/93] [Batch 100/539] [D loss: 0.409064] [G loss: 0.185761] [ema: 0.999588] 
[Epoch 31/93] [Batch 200/539] [D loss: 0.408908] [G loss: 0.147940] [ema: 0.999590] 
[Epoch 31/93] [Batch 300/539] [D loss: 0.435061] [G loss: 0.173688] [ema: 0.999593] 
[Epoch 31/93] [Batch 400/539] [D loss: 0.362334] [G loss: 0.197230] [ema: 0.999595] 
[Epoch 31/93] [Batch 500/539] [D loss: 0.442509] [G loss: 0.161083] [ema: 0.999597] 
[Epoch 32/93] [Batch 0/539] [D loss: 0.408743] [G loss: 0.177345] [ema: 0.999598] 
[Epoch 32/93] [Batch 100/539] [D loss: 0.448080] [G loss: 0.197712] [ema: 0.999601] 
[Epoch 32/93] [Batch 200/539] [D loss: 0.479551] [G loss: 0.157167] [ema: 0.999603] 
[Epoch 32/93] [Batch 300/539] [D loss: 0.378414] [G loss: 0.153009] [ema: 0.999605] 
[Epoch 32/93] [Batch 400/539] [D loss: 0.379834] [G loss: 0.192416] [ema: 0.999607] 
[Epoch 32/93] [Batch 500/539] [D loss: 0.386805] [G loss: 0.167341] [ema: 0.999610] 
[Epoch 33/93] [Batch 0/539] [D loss: 0.412380] [G loss: 0.194156] [ema: 0.999610] 
[Epoch 33/93] [Batch 100/539] [D loss: 0.404002] [G loss: 0.186396] [ema: 0.999613] 
[Epoch 33/93] [Batch 200/539] [D loss: 0.406295] [G loss: 0.171639] [ema: 0.999615] 
[Epoch 33/93] [Batch 300/539] [D loss: 0.438097] [G loss: 0.193786] [ema: 0.999617] 
[Epoch 33/93] [Batch 400/539] [D loss: 0.413512] [G loss: 0.145480] [ema: 0.999619] 
[Epoch 33/93] [Batch 500/539] [D loss: 0.342858] [G loss: 0.202132] [ema: 0.999621] 
[Epoch 34/93] [Batch 0/539] [D loss: 0.371899] [G loss: 0.198209] [ema: 0.999622] 
[Epoch 34/93] [Batch 100/539] [D loss: 0.416219] [G loss: 0.175865] [ema: 0.999624] 
[Epoch 34/93] [Batch 200/539] [D loss: 0.387870] [G loss: 0.168982] [ema: 0.999626] 
[Epoch 34/93] [Batch 300/539] [D loss: 0.374765] [G loss: 0.200670] [ema: 0.999628] 
[Epoch 34/93] [Batch 400/539] [D loss: 0.414509] [G loss: 0.189161] [ema: 0.999630] 
[Epoch 34/93] [Batch 500/539] [D loss: 0.392918] [G loss: 0.181135] [ema: 0.999632] 
[Epoch 35/93] [Batch 0/539] [D loss: 0.448960] [G loss: 0.185299] [ema: 0.999633] 
[Epoch 35/93] [Batch 100/539] [D loss: 0.366463] [G loss: 0.190983] [ema: 0.999635] 
[Epoch 35/93] [Batch 200/539] [D loss: 0.388197] [G loss: 0.189622] [ema: 0.999636] 
[Epoch 35/93] [Batch 300/539] [D loss: 0.406274] [G loss: 0.199173] [ema: 0.999638] 
[Epoch 35/93] [Batch 400/539] [D loss: 0.396344] [G loss: 0.200385] [ema: 0.999640] 
[Epoch 35/93] [Batch 500/539] [D loss: 0.361998] [G loss: 0.163753] [ema: 0.999642] 
[Epoch 36/93] [Batch 0/539] [D loss: 0.380851] [G loss: 0.181015] [ema: 0.999643] 
[Epoch 36/93] [Batch 100/539] [D loss: 0.366635] [G loss: 0.214488] [ema: 0.999645] 
[Epoch 36/93] [Batch 200/539] [D loss: 0.413152] [G loss: 0.192320] [ema: 0.999646] 
[Epoch 36/93] [Batch 300/539] [D loss: 0.323675] [G loss: 0.192230] [ema: 0.999648] 
[Epoch 36/93] [Batch 400/539] [D loss: 0.435881] [G loss: 0.194310] [ema: 0.999650] 
[Epoch 36/93] [Batch 500/539] [D loss: 0.325940] [G loss: 0.175660] [ema: 0.999652] 
[Epoch 37/93] [Batch 0/539] [D loss: 0.429321] [G loss: 0.203884] [ema: 0.999652] 
[Epoch 37/93] [Batch 100/539] [D loss: 0.402210] [G loss: 0.175679] [ema: 0.999654] 
[Epoch 37/93] [Batch 200/539] [D loss: 0.369964] [G loss: 0.184140] [ema: 0.999656] 
[Epoch 37/93] [Batch 300/539] [D loss: 0.432728] [G loss: 0.178584] [ema: 0.999658] 
[Epoch 37/93] [Batch 400/539] [D loss: 0.352332] [G loss: 0.159131] [ema: 0.999659] 
[Epoch 37/93] [Batch 500/539] [D loss: 0.426492] [G loss: 0.187757] [ema: 0.999661] 
[Epoch 38/93] [Batch 0/539] [D loss: 0.360808] [G loss: 0.192899] [ema: 0.999662] 
[Epoch 38/93] [Batch 100/539] [D loss: 0.375495] [G loss: 0.182232] [ema: 0.999663] 
[Epoch 38/93] [Batch 200/539] [D loss: 0.437025] [G loss: 0.234060] [ema: 0.999665] 
[Epoch 38/93] [Batch 300/539] [D loss: 0.359212] [G loss: 0.187660] [ema: 0.999667] 
[Epoch 38/93] [Batch 400/539] [D loss: 0.399986] [G loss: 0.184289] [ema: 0.999668] 
[Epoch 38/93] [Batch 500/539] [D loss: 0.374965] [G loss: 0.226240] [ema: 0.999670] 
[Epoch 39/93] [Batch 0/539] [D loss: 0.381595] [G loss: 0.196033] [ema: 0.999670] 
[Epoch 39/93] [Batch 100/539] [D loss: 0.378904] [G loss: 0.181883] [ema: 0.999672] 
[Epoch 39/93] [Batch 200/539] [D loss: 0.319781] [G loss: 0.225303] [ema: 0.999673] 
[Epoch 39/93] [Batch 300/539] [D loss: 0.364137] [G loss: 0.193767] [ema: 0.999675] 
[Epoch 39/93] [Batch 400/539] [D loss: 0.358302] [G loss: 0.206681] [ema: 0.999676] 
[Epoch 39/93] [Batch 500/539] [D loss: 0.373505] [G loss: 0.165916] [ema: 0.999678] 
[Epoch 40/93] [Batch 0/539] [D loss: 0.378972] [G loss: 0.180685] [ema: 0.999679] 
[Epoch 40/93] [Batch 100/539] [D loss: 0.383953] [G loss: 0.241604] [ema: 0.999680] 
[Epoch 40/93] [Batch 200/539] [D loss: 0.370506] [G loss: 0.165853] [ema: 0.999682] 
[Epoch 40/93] [Batch 300/539] [D loss: 0.405942] [G loss: 0.190150] [ema: 0.999683] 
[Epoch 40/93] [Batch 400/539] [D loss: 0.324516] [G loss: 0.225113] [ema: 0.999684] 
[Epoch 40/93] [Batch 500/539] [D loss: 0.344152] [G loss: 0.187221] [ema: 0.999686] 
[Epoch 41/93] [Batch 0/539] [D loss: 0.352327] [G loss: 0.198154] [ema: 0.999686] 
[Epoch 41/93] [Batch 100/539] [D loss: 0.363710] [G loss: 0.192884] [ema: 0.999688] 
[Epoch 41/93] [Batch 200/539] [D loss: 0.371290] [G loss: 0.182930] [ema: 0.999689] 
[Epoch 41/93] [Batch 300/539] [D loss: 0.379954] [G loss: 0.209103] [ema: 0.999691] 
[Epoch 41/93] [Batch 400/539] [D loss: 0.382957] [G loss: 0.196349] [ema: 0.999692] 
[Epoch 41/93] [Batch 500/539] [D loss: 0.352572] [G loss: 0.217275] [ema: 0.999693] 
[Epoch 42/93] [Batch 0/539] [D loss: 0.398111] [G loss: 0.203685] [ema: 0.999694] 
[Epoch 42/93] [Batch 100/539] [D loss: 0.356761] [G loss: 0.205706] [ema: 0.999695] 
[Epoch 42/93] [Batch 200/539] [D loss: 0.436118] [G loss: 0.202150] [ema: 0.999697] 
[Epoch 42/93] [Batch 300/539] [D loss: 0.414469] [G loss: 0.210462] [ema: 0.999698] 
[Epoch 42/93] [Batch 400/539] [D loss: 0.401209] [G loss: 0.211082] [ema: 0.999699] 
[Epoch 42/93] [Batch 500/539] [D loss: 0.334877] [G loss: 0.221223] [ema: 0.999700] 
[Epoch 43/93] [Batch 0/539] [D loss: 0.338056] [G loss: 0.196447] [ema: 0.999701] 
[Epoch 43/93] [Batch 100/539] [D loss: 0.360699] [G loss: 0.222462] [ema: 0.999702] 
[Epoch 43/93] [Batch 200/539] [D loss: 0.410001] [G loss: 0.159730] [ema: 0.999704] 
[Epoch 43/93] [Batch 300/539] [D loss: 0.303042] [G loss: 0.196009] [ema: 0.999705] 
[Epoch 43/93] [Batch 400/539] [D loss: 0.403463] [G loss: 0.178081] [ema: 0.999706] 
[Epoch 43/93] [Batch 500/539] [D loss: 0.459314] [G loss: 0.222714] [ema: 0.999707] 
[Epoch 44/93] [Batch 0/539] [D loss: 0.401136] [G loss: 0.200397] [ema: 0.999708] 
[Epoch 44/93] [Batch 100/539] [D loss: 0.405742] [G loss: 0.196503] [ema: 0.999709] 
[Epoch 44/93] [Batch 200/539] [D loss: 0.356093] [G loss: 0.203837] [ema: 0.999710] 
[Epoch 44/93] [Batch 300/539] [D loss: 0.294745] [G loss: 0.182498] [ema: 0.999711] 
[Epoch 44/93] [Batch 400/539] [D loss: 0.417586] [G loss: 0.225517] [ema: 0.999713] 
[Epoch 44/93] [Batch 500/539] [D loss: 0.333789] [G loss: 0.218766] [ema: 0.999714] 
[Epoch 45/93] [Batch 0/539] [D loss: 0.331740] [G loss: 0.196361] [ema: 0.999714] 
[Epoch 45/93] [Batch 100/539] [D loss: 0.344427] [G loss: 0.213346] [ema: 0.999715] 
[Epoch 45/93] [Batch 200/539] [D loss: 0.425663] [G loss: 0.181251] [ema: 0.999717] 
[Epoch 45/93] [Batch 300/539] [D loss: 0.323771] [G loss: 0.191134] [ema: 0.999718] 
[Epoch 45/93] [Batch 400/539] [D loss: 0.333396] [G loss: 0.216393] [ema: 0.999719] 
[Epoch 45/93] [Batch 500/539] [D loss: 0.327190] [G loss: 0.206902] [ema: 0.999720] 
[Epoch 46/93] [Batch 0/539] [D loss: 0.347396] [G loss: 0.210865] [ema: 0.999720] 
[Epoch 46/93] [Batch 100/539] [D loss: 0.355967] [G loss: 0.193267] [ema: 0.999722] 
[Epoch 46/93] [Batch 200/539] [D loss: 0.355078] [G loss: 0.176969] [ema: 0.999723] 
[Epoch 46/93] [Batch 300/539] [D loss: 0.376767] [G loss: 0.190490] [ema: 0.999724] 
[Epoch 46/93] [Batch 400/539] [D loss: 0.395572] [G loss: 0.192952] [ema: 0.999725] 
[Epoch 46/93] [Batch 500/539] [D loss: 0.400500] [G loss: 0.187086] [ema: 0.999726] 
[Epoch 47/93] [Batch 0/539] [D loss: 0.336620] [G loss: 0.229077] [ema: 0.999726] 
[Epoch 47/93] [Batch 100/539] [D loss: 0.359533] [G loss: 0.191791] [ema: 0.999727] 
[Epoch 47/93] [Batch 200/539] [D loss: 0.410574] [G loss: 0.215505] [ema: 0.999729] 
[Epoch 47/93] [Batch 300/539] [D loss: 0.359518] [G loss: 0.195239] [ema: 0.999730] 
[Epoch 47/93] [Batch 400/539] [D loss: 0.325099] [G loss: 0.202631] [ema: 0.999731] 
[Epoch 47/93] [Batch 500/539] [D loss: 0.386186] [G loss: 0.224188] [ema: 0.999732] 



Saving checkpoint 3 in logs/daghar_50000_60_100/walk_50000_D_60_2024_10_23_18_27_33/Model



[Epoch 48/93] [Batch 0/539] [D loss: 0.361208] [G loss: 0.177350] [ema: 0.999732] 
[Epoch 48/93] [Batch 100/539] [D loss: 0.400192] [G loss: 0.185234] [ema: 0.999733] 
[Epoch 48/93] [Batch 200/539] [D loss: 0.387302] [G loss: 0.182935] [ema: 0.999734] 
[Epoch 48/93] [Batch 300/539] [D loss: 0.328748] [G loss: 0.209049] [ema: 0.999735] 
[Epoch 48/93] [Batch 400/539] [D loss: 0.320012] [G loss: 0.230632] [ema: 0.999736] 
[Epoch 48/93] [Batch 500/539] [D loss: 0.383160] [G loss: 0.167081] [ema: 0.999737] 
[Epoch 49/93] [Batch 0/539] [D loss: 0.345494] [G loss: 0.206954] [ema: 0.999738] 
[Epoch 49/93] [Batch 100/539] [D loss: 0.372567] [G loss: 0.192295] [ema: 0.999739] 
[Epoch 49/93] [Batch 200/539] [D loss: 0.386555] [G loss: 0.182930] [ema: 0.999740] 
[Epoch 49/93] [Batch 300/539] [D loss: 0.362789] [G loss: 0.200876] [ema: 0.999741] 
[Epoch 49/93] [Batch 400/539] [D loss: 0.364394] [G loss: 0.156407] [ema: 0.999742] 
[Epoch 49/93] [Batch 500/539] [D loss: 0.382795] [G loss: 0.181165] [ema: 0.999742] 
[Epoch 50/93] [Batch 0/539] [D loss: 0.367073] [G loss: 0.187426] [ema: 0.999743] 
[Epoch 50/93] [Batch 100/539] [D loss: 0.328119] [G loss: 0.210623] [ema: 0.999744] 
[Epoch 50/93] [Batch 200/539] [D loss: 0.372564] [G loss: 0.155460] [ema: 0.999745] 
[Epoch 50/93] [Batch 300/539] [D loss: 0.337356] [G loss: 0.246297] [ema: 0.999746] 
[Epoch 50/93] [Batch 400/539] [D loss: 0.283391] [G loss: 0.183719] [ema: 0.999747] 
[Epoch 50/93] [Batch 500/539] [D loss: 0.329868] [G loss: 0.181940] [ema: 0.999748] 
[Epoch 51/93] [Batch 0/539] [D loss: 0.352583] [G loss: 0.195563] [ema: 0.999748] 
[Epoch 51/93] [Batch 100/539] [D loss: 0.384448] [G loss: 0.195393] [ema: 0.999749] 
[Epoch 51/93] [Batch 200/539] [D loss: 0.379169] [G loss: 0.171663] [ema: 0.999750] 
[Epoch 51/93] [Batch 300/539] [D loss: 0.298464] [G loss: 0.203370] [ema: 0.999751] 
[Epoch 51/93] [Batch 400/539] [D loss: 0.387664] [G loss: 0.205973] [ema: 0.999751] 
[Epoch 51/93] [Batch 500/539] [D loss: 0.352552] [G loss: 0.220123] [ema: 0.999752] 
[Epoch 52/93] [Batch 0/539] [D loss: 0.369582] [G loss: 0.212550] [ema: 0.999753] 
[Epoch 52/93] [Batch 100/539] [D loss: 0.349737] [G loss: 0.223828] [ema: 0.999754] 
[Epoch 52/93] [Batch 200/539] [D loss: 0.345551] [G loss: 0.196149] [ema: 0.999754] 
[Epoch 52/93] [Batch 300/539] [D loss: 0.321429] [G loss: 0.189613] [ema: 0.999755] 
[Epoch 52/93] [Batch 400/539] [D loss: 0.306809] [G loss: 0.197248] [ema: 0.999756] 
[Epoch 52/93] [Batch 500/539] [D loss: 0.392606] [G loss: 0.208807] [ema: 0.999757] 
[Epoch 53/93] [Batch 0/539] [D loss: 0.360802] [G loss: 0.195766] [ema: 0.999757] 
[Epoch 53/93] [Batch 100/539] [D loss: 0.391804] [G loss: 0.224572] [ema: 0.999758] 
[Epoch 53/93] [Batch 200/539] [D loss: 0.331651] [G loss: 0.200267] [ema: 0.999759] 
[Epoch 53/93] [Batch 300/539] [D loss: 0.318490] [G loss: 0.188598] [ema: 0.999760] 
[Epoch 53/93] [Batch 400/539] [D loss: 0.357796] [G loss: 0.200300] [ema: 0.999761] 
[Epoch 53/93] [Batch 500/539] [D loss: 0.424785] [G loss: 0.205096] [ema: 0.999762] 
[Epoch 54/93] [Batch 0/539] [D loss: 0.387841] [G loss: 0.190848] [ema: 0.999762] 
[Epoch 54/93] [Batch 100/539] [D loss: 0.353639] [G loss: 0.207576] [ema: 0.999763] 
[Epoch 54/93] [Batch 200/539] [D loss: 0.397173] [G loss: 0.215437] [ema: 0.999764] 
[Epoch 54/93] [Batch 300/539] [D loss: 0.346264] [G loss: 0.218851] [ema: 0.999764] 
[Epoch 54/93] [Batch 400/539] [D loss: 0.367961] [G loss: 0.198217] [ema: 0.999765] 
[Epoch 54/93] [Batch 500/539] [D loss: 0.322069] [G loss: 0.215711] [ema: 0.999766] 
[Epoch 55/93] [Batch 0/539] [D loss: 0.311614] [G loss: 0.212837] [ema: 0.999766] 
[Epoch 55/93] [Batch 100/539] [D loss: 0.325724] [G loss: 0.195526] [ema: 0.999767] 
[Epoch 55/93] [Batch 200/539] [D loss: 0.409183] [G loss: 0.210314] [ema: 0.999768] 
[Epoch 55/93] [Batch 300/539] [D loss: 0.340614] [G loss: 0.166174] [ema: 0.999769] 
[Epoch 55/93] [Batch 400/539] [D loss: 0.351846] [G loss: 0.224496] [ema: 0.999769] 
[Epoch 55/93] [Batch 500/539] [D loss: 0.398178] [G loss: 0.211218] [ema: 0.999770] 
[Epoch 56/93] [Batch 0/539] [D loss: 0.386612] [G loss: 0.164390] [ema: 0.999770] 
[Epoch 56/93] [Batch 100/539] [D loss: 0.317993] [G loss: 0.206544] [ema: 0.999771] 
[Epoch 56/93] [Batch 200/539] [D loss: 0.299883] [G loss: 0.232826] [ema: 0.999772] 
[Epoch 56/93] [Batch 300/539] [D loss: 0.358712] [G loss: 0.182907] [ema: 0.999773] 
[Epoch 56/93] [Batch 400/539] [D loss: 0.337589] [G loss: 0.212126] [ema: 0.999773] 
[Epoch 56/93] [Batch 500/539] [D loss: 0.314691] [G loss: 0.219209] [ema: 0.999774] 
[Epoch 57/93] [Batch 0/539] [D loss: 0.335819] [G loss: 0.213385] [ema: 0.999774] 
[Epoch 57/93] [Batch 100/539] [D loss: 0.393516] [G loss: 0.190836] [ema: 0.999775] 
[Epoch 57/93] [Batch 200/539] [D loss: 0.340468] [G loss: 0.207131] [ema: 0.999776] 
[Epoch 57/93] [Batch 300/539] [D loss: 0.450157] [G loss: 0.174037] [ema: 0.999777] 
[Epoch 57/93] [Batch 400/539] [D loss: 0.318531] [G loss: 0.224567] [ema: 0.999777] 
[Epoch 57/93] [Batch 500/539] [D loss: 0.319648] [G loss: 0.183901] [ema: 0.999778] 
[Epoch 58/93] [Batch 0/539] [D loss: 0.372482] [G loss: 0.196808] [ema: 0.999778] 
[Epoch 58/93] [Batch 100/539] [D loss: 0.352184] [G loss: 0.205651] [ema: 0.999779] 
[Epoch 58/93] [Batch 200/539] [D loss: 0.311905] [G loss: 0.179475] [ema: 0.999780] 
[Epoch 58/93] [Batch 300/539] [D loss: 0.348762] [G loss: 0.179718] [ema: 0.999780] 
[Epoch 58/93] [Batch 400/539] [D loss: 0.342753] [G loss: 0.219936] [ema: 0.999781] 
[Epoch 58/93] [Batch 500/539] [D loss: 0.362742] [G loss: 0.180556] [ema: 0.999782] 
[Epoch 59/93] [Batch 0/539] [D loss: 0.355925] [G loss: 0.218307] [ema: 0.999782] 
[Epoch 59/93] [Batch 100/539] [D loss: 0.343824] [G loss: 0.177725] [ema: 0.999783] 
[Epoch 59/93] [Batch 200/539] [D loss: 0.345857] [G loss: 0.209428] [ema: 0.999783] 
[Epoch 59/93] [Batch 300/539] [D loss: 0.350657] [G loss: 0.168628] [ema: 0.999784] 
[Epoch 59/93] [Batch 400/539] [D loss: 0.374696] [G loss: 0.201157] [ema: 0.999785] 
[Epoch 59/93] [Batch 500/539] [D loss: 0.358177] [G loss: 0.169001] [ema: 0.999785] 
[Epoch 60/93] [Batch 0/539] [D loss: 0.341899] [G loss: 0.213328] [ema: 0.999786] 
[Epoch 60/93] [Batch 100/539] [D loss: 0.361941] [G loss: 0.201952] [ema: 0.999786] 
[Epoch 60/93] [Batch 200/539] [D loss: 0.393726] [G loss: 0.217701] [ema: 0.999787] 
[Epoch 60/93] [Batch 300/539] [D loss: 0.410046] [G loss: 0.233083] [ema: 0.999788] 
[Epoch 60/93] [Batch 400/539] [D loss: 0.344429] [G loss: 0.208291] [ema: 0.999788] 
[Epoch 60/93] [Batch 500/539] [D loss: 0.390595] [G loss: 0.177332] [ema: 0.999789] 
[Epoch 61/93] [Batch 0/539] [D loss: 0.396130] [G loss: 0.207816] [ema: 0.999789] 
[Epoch 61/93] [Batch 100/539] [D loss: 0.358019] [G loss: 0.178291] [ema: 0.999790] 
[Epoch 61/93] [Batch 200/539] [D loss: 0.343789] [G loss: 0.213914] [ema: 0.999790] 
[Epoch 61/93] [Batch 300/539] [D loss: 0.359663] [G loss: 0.221115] [ema: 0.999791] 
[Epoch 61/93] [Batch 400/539] [D loss: 0.360150] [G loss: 0.206490] [ema: 0.999792] 
[Epoch 61/93] [Batch 500/539] [D loss: 0.356673] [G loss: 0.205140] [ema: 0.999792] 
[Epoch 62/93] [Batch 0/539] [D loss: 0.371547] [G loss: 0.189014] [ema: 0.999793] 
[Epoch 62/93] [Batch 100/539] [D loss: 0.344035] [G loss: 0.215335] [ema: 0.999793] 
[Epoch 62/93] [Batch 200/539] [D loss: 0.353338] [G loss: 0.213379] [ema: 0.999794] 
[Epoch 62/93] [Batch 300/539] [D loss: 0.351768] [G loss: 0.191472] [ema: 0.999794] 
[Epoch 62/93] [Batch 400/539] [D loss: 0.334153] [G loss: 0.215822] [ema: 0.999795] 
[Epoch 62/93] [Batch 500/539] [D loss: 0.384206] [G loss: 0.198781] [ema: 0.999796] 
[Epoch 63/93] [Batch 0/539] [D loss: 0.449748] [G loss: 0.185871] [ema: 0.999796] 
[Epoch 63/93] [Batch 100/539] [D loss: 0.373788] [G loss: 0.202128] [ema: 0.999796] 
[Epoch 63/93] [Batch 200/539] [D loss: 0.349934] [G loss: 0.183628] [ema: 0.999797] 
[Epoch 63/93] [Batch 300/539] [D loss: 0.332021] [G loss: 0.192705] [ema: 0.999798] 
[Epoch 63/93] [Batch 400/539] [D loss: 0.383040] [G loss: 0.198622] [ema: 0.999798] 
[Epoch 63/93] [Batch 500/539] [D loss: 0.349200] [G loss: 0.186193] [ema: 0.999799] 
[Epoch 64/93] [Batch 0/539] [D loss: 0.393929] [G loss: 0.212385] [ema: 0.999799] 
[Epoch 64/93] [Batch 100/539] [D loss: 0.357933] [G loss: 0.190001] [ema: 0.999800] 
[Epoch 64/93] [Batch 200/539] [D loss: 0.313234] [G loss: 0.204917] [ema: 0.999800] 
[Epoch 64/93] [Batch 300/539] [D loss: 0.415721] [G loss: 0.228616] [ema: 0.999801] 
[Epoch 64/93] [Batch 400/539] [D loss: 0.353861] [G loss: 0.215199] [ema: 0.999801] 
[Epoch 64/93] [Batch 500/539] [D loss: 0.406828] [G loss: 0.196043] [ema: 0.999802] 
[Epoch 65/93] [Batch 0/539] [D loss: 0.320173] [G loss: 0.233938] [ema: 0.999802] 
[Epoch 65/93] [Batch 100/539] [D loss: 0.333808] [G loss: 0.235317] [ema: 0.999803] 
[Epoch 65/93] [Batch 200/539] [D loss: 0.278188] [G loss: 0.232849] [ema: 0.999803] 
[Epoch 65/93] [Batch 300/539] [D loss: 0.319819] [G loss: 0.159883] [ema: 0.999804] 
[Epoch 65/93] [Batch 400/539] [D loss: 0.293759] [G loss: 0.233827] [ema: 0.999804] 
[Epoch 65/93] [Batch 500/539] [D loss: 0.294222] [G loss: 0.212657] [ema: 0.999805] 
[Epoch 66/93] [Batch 0/539] [D loss: 0.303696] [G loss: 0.203519] [ema: 0.999805] 
[Epoch 66/93] [Batch 100/539] [D loss: 0.362142] [G loss: 0.215414] [ema: 0.999806] 
[Epoch 66/93] [Batch 200/539] [D loss: 0.396318] [G loss: 0.197644] [ema: 0.999806] 
[Epoch 66/93] [Batch 300/539] [D loss: 0.367286] [G loss: 0.189539] [ema: 0.999807] 
[Epoch 66/93] [Batch 400/539] [D loss: 0.334452] [G loss: 0.207039] [ema: 0.999807] 
[Epoch 66/93] [Batch 500/539] [D loss: 0.364837] [G loss: 0.226379] [ema: 0.999808] 
[Epoch 67/93] [Batch 0/539] [D loss: 0.357762] [G loss: 0.218182] [ema: 0.999808] 
[Epoch 67/93] [Batch 100/539] [D loss: 0.340841] [G loss: 0.236039] [ema: 0.999809] 
[Epoch 67/93] [Batch 200/539] [D loss: 0.352297] [G loss: 0.191591] [ema: 0.999809] 
[Epoch 67/93] [Batch 300/539] [D loss: 0.352211] [G loss: 0.233082] [ema: 0.999810] 
[Epoch 67/93] [Batch 400/539] [D loss: 0.359238] [G loss: 0.236640] [ema: 0.999810] 
[Epoch 67/93] [Batch 500/539] [D loss: 0.315105] [G loss: 0.158750] [ema: 0.999811] 
[Epoch 68/93] [Batch 0/539] [D loss: 0.344800] [G loss: 0.204480] [ema: 0.999811] 
[Epoch 68/93] [Batch 100/539] [D loss: 0.374366] [G loss: 0.193283] [ema: 0.999811] 
[Epoch 68/93] [Batch 200/539] [D loss: 0.327737] [G loss: 0.195622] [ema: 0.999812] 
[Epoch 68/93] [Batch 300/539] [D loss: 0.355041] [G loss: 0.203715] [ema: 0.999812] 
[Epoch 68/93] [Batch 400/539] [D loss: 0.388621] [G loss: 0.184115] [ema: 0.999813] 
[Epoch 68/93] [Batch 500/539] [D loss: 0.361882] [G loss: 0.203341] [ema: 0.999813] 
[Epoch 69/93] [Batch 0/539] [D loss: 0.324373] [G loss: 0.195169] [ema: 0.999814] 
[Epoch 69/93] [Batch 100/539] [D loss: 0.372756] [G loss: 0.193216] [ema: 0.999814] 
[Epoch 69/93] [Batch 200/539] [D loss: 0.357209] [G loss: 0.178452] [ema: 0.999815] 
[Epoch 69/93] [Batch 300/539] [D loss: 0.342852] [G loss: 0.212994] [ema: 0.999815] 
[Epoch 69/93] [Batch 400/539] [D loss: 0.401651] [G loss: 0.187349] [ema: 0.999816] 
[Epoch 69/93] [Batch 500/539] [D loss: 0.368631] [G loss: 0.218692] [ema: 0.999816] 
[Epoch 70/93] [Batch 0/539] [D loss: 0.346961] [G loss: 0.173779] [ema: 0.999816] 
[Epoch 70/93] [Batch 100/539] [D loss: 0.358481] [G loss: 0.232800] [ema: 0.999817] 
[Epoch 70/93] [Batch 200/539] [D loss: 0.319483] [G loss: 0.208370] [ema: 0.999817] 
[Epoch 70/93] [Batch 300/539] [D loss: 0.302772] [G loss: 0.242156] [ema: 0.999818] 
[Epoch 70/93] [Batch 400/539] [D loss: 0.330291] [G loss: 0.204814] [ema: 0.999818] 
[Epoch 70/93] [Batch 500/539] [D loss: 0.352709] [G loss: 0.185711] [ema: 0.999819] 
[Epoch 71/93] [Batch 0/539] [D loss: 0.341151] [G loss: 0.190518] [ema: 0.999819] 
[Epoch 71/93] [Batch 100/539] [D loss: 0.317323] [G loss: 0.208282] [ema: 0.999819] 
[Epoch 71/93] [Batch 200/539] [D loss: 0.332229] [G loss: 0.218051] [ema: 0.999820] 
[Epoch 71/93] [Batch 300/539] [D loss: 0.355147] [G loss: 0.188908] [ema: 0.999820] 
[Epoch 71/93] [Batch 400/539] [D loss: 0.329279] [G loss: 0.209102] [ema: 0.999821] 
[Epoch 71/93] [Batch 500/539] [D loss: 0.317481] [G loss: 0.189818] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_50000_60_100/walk_50000_D_60_2024_10_23_18_27_33/Model



[Epoch 72/93] [Batch 0/539] [D loss: 0.320959] [G loss: 0.196363] [ema: 0.999821] 
[Epoch 72/93] [Batch 100/539] [D loss: 0.311039] [G loss: 0.217727] [ema: 0.999822] 
[Epoch 72/93] [Batch 200/539] [D loss: 0.371301] [G loss: 0.204719] [ema: 0.999822] 
[Epoch 72/93] [Batch 300/539] [D loss: 0.354436] [G loss: 0.215047] [ema: 0.999823] 
[Epoch 72/93] [Batch 400/539] [D loss: 0.361472] [G loss: 0.190901] [ema: 0.999823] 
[Epoch 72/93] [Batch 500/539] [D loss: 0.342738] [G loss: 0.217500] [ema: 0.999824] 
[Epoch 73/93] [Batch 0/539] [D loss: 0.440959] [G loss: 0.189150] [ema: 0.999824] 
[Epoch 73/93] [Batch 100/539] [D loss: 0.407381] [G loss: 0.182502] [ema: 0.999824] 
[Epoch 73/93] [Batch 200/539] [D loss: 0.335265] [G loss: 0.189903] [ema: 0.999825] 
[Epoch 73/93] [Batch 300/539] [D loss: 0.330391] [G loss: 0.211657] [ema: 0.999825] 
[Epoch 73/93] [Batch 400/539] [D loss: 0.411029] [G loss: 0.230537] [ema: 0.999826] 
[Epoch 73/93] [Batch 500/539] [D loss: 0.336127] [G loss: 0.215906] [ema: 0.999826] 
[Epoch 74/93] [Batch 0/539] [D loss: 0.368817] [G loss: 0.223648] [ema: 0.999826] 
[Epoch 74/93] [Batch 100/539] [D loss: 0.387934] [G loss: 0.180824] [ema: 0.999827] 
[Epoch 74/93] [Batch 200/539] [D loss: 0.354797] [G loss: 0.213354] [ema: 0.999827] 
[Epoch 74/93] [Batch 300/539] [D loss: 0.335608] [G loss: 0.218096] [ema: 0.999828] 
[Epoch 74/93] [Batch 400/539] [D loss: 0.371877] [G loss: 0.210763] [ema: 0.999828] 
[Epoch 74/93] [Batch 500/539] [D loss: 0.308846] [G loss: 0.186397] [ema: 0.999828] 
[Epoch 75/93] [Batch 0/539] [D loss: 0.374250] [G loss: 0.201207] [ema: 0.999829] 
[Epoch 75/93] [Batch 100/539] [D loss: 0.347822] [G loss: 0.202087] [ema: 0.999829] 
[Epoch 75/93] [Batch 200/539] [D loss: 0.380807] [G loss: 0.229213] [ema: 0.999829] 
[Epoch 75/93] [Batch 300/539] [D loss: 0.358157] [G loss: 0.218928] [ema: 0.999830] 
[Epoch 75/93] [Batch 400/539] [D loss: 0.344754] [G loss: 0.234331] [ema: 0.999830] 
[Epoch 75/93] [Batch 500/539] [D loss: 0.410095] [G loss: 0.180793] [ema: 0.999831] 
[Epoch 76/93] [Batch 0/539] [D loss: 0.363376] [G loss: 0.186626] [ema: 0.999831] 
[Epoch 76/93] [Batch 100/539] [D loss: 0.333997] [G loss: 0.194916] [ema: 0.999831] 
[Epoch 76/93] [Batch 200/539] [D loss: 0.327665] [G loss: 0.210402] [ema: 0.999832] 
[Epoch 76/93] [Batch 300/539] [D loss: 0.351925] [G loss: 0.220572] [ema: 0.999832] 
[Epoch 76/93] [Batch 400/539] [D loss: 0.363895] [G loss: 0.211799] [ema: 0.999832] 
[Epoch 76/93] [Batch 500/539] [D loss: 0.347737] [G loss: 0.240950] [ema: 0.999833] 
[Epoch 77/93] [Batch 0/539] [D loss: 0.309999] [G loss: 0.223037] [ema: 0.999833] 
[Epoch 77/93] [Batch 100/539] [D loss: 0.391427] [G loss: 0.218799] [ema: 0.999833] 
[Epoch 77/93] [Batch 200/539] [D loss: 0.305915] [G loss: 0.195210] [ema: 0.999834] 
[Epoch 77/93] [Batch 300/539] [D loss: 0.361009] [G loss: 0.211832] [ema: 0.999834] 
[Epoch 77/93] [Batch 400/539] [D loss: 0.311788] [G loss: 0.207055] [ema: 0.999835] 
[Epoch 77/93] [Batch 500/539] [D loss: 0.327018] [G loss: 0.223556] [ema: 0.999835] 
[Epoch 78/93] [Batch 0/539] [D loss: 0.394864] [G loss: 0.224878] [ema: 0.999835] 
[Epoch 78/93] [Batch 100/539] [D loss: 0.297238] [G loss: 0.213024] [ema: 0.999836] 
[Epoch 78/93] [Batch 200/539] [D loss: 0.312546] [G loss: 0.185717] [ema: 0.999836] 
[Epoch 78/93] [Batch 300/539] [D loss: 0.337287] [G loss: 0.183996] [ema: 0.999836] 
[Epoch 78/93] [Batch 400/539] [D loss: 0.342433] [G loss: 0.251977] [ema: 0.999837] 
[Epoch 78/93] [Batch 500/539] [D loss: 0.335572] [G loss: 0.201769] [ema: 0.999837] 
[Epoch 79/93] [Batch 0/539] [D loss: 0.376994] [G loss: 0.223239] [ema: 0.999837] 
[Epoch 79/93] [Batch 100/539] [D loss: 0.328601] [G loss: 0.226894] [ema: 0.999838] 
[Epoch 79/93] [Batch 200/539] [D loss: 0.349558] [G loss: 0.180779] [ema: 0.999838] 
[Epoch 79/93] [Batch 300/539] [D loss: 0.336861] [G loss: 0.183560] [ema: 0.999838] 
[Epoch 79/93] [Batch 400/539] [D loss: 0.440483] [G loss: 0.189310] [ema: 0.999839] 
[Epoch 79/93] [Batch 500/539] [D loss: 0.362681] [G loss: 0.198323] [ema: 0.999839] 
[Epoch 80/93] [Batch 0/539] [D loss: 0.309627] [G loss: 0.208044] [ema: 0.999839] 
[Epoch 80/93] [Batch 100/539] [D loss: 0.293767] [G loss: 0.226835] [ema: 0.999840] 
[Epoch 80/93] [Batch 200/539] [D loss: 0.417629] [G loss: 0.201026] [ema: 0.999840] 
[Epoch 80/93] [Batch 300/539] [D loss: 0.344865] [G loss: 0.197124] [ema: 0.999840] 
[Epoch 80/93] [Batch 400/539] [D loss: 0.358790] [G loss: 0.221239] [ema: 0.999841] 
[Epoch 80/93] [Batch 500/539] [D loss: 0.391167] [G loss: 0.220983] [ema: 0.999841] 
[Epoch 81/93] [Batch 0/539] [D loss: 0.419592] [G loss: 0.198767] [ema: 0.999841] 
[Epoch 81/93] [Batch 100/539] [D loss: 0.314447] [G loss: 0.211570] [ema: 0.999842] 
[Epoch 81/93] [Batch 200/539] [D loss: 0.454228] [G loss: 0.218175] [ema: 0.999842] 
[Epoch 81/93] [Batch 300/539] [D loss: 0.333911] [G loss: 0.215662] [ema: 0.999842] 
[Epoch 81/93] [Batch 400/539] [D loss: 0.338562] [G loss: 0.226830] [ema: 0.999843] 
[Epoch 81/93] [Batch 500/539] [D loss: 0.360284] [G loss: 0.173898] [ema: 0.999843] 
[Epoch 82/93] [Batch 0/539] [D loss: 0.320192] [G loss: 0.198993] [ema: 0.999843] 
[Epoch 82/93] [Batch 100/539] [D loss: 0.363199] [G loss: 0.203130] [ema: 0.999844] 
[Epoch 82/93] [Batch 200/539] [D loss: 0.304545] [G loss: 0.178545] [ema: 0.999844] 
[Epoch 82/93] [Batch 300/539] [D loss: 0.348260] [G loss: 0.228456] [ema: 0.999844] 
[Epoch 82/93] [Batch 400/539] [D loss: 0.383288] [G loss: 0.195934] [ema: 0.999845] 
[Epoch 82/93] [Batch 500/539] [D loss: 0.276090] [G loss: 0.198701] [ema: 0.999845] 
[Epoch 83/93] [Batch 0/539] [D loss: 0.348802] [G loss: 0.186936] [ema: 0.999845] 
[Epoch 83/93] [Batch 100/539] [D loss: 0.385536] [G loss: 0.192169] [ema: 0.999845] 
[Epoch 83/93] [Batch 200/539] [D loss: 0.346227] [G loss: 0.191265] [ema: 0.999846] 
[Epoch 83/93] [Batch 300/539] [D loss: 0.396610] [G loss: 0.189823] [ema: 0.999846] 
[Epoch 83/93] [Batch 400/539] [D loss: 0.330331] [G loss: 0.205634] [ema: 0.999846] 
[Epoch 83/93] [Batch 500/539] [D loss: 0.430983] [G loss: 0.195518] [ema: 0.999847] 
[Epoch 84/93] [Batch 0/539] [D loss: 0.364120] [G loss: 0.169684] [ema: 0.999847] 
[Epoch 84/93] [Batch 100/539] [D loss: 0.392202] [G loss: 0.203347] [ema: 0.999847] 
[Epoch 84/93] [Batch 200/539] [D loss: 0.320267] [G loss: 0.192950] [ema: 0.999848] 
[Epoch 84/93] [Batch 300/539] [D loss: 0.318720] [G loss: 0.220628] [ema: 0.999848] 
[Epoch 84/93] [Batch 400/539] [D loss: 0.322441] [G loss: 0.197800] [ema: 0.999848] 
[Epoch 84/93] [Batch 500/539] [D loss: 0.309472] [G loss: 0.196393] [ema: 0.999849] 
[Epoch 85/93] [Batch 0/539] [D loss: 0.363259] [G loss: 0.176731] [ema: 0.999849] 
[Epoch 85/93] [Batch 100/539] [D loss: 0.324168] [G loss: 0.216460] [ema: 0.999849] 
[Epoch 85/93] [Batch 200/539] [D loss: 0.360727] [G loss: 0.190296] [ema: 0.999849] 
[Epoch 85/93] [Batch 300/539] [D loss: 0.353077] [G loss: 0.202330] [ema: 0.999850] 
[Epoch 85/93] [Batch 400/539] [D loss: 0.337522] [G loss: 0.244330] [ema: 0.999850] 
[Epoch 85/93] [Batch 500/539] [D loss: 0.360655] [G loss: 0.202891] [ema: 0.999850] 
[Epoch 86/93] [Batch 0/539] [D loss: 0.434674] [G loss: 0.204363] [ema: 0.999850] 
[Epoch 86/93] [Batch 100/539] [D loss: 0.377404] [G loss: 0.192623] [ema: 0.999851] 
[Epoch 86/93] [Batch 200/539] [D loss: 0.339719] [G loss: 0.211770] [ema: 0.999851] 
[Epoch 86/93] [Batch 300/539] [D loss: 0.342193] [G loss: 0.211401] [ema: 0.999851] 
[Epoch 86/93] [Batch 400/539] [D loss: 0.355180] [G loss: 0.226778] [ema: 0.999852] 
[Epoch 86/93] [Batch 500/539] [D loss: 0.298386] [G loss: 0.218904] [ema: 0.999852] 
[Epoch 87/93] [Batch 0/539] [D loss: 0.393267] [G loss: 0.233120] [ema: 0.999852] 
[Epoch 87/93] [Batch 100/539] [D loss: 0.335199] [G loss: 0.210410] [ema: 0.999853] 
[Epoch 87/93] [Batch 200/539] [D loss: 0.310087] [G loss: 0.229333] [ema: 0.999853] 
[Epoch 87/93] [Batch 300/539] [D loss: 0.339312] [G loss: 0.187613] [ema: 0.999853] 
[Epoch 87/93] [Batch 400/539] [D loss: 0.322840] [G loss: 0.187267] [ema: 0.999853] 
[Epoch 87/93] [Batch 500/539] [D loss: 0.389225] [G loss: 0.195441] [ema: 0.999854] 
[Epoch 88/93] [Batch 0/539] [D loss: 0.302854] [G loss: 0.199886] [ema: 0.999854] 
[Epoch 88/93] [Batch 100/539] [D loss: 0.386828] [G loss: 0.187976] [ema: 0.999854] 
[Epoch 88/93] [Batch 200/539] [D loss: 0.344565] [G loss: 0.209799] [ema: 0.999854] 
[Epoch 88/93] [Batch 300/539] [D loss: 0.358976] [G loss: 0.199381] [ema: 0.999855] 
[Epoch 88/93] [Batch 400/539] [D loss: 0.335073] [G loss: 0.210083] [ema: 0.999855] 
[Epoch 88/93] [Batch 500/539] [D loss: 0.309715] [G loss: 0.235336] [ema: 0.999855] 
[Epoch 89/93] [Batch 0/539] [D loss: 0.390445] [G loss: 0.171113] [ema: 0.999856] 
[Epoch 89/93] [Batch 100/539] [D loss: 0.351704] [G loss: 0.238652] [ema: 0.999856] 
[Epoch 89/93] [Batch 200/539] [D loss: 0.338974] [G loss: 0.180113] [ema: 0.999856] 
[Epoch 89/93] [Batch 300/539] [D loss: 0.351707] [G loss: 0.165295] [ema: 0.999856] 
[Epoch 89/93] [Batch 400/539] [D loss: 0.320737] [G loss: 0.222328] [ema: 0.999857] 
[Epoch 89/93] [Batch 500/539] [D loss: 0.411536] [G loss: 0.184638] [ema: 0.999857] 
[Epoch 90/93] [Batch 0/539] [D loss: 0.355150] [G loss: 0.169537] [ema: 0.999857] 
[Epoch 90/93] [Batch 100/539] [D loss: 0.309603] [G loss: 0.224156] [ema: 0.999857] 
[Epoch 90/93] [Batch 200/539] [D loss: 0.376826] [G loss: 0.195416] [ema: 0.999858] 
[Epoch 90/93] [Batch 300/539] [D loss: 0.321083] [G loss: 0.204004] [ema: 0.999858] 
[Epoch 90/93] [Batch 400/539] [D loss: 0.408082] [G loss: 0.183271] [ema: 0.999858] 
[Epoch 90/93] [Batch 500/539] [D loss: 0.326003] [G loss: 0.201244] [ema: 0.999859] 
[Epoch 91/93] [Batch 0/539] [D loss: 0.355761] [G loss: 0.201609] [ema: 0.999859] 
[Epoch 91/93] [Batch 100/539] [D loss: 0.321640] [G loss: 0.188184] [ema: 0.999859] 
[Epoch 91/93] [Batch 200/539] [D loss: 0.365388] [G loss: 0.204148] [ema: 0.999859] 
[Epoch 91/93] [Batch 300/539] [D loss: 0.281849] [G loss: 0.206727] [ema: 0.999860] 
[Epoch 91/93] [Batch 400/539] [D loss: 0.366683] [G loss: 0.219316] [ema: 0.999860] 
[Epoch 91/93] [Batch 500/539] [D loss: 0.357708] [G loss: 0.224554] [ema: 0.999860] 
[Epoch 92/93] [Batch 0/539] [D loss: 0.381340] [G loss: 0.226762] [ema: 0.999860] 
[Epoch 92/93] [Batch 100/539] [D loss: 0.376121] [G loss: 0.228174] [ema: 0.999861] 
[Epoch 92/93] [Batch 200/539] [D loss: 0.334373] [G loss: 0.217680] [ema: 0.999861] 
[Epoch 92/93] [Batch 300/539] [D loss: 0.290000] [G loss: 0.203661] [ema: 0.999861] 
[Epoch 92/93] [Batch 400/539] [D loss: 0.326406] [G loss: 0.183774] [ema: 0.999861] 
[Epoch 92/93] [Batch 500/539] [D loss: 0.359885] [G loss: 0.221068] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
downstairs training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
downstairs
daghar
return single class data and labels, class is downstairs
data shape is (6427, 3, 1, 60)
label shape is (6427,)
402
Epochs between checkpoint: 32



Saving checkpoint 1 in logs/daghar_50000_60_100/downstairs_50000_D_60_2024_10_23_19_00_36/Model



[Epoch 0/125] [Batch 0/402] [D loss: 1.211918] [G loss: 0.392959] [ema: 0.000000] 
[Epoch 0/125] [Batch 100/402] [D loss: 0.380643] [G loss: 0.249345] [ema: 0.933033] 
[Epoch 0/125] [Batch 200/402] [D loss: 0.386856] [G loss: 0.194629] [ema: 0.965936] 
[Epoch 0/125] [Batch 300/402] [D loss: 0.340061] [G loss: 0.200918] [ema: 0.977160] 
[Epoch 0/125] [Batch 400/402] [D loss: 0.363718] [G loss: 0.194916] [ema: 0.982821] 
[Epoch 1/125] [Batch 0/402] [D loss: 0.315016] [G loss: 0.226615] [ema: 0.982905] 
[Epoch 1/125] [Batch 100/402] [D loss: 0.380068] [G loss: 0.200462] [ema: 0.986287] 
[Epoch 1/125] [Batch 200/402] [D loss: 0.378551] [G loss: 0.191481] [ema: 0.988552] 
[Epoch 1/125] [Batch 300/402] [D loss: 0.364512] [G loss: 0.194679] [ema: 0.990175] 
[Epoch 1/125] [Batch 400/402] [D loss: 0.399325] [G loss: 0.193386] [ema: 0.991395] 
[Epoch 2/125] [Batch 0/402] [D loss: 0.373178] [G loss: 0.206142] [ema: 0.991416] 
[Epoch 2/125] [Batch 100/402] [D loss: 0.504367] [G loss: 0.132937] [ema: 0.992362] 
[Epoch 2/125] [Batch 200/402] [D loss: 0.536614] [G loss: 0.154969] [ema: 0.993120] 
[Epoch 2/125] [Batch 300/402] [D loss: 0.470996] [G loss: 0.186869] [ema: 0.993741] 
[Epoch 2/125] [Batch 400/402] [D loss: 0.369443] [G loss: 0.230473] [ema: 0.994260] 
[Epoch 3/125] [Batch 0/402] [D loss: 0.436620] [G loss: 0.225696] [ema: 0.994269] 
[Epoch 3/125] [Batch 100/402] [D loss: 0.392145] [G loss: 0.153524] [ema: 0.994707] 
[Epoch 3/125] [Batch 200/402] [D loss: 0.401447] [G loss: 0.178848] [ema: 0.995082] 
[Epoch 3/125] [Batch 300/402] [D loss: 0.429681] [G loss: 0.180373] [ema: 0.995408] 
[Epoch 3/125] [Batch 400/402] [D loss: 0.413107] [G loss: 0.120587] [ema: 0.995693] 
[Epoch 4/125] [Batch 0/402] [D loss: 0.464550] [G loss: 0.231577] [ema: 0.995699] 
[Epoch 4/125] [Batch 100/402] [D loss: 0.459501] [G loss: 0.152969] [ema: 0.995950] 
[Epoch 4/125] [Batch 200/402] [D loss: 0.505182] [G loss: 0.145261] [ema: 0.996174] 
[Epoch 4/125] [Batch 300/402] [D loss: 0.483725] [G loss: 0.131153] [ema: 0.996374] 
[Epoch 4/125] [Batch 400/402] [D loss: 0.485386] [G loss: 0.154991] [ema: 0.996554] 
[Epoch 5/125] [Batch 0/402] [D loss: 0.461626] [G loss: 0.187176] [ema: 0.996557] 
[Epoch 5/125] [Batch 100/402] [D loss: 0.495388] [G loss: 0.142094] [ema: 0.996720] 
[Epoch 5/125] [Batch 200/402] [D loss: 0.488109] [G loss: 0.126767] [ema: 0.996869] 
[Epoch 5/125] [Batch 300/402] [D loss: 0.428584] [G loss: 0.156357] [ema: 0.997004] 
[Epoch 5/125] [Batch 400/402] [D loss: 0.438175] [G loss: 0.161136] [ema: 0.997128] 
[Epoch 6/125] [Batch 0/402] [D loss: 0.418514] [G loss: 0.165494] [ema: 0.997130] 
[Epoch 6/125] [Batch 100/402] [D loss: 0.440380] [G loss: 0.127284] [ema: 0.997244] 
[Epoch 6/125] [Batch 200/402] [D loss: 0.473773] [G loss: 0.140895] [ema: 0.997350] 
[Epoch 6/125] [Batch 300/402] [D loss: 0.578126] [G loss: 0.142982] [ema: 0.997447] 
[Epoch 6/125] [Batch 400/402] [D loss: 0.541017] [G loss: 0.143302] [ema: 0.997538] 
[Epoch 7/125] [Batch 0/402] [D loss: 0.531958] [G loss: 0.128616] [ema: 0.997540] 
[Epoch 7/125] [Batch 100/402] [D loss: 0.528750] [G loss: 0.142781] [ema: 0.997624] 
[Epoch 7/125] [Batch 200/402] [D loss: 0.428343] [G loss: 0.127040] [ema: 0.997703] 
[Epoch 7/125] [Batch 300/402] [D loss: 0.552763] [G loss: 0.144552] [ema: 0.997777] 
[Epoch 7/125] [Batch 400/402] [D loss: 0.538779] [G loss: 0.142342] [ema: 0.997846] 
[Epoch 8/125] [Batch 0/402] [D loss: 0.574851] [G loss: 0.152957] [ema: 0.997847] 
[Epoch 8/125] [Batch 100/402] [D loss: 0.552980] [G loss: 0.120199] [ema: 0.997912] 
[Epoch 8/125] [Batch 200/402] [D loss: 0.542961] [G loss: 0.122591] [ema: 0.997973] 
[Epoch 8/125] [Batch 300/402] [D loss: 0.459355] [G loss: 0.138135] [ema: 0.998031] 
[Epoch 8/125] [Batch 400/402] [D loss: 0.581017] [G loss: 0.118910] [ema: 0.998085] 
[Epoch 9/125] [Batch 0/402] [D loss: 0.515165] [G loss: 0.119604] [ema: 0.998086] 
[Epoch 9/125] [Batch 100/402] [D loss: 0.546027] [G loss: 0.144893] [ema: 0.998137] 
[Epoch 9/125] [Batch 200/402] [D loss: 0.536988] [G loss: 0.125245] [ema: 0.998186] 
[Epoch 9/125] [Batch 300/402] [D loss: 0.495950] [G loss: 0.121542] [ema: 0.998232] 
[Epoch 9/125] [Batch 400/402] [D loss: 0.477160] [G loss: 0.123542] [ema: 0.998276] 
[Epoch 10/125] [Batch 0/402] [D loss: 0.562641] [G loss: 0.120563] [ema: 0.998277] 
[Epoch 10/125] [Batch 100/402] [D loss: 0.539942] [G loss: 0.132258] [ema: 0.998319] 
[Epoch 10/125] [Batch 200/402] [D loss: 0.447599] [G loss: 0.138762] [ema: 0.998359] 
[Epoch 10/125] [Batch 300/402] [D loss: 0.475290] [G loss: 0.138103] [ema: 0.998397] 
[Epoch 10/125] [Batch 400/402] [D loss: 0.560202] [G loss: 0.129058] [ema: 0.998433] 
[Epoch 11/125] [Batch 0/402] [D loss: 0.499236] [G loss: 0.164541] [ema: 0.998434] 
[Epoch 11/125] [Batch 100/402] [D loss: 0.438129] [G loss: 0.139195] [ema: 0.998468] 
[Epoch 11/125] [Batch 200/402] [D loss: 0.524338] [G loss: 0.131613] [ema: 0.998501] 
[Epoch 11/125] [Batch 300/402] [D loss: 0.475953] [G loss: 0.137118] [ema: 0.998533] 
[Epoch 11/125] [Batch 400/402] [D loss: 0.508549] [G loss: 0.145473] [ema: 0.998564] 
[Epoch 12/125] [Batch 0/402] [D loss: 0.459404] [G loss: 0.142388] [ema: 0.998564] 
[Epoch 12/125] [Batch 100/402] [D loss: 0.495886] [G loss: 0.156670] [ema: 0.998593] 
[Epoch 12/125] [Batch 200/402] [D loss: 0.489395] [G loss: 0.118149] [ema: 0.998621] 
[Epoch 12/125] [Batch 300/402] [D loss: 0.513418] [G loss: 0.154275] [ema: 0.998648] 
[Epoch 12/125] [Batch 400/402] [D loss: 0.475281] [G loss: 0.136201] [ema: 0.998674] 
[Epoch 13/125] [Batch 0/402] [D loss: 0.511665] [G loss: 0.150084] [ema: 0.998675] 
[Epoch 13/125] [Batch 100/402] [D loss: 0.424248] [G loss: 0.146857] [ema: 0.998699] 
[Epoch 13/125] [Batch 200/402] [D loss: 0.475746] [G loss: 0.124971] [ema: 0.998723] 
[Epoch 13/125] [Batch 300/402] [D loss: 0.501640] [G loss: 0.128397] [ema: 0.998746] 
[Epoch 13/125] [Batch 400/402] [D loss: 0.466766] [G loss: 0.129886] [ema: 0.998769] 
[Epoch 14/125] [Batch 0/402] [D loss: 0.472096] [G loss: 0.149310] [ema: 0.998769] 
[Epoch 14/125] [Batch 100/402] [D loss: 0.445767] [G loss: 0.162620] [ema: 0.998791] 
[Epoch 14/125] [Batch 200/402] [D loss: 0.441824] [G loss: 0.165232] [ema: 0.998811] 
[Epoch 14/125] [Batch 300/402] [D loss: 0.502708] [G loss: 0.141379] [ema: 0.998831] 
[Epoch 14/125] [Batch 400/402] [D loss: 0.443622] [G loss: 0.147746] [ema: 0.998851] 
[Epoch 15/125] [Batch 0/402] [D loss: 0.511316] [G loss: 0.155500] [ema: 0.998851] 
[Epoch 15/125] [Batch 100/402] [D loss: 0.455124] [G loss: 0.157548] [ema: 0.998870] 
[Epoch 15/125] [Batch 200/402] [D loss: 0.460445] [G loss: 0.149612] [ema: 0.998888] 
[Epoch 15/125] [Batch 300/402] [D loss: 0.490185] [G loss: 0.155298] [ema: 0.998906] 
[Epoch 15/125] [Batch 400/402] [D loss: 0.453007] [G loss: 0.171039] [ema: 0.998923] 
[Epoch 16/125] [Batch 0/402] [D loss: 0.452537] [G loss: 0.146267] [ema: 0.998923] 
[Epoch 16/125] [Batch 100/402] [D loss: 0.474627] [G loss: 0.144138] [ema: 0.998939] 
[Epoch 16/125] [Batch 200/402] [D loss: 0.396090] [G loss: 0.157760] [ema: 0.998955] 
[Epoch 16/125] [Batch 300/402] [D loss: 0.446667] [G loss: 0.173744] [ema: 0.998971] 
[Epoch 16/125] [Batch 400/402] [D loss: 0.422252] [G loss: 0.175613] [ema: 0.998986] 
[Epoch 17/125] [Batch 0/402] [D loss: 0.465594] [G loss: 0.166841] [ema: 0.998986] 
[Epoch 17/125] [Batch 100/402] [D loss: 0.507250] [G loss: 0.160080] [ema: 0.999001] 
[Epoch 17/125] [Batch 200/402] [D loss: 0.457700] [G loss: 0.138115] [ema: 0.999015] 
[Epoch 17/125] [Batch 300/402] [D loss: 0.450077] [G loss: 0.170149] [ema: 0.999029] 
[Epoch 17/125] [Batch 400/402] [D loss: 0.421975] [G loss: 0.170488] [ema: 0.999042] 
[Epoch 18/125] [Batch 0/402] [D loss: 0.444873] [G loss: 0.177867] [ema: 0.999043] 
[Epoch 18/125] [Batch 100/402] [D loss: 0.486370] [G loss: 0.172101] [ema: 0.999056] 
[Epoch 18/125] [Batch 200/402] [D loss: 0.465643] [G loss: 0.142171] [ema: 0.999068] 
[Epoch 18/125] [Batch 300/402] [D loss: 0.477460] [G loss: 0.138736] [ema: 0.999081] 
[Epoch 18/125] [Batch 400/402] [D loss: 0.404996] [G loss: 0.182604] [ema: 0.999093] 
[Epoch 19/125] [Batch 0/402] [D loss: 0.462709] [G loss: 0.157161] [ema: 0.999093] 
[Epoch 19/125] [Batch 100/402] [D loss: 0.449419] [G loss: 0.157727] [ema: 0.999105] 
[Epoch 19/125] [Batch 200/402] [D loss: 0.449385] [G loss: 0.194317] [ema: 0.999116] 
[Epoch 19/125] [Batch 300/402] [D loss: 0.461937] [G loss: 0.150007] [ema: 0.999127] 
[Epoch 19/125] [Batch 400/402] [D loss: 0.530586] [G loss: 0.149519] [ema: 0.999138] 
[Epoch 20/125] [Batch 0/402] [D loss: 0.472076] [G loss: 0.156235] [ema: 0.999138] 
[Epoch 20/125] [Batch 100/402] [D loss: 0.446456] [G loss: 0.159458] [ema: 0.999149] 
[Epoch 20/125] [Batch 200/402] [D loss: 0.431493] [G loss: 0.153546] [ema: 0.999159] 
[Epoch 20/125] [Batch 300/402] [D loss: 0.451258] [G loss: 0.126011] [ema: 0.999169] 
[Epoch 20/125] [Batch 400/402] [D loss: 0.445561] [G loss: 0.146675] [ema: 0.999179] 
[Epoch 21/125] [Batch 0/402] [D loss: 0.433722] [G loss: 0.174005] [ema: 0.999179] 
[Epoch 21/125] [Batch 100/402] [D loss: 0.442324] [G loss: 0.160037] [ema: 0.999189] 
[Epoch 21/125] [Batch 200/402] [D loss: 0.452030] [G loss: 0.150960] [ema: 0.999198] 
[Epoch 21/125] [Batch 300/402] [D loss: 0.527159] [G loss: 0.169633] [ema: 0.999207] 
[Epoch 21/125] [Batch 400/402] [D loss: 0.409199] [G loss: 0.180151] [ema: 0.999216] 
[Epoch 22/125] [Batch 0/402] [D loss: 0.407072] [G loss: 0.190867] [ema: 0.999217] 
[Epoch 22/125] [Batch 100/402] [D loss: 0.476702] [G loss: 0.186652] [ema: 0.999225] 
[Epoch 22/125] [Batch 200/402] [D loss: 0.445212] [G loss: 0.162050] [ema: 0.999234] 
[Epoch 22/125] [Batch 300/402] [D loss: 0.453685] [G loss: 0.147999] [ema: 0.999242] 
[Epoch 22/125] [Batch 400/402] [D loss: 0.441065] [G loss: 0.151191] [ema: 0.999250] 
[Epoch 23/125] [Batch 0/402] [D loss: 0.448253] [G loss: 0.170149] [ema: 0.999251] 
[Epoch 23/125] [Batch 100/402] [D loss: 0.439154] [G loss: 0.185889] [ema: 0.999259] 
[Epoch 23/125] [Batch 200/402] [D loss: 0.470725] [G loss: 0.130237] [ema: 0.999266] 
[Epoch 23/125] [Batch 300/402] [D loss: 0.525522] [G loss: 0.145588] [ema: 0.999274] 
[Epoch 23/125] [Batch 400/402] [D loss: 0.445325] [G loss: 0.135142] [ema: 0.999282] 
[Epoch 24/125] [Batch 0/402] [D loss: 0.467581] [G loss: 0.145096] [ema: 0.999282] 
[Epoch 24/125] [Batch 100/402] [D loss: 0.456347] [G loss: 0.172869] [ema: 0.999289] 
[Epoch 24/125] [Batch 200/402] [D loss: 0.431233] [G loss: 0.182870] [ema: 0.999296] 
[Epoch 24/125] [Batch 300/402] [D loss: 0.407702] [G loss: 0.159555] [ema: 0.999303] 
[Epoch 24/125] [Batch 400/402] [D loss: 0.479385] [G loss: 0.155382] [ema: 0.999310] 
[Epoch 25/125] [Batch 0/402] [D loss: 0.429734] [G loss: 0.140460] [ema: 0.999311] 
[Epoch 25/125] [Batch 100/402] [D loss: 0.444484] [G loss: 0.149583] [ema: 0.999317] 
[Epoch 25/125] [Batch 200/402] [D loss: 0.418956] [G loss: 0.162223] [ema: 0.999324] 
[Epoch 25/125] [Batch 300/402] [D loss: 0.413740] [G loss: 0.168932] [ema: 0.999331] 
[Epoch 25/125] [Batch 400/402] [D loss: 0.467419] [G loss: 0.174895] [ema: 0.999337] 
[Epoch 26/125] [Batch 0/402] [D loss: 0.450773] [G loss: 0.155069] [ema: 0.999337] 
[Epoch 26/125] [Batch 100/402] [D loss: 0.472027] [G loss: 0.151387] [ema: 0.999343] 
[Epoch 26/125] [Batch 200/402] [D loss: 0.394650] [G loss: 0.178888] [ema: 0.999349] 
[Epoch 26/125] [Batch 300/402] [D loss: 0.431803] [G loss: 0.167113] [ema: 0.999356] 
[Epoch 26/125] [Batch 400/402] [D loss: 0.544868] [G loss: 0.150821] [ema: 0.999361] 
[Epoch 27/125] [Batch 0/402] [D loss: 0.464702] [G loss: 0.116721] [ema: 0.999362] 
[Epoch 27/125] [Batch 100/402] [D loss: 0.491344] [G loss: 0.162548] [ema: 0.999367] 
[Epoch 27/125] [Batch 200/402] [D loss: 0.501283] [G loss: 0.146170] [ema: 0.999373] 
[Epoch 27/125] [Batch 300/402] [D loss: 0.478614] [G loss: 0.139176] [ema: 0.999379] 
[Epoch 27/125] [Batch 400/402] [D loss: 0.521768] [G loss: 0.140866] [ema: 0.999384] 
[Epoch 28/125] [Batch 0/402] [D loss: 0.512958] [G loss: 0.149436] [ema: 0.999384] 
[Epoch 28/125] [Batch 100/402] [D loss: 0.459202] [G loss: 0.119589] [ema: 0.999390] 
[Epoch 28/125] [Batch 200/402] [D loss: 0.489779] [G loss: 0.139251] [ema: 0.999395] 
[Epoch 28/125] [Batch 300/402] [D loss: 0.453879] [G loss: 0.167092] [ema: 0.999400] 
[Epoch 28/125] [Batch 400/402] [D loss: 0.485071] [G loss: 0.134685] [ema: 0.999406] 
[Epoch 29/125] [Batch 0/402] [D loss: 0.409784] [G loss: 0.202354] [ema: 0.999406] 
[Epoch 29/125] [Batch 100/402] [D loss: 0.545001] [G loss: 0.143114] [ema: 0.999411] 
[Epoch 29/125] [Batch 200/402] [D loss: 0.408717] [G loss: 0.144130] [ema: 0.999416] 
[Epoch 29/125] [Batch 300/402] [D loss: 0.415172] [G loss: 0.112942] [ema: 0.999421] 
[Epoch 29/125] [Batch 400/402] [D loss: 0.446898] [G loss: 0.168106] [ema: 0.999425] 
[Epoch 30/125] [Batch 0/402] [D loss: 0.457822] [G loss: 0.187588] [ema: 0.999425] 
[Epoch 30/125] [Batch 100/402] [D loss: 0.447362] [G loss: 0.151240] [ema: 0.999430] 
[Epoch 30/125] [Batch 200/402] [D loss: 0.502208] [G loss: 0.155448] [ema: 0.999435] 
[Epoch 30/125] [Batch 300/402] [D loss: 0.436521] [G loss: 0.174702] [ema: 0.999439] 
[Epoch 30/125] [Batch 400/402] [D loss: 0.447881] [G loss: 0.153808] [ema: 0.999444] 
[Epoch 31/125] [Batch 0/402] [D loss: 0.418726] [G loss: 0.174996] [ema: 0.999444] 
[Epoch 31/125] [Batch 100/402] [D loss: 0.469510] [G loss: 0.179112] [ema: 0.999448] 
[Epoch 31/125] [Batch 200/402] [D loss: 0.414545] [G loss: 0.201540] [ema: 0.999453] 
[Epoch 31/125] [Batch 300/402] [D loss: 0.368833] [G loss: 0.181954] [ema: 0.999457] 
[Epoch 31/125] [Batch 400/402] [D loss: 0.449127] [G loss: 0.139698] [ema: 0.999461] 



Saving checkpoint 2 in logs/daghar_50000_60_100/downstairs_50000_D_60_2024_10_23_19_00_36/Model



[Epoch 32/125] [Batch 0/402] [D loss: 0.421395] [G loss: 0.149232] [ema: 0.999461] 
[Epoch 32/125] [Batch 100/402] [D loss: 0.436532] [G loss: 0.171674] [ema: 0.999465] 
[Epoch 32/125] [Batch 200/402] [D loss: 0.454513] [G loss: 0.195868] [ema: 0.999470] 
[Epoch 32/125] [Batch 300/402] [D loss: 0.445505] [G loss: 0.190648] [ema: 0.999474] 
[Epoch 32/125] [Batch 400/402] [D loss: 0.392179] [G loss: 0.220851] [ema: 0.999478] 
[Epoch 33/125] [Batch 0/402] [D loss: 0.386980] [G loss: 0.171349] [ema: 0.999478] 
[Epoch 33/125] [Batch 100/402] [D loss: 0.442368] [G loss: 0.155607] [ema: 0.999482] 
[Epoch 33/125] [Batch 200/402] [D loss: 0.463510] [G loss: 0.145961] [ema: 0.999485] 
[Epoch 33/125] [Batch 300/402] [D loss: 0.393920] [G loss: 0.213342] [ema: 0.999489] 
[Epoch 33/125] [Batch 400/402] [D loss: 0.398218] [G loss: 0.131214] [ema: 0.999493] 
[Epoch 34/125] [Batch 0/402] [D loss: 0.478834] [G loss: 0.177272] [ema: 0.999493] 
[Epoch 34/125] [Batch 100/402] [D loss: 0.421729] [G loss: 0.173969] [ema: 0.999497] 
[Epoch 34/125] [Batch 200/402] [D loss: 0.411800] [G loss: 0.185011] [ema: 0.999500] 
[Epoch 34/125] [Batch 300/402] [D loss: 0.452213] [G loss: 0.138064] [ema: 0.999504] 
[Epoch 34/125] [Batch 400/402] [D loss: 0.405791] [G loss: 0.221903] [ema: 0.999507] 
[Epoch 35/125] [Batch 0/402] [D loss: 0.328893] [G loss: 0.185760] [ema: 0.999507] 
[Epoch 35/125] [Batch 100/402] [D loss: 0.424529] [G loss: 0.222822] [ema: 0.999511] 
[Epoch 35/125] [Batch 200/402] [D loss: 0.375982] [G loss: 0.178993] [ema: 0.999514] 
[Epoch 35/125] [Batch 300/402] [D loss: 0.400593] [G loss: 0.201186] [ema: 0.999518] 
[Epoch 35/125] [Batch 400/402] [D loss: 0.425115] [G loss: 0.197521] [ema: 0.999521] 
[Epoch 36/125] [Batch 0/402] [D loss: 0.361390] [G loss: 0.183614] [ema: 0.999521] 
[Epoch 36/125] [Batch 100/402] [D loss: 0.393865] [G loss: 0.178057] [ema: 0.999524] 
[Epoch 36/125] [Batch 200/402] [D loss: 0.377092] [G loss: 0.178312] [ema: 0.999528] 
[Epoch 36/125] [Batch 300/402] [D loss: 0.459208] [G loss: 0.169170] [ema: 0.999531] 
[Epoch 36/125] [Batch 400/402] [D loss: 0.338682] [G loss: 0.181796] [ema: 0.999534] 
[Epoch 37/125] [Batch 0/402] [D loss: 0.400888] [G loss: 0.239309] [ema: 0.999534] 
[Epoch 37/125] [Batch 100/402] [D loss: 0.395742] [G loss: 0.225332] [ema: 0.999537] 
[Epoch 37/125] [Batch 200/402] [D loss: 0.347810] [G loss: 0.188408] [ema: 0.999540] 
[Epoch 37/125] [Batch 300/402] [D loss: 0.360665] [G loss: 0.175269] [ema: 0.999543] 
[Epoch 37/125] [Batch 400/402] [D loss: 0.385021] [G loss: 0.205906] [ema: 0.999546] 
[Epoch 38/125] [Batch 0/402] [D loss: 0.391547] [G loss: 0.192860] [ema: 0.999546] 
[Epoch 38/125] [Batch 100/402] [D loss: 0.353916] [G loss: 0.207068] [ema: 0.999549] 
[Epoch 38/125] [Batch 200/402] [D loss: 0.377205] [G loss: 0.207702] [ema: 0.999552] 
[Epoch 38/125] [Batch 300/402] [D loss: 0.408542] [G loss: 0.193654] [ema: 0.999555] 
[Epoch 38/125] [Batch 400/402] [D loss: 0.429270] [G loss: 0.194972] [ema: 0.999558] 
[Epoch 39/125] [Batch 0/402] [D loss: 0.396166] [G loss: 0.206728] [ema: 0.999558] 
[Epoch 39/125] [Batch 100/402] [D loss: 0.370974] [G loss: 0.181923] [ema: 0.999561] 
[Epoch 39/125] [Batch 200/402] [D loss: 0.419295] [G loss: 0.173817] [ema: 0.999564] 
[Epoch 39/125] [Batch 300/402] [D loss: 0.409940] [G loss: 0.176411] [ema: 0.999566] 
[Epoch 39/125] [Batch 400/402] [D loss: 0.376692] [G loss: 0.196963] [ema: 0.999569] 
[Epoch 40/125] [Batch 0/402] [D loss: 0.378505] [G loss: 0.188291] [ema: 0.999569] 
[Epoch 40/125] [Batch 100/402] [D loss: 0.401682] [G loss: 0.182480] [ema: 0.999572] 
[Epoch 40/125] [Batch 200/402] [D loss: 0.358281] [G loss: 0.169662] [ema: 0.999574] 
[Epoch 40/125] [Batch 300/402] [D loss: 0.387869] [G loss: 0.196347] [ema: 0.999577] 
[Epoch 40/125] [Batch 400/402] [D loss: 0.323522] [G loss: 0.225475] [ema: 0.999579] 
[Epoch 41/125] [Batch 0/402] [D loss: 0.277844] [G loss: 0.215655] [ema: 0.999580] 
[Epoch 41/125] [Batch 100/402] [D loss: 0.342727] [G loss: 0.164098] [ema: 0.999582] 
[Epoch 41/125] [Batch 200/402] [D loss: 0.294034] [G loss: 0.196613] [ema: 0.999585] 
[Epoch 41/125] [Batch 300/402] [D loss: 0.383093] [G loss: 0.182525] [ema: 0.999587] 
[Epoch 41/125] [Batch 400/402] [D loss: 0.339888] [G loss: 0.193724] [ema: 0.999590] 
[Epoch 42/125] [Batch 0/402] [D loss: 0.391425] [G loss: 0.143880] [ema: 0.999590] 
[Epoch 42/125] [Batch 100/402] [D loss: 0.434650] [G loss: 0.200878] [ema: 0.999592] 
[Epoch 42/125] [Batch 200/402] [D loss: 0.358549] [G loss: 0.199811] [ema: 0.999594] 
[Epoch 42/125] [Batch 300/402] [D loss: 0.354927] [G loss: 0.192585] [ema: 0.999597] 
[Epoch 42/125] [Batch 400/402] [D loss: 0.378030] [G loss: 0.154793] [ema: 0.999599] 
[Epoch 43/125] [Batch 0/402] [D loss: 0.370347] [G loss: 0.219505] [ema: 0.999599] 
[Epoch 43/125] [Batch 100/402] [D loss: 0.333080] [G loss: 0.205669] [ema: 0.999601] 
[Epoch 43/125] [Batch 200/402] [D loss: 0.349293] [G loss: 0.218488] [ema: 0.999604] 
[Epoch 43/125] [Batch 300/402] [D loss: 0.347336] [G loss: 0.196695] [ema: 0.999606] 
[Epoch 43/125] [Batch 400/402] [D loss: 0.342905] [G loss: 0.186554] [ema: 0.999608] 
[Epoch 44/125] [Batch 0/402] [D loss: 0.359884] [G loss: 0.207194] [ema: 0.999608] 
[Epoch 44/125] [Batch 100/402] [D loss: 0.342950] [G loss: 0.220050] [ema: 0.999610] 
[Epoch 44/125] [Batch 200/402] [D loss: 0.392581] [G loss: 0.179431] [ema: 0.999613] 
[Epoch 44/125] [Batch 300/402] [D loss: 0.361559] [G loss: 0.187137] [ema: 0.999615] 
[Epoch 44/125] [Batch 400/402] [D loss: 0.379930] [G loss: 0.205992] [ema: 0.999617] 
[Epoch 45/125] [Batch 0/402] [D loss: 0.359911] [G loss: 0.204895] [ema: 0.999617] 
[Epoch 45/125] [Batch 100/402] [D loss: 0.318828] [G loss: 0.192057] [ema: 0.999619] 
[Epoch 45/125] [Batch 200/402] [D loss: 0.353979] [G loss: 0.187738] [ema: 0.999621] 
[Epoch 45/125] [Batch 300/402] [D loss: 0.336275] [G loss: 0.197376] [ema: 0.999623] 
[Epoch 45/125] [Batch 400/402] [D loss: 0.334438] [G loss: 0.201922] [ema: 0.999625] 
[Epoch 46/125] [Batch 0/402] [D loss: 0.324536] [G loss: 0.209629] [ema: 0.999625] 
[Epoch 46/125] [Batch 100/402] [D loss: 0.368920] [G loss: 0.217615] [ema: 0.999627] 
[Epoch 46/125] [Batch 200/402] [D loss: 0.345778] [G loss: 0.236121] [ema: 0.999629] 
[Epoch 46/125] [Batch 300/402] [D loss: 0.353607] [G loss: 0.181722] [ema: 0.999631] 
[Epoch 46/125] [Batch 400/402] [D loss: 0.313109] [G loss: 0.197151] [ema: 0.999633] 
[Epoch 47/125] [Batch 0/402] [D loss: 0.369760] [G loss: 0.197133] [ema: 0.999633] 
[Epoch 47/125] [Batch 100/402] [D loss: 0.412323] [G loss: 0.184383] [ema: 0.999635] 
[Epoch 47/125] [Batch 200/402] [D loss: 0.388463] [G loss: 0.173068] [ema: 0.999637] 
[Epoch 47/125] [Batch 300/402] [D loss: 0.443943] [G loss: 0.186148] [ema: 0.999639] 
[Epoch 47/125] [Batch 400/402] [D loss: 0.377823] [G loss: 0.178060] [ema: 0.999641] 
[Epoch 48/125] [Batch 0/402] [D loss: 0.372624] [G loss: 0.184795] [ema: 0.999641] 
[Epoch 48/125] [Batch 100/402] [D loss: 0.368681] [G loss: 0.185975] [ema: 0.999643] 
[Epoch 48/125] [Batch 200/402] [D loss: 0.431938] [G loss: 0.173433] [ema: 0.999645] 
[Epoch 48/125] [Batch 300/402] [D loss: 0.340632] [G loss: 0.178276] [ema: 0.999646] 
[Epoch 48/125] [Batch 400/402] [D loss: 0.376834] [G loss: 0.131934] [ema: 0.999648] 
[Epoch 49/125] [Batch 0/402] [D loss: 0.371999] [G loss: 0.180902] [ema: 0.999648] 
[Epoch 49/125] [Batch 100/402] [D loss: 0.397792] [G loss: 0.200849] [ema: 0.999650] 
[Epoch 49/125] [Batch 200/402] [D loss: 0.406453] [G loss: 0.198610] [ema: 0.999652] 
[Epoch 49/125] [Batch 300/402] [D loss: 0.460646] [G loss: 0.156801] [ema: 0.999653] 
[Epoch 49/125] [Batch 400/402] [D loss: 0.382647] [G loss: 0.189849] [ema: 0.999655] 
[Epoch 50/125] [Batch 0/402] [D loss: 0.316059] [G loss: 0.207622] [ema: 0.999655] 
[Epoch 50/125] [Batch 100/402] [D loss: 0.366052] [G loss: 0.193789] [ema: 0.999657] 
[Epoch 50/125] [Batch 200/402] [D loss: 0.385996] [G loss: 0.173074] [ema: 0.999659] 
[Epoch 50/125] [Batch 300/402] [D loss: 0.401120] [G loss: 0.148532] [ema: 0.999660] 
[Epoch 50/125] [Batch 400/402] [D loss: 0.379279] [G loss: 0.193486] [ema: 0.999662] 
[Epoch 51/125] [Batch 0/402] [D loss: 0.378741] [G loss: 0.217827] [ema: 0.999662] 
[Epoch 51/125] [Batch 100/402] [D loss: 0.378064] [G loss: 0.158836] [ema: 0.999664] 
[Epoch 51/125] [Batch 200/402] [D loss: 0.389923] [G loss: 0.169930] [ema: 0.999665] 
[Epoch 51/125] [Batch 300/402] [D loss: 0.429239] [G loss: 0.156594] [ema: 0.999667] 
[Epoch 51/125] [Batch 400/402] [D loss: 0.454459] [G loss: 0.172166] [ema: 0.999668] 
[Epoch 52/125] [Batch 0/402] [D loss: 0.374497] [G loss: 0.178876] [ema: 0.999668] 
[Epoch 52/125] [Batch 100/402] [D loss: 0.468547] [G loss: 0.151802] [ema: 0.999670] 
[Epoch 52/125] [Batch 200/402] [D loss: 0.431651] [G loss: 0.157253] [ema: 0.999672] 
[Epoch 52/125] [Batch 300/402] [D loss: 0.487136] [G loss: 0.185371] [ema: 0.999673] 
[Epoch 52/125] [Batch 400/402] [D loss: 0.416110] [G loss: 0.167318] [ema: 0.999675] 
[Epoch 53/125] [Batch 0/402] [D loss: 0.451087] [G loss: 0.161633] [ema: 0.999675] 
[Epoch 53/125] [Batch 100/402] [D loss: 0.390591] [G loss: 0.164896] [ema: 0.999676] 
[Epoch 53/125] [Batch 200/402] [D loss: 0.451503] [G loss: 0.184614] [ema: 0.999678] 
[Epoch 53/125] [Batch 300/402] [D loss: 0.439442] [G loss: 0.138811] [ema: 0.999679] 
[Epoch 53/125] [Batch 400/402] [D loss: 0.537581] [G loss: 0.153325] [ema: 0.999681] 
[Epoch 54/125] [Batch 0/402] [D loss: 0.446410] [G loss: 0.167971] [ema: 0.999681] 
[Epoch 54/125] [Batch 100/402] [D loss: 0.456078] [G loss: 0.171197] [ema: 0.999682] 
[Epoch 54/125] [Batch 200/402] [D loss: 0.384250] [G loss: 0.175292] [ema: 0.999684] 
[Epoch 54/125] [Batch 300/402] [D loss: 0.470397] [G loss: 0.174579] [ema: 0.999685] 
[Epoch 54/125] [Batch 400/402] [D loss: 0.386374] [G loss: 0.166622] [ema: 0.999687] 
[Epoch 55/125] [Batch 0/402] [D loss: 0.448050] [G loss: 0.169780] [ema: 0.999687] 
[Epoch 55/125] [Batch 100/402] [D loss: 0.453149] [G loss: 0.173304] [ema: 0.999688] 
[Epoch 55/125] [Batch 200/402] [D loss: 0.450479] [G loss: 0.165343] [ema: 0.999689] 
[Epoch 55/125] [Batch 300/402] [D loss: 0.494827] [G loss: 0.194767] [ema: 0.999691] 
[Epoch 55/125] [Batch 400/402] [D loss: 0.510872] [G loss: 0.163183] [ema: 0.999692] 
[Epoch 56/125] [Batch 0/402] [D loss: 0.373133] [G loss: 0.166460] [ema: 0.999692] 
[Epoch 56/125] [Batch 100/402] [D loss: 0.406798] [G loss: 0.174930] [ema: 0.999694] 
[Epoch 56/125] [Batch 200/402] [D loss: 0.484924] [G loss: 0.150675] [ema: 0.999695] 
[Epoch 56/125] [Batch 300/402] [D loss: 0.396170] [G loss: 0.189013] [ema: 0.999696] 
[Epoch 56/125] [Batch 400/402] [D loss: 0.489508] [G loss: 0.170916] [ema: 0.999698] 
[Epoch 57/125] [Batch 0/402] [D loss: 0.399042] [G loss: 0.164594] [ema: 0.999698] 
[Epoch 57/125] [Batch 100/402] [D loss: 0.421225] [G loss: 0.190300] [ema: 0.999699] 
[Epoch 57/125] [Batch 200/402] [D loss: 0.439759] [G loss: 0.182563] [ema: 0.999700] 
[Epoch 57/125] [Batch 300/402] [D loss: 0.441272] [G loss: 0.150461] [ema: 0.999701] 
[Epoch 57/125] [Batch 400/402] [D loss: 0.410995] [G loss: 0.149778] [ema: 0.999703] 
[Epoch 58/125] [Batch 0/402] [D loss: 0.388194] [G loss: 0.184800] [ema: 0.999703] 
[Epoch 58/125] [Batch 100/402] [D loss: 0.433472] [G loss: 0.170143] [ema: 0.999704] 
[Epoch 58/125] [Batch 200/402] [D loss: 0.435670] [G loss: 0.192852] [ema: 0.999705] 
[Epoch 58/125] [Batch 300/402] [D loss: 0.521865] [G loss: 0.173199] [ema: 0.999707] 
[Epoch 58/125] [Batch 400/402] [D loss: 0.359003] [G loss: 0.157370] [ema: 0.999708] 
[Epoch 59/125] [Batch 0/402] [D loss: 0.441684] [G loss: 0.180095] [ema: 0.999708] 
[Epoch 59/125] [Batch 100/402] [D loss: 0.425513] [G loss: 0.192194] [ema: 0.999709] 
[Epoch 59/125] [Batch 200/402] [D loss: 0.437164] [G loss: 0.165171] [ema: 0.999710] 
[Epoch 59/125] [Batch 300/402] [D loss: 0.401053] [G loss: 0.210915] [ema: 0.999711] 
[Epoch 59/125] [Batch 400/402] [D loss: 0.445303] [G loss: 0.191659] [ema: 0.999713] 
[Epoch 60/125] [Batch 0/402] [D loss: 0.468612] [G loss: 0.183446] [ema: 0.999713] 
[Epoch 60/125] [Batch 100/402] [D loss: 0.472146] [G loss: 0.179948] [ema: 0.999714] 
[Epoch 60/125] [Batch 200/402] [D loss: 0.428670] [G loss: 0.176863] [ema: 0.999715] 
[Epoch 60/125] [Batch 300/402] [D loss: 0.386558] [G loss: 0.166074] [ema: 0.999716] 
[Epoch 60/125] [Batch 400/402] [D loss: 0.378300] [G loss: 0.166273] [ema: 0.999717] 
[Epoch 61/125] [Batch 0/402] [D loss: 0.464718] [G loss: 0.189287] [ema: 0.999717] 
[Epoch 61/125] [Batch 100/402] [D loss: 0.389142] [G loss: 0.180510] [ema: 0.999719] 
[Epoch 61/125] [Batch 200/402] [D loss: 0.402690] [G loss: 0.189538] [ema: 0.999720] 
[Epoch 61/125] [Batch 300/402] [D loss: 0.477006] [G loss: 0.161273] [ema: 0.999721] 
[Epoch 61/125] [Batch 400/402] [D loss: 0.396981] [G loss: 0.165375] [ema: 0.999722] 
[Epoch 62/125] [Batch 0/402] [D loss: 0.419817] [G loss: 0.167517] [ema: 0.999722] 
[Epoch 62/125] [Batch 100/402] [D loss: 0.405462] [G loss: 0.215930] [ema: 0.999723] 
[Epoch 62/125] [Batch 200/402] [D loss: 0.457293] [G loss: 0.160217] [ema: 0.999724] 
[Epoch 62/125] [Batch 300/402] [D loss: 0.401221] [G loss: 0.204088] [ema: 0.999725] 
[Epoch 62/125] [Batch 400/402] [D loss: 0.390222] [G loss: 0.201552] [ema: 0.999726] 
[Epoch 63/125] [Batch 0/402] [D loss: 0.411416] [G loss: 0.190842] [ema: 0.999726] 
[Epoch 63/125] [Batch 100/402] [D loss: 0.355099] [G loss: 0.160131] [ema: 0.999727] 
[Epoch 63/125] [Batch 200/402] [D loss: 0.384522] [G loss: 0.166883] [ema: 0.999728] 
[Epoch 63/125] [Batch 300/402] [D loss: 0.356473] [G loss: 0.195936] [ema: 0.999730] 
[Epoch 63/125] [Batch 400/402] [D loss: 0.404398] [G loss: 0.150361] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_50000_60_100/downstairs_50000_D_60_2024_10_23_19_00_36/Model



[Epoch 64/125] [Batch 0/402] [D loss: 0.339496] [G loss: 0.191835] [ema: 0.999731] 
[Epoch 64/125] [Batch 100/402] [D loss: 0.396745] [G loss: 0.176680] [ema: 0.999732] 
[Epoch 64/125] [Batch 200/402] [D loss: 0.422735] [G loss: 0.172198] [ema: 0.999733] 
[Epoch 64/125] [Batch 300/402] [D loss: 0.387434] [G loss: 0.171736] [ema: 0.999734] 
[Epoch 64/125] [Batch 400/402] [D loss: 0.431173] [G loss: 0.193154] [ema: 0.999735] 
[Epoch 65/125] [Batch 0/402] [D loss: 0.383650] [G loss: 0.179677] [ema: 0.999735] 
[Epoch 65/125] [Batch 100/402] [D loss: 0.435002] [G loss: 0.153611] [ema: 0.999736] 
[Epoch 65/125] [Batch 200/402] [D loss: 0.423856] [G loss: 0.172022] [ema: 0.999737] 
[Epoch 65/125] [Batch 300/402] [D loss: 0.342004] [G loss: 0.213505] [ema: 0.999738] 
[Epoch 65/125] [Batch 400/402] [D loss: 0.418383] [G loss: 0.204928] [ema: 0.999739] 
[Epoch 66/125] [Batch 0/402] [D loss: 0.391643] [G loss: 0.220717] [ema: 0.999739] 
[Epoch 66/125] [Batch 100/402] [D loss: 0.356757] [G loss: 0.144550] [ema: 0.999740] 
[Epoch 66/125] [Batch 200/402] [D loss: 0.469272] [G loss: 0.193288] [ema: 0.999741] 
[Epoch 66/125] [Batch 300/402] [D loss: 0.400794] [G loss: 0.208384] [ema: 0.999742] 
[Epoch 66/125] [Batch 400/402] [D loss: 0.382540] [G loss: 0.192122] [ema: 0.999743] 
[Epoch 67/125] [Batch 0/402] [D loss: 0.352090] [G loss: 0.198137] [ema: 0.999743] 
[Epoch 67/125] [Batch 100/402] [D loss: 0.366552] [G loss: 0.198453] [ema: 0.999744] 
[Epoch 67/125] [Batch 200/402] [D loss: 0.401287] [G loss: 0.157761] [ema: 0.999745] 
[Epoch 67/125] [Batch 300/402] [D loss: 0.342301] [G loss: 0.199673] [ema: 0.999746] 
[Epoch 67/125] [Batch 400/402] [D loss: 0.407924] [G loss: 0.172965] [ema: 0.999746] 
[Epoch 68/125] [Batch 0/402] [D loss: 0.387759] [G loss: 0.166148] [ema: 0.999746] 
[Epoch 68/125] [Batch 100/402] [D loss: 0.351475] [G loss: 0.167164] [ema: 0.999747] 
[Epoch 68/125] [Batch 200/402] [D loss: 0.375465] [G loss: 0.149772] [ema: 0.999748] 
[Epoch 68/125] [Batch 300/402] [D loss: 0.417830] [G loss: 0.189637] [ema: 0.999749] 
[Epoch 68/125] [Batch 400/402] [D loss: 0.432199] [G loss: 0.166833] [ema: 0.999750] 
[Epoch 69/125] [Batch 0/402] [D loss: 0.396304] [G loss: 0.167688] [ema: 0.999750] 
[Epoch 69/125] [Batch 100/402] [D loss: 0.362272] [G loss: 0.184287] [ema: 0.999751] 
[Epoch 69/125] [Batch 200/402] [D loss: 0.437278] [G loss: 0.186299] [ema: 0.999752] 
[Epoch 69/125] [Batch 300/402] [D loss: 0.431615] [G loss: 0.190549] [ema: 0.999753] 
[Epoch 69/125] [Batch 400/402] [D loss: 0.394069] [G loss: 0.147782] [ema: 0.999754] 
[Epoch 70/125] [Batch 0/402] [D loss: 0.378154] [G loss: 0.191765] [ema: 0.999754] 
[Epoch 70/125] [Batch 100/402] [D loss: 0.342147] [G loss: 0.152587] [ema: 0.999755] 
[Epoch 70/125] [Batch 200/402] [D loss: 0.335999] [G loss: 0.215559] [ema: 0.999755] 
[Epoch 70/125] [Batch 300/402] [D loss: 0.455243] [G loss: 0.188481] [ema: 0.999756] 
[Epoch 70/125] [Batch 400/402] [D loss: 0.368186] [G loss: 0.216090] [ema: 0.999757] 
[Epoch 71/125] [Batch 0/402] [D loss: 0.351324] [G loss: 0.216192] [ema: 0.999757] 
[Epoch 71/125] [Batch 100/402] [D loss: 0.402551] [G loss: 0.220603] [ema: 0.999758] 
[Epoch 71/125] [Batch 200/402] [D loss: 0.353586] [G loss: 0.234069] [ema: 0.999759] 
[Epoch 71/125] [Batch 300/402] [D loss: 0.340909] [G loss: 0.177731] [ema: 0.999760] 
[Epoch 71/125] [Batch 400/402] [D loss: 0.389220] [G loss: 0.216817] [ema: 0.999761] 
[Epoch 72/125] [Batch 0/402] [D loss: 0.388826] [G loss: 0.221069] [ema: 0.999761] 
[Epoch 72/125] [Batch 100/402] [D loss: 0.378798] [G loss: 0.183169] [ema: 0.999761] 
[Epoch 72/125] [Batch 200/402] [D loss: 0.420626] [G loss: 0.171565] [ema: 0.999762] 
[Epoch 72/125] [Batch 300/402] [D loss: 0.353355] [G loss: 0.212705] [ema: 0.999763] 
[Epoch 72/125] [Batch 400/402] [D loss: 0.406740] [G loss: 0.163941] [ema: 0.999764] 
[Epoch 73/125] [Batch 0/402] [D loss: 0.355153] [G loss: 0.159140] [ema: 0.999764] 
[Epoch 73/125] [Batch 100/402] [D loss: 0.416596] [G loss: 0.185770] [ema: 0.999765] 
[Epoch 73/125] [Batch 200/402] [D loss: 0.419022] [G loss: 0.170941] [ema: 0.999765] 
[Epoch 73/125] [Batch 300/402] [D loss: 0.397740] [G loss: 0.222255] [ema: 0.999766] 
[Epoch 73/125] [Batch 400/402] [D loss: 0.357671] [G loss: 0.179267] [ema: 0.999767] 
[Epoch 74/125] [Batch 0/402] [D loss: 0.389993] [G loss: 0.166875] [ema: 0.999767] 
[Epoch 74/125] [Batch 100/402] [D loss: 0.368277] [G loss: 0.192256] [ema: 0.999768] 
[Epoch 74/125] [Batch 200/402] [D loss: 0.399524] [G loss: 0.186071] [ema: 0.999769] 
[Epoch 74/125] [Batch 300/402] [D loss: 0.397340] [G loss: 0.212901] [ema: 0.999769] 
[Epoch 74/125] [Batch 400/402] [D loss: 0.395595] [G loss: 0.179948] [ema: 0.999770] 
[Epoch 75/125] [Batch 0/402] [D loss: 0.343116] [G loss: 0.179076] [ema: 0.999770] 
[Epoch 75/125] [Batch 100/402] [D loss: 0.348430] [G loss: 0.153212] [ema: 0.999771] 
[Epoch 75/125] [Batch 200/402] [D loss: 0.421559] [G loss: 0.171855] [ema: 0.999772] 
[Epoch 75/125] [Batch 300/402] [D loss: 0.393272] [G loss: 0.173532] [ema: 0.999772] 
[Epoch 75/125] [Batch 400/402] [D loss: 0.359808] [G loss: 0.207642] [ema: 0.999773] 
[Epoch 76/125] [Batch 0/402] [D loss: 0.394331] [G loss: 0.204464] [ema: 0.999773] 
[Epoch 76/125] [Batch 100/402] [D loss: 0.347773] [G loss: 0.174894] [ema: 0.999774] 
[Epoch 76/125] [Batch 200/402] [D loss: 0.374387] [G loss: 0.180440] [ema: 0.999775] 
[Epoch 76/125] [Batch 300/402] [D loss: 0.381023] [G loss: 0.232791] [ema: 0.999775] 
[Epoch 76/125] [Batch 400/402] [D loss: 0.486877] [G loss: 0.148803] [ema: 0.999776] 
[Epoch 77/125] [Batch 0/402] [D loss: 0.453693] [G loss: 0.164178] [ema: 0.999776] 
[Epoch 77/125] [Batch 100/402] [D loss: 0.341966] [G loss: 0.208741] [ema: 0.999777] 
[Epoch 77/125] [Batch 200/402] [D loss: 0.388986] [G loss: 0.163098] [ema: 0.999778] 
[Epoch 77/125] [Batch 300/402] [D loss: 0.460889] [G loss: 0.207910] [ema: 0.999778] 
[Epoch 77/125] [Batch 400/402] [D loss: 0.351712] [G loss: 0.195610] [ema: 0.999779] 
[Epoch 78/125] [Batch 0/402] [D loss: 0.369418] [G loss: 0.189124] [ema: 0.999779] 
[Epoch 78/125] [Batch 100/402] [D loss: 0.339553] [G loss: 0.241248] [ema: 0.999780] 
[Epoch 78/125] [Batch 200/402] [D loss: 0.376471] [G loss: 0.178072] [ema: 0.999780] 
[Epoch 78/125] [Batch 300/402] [D loss: 0.416141] [G loss: 0.204417] [ema: 0.999781] 
[Epoch 78/125] [Batch 400/402] [D loss: 0.387444] [G loss: 0.218110] [ema: 0.999782] 
[Epoch 79/125] [Batch 0/402] [D loss: 0.309528] [G loss: 0.174923] [ema: 0.999782] 
[Epoch 79/125] [Batch 100/402] [D loss: 0.333192] [G loss: 0.206973] [ema: 0.999782] 
[Epoch 79/125] [Batch 200/402] [D loss: 0.393912] [G loss: 0.194340] [ema: 0.999783] 
[Epoch 79/125] [Batch 300/402] [D loss: 0.340739] [G loss: 0.198360] [ema: 0.999784] 
[Epoch 79/125] [Batch 400/402] [D loss: 0.423495] [G loss: 0.150538] [ema: 0.999784] 
[Epoch 80/125] [Batch 0/402] [D loss: 0.399695] [G loss: 0.180319] [ema: 0.999784] 
[Epoch 80/125] [Batch 100/402] [D loss: 0.381816] [G loss: 0.191472] [ema: 0.999785] 
[Epoch 80/125] [Batch 200/402] [D loss: 0.351164] [G loss: 0.191050] [ema: 0.999786] 
[Epoch 80/125] [Batch 300/402] [D loss: 0.336054] [G loss: 0.231719] [ema: 0.999786] 
[Epoch 80/125] [Batch 400/402] [D loss: 0.372079] [G loss: 0.175080] [ema: 0.999787] 
[Epoch 81/125] [Batch 0/402] [D loss: 0.356532] [G loss: 0.189309] [ema: 0.999787] 
[Epoch 81/125] [Batch 100/402] [D loss: 0.390579] [G loss: 0.209764] [ema: 0.999788] 
[Epoch 81/125] [Batch 200/402] [D loss: 0.433634] [G loss: 0.165190] [ema: 0.999788] 
[Epoch 81/125] [Batch 300/402] [D loss: 0.308489] [G loss: 0.186678] [ema: 0.999789] 
[Epoch 81/125] [Batch 400/402] [D loss: 0.432936] [G loss: 0.189320] [ema: 0.999790] 
[Epoch 82/125] [Batch 0/402] [D loss: 0.437273] [G loss: 0.208428] [ema: 0.999790] 
[Epoch 82/125] [Batch 100/402] [D loss: 0.378385] [G loss: 0.190661] [ema: 0.999790] 
[Epoch 82/125] [Batch 200/402] [D loss: 0.408000] [G loss: 0.193968] [ema: 0.999791] 
[Epoch 82/125] [Batch 300/402] [D loss: 0.330050] [G loss: 0.206402] [ema: 0.999792] 
[Epoch 82/125] [Batch 400/402] [D loss: 0.381155] [G loss: 0.192616] [ema: 0.999792] 
[Epoch 83/125] [Batch 0/402] [D loss: 0.367900] [G loss: 0.171806] [ema: 0.999792] 
[Epoch 83/125] [Batch 100/402] [D loss: 0.333780] [G loss: 0.201200] [ema: 0.999793] 
[Epoch 83/125] [Batch 200/402] [D loss: 0.422547] [G loss: 0.177050] [ema: 0.999794] 
[Epoch 83/125] [Batch 300/402] [D loss: 0.340340] [G loss: 0.221486] [ema: 0.999794] 
[Epoch 83/125] [Batch 400/402] [D loss: 0.347924] [G loss: 0.199963] [ema: 0.999795] 
[Epoch 84/125] [Batch 0/402] [D loss: 0.440173] [G loss: 0.210092] [ema: 0.999795] 
[Epoch 84/125] [Batch 100/402] [D loss: 0.400181] [G loss: 0.186664] [ema: 0.999795] 
[Epoch 84/125] [Batch 200/402] [D loss: 0.371001] [G loss: 0.193892] [ema: 0.999796] 
[Epoch 84/125] [Batch 300/402] [D loss: 0.426246] [G loss: 0.181514] [ema: 0.999797] 
[Epoch 84/125] [Batch 400/402] [D loss: 0.319833] [G loss: 0.177727] [ema: 0.999797] 
[Epoch 85/125] [Batch 0/402] [D loss: 0.431135] [G loss: 0.183701] [ema: 0.999797] 
[Epoch 85/125] [Batch 100/402] [D loss: 0.348491] [G loss: 0.205985] [ema: 0.999798] 
[Epoch 85/125] [Batch 200/402] [D loss: 0.366667] [G loss: 0.190894] [ema: 0.999798] 
[Epoch 85/125] [Batch 300/402] [D loss: 0.410583] [G loss: 0.198885] [ema: 0.999799] 
[Epoch 85/125] [Batch 400/402] [D loss: 0.356459] [G loss: 0.189979] [ema: 0.999800] 
[Epoch 86/125] [Batch 0/402] [D loss: 0.405328] [G loss: 0.184114] [ema: 0.999800] 
[Epoch 86/125] [Batch 100/402] [D loss: 0.333834] [G loss: 0.188275] [ema: 0.999800] 
[Epoch 86/125] [Batch 200/402] [D loss: 0.370725] [G loss: 0.183285] [ema: 0.999801] 
[Epoch 86/125] [Batch 300/402] [D loss: 0.360772] [G loss: 0.184516] [ema: 0.999801] 
[Epoch 86/125] [Batch 400/402] [D loss: 0.378473] [G loss: 0.206407] [ema: 0.999802] 
[Epoch 87/125] [Batch 0/402] [D loss: 0.407001] [G loss: 0.222041] [ema: 0.999802] 
[Epoch 87/125] [Batch 100/402] [D loss: 0.367695] [G loss: 0.226004] [ema: 0.999802] 
[Epoch 87/125] [Batch 200/402] [D loss: 0.382991] [G loss: 0.194052] [ema: 0.999803] 
[Epoch 87/125] [Batch 300/402] [D loss: 0.376229] [G loss: 0.200358] [ema: 0.999804] 
[Epoch 87/125] [Batch 400/402] [D loss: 0.375564] [G loss: 0.209645] [ema: 0.999804] 
[Epoch 88/125] [Batch 0/402] [D loss: 0.367741] [G loss: 0.193251] [ema: 0.999804] 
[Epoch 88/125] [Batch 100/402] [D loss: 0.289446] [G loss: 0.197260] [ema: 0.999805] 
[Epoch 88/125] [Batch 200/402] [D loss: 0.385748] [G loss: 0.225230] [ema: 0.999805] 
[Epoch 88/125] [Batch 300/402] [D loss: 0.314158] [G loss: 0.233381] [ema: 0.999806] 
[Epoch 88/125] [Batch 400/402] [D loss: 0.312260] [G loss: 0.240236] [ema: 0.999806] 
[Epoch 89/125] [Batch 0/402] [D loss: 0.354703] [G loss: 0.232001] [ema: 0.999806] 
[Epoch 89/125] [Batch 100/402] [D loss: 0.351513] [G loss: 0.203600] [ema: 0.999807] 
[Epoch 89/125] [Batch 200/402] [D loss: 0.423034] [G loss: 0.188733] [ema: 0.999807] 
[Epoch 89/125] [Batch 300/402] [D loss: 0.391522] [G loss: 0.215180] [ema: 0.999808] 
[Epoch 89/125] [Batch 400/402] [D loss: 0.356900] [G loss: 0.203194] [ema: 0.999808] 
[Epoch 90/125] [Batch 0/402] [D loss: 0.366729] [G loss: 0.177906] [ema: 0.999808] 
[Epoch 90/125] [Batch 100/402] [D loss: 0.356282] [G loss: 0.199149] [ema: 0.999809] 
[Epoch 90/125] [Batch 200/402] [D loss: 0.336587] [G loss: 0.196417] [ema: 0.999809] 
[Epoch 90/125] [Batch 300/402] [D loss: 0.340332] [G loss: 0.175715] [ema: 0.999810] 
[Epoch 90/125] [Batch 400/402] [D loss: 0.315491] [G loss: 0.178056] [ema: 0.999811] 
[Epoch 91/125] [Batch 0/402] [D loss: 0.340282] [G loss: 0.192623] [ema: 0.999811] 
[Epoch 91/125] [Batch 100/402] [D loss: 0.377930] [G loss: 0.164065] [ema: 0.999811] 
[Epoch 91/125] [Batch 200/402] [D loss: 0.411343] [G loss: 0.208397] [ema: 0.999812] 
[Epoch 91/125] [Batch 300/402] [D loss: 0.340298] [G loss: 0.172796] [ema: 0.999812] 
[Epoch 91/125] [Batch 400/402] [D loss: 0.360553] [G loss: 0.215103] [ema: 0.999813] 
[Epoch 92/125] [Batch 0/402] [D loss: 0.359748] [G loss: 0.213295] [ema: 0.999813] 
[Epoch 92/125] [Batch 100/402] [D loss: 0.309252] [G loss: 0.183966] [ema: 0.999813] 
[Epoch 92/125] [Batch 200/402] [D loss: 0.364311] [G loss: 0.247958] [ema: 0.999814] 
[Epoch 92/125] [Batch 300/402] [D loss: 0.426735] [G loss: 0.216325] [ema: 0.999814] 
[Epoch 92/125] [Batch 400/402] [D loss: 0.313195] [G loss: 0.207011] [ema: 0.999815] 
[Epoch 93/125] [Batch 0/402] [D loss: 0.328345] [G loss: 0.202758] [ema: 0.999815] 
[Epoch 93/125] [Batch 100/402] [D loss: 0.326786] [G loss: 0.188807] [ema: 0.999815] 
[Epoch 93/125] [Batch 200/402] [D loss: 0.357472] [G loss: 0.218017] [ema: 0.999816] 
[Epoch 93/125] [Batch 300/402] [D loss: 0.350852] [G loss: 0.192618] [ema: 0.999816] 
[Epoch 93/125] [Batch 400/402] [D loss: 0.384184] [G loss: 0.228193] [ema: 0.999817] 
[Epoch 94/125] [Batch 0/402] [D loss: 0.374723] [G loss: 0.180621] [ema: 0.999817] 
[Epoch 94/125] [Batch 100/402] [D loss: 0.377315] [G loss: 0.211781] [ema: 0.999817] 
[Epoch 94/125] [Batch 200/402] [D loss: 0.410687] [G loss: 0.195536] [ema: 0.999818] 
[Epoch 94/125] [Batch 300/402] [D loss: 0.328420] [G loss: 0.190821] [ema: 0.999818] 
[Epoch 94/125] [Batch 400/402] [D loss: 0.374690] [G loss: 0.217087] [ema: 0.999819] 
[Epoch 95/125] [Batch 0/402] [D loss: 0.403303] [G loss: 0.243079] [ema: 0.999819] 
[Epoch 95/125] [Batch 100/402] [D loss: 0.422731] [G loss: 0.196136] [ema: 0.999819] 
[Epoch 95/125] [Batch 200/402] [D loss: 0.368559] [G loss: 0.232461] [ema: 0.999819] 
[Epoch 95/125] [Batch 300/402] [D loss: 0.363416] [G loss: 0.177689] [ema: 0.999820] 
[Epoch 95/125] [Batch 400/402] [D loss: 0.361640] [G loss: 0.207051] [ema: 0.999820] 



Saving checkpoint 4 in logs/daghar_50000_60_100/downstairs_50000_D_60_2024_10_23_19_00_36/Model



[Epoch 96/125] [Batch 0/402] [D loss: 0.433625] [G loss: 0.200462] [ema: 0.999820] 
[Epoch 96/125] [Batch 100/402] [D loss: 0.358584] [G loss: 0.213778] [ema: 0.999821] 
[Epoch 96/125] [Batch 200/402] [D loss: 0.352849] [G loss: 0.179125] [ema: 0.999821] 
[Epoch 96/125] [Batch 300/402] [D loss: 0.435322] [G loss: 0.191659] [ema: 0.999822] 
[Epoch 96/125] [Batch 400/402] [D loss: 0.392913] [G loss: 0.216365] [ema: 0.999822] 
[Epoch 97/125] [Batch 0/402] [D loss: 0.325241] [G loss: 0.212603] [ema: 0.999822] 
[Epoch 97/125] [Batch 100/402] [D loss: 0.332803] [G loss: 0.222887] [ema: 0.999823] 
[Epoch 97/125] [Batch 200/402] [D loss: 0.374549] [G loss: 0.231125] [ema: 0.999823] 
[Epoch 97/125] [Batch 300/402] [D loss: 0.386606] [G loss: 0.218318] [ema: 0.999824] 
[Epoch 97/125] [Batch 400/402] [D loss: 0.405286] [G loss: 0.224012] [ema: 0.999824] 
[Epoch 98/125] [Batch 0/402] [D loss: 0.355344] [G loss: 0.231350] [ema: 0.999824] 
[Epoch 98/125] [Batch 100/402] [D loss: 0.302190] [G loss: 0.212829] [ema: 0.999825] 
[Epoch 98/125] [Batch 200/402] [D loss: 0.361638] [G loss: 0.206938] [ema: 0.999825] 
[Epoch 98/125] [Batch 300/402] [D loss: 0.344484] [G loss: 0.198960] [ema: 0.999825] 
[Epoch 98/125] [Batch 400/402] [D loss: 0.348264] [G loss: 0.205512] [ema: 0.999826] 
[Epoch 99/125] [Batch 0/402] [D loss: 0.445770] [G loss: 0.199335] [ema: 0.999826] 
[Epoch 99/125] [Batch 100/402] [D loss: 0.349455] [G loss: 0.208187] [ema: 0.999826] 
[Epoch 99/125] [Batch 200/402] [D loss: 0.363175] [G loss: 0.211140] [ema: 0.999827] 
[Epoch 99/125] [Batch 300/402] [D loss: 0.333123] [G loss: 0.234200] [ema: 0.999827] 
[Epoch 99/125] [Batch 400/402] [D loss: 0.340156] [G loss: 0.193217] [ema: 0.999828] 
[Epoch 100/125] [Batch 0/402] [D loss: 0.380813] [G loss: 0.242239] [ema: 0.999828] 
[Epoch 100/125] [Batch 100/402] [D loss: 0.356366] [G loss: 0.169444] [ema: 0.999828] 
[Epoch 100/125] [Batch 200/402] [D loss: 0.406724] [G loss: 0.224261] [ema: 0.999828] 
[Epoch 100/125] [Batch 300/402] [D loss: 0.349498] [G loss: 0.216734] [ema: 0.999829] 
[Epoch 100/125] [Batch 400/402] [D loss: 0.362779] [G loss: 0.193479] [ema: 0.999829] 
[Epoch 101/125] [Batch 0/402] [D loss: 0.404231] [G loss: 0.181453] [ema: 0.999829] 
[Epoch 101/125] [Batch 100/402] [D loss: 0.290262] [G loss: 0.213470] [ema: 0.999830] 
[Epoch 101/125] [Batch 200/402] [D loss: 0.342915] [G loss: 0.183607] [ema: 0.999830] 
[Epoch 101/125] [Batch 300/402] [D loss: 0.355191] [G loss: 0.244383] [ema: 0.999831] 
[Epoch 101/125] [Batch 400/402] [D loss: 0.371837] [G loss: 0.194625] [ema: 0.999831] 
[Epoch 102/125] [Batch 0/402] [D loss: 0.379874] [G loss: 0.205915] [ema: 0.999831] 
[Epoch 102/125] [Batch 100/402] [D loss: 0.303259] [G loss: 0.219865] [ema: 0.999831] 
[Epoch 102/125] [Batch 200/402] [D loss: 0.389332] [G loss: 0.177571] [ema: 0.999832] 
[Epoch 102/125] [Batch 300/402] [D loss: 0.298374] [G loss: 0.202843] [ema: 0.999832] 
[Epoch 102/125] [Batch 400/402] [D loss: 0.353831] [G loss: 0.200601] [ema: 0.999833] 
[Epoch 103/125] [Batch 0/402] [D loss: 0.405336] [G loss: 0.192942] [ema: 0.999833] 
[Epoch 103/125] [Batch 100/402] [D loss: 0.384693] [G loss: 0.205777] [ema: 0.999833] 
[Epoch 103/125] [Batch 200/402] [D loss: 0.347319] [G loss: 0.211335] [ema: 0.999833] 
[Epoch 103/125] [Batch 300/402] [D loss: 0.360781] [G loss: 0.231942] [ema: 0.999834] 
[Epoch 103/125] [Batch 400/402] [D loss: 0.401048] [G loss: 0.208359] [ema: 0.999834] 
[Epoch 104/125] [Batch 0/402] [D loss: 0.358928] [G loss: 0.200409] [ema: 0.999834] 
[Epoch 104/125] [Batch 100/402] [D loss: 0.372506] [G loss: 0.164012] [ema: 0.999835] 
[Epoch 104/125] [Batch 200/402] [D loss: 0.427278] [G loss: 0.202572] [ema: 0.999835] 
[Epoch 104/125] [Batch 300/402] [D loss: 0.362536] [G loss: 0.205687] [ema: 0.999835] 
[Epoch 104/125] [Batch 400/402] [D loss: 0.414000] [G loss: 0.169626] [ema: 0.999836] 
[Epoch 105/125] [Batch 0/402] [D loss: 0.374481] [G loss: 0.209008] [ema: 0.999836] 
[Epoch 105/125] [Batch 100/402] [D loss: 0.399214] [G loss: 0.196636] [ema: 0.999836] 
[Epoch 105/125] [Batch 200/402] [D loss: 0.352021] [G loss: 0.229260] [ema: 0.999837] 
[Epoch 105/125] [Batch 300/402] [D loss: 0.358106] [G loss: 0.235402] [ema: 0.999837] 
[Epoch 105/125] [Batch 400/402] [D loss: 0.331584] [G loss: 0.223606] [ema: 0.999837] 
[Epoch 106/125] [Batch 0/402] [D loss: 0.355427] [G loss: 0.200014] [ema: 0.999837] 
[Epoch 106/125] [Batch 100/402] [D loss: 0.370846] [G loss: 0.192368] [ema: 0.999838] 
[Epoch 106/125] [Batch 200/402] [D loss: 0.342346] [G loss: 0.201757] [ema: 0.999838] 
[Epoch 106/125] [Batch 300/402] [D loss: 0.392755] [G loss: 0.200577] [ema: 0.999838] 
[Epoch 106/125] [Batch 400/402] [D loss: 0.300464] [G loss: 0.197431] [ema: 0.999839] 
[Epoch 107/125] [Batch 0/402] [D loss: 0.392099] [G loss: 0.216394] [ema: 0.999839] 
[Epoch 107/125] [Batch 100/402] [D loss: 0.360241] [G loss: 0.208452] [ema: 0.999839] 
[Epoch 107/125] [Batch 200/402] [D loss: 0.372953] [G loss: 0.190017] [ema: 0.999840] 
[Epoch 107/125] [Batch 300/402] [D loss: 0.383014] [G loss: 0.193002] [ema: 0.999840] 
[Epoch 107/125] [Batch 400/402] [D loss: 0.377058] [G loss: 0.204689] [ema: 0.999840] 
[Epoch 108/125] [Batch 0/402] [D loss: 0.351166] [G loss: 0.214182] [ema: 0.999840] 
[Epoch 108/125] [Batch 100/402] [D loss: 0.371920] [G loss: 0.195989] [ema: 0.999841] 
[Epoch 108/125] [Batch 200/402] [D loss: 0.341064] [G loss: 0.224859] [ema: 0.999841] 
[Epoch 108/125] [Batch 300/402] [D loss: 0.357614] [G loss: 0.199476] [ema: 0.999841] 
[Epoch 108/125] [Batch 400/402] [D loss: 0.349414] [G loss: 0.217351] [ema: 0.999842] 
[Epoch 109/125] [Batch 0/402] [D loss: 0.356330] [G loss: 0.230031] [ema: 0.999842] 
[Epoch 109/125] [Batch 100/402] [D loss: 0.340252] [G loss: 0.199876] [ema: 0.999842] 
[Epoch 109/125] [Batch 200/402] [D loss: 0.310472] [G loss: 0.225927] [ema: 0.999843] 
[Epoch 109/125] [Batch 300/402] [D loss: 0.329679] [G loss: 0.191017] [ema: 0.999843] 
[Epoch 109/125] [Batch 400/402] [D loss: 0.433917] [G loss: 0.149201] [ema: 0.999843] 
[Epoch 110/125] [Batch 0/402] [D loss: 0.390889] [G loss: 0.202606] [ema: 0.999843] 
[Epoch 110/125] [Batch 100/402] [D loss: 0.324621] [G loss: 0.203749] [ema: 0.999844] 
[Epoch 110/125] [Batch 200/402] [D loss: 0.344752] [G loss: 0.169755] [ema: 0.999844] 
[Epoch 110/125] [Batch 300/402] [D loss: 0.340512] [G loss: 0.174040] [ema: 0.999844] 
[Epoch 110/125] [Batch 400/402] [D loss: 0.435207] [G loss: 0.193271] [ema: 0.999845] 
[Epoch 111/125] [Batch 0/402] [D loss: 0.362328] [G loss: 0.236411] [ema: 0.999845] 
[Epoch 111/125] [Batch 100/402] [D loss: 0.360010] [G loss: 0.199987] [ema: 0.999845] 
[Epoch 111/125] [Batch 200/402] [D loss: 0.329628] [G loss: 0.184060] [ema: 0.999845] 
[Epoch 111/125] [Batch 300/402] [D loss: 0.402192] [G loss: 0.179017] [ema: 0.999846] 
[Epoch 111/125] [Batch 400/402] [D loss: 0.372544] [G loss: 0.179468] [ema: 0.999846] 
[Epoch 112/125] [Batch 0/402] [D loss: 0.341616] [G loss: 0.219106] [ema: 0.999846] 
[Epoch 112/125] [Batch 100/402] [D loss: 0.360656] [G loss: 0.189035] [ema: 0.999846] 
[Epoch 112/125] [Batch 200/402] [D loss: 0.328984] [G loss: 0.185058] [ema: 0.999847] 
[Epoch 112/125] [Batch 300/402] [D loss: 0.387488] [G loss: 0.194390] [ema: 0.999847] 
[Epoch 112/125] [Batch 400/402] [D loss: 0.400736] [G loss: 0.188820] [ema: 0.999847] 
[Epoch 113/125] [Batch 0/402] [D loss: 0.369140] [G loss: 0.210728] [ema: 0.999847] 
[Epoch 113/125] [Batch 100/402] [D loss: 0.374154] [G loss: 0.208422] [ema: 0.999848] 
[Epoch 113/125] [Batch 200/402] [D loss: 0.395732] [G loss: 0.215619] [ema: 0.999848] 
[Epoch 113/125] [Batch 300/402] [D loss: 0.394319] [G loss: 0.199857] [ema: 0.999848] 
[Epoch 113/125] [Batch 400/402] [D loss: 0.345097] [G loss: 0.191284] [ema: 0.999849] 
[Epoch 114/125] [Batch 0/402] [D loss: 0.383619] [G loss: 0.225747] [ema: 0.999849] 
[Epoch 114/125] [Batch 100/402] [D loss: 0.373200] [G loss: 0.192932] [ema: 0.999849] 
[Epoch 114/125] [Batch 200/402] [D loss: 0.347792] [G loss: 0.196993] [ema: 0.999849] 
[Epoch 114/125] [Batch 300/402] [D loss: 0.378152] [G loss: 0.181884] [ema: 0.999850] 
[Epoch 114/125] [Batch 400/402] [D loss: 0.383740] [G loss: 0.221634] [ema: 0.999850] 
[Epoch 115/125] [Batch 0/402] [D loss: 0.340956] [G loss: 0.247416] [ema: 0.999850] 
[Epoch 115/125] [Batch 100/402] [D loss: 0.371199] [G loss: 0.212844] [ema: 0.999850] 
[Epoch 115/125] [Batch 200/402] [D loss: 0.386350] [G loss: 0.212146] [ema: 0.999851] 
[Epoch 115/125] [Batch 300/402] [D loss: 0.409033] [G loss: 0.217302] [ema: 0.999851] 
[Epoch 115/125] [Batch 400/402] [D loss: 0.426453] [G loss: 0.163349] [ema: 0.999851] 
[Epoch 116/125] [Batch 0/402] [D loss: 0.359882] [G loss: 0.213595] [ema: 0.999851] 
[Epoch 116/125] [Batch 100/402] [D loss: 0.326641] [G loss: 0.218349] [ema: 0.999852] 
[Epoch 116/125] [Batch 200/402] [D loss: 0.365173] [G loss: 0.181690] [ema: 0.999852] 
[Epoch 116/125] [Batch 300/402] [D loss: 0.313816] [G loss: 0.190294] [ema: 0.999852] 
[Epoch 116/125] [Batch 400/402] [D loss: 0.382094] [G loss: 0.189983] [ema: 0.999853] 
[Epoch 117/125] [Batch 0/402] [D loss: 0.344075] [G loss: 0.238814] [ema: 0.999853] 
[Epoch 117/125] [Batch 100/402] [D loss: 0.440437] [G loss: 0.213688] [ema: 0.999853] 
[Epoch 117/125] [Batch 200/402] [D loss: 0.404005] [G loss: 0.190201] [ema: 0.999853] 
[Epoch 117/125] [Batch 300/402] [D loss: 0.435282] [G loss: 0.210934] [ema: 0.999854] 
[Epoch 117/125] [Batch 400/402] [D loss: 0.361084] [G loss: 0.195358] [ema: 0.999854] 
[Epoch 118/125] [Batch 0/402] [D loss: 0.298225] [G loss: 0.212316] [ema: 0.999854] 
[Epoch 118/125] [Batch 100/402] [D loss: 0.382415] [G loss: 0.193668] [ema: 0.999854] 
[Epoch 118/125] [Batch 200/402] [D loss: 0.390205] [G loss: 0.206458] [ema: 0.999855] 
[Epoch 118/125] [Batch 300/402] [D loss: 0.424097] [G loss: 0.203281] [ema: 0.999855] 
[Epoch 118/125] [Batch 400/402] [D loss: 0.316136] [G loss: 0.205745] [ema: 0.999855] 
[Epoch 119/125] [Batch 0/402] [D loss: 0.399096] [G loss: 0.236464] [ema: 0.999855] 
[Epoch 119/125] [Batch 100/402] [D loss: 0.339026] [G loss: 0.216512] [ema: 0.999855] 
[Epoch 119/125] [Batch 200/402] [D loss: 0.377753] [G loss: 0.194288] [ema: 0.999856] 
[Epoch 119/125] [Batch 300/402] [D loss: 0.393466] [G loss: 0.199233] [ema: 0.999856] 
[Epoch 119/125] [Batch 400/402] [D loss: 0.379602] [G loss: 0.214261] [ema: 0.999856] 
[Epoch 120/125] [Batch 0/402] [D loss: 0.322716] [G loss: 0.215650] [ema: 0.999856] 
[Epoch 120/125] [Batch 100/402] [D loss: 0.412324] [G loss: 0.179101] [ema: 0.999857] 
[Epoch 120/125] [Batch 200/402] [D loss: 0.376798] [G loss: 0.191437] [ema: 0.999857] 
[Epoch 120/125] [Batch 300/402] [D loss: 0.442899] [G loss: 0.187068] [ema: 0.999857] 
[Epoch 120/125] [Batch 400/402] [D loss: 0.330504] [G loss: 0.189467] [ema: 0.999858] 
[Epoch 121/125] [Batch 0/402] [D loss: 0.394824] [G loss: 0.196225] [ema: 0.999858] 
[Epoch 121/125] [Batch 100/402] [D loss: 0.321703] [G loss: 0.199194] [ema: 0.999858] 
[Epoch 121/125] [Batch 200/402] [D loss: 0.388837] [G loss: 0.192019] [ema: 0.999858] 
[Epoch 121/125] [Batch 300/402] [D loss: 0.318985] [G loss: 0.205099] [ema: 0.999858] 
[Epoch 121/125] [Batch 400/402] [D loss: 0.362893] [G loss: 0.209393] [ema: 0.999859] 
[Epoch 122/125] [Batch 0/402] [D loss: 0.315979] [G loss: 0.174715] [ema: 0.999859] 
[Epoch 122/125] [Batch 100/402] [D loss: 0.386124] [G loss: 0.185731] [ema: 0.999859] 
[Epoch 122/125] [Batch 200/402] [D loss: 0.368549] [G loss: 0.217908] [ema: 0.999859] 
[Epoch 122/125] [Batch 300/402] [D loss: 0.359880] [G loss: 0.218963] [ema: 0.999860] 
[Epoch 122/125] [Batch 400/402] [D loss: 0.365953] [G loss: 0.198010] [ema: 0.999860] 
[Epoch 123/125] [Batch 0/402] [D loss: 0.403193] [G loss: 0.218747] [ema: 0.999860] 
[Epoch 123/125] [Batch 100/402] [D loss: 0.295583] [G loss: 0.170900] [ema: 0.999860] 
[Epoch 123/125] [Batch 200/402] [D loss: 0.371351] [G loss: 0.203916] [ema: 0.999860] 
[Epoch 123/125] [Batch 300/402] [D loss: 0.358079] [G loss: 0.185396] [ema: 0.999861] 
[Epoch 123/125] [Batch 400/402] [D loss: 0.366522] [G loss: 0.197199] [ema: 0.999861] 
[Epoch 124/125] [Batch 0/402] [D loss: 0.371738] [G loss: 0.190898] [ema: 0.999861] 
[Epoch 124/125] [Batch 100/402] [D loss: 0.334918] [G loss: 0.199925] [ema: 0.999861] 
[Epoch 124/125] [Batch 200/402] [D loss: 0.359917] [G loss: 0.193919] [ema: 0.999862] 
[Epoch 124/125] [Batch 300/402] [D loss: 0.424521] [G loss: 0.191231] [ema: 0.999862] 
[Epoch 124/125] [Batch 400/402] [D loss: 0.340411] [G loss: 0.186195] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
sit training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
sit
daghar
return single class data and labels, class is sit
data shape is (8614, 3, 1, 60)
label shape is (8614,)
539
Epochs between checkpoint: 24



Saving checkpoint 1 in logs/daghar_50000_60_100/sit_50000_D_60_2024_10_23_19_34_15/Model



[Epoch 0/93] [Batch 0/539] [D loss: 1.101219] [G loss: 0.317199] [ema: 0.000000] 
[Epoch 0/93] [Batch 100/539] [D loss: 0.562431] [G loss: 0.166641] [ema: 0.933033] 
[Epoch 0/93] [Batch 200/539] [D loss: 0.562999] [G loss: 0.145966] [ema: 0.965936] 
[Epoch 0/93] [Batch 300/539] [D loss: 0.610680] [G loss: 0.119062] [ema: 0.977160] 
[Epoch 0/93] [Batch 400/539] [D loss: 0.608101] [G loss: 0.100908] [ema: 0.982821] 
[Epoch 0/93] [Batch 500/539] [D loss: 0.604816] [G loss: 0.107007] [ema: 0.986233] 
[Epoch 1/93] [Batch 0/539] [D loss: 0.523719] [G loss: 0.151805] [ema: 0.987222] 
[Epoch 1/93] [Batch 100/539] [D loss: 0.511673] [G loss: 0.155992] [ema: 0.989211] 
[Epoch 1/93] [Batch 200/539] [D loss: 0.493337] [G loss: 0.122074] [ema: 0.990664] 
[Epoch 1/93] [Batch 300/539] [D loss: 0.481525] [G loss: 0.126414] [ema: 0.991772] 
[Epoch 1/93] [Batch 400/539] [D loss: 0.502376] [G loss: 0.139859] [ema: 0.992645] 
[Epoch 1/93] [Batch 500/539] [D loss: 0.487840] [G loss: 0.111786] [ema: 0.993351] 
[Epoch 2/93] [Batch 0/539] [D loss: 0.582026] [G loss: 0.141669] [ema: 0.993591] 
[Epoch 2/93] [Batch 100/539] [D loss: 0.508395] [G loss: 0.122274] [ema: 0.994133] 
[Epoch 2/93] [Batch 200/539] [D loss: 0.553372] [G loss: 0.125528] [ema: 0.994591] 
[Epoch 2/93] [Batch 300/539] [D loss: 0.479566] [G loss: 0.136682] [ema: 0.994983] 
[Epoch 2/93] [Batch 400/539] [D loss: 0.580017] [G loss: 0.109155] [ema: 0.995321] 
[Epoch 2/93] [Batch 500/539] [D loss: 0.513268] [G loss: 0.113075] [ema: 0.995617] 
[Epoch 3/93] [Batch 0/539] [D loss: 0.532542] [G loss: 0.136732] [ema: 0.995723] 
[Epoch 3/93] [Batch 100/539] [D loss: 0.595213] [G loss: 0.109115] [ema: 0.995971] 
[Epoch 3/93] [Batch 200/539] [D loss: 0.619545] [G loss: 0.101373] [ema: 0.996192] 
[Epoch 3/93] [Batch 300/539] [D loss: 0.569860] [G loss: 0.118196] [ema: 0.996391] 
[Epoch 3/93] [Batch 400/539] [D loss: 0.583809] [G loss: 0.112273] [ema: 0.996569] 
[Epoch 3/93] [Batch 500/539] [D loss: 0.523339] [G loss: 0.111477] [ema: 0.996731] 
[Epoch 4/93] [Batch 0/539] [D loss: 0.539242] [G loss: 0.141767] [ema: 0.996790] 
[Epoch 4/93] [Batch 100/539] [D loss: 0.564475] [G loss: 0.145790] [ema: 0.996932] 
[Epoch 4/93] [Batch 200/539] [D loss: 0.559853] [G loss: 0.118597] [ema: 0.997062] 
[Epoch 4/93] [Batch 300/539] [D loss: 0.589907] [G loss: 0.111212] [ema: 0.997182] 
[Epoch 4/93] [Batch 400/539] [D loss: 0.547704] [G loss: 0.119959] [ema: 0.997292] 
[Epoch 4/93] [Batch 500/539] [D loss: 0.511724] [G loss: 0.124071] [ema: 0.997394] 
[Epoch 5/93] [Batch 0/539] [D loss: 0.573991] [G loss: 0.106144] [ema: 0.997431] 
[Epoch 5/93] [Batch 100/539] [D loss: 0.575657] [G loss: 0.117561] [ema: 0.997523] 
[Epoch 5/93] [Batch 200/539] [D loss: 0.542498] [G loss: 0.115762] [ema: 0.997609] 
[Epoch 5/93] [Batch 300/539] [D loss: 0.523450] [G loss: 0.131020] [ema: 0.997688] 
[Epoch 5/93] [Batch 400/539] [D loss: 0.547929] [G loss: 0.112493] [ema: 0.997763] 
[Epoch 5/93] [Batch 500/539] [D loss: 0.559160] [G loss: 0.116514] [ema: 0.997833] 
[Epoch 6/93] [Batch 0/539] [D loss: 0.504281] [G loss: 0.135705] [ema: 0.997859] 
[Epoch 6/93] [Batch 100/539] [D loss: 0.568926] [G loss: 0.117605] [ema: 0.997923] 
[Epoch 6/93] [Batch 200/539] [D loss: 0.526878] [G loss: 0.117266] [ema: 0.997984] 
[Epoch 6/93] [Batch 300/539] [D loss: 0.551397] [G loss: 0.112312] [ema: 0.998041] 
[Epoch 6/93] [Batch 400/539] [D loss: 0.507215] [G loss: 0.128009] [ema: 0.998094] 
[Epoch 6/93] [Batch 500/539] [D loss: 0.578914] [G loss: 0.109619] [ema: 0.998145] 
[Epoch 7/93] [Batch 0/539] [D loss: 0.597027] [G loss: 0.106911] [ema: 0.998165] 
[Epoch 7/93] [Batch 100/539] [D loss: 0.540249] [G loss: 0.103712] [ema: 0.998212] 
[Epoch 7/93] [Batch 200/539] [D loss: 0.520776] [G loss: 0.132911] [ema: 0.998257] 
[Epoch 7/93] [Batch 300/539] [D loss: 0.525374] [G loss: 0.117727] [ema: 0.998300] 
[Epoch 7/93] [Batch 400/539] [D loss: 0.524144] [G loss: 0.110250] [ema: 0.998340] 
[Epoch 7/93] [Batch 500/539] [D loss: 0.541705] [G loss: 0.132201] [ema: 0.998379] 
[Epoch 8/93] [Batch 0/539] [D loss: 0.555356] [G loss: 0.131398] [ema: 0.998394] 
[Epoch 8/93] [Batch 100/539] [D loss: 0.556047] [G loss: 0.114133] [ema: 0.998430] 
[Epoch 8/93] [Batch 200/539] [D loss: 0.525714] [G loss: 0.118672] [ema: 0.998465] 
[Epoch 8/93] [Batch 300/539] [D loss: 0.485286] [G loss: 0.134649] [ema: 0.998498] 
[Epoch 8/93] [Batch 400/539] [D loss: 0.501549] [G loss: 0.168454] [ema: 0.998530] 
[Epoch 8/93] [Batch 500/539] [D loss: 0.496713] [G loss: 0.179790] [ema: 0.998561] 
[Epoch 9/93] [Batch 0/539] [D loss: 0.493885] [G loss: 0.124888] [ema: 0.998572] 
[Epoch 9/93] [Batch 100/539] [D loss: 0.476584] [G loss: 0.182161] [ema: 0.998601] 
[Epoch 9/93] [Batch 200/539] [D loss: 0.469507] [G loss: 0.093834] [ema: 0.998629] 
[Epoch 9/93] [Batch 300/539] [D loss: 0.429165] [G loss: 0.140530] [ema: 0.998655] 
[Epoch 9/93] [Batch 400/539] [D loss: 0.495879] [G loss: 0.134646] [ema: 0.998681] 
[Epoch 9/93] [Batch 500/539] [D loss: 0.396625] [G loss: 0.188570] [ema: 0.998705] 
[Epoch 10/93] [Batch 0/539] [D loss: 0.504879] [G loss: 0.150275] [ema: 0.998715] 
[Epoch 10/93] [Batch 100/539] [D loss: 0.481139] [G loss: 0.144724] [ema: 0.998738] 
[Epoch 10/93] [Batch 200/539] [D loss: 0.500761] [G loss: 0.124055] [ema: 0.998761] 
[Epoch 10/93] [Batch 300/539] [D loss: 0.507211] [G loss: 0.148862] [ema: 0.998783] 
[Epoch 10/93] [Batch 400/539] [D loss: 0.483037] [G loss: 0.153038] [ema: 0.998804] 
[Epoch 10/93] [Batch 500/539] [D loss: 0.485436] [G loss: 0.119176] [ema: 0.998824] 
[Epoch 11/93] [Batch 0/539] [D loss: 0.452076] [G loss: 0.163708] [ema: 0.998832] 
[Epoch 11/93] [Batch 100/539] [D loss: 0.422904] [G loss: 0.154720] [ema: 0.998851] 
[Epoch 11/93] [Batch 200/539] [D loss: 0.482128] [G loss: 0.151889] [ema: 0.998870] 
[Epoch 11/93] [Batch 300/539] [D loss: 0.458675] [G loss: 0.147699] [ema: 0.998888] 
[Epoch 11/93] [Batch 400/539] [D loss: 0.478329] [G loss: 0.163817] [ema: 0.998905] 
[Epoch 11/93] [Batch 500/539] [D loss: 0.496984] [G loss: 0.150626] [ema: 0.998922] 
[Epoch 12/93] [Batch 0/539] [D loss: 0.454165] [G loss: 0.147720] [ema: 0.998929] 
[Epoch 12/93] [Batch 100/539] [D loss: 0.507721] [G loss: 0.156566] [ema: 0.998945] 
[Epoch 12/93] [Batch 200/539] [D loss: 0.456298] [G loss: 0.146925] [ema: 0.998961] 
[Epoch 12/93] [Batch 300/539] [D loss: 0.464193] [G loss: 0.120548] [ema: 0.998976] 
[Epoch 12/93] [Batch 400/539] [D loss: 0.453641] [G loss: 0.149508] [ema: 0.998991] 
[Epoch 12/93] [Batch 500/539] [D loss: 0.440335] [G loss: 0.135661] [ema: 0.999006] 
[Epoch 13/93] [Batch 0/539] [D loss: 0.454671] [G loss: 0.188781] [ema: 0.999011] 
[Epoch 13/93] [Batch 100/539] [D loss: 0.435022] [G loss: 0.153295] [ema: 0.999025] 
[Epoch 13/93] [Batch 200/539] [D loss: 0.425043] [G loss: 0.142204] [ema: 0.999039] 
[Epoch 13/93] [Batch 300/539] [D loss: 0.467721] [G loss: 0.126687] [ema: 0.999052] 
[Epoch 13/93] [Batch 400/539] [D loss: 0.485475] [G loss: 0.145937] [ema: 0.999065] 
[Epoch 13/93] [Batch 500/539] [D loss: 0.392004] [G loss: 0.157980] [ema: 0.999077] 
[Epoch 14/93] [Batch 0/539] [D loss: 0.456271] [G loss: 0.170655] [ema: 0.999082] 
[Epoch 14/93] [Batch 100/539] [D loss: 0.445856] [G loss: 0.149732] [ema: 0.999094] 
[Epoch 14/93] [Batch 200/539] [D loss: 0.447778] [G loss: 0.156476] [ema: 0.999106] 
[Epoch 14/93] [Batch 300/539] [D loss: 0.527481] [G loss: 0.154847] [ema: 0.999117] 
[Epoch 14/93] [Batch 400/539] [D loss: 0.419647] [G loss: 0.139515] [ema: 0.999128] 
[Epoch 14/93] [Batch 500/539] [D loss: 0.436426] [G loss: 0.179804] [ema: 0.999139] 
[Epoch 15/93] [Batch 0/539] [D loss: 0.395540] [G loss: 0.189719] [ema: 0.999143] 
[Epoch 15/93] [Batch 100/539] [D loss: 0.450884] [G loss: 0.156067] [ema: 0.999154] 
[Epoch 15/93] [Batch 200/539] [D loss: 0.447573] [G loss: 0.152939] [ema: 0.999164] 
[Epoch 15/93] [Batch 300/539] [D loss: 0.475773] [G loss: 0.140706] [ema: 0.999174] 
[Epoch 15/93] [Batch 400/539] [D loss: 0.447263] [G loss: 0.136301] [ema: 0.999183] 
[Epoch 15/93] [Batch 500/539] [D loss: 0.459839] [G loss: 0.158236] [ema: 0.999193] 
[Epoch 16/93] [Batch 0/539] [D loss: 0.442207] [G loss: 0.151900] [ema: 0.999197] 
[Epoch 16/93] [Batch 100/539] [D loss: 0.493451] [G loss: 0.155314] [ema: 0.999206] 
[Epoch 16/93] [Batch 200/539] [D loss: 0.478505] [G loss: 0.150455] [ema: 0.999215] 
[Epoch 16/93] [Batch 300/539] [D loss: 0.443620] [G loss: 0.168654] [ema: 0.999224] 
[Epoch 16/93] [Batch 400/539] [D loss: 0.407167] [G loss: 0.157888] [ema: 0.999232] 
[Epoch 16/93] [Batch 500/539] [D loss: 0.449681] [G loss: 0.149572] [ema: 0.999241] 
[Epoch 17/93] [Batch 0/539] [D loss: 0.486319] [G loss: 0.163265] [ema: 0.999244] 
[Epoch 17/93] [Batch 100/539] [D loss: 0.519759] [G loss: 0.175631] [ema: 0.999252] 
[Epoch 17/93] [Batch 200/539] [D loss: 0.475183] [G loss: 0.164339] [ema: 0.999260] 
[Epoch 17/93] [Batch 300/539] [D loss: 0.464697] [G loss: 0.170786] [ema: 0.999268] 
[Epoch 17/93] [Batch 400/539] [D loss: 0.478647] [G loss: 0.152220] [ema: 0.999275] 
[Epoch 17/93] [Batch 500/539] [D loss: 0.480897] [G loss: 0.170638] [ema: 0.999283] 
[Epoch 18/93] [Batch 0/539] [D loss: 0.419843] [G loss: 0.140559] [ema: 0.999286] 
[Epoch 18/93] [Batch 100/539] [D loss: 0.466375] [G loss: 0.171651] [ema: 0.999293] 
[Epoch 18/93] [Batch 200/539] [D loss: 0.534303] [G loss: 0.141058] [ema: 0.999300] 
[Epoch 18/93] [Batch 300/539] [D loss: 0.435091] [G loss: 0.144955] [ema: 0.999307] 
[Epoch 18/93] [Batch 400/539] [D loss: 0.488862] [G loss: 0.145274] [ema: 0.999314] 
[Epoch 18/93] [Batch 500/539] [D loss: 0.442734] [G loss: 0.138666] [ema: 0.999321] 
[Epoch 19/93] [Batch 0/539] [D loss: 0.530592] [G loss: 0.144257] [ema: 0.999323] 
[Epoch 19/93] [Batch 100/539] [D loss: 0.472389] [G loss: 0.166900] [ema: 0.999330] 
[Epoch 19/93] [Batch 200/539] [D loss: 0.481020] [G loss: 0.161669] [ema: 0.999336] 
[Epoch 19/93] [Batch 300/539] [D loss: 0.385835] [G loss: 0.170072] [ema: 0.999343] 
[Epoch 19/93] [Batch 400/539] [D loss: 0.439429] [G loss: 0.142619] [ema: 0.999349] 
[Epoch 19/93] [Batch 500/539] [D loss: 0.519064] [G loss: 0.111126] [ema: 0.999355] 
[Epoch 20/93] [Batch 0/539] [D loss: 0.452959] [G loss: 0.169496] [ema: 0.999357] 
[Epoch 20/93] [Batch 100/539] [D loss: 0.476384] [G loss: 0.142181] [ema: 0.999363] 
[Epoch 20/93] [Batch 200/539] [D loss: 0.403288] [G loss: 0.155813] [ema: 0.999369] 
[Epoch 20/93] [Batch 300/539] [D loss: 0.469324] [G loss: 0.151593] [ema: 0.999375] 
[Epoch 20/93] [Batch 400/539] [D loss: 0.470558] [G loss: 0.133224] [ema: 0.999380] 
[Epoch 20/93] [Batch 500/539] [D loss: 0.470583] [G loss: 0.180781] [ema: 0.999386] 
[Epoch 21/93] [Batch 0/539] [D loss: 0.460248] [G loss: 0.171801] [ema: 0.999388] 
[Epoch 21/93] [Batch 100/539] [D loss: 0.461721] [G loss: 0.146455] [ema: 0.999393] 
[Epoch 21/93] [Batch 200/539] [D loss: 0.452415] [G loss: 0.133178] [ema: 0.999398] 
[Epoch 21/93] [Batch 300/539] [D loss: 0.514897] [G loss: 0.134717] [ema: 0.999404] 
[Epoch 21/93] [Batch 400/539] [D loss: 0.510335] [G loss: 0.137147] [ema: 0.999409] 
[Epoch 21/93] [Batch 500/539] [D loss: 0.455246] [G loss: 0.182933] [ema: 0.999414] 
[Epoch 22/93] [Batch 0/539] [D loss: 0.468243] [G loss: 0.184424] [ema: 0.999416] 
[Epoch 22/93] [Batch 100/539] [D loss: 0.477860] [G loss: 0.168589] [ema: 0.999421] 
[Epoch 22/93] [Batch 200/539] [D loss: 0.550826] [G loss: 0.136936] [ema: 0.999425] 
[Epoch 22/93] [Batch 300/539] [D loss: 0.412347] [G loss: 0.135561] [ema: 0.999430] 
[Epoch 22/93] [Batch 400/539] [D loss: 0.425649] [G loss: 0.161421] [ema: 0.999435] 
[Epoch 22/93] [Batch 500/539] [D loss: 0.518181] [G loss: 0.155291] [ema: 0.999439] 
[Epoch 23/93] [Batch 0/539] [D loss: 0.495557] [G loss: 0.153629] [ema: 0.999441] 
[Epoch 23/93] [Batch 100/539] [D loss: 0.445792] [G loss: 0.146557] [ema: 0.999446] 
[Epoch 23/93] [Batch 200/539] [D loss: 0.459727] [G loss: 0.155875] [ema: 0.999450] 
[Epoch 23/93] [Batch 300/539] [D loss: 0.451162] [G loss: 0.171150] [ema: 0.999454] 
[Epoch 23/93] [Batch 400/539] [D loss: 0.464079] [G loss: 0.148592] [ema: 0.999458] 
[Epoch 23/93] [Batch 500/539] [D loss: 0.512603] [G loss: 0.136733] [ema: 0.999463] 



Saving checkpoint 2 in logs/daghar_50000_60_100/sit_50000_D_60_2024_10_23_19_34_15/Model



[Epoch 24/93] [Batch 0/539] [D loss: 0.482027] [G loss: 0.178274] [ema: 0.999464] 
[Epoch 24/93] [Batch 100/539] [D loss: 0.463568] [G loss: 0.134782] [ema: 0.999468] 
[Epoch 24/93] [Batch 200/539] [D loss: 0.468913] [G loss: 0.165704] [ema: 0.999472] 
[Epoch 24/93] [Batch 300/539] [D loss: 0.487547] [G loss: 0.161187] [ema: 0.999476] 
[Epoch 24/93] [Batch 400/539] [D loss: 0.434251] [G loss: 0.120275] [ema: 0.999480] 
[Epoch 24/93] [Batch 500/539] [D loss: 0.476129] [G loss: 0.127509] [ema: 0.999484] 
[Epoch 25/93] [Batch 0/539] [D loss: 0.496005] [G loss: 0.152282] [ema: 0.999486] 
[Epoch 25/93] [Batch 100/539] [D loss: 0.478962] [G loss: 0.137473] [ema: 0.999490] 
[Epoch 25/93] [Batch 200/539] [D loss: 0.550896] [G loss: 0.145917] [ema: 0.999493] 
[Epoch 25/93] [Batch 300/539] [D loss: 0.461017] [G loss: 0.162440] [ema: 0.999497] 
[Epoch 25/93] [Batch 400/539] [D loss: 0.472660] [G loss: 0.138653] [ema: 0.999501] 
[Epoch 25/93] [Batch 500/539] [D loss: 0.519192] [G loss: 0.138867] [ema: 0.999504] 
[Epoch 26/93] [Batch 0/539] [D loss: 0.423493] [G loss: 0.186172] [ema: 0.999506] 
[Epoch 26/93] [Batch 100/539] [D loss: 0.454917] [G loss: 0.152415] [ema: 0.999509] 
[Epoch 26/93] [Batch 200/539] [D loss: 0.444796] [G loss: 0.141093] [ema: 0.999512] 
[Epoch 26/93] [Batch 300/539] [D loss: 0.444246] [G loss: 0.152223] [ema: 0.999516] 
[Epoch 26/93] [Batch 400/539] [D loss: 0.488996] [G loss: 0.152075] [ema: 0.999519] 
[Epoch 26/93] [Batch 500/539] [D loss: 0.448025] [G loss: 0.159066] [ema: 0.999523] 
[Epoch 27/93] [Batch 0/539] [D loss: 0.511554] [G loss: 0.170003] [ema: 0.999524] 
[Epoch 27/93] [Batch 100/539] [D loss: 0.444020] [G loss: 0.141888] [ema: 0.999527] 
[Epoch 27/93] [Batch 200/539] [D loss: 0.560737] [G loss: 0.139318] [ema: 0.999530] 
[Epoch 27/93] [Batch 300/539] [D loss: 0.464807] [G loss: 0.148975] [ema: 0.999533] 
[Epoch 27/93] [Batch 400/539] [D loss: 0.437971] [G loss: 0.131933] [ema: 0.999537] 
[Epoch 27/93] [Batch 500/539] [D loss: 0.452658] [G loss: 0.145517] [ema: 0.999540] 
[Epoch 28/93] [Batch 0/539] [D loss: 0.467903] [G loss: 0.138948] [ema: 0.999541] 
[Epoch 28/93] [Batch 100/539] [D loss: 0.501109] [G loss: 0.133766] [ema: 0.999544] 
[Epoch 28/93] [Batch 200/539] [D loss: 0.514624] [G loss: 0.147891] [ema: 0.999547] 
[Epoch 28/93] [Batch 300/539] [D loss: 0.492368] [G loss: 0.158092] [ema: 0.999550] 
[Epoch 28/93] [Batch 400/539] [D loss: 0.459047] [G loss: 0.150165] [ema: 0.999553] 
[Epoch 28/93] [Batch 500/539] [D loss: 0.463186] [G loss: 0.163233] [ema: 0.999556] 
[Epoch 29/93] [Batch 0/539] [D loss: 0.438370] [G loss: 0.140027] [ema: 0.999557] 
[Epoch 29/93] [Batch 100/539] [D loss: 0.449658] [G loss: 0.143188] [ema: 0.999559] 
[Epoch 29/93] [Batch 200/539] [D loss: 0.437510] [G loss: 0.171405] [ema: 0.999562] 
[Epoch 29/93] [Batch 300/539] [D loss: 0.454594] [G loss: 0.153657] [ema: 0.999565] 
[Epoch 29/93] [Batch 400/539] [D loss: 0.437103] [G loss: 0.166130] [ema: 0.999568] 
[Epoch 29/93] [Batch 500/539] [D loss: 0.441694] [G loss: 0.163576] [ema: 0.999570] 
[Epoch 30/93] [Batch 0/539] [D loss: 0.484150] [G loss: 0.132260] [ema: 0.999571] 
[Epoch 30/93] [Batch 100/539] [D loss: 0.498331] [G loss: 0.141725] [ema: 0.999574] 
[Epoch 30/93] [Batch 200/539] [D loss: 0.487996] [G loss: 0.162592] [ema: 0.999577] 
[Epoch 30/93] [Batch 300/539] [D loss: 0.469940] [G loss: 0.168769] [ema: 0.999579] 
[Epoch 30/93] [Batch 400/539] [D loss: 0.470960] [G loss: 0.131904] [ema: 0.999582] 
[Epoch 30/93] [Batch 500/539] [D loss: 0.473387] [G loss: 0.135273] [ema: 0.999584] 
[Epoch 31/93] [Batch 0/539] [D loss: 0.471723] [G loss: 0.153633] [ema: 0.999585] 
[Epoch 31/93] [Batch 100/539] [D loss: 0.438950] [G loss: 0.151248] [ema: 0.999588] 
[Epoch 31/93] [Batch 200/539] [D loss: 0.466837] [G loss: 0.138898] [ema: 0.999590] 
[Epoch 31/93] [Batch 300/539] [D loss: 0.543752] [G loss: 0.125401] [ema: 0.999593] 
[Epoch 31/93] [Batch 400/539] [D loss: 0.494553] [G loss: 0.140137] [ema: 0.999595] 
[Epoch 31/93] [Batch 500/539] [D loss: 0.551578] [G loss: 0.154880] [ema: 0.999597] 
[Epoch 32/93] [Batch 0/539] [D loss: 0.498645] [G loss: 0.136796] [ema: 0.999598] 
[Epoch 32/93] [Batch 100/539] [D loss: 0.506052] [G loss: 0.118011] [ema: 0.999601] 
[Epoch 32/93] [Batch 200/539] [D loss: 0.498966] [G loss: 0.136377] [ema: 0.999603] 
[Epoch 32/93] [Batch 300/539] [D loss: 0.485234] [G loss: 0.150567] [ema: 0.999605] 
[Epoch 32/93] [Batch 400/539] [D loss: 0.471177] [G loss: 0.143213] [ema: 0.999607] 
[Epoch 32/93] [Batch 500/539] [D loss: 0.529146] [G loss: 0.127475] [ema: 0.999610] 
[Epoch 33/93] [Batch 0/539] [D loss: 0.484026] [G loss: 0.140718] [ema: 0.999610] 
[Epoch 33/93] [Batch 100/539] [D loss: 0.501111] [G loss: 0.147768] [ema: 0.999613] 
[Epoch 33/93] [Batch 200/539] [D loss: 0.443876] [G loss: 0.165375] [ema: 0.999615] 
[Epoch 33/93] [Batch 300/539] [D loss: 0.497147] [G loss: 0.134788] [ema: 0.999617] 
[Epoch 33/93] [Batch 400/539] [D loss: 0.486511] [G loss: 0.139714] [ema: 0.999619] 
[Epoch 33/93] [Batch 500/539] [D loss: 0.459056] [G loss: 0.146925] [ema: 0.999621] 
[Epoch 34/93] [Batch 0/539] [D loss: 0.542489] [G loss: 0.138605] [ema: 0.999622] 
[Epoch 34/93] [Batch 100/539] [D loss: 0.498637] [G loss: 0.140392] [ema: 0.999624] 
[Epoch 34/93] [Batch 200/539] [D loss: 0.520889] [G loss: 0.156866] [ema: 0.999626] 
[Epoch 34/93] [Batch 300/539] [D loss: 0.514743] [G loss: 0.132979] [ema: 0.999628] 
[Epoch 34/93] [Batch 400/539] [D loss: 0.491821] [G loss: 0.134792] [ema: 0.999630] 
[Epoch 34/93] [Batch 500/539] [D loss: 0.489078] [G loss: 0.128765] [ema: 0.999632] 
[Epoch 35/93] [Batch 0/539] [D loss: 0.594300] [G loss: 0.142133] [ema: 0.999633] 
[Epoch 35/93] [Batch 100/539] [D loss: 0.468434] [G loss: 0.155013] [ema: 0.999635] 
[Epoch 35/93] [Batch 200/539] [D loss: 0.523558] [G loss: 0.139531] [ema: 0.999636] 
[Epoch 35/93] [Batch 300/539] [D loss: 0.488937] [G loss: 0.154233] [ema: 0.999638] 
[Epoch 35/93] [Batch 400/539] [D loss: 0.481731] [G loss: 0.115936] [ema: 0.999640] 
[Epoch 35/93] [Batch 500/539] [D loss: 0.561363] [G loss: 0.132269] [ema: 0.999642] 
[Epoch 36/93] [Batch 0/539] [D loss: 0.528334] [G loss: 0.136418] [ema: 0.999643] 
[Epoch 36/93] [Batch 100/539] [D loss: 0.475798] [G loss: 0.147294] [ema: 0.999645] 
[Epoch 36/93] [Batch 200/539] [D loss: 0.470127] [G loss: 0.135314] [ema: 0.999646] 
[Epoch 36/93] [Batch 300/539] [D loss: 0.476281] [G loss: 0.151117] [ema: 0.999648] 
[Epoch 36/93] [Batch 400/539] [D loss: 0.508224] [G loss: 0.132843] [ema: 0.999650] 
[Epoch 36/93] [Batch 500/539] [D loss: 0.466625] [G loss: 0.129321] [ema: 0.999652] 
[Epoch 37/93] [Batch 0/539] [D loss: 0.569269] [G loss: 0.126320] [ema: 0.999652] 
[Epoch 37/93] [Batch 100/539] [D loss: 0.471026] [G loss: 0.145385] [ema: 0.999654] 
[Epoch 37/93] [Batch 200/539] [D loss: 0.490162] [G loss: 0.134426] [ema: 0.999656] 
[Epoch 37/93] [Batch 300/539] [D loss: 0.493487] [G loss: 0.140917] [ema: 0.999658] 
[Epoch 37/93] [Batch 400/539] [D loss: 0.540252] [G loss: 0.151301] [ema: 0.999659] 
[Epoch 37/93] [Batch 500/539] [D loss: 0.509963] [G loss: 0.132071] [ema: 0.999661] 
[Epoch 38/93] [Batch 0/539] [D loss: 0.485288] [G loss: 0.140360] [ema: 0.999662] 
[Epoch 38/93] [Batch 100/539] [D loss: 0.515516] [G loss: 0.150223] [ema: 0.999663] 
[Epoch 38/93] [Batch 200/539] [D loss: 0.526930] [G loss: 0.130750] [ema: 0.999665] 
[Epoch 38/93] [Batch 300/539] [D loss: 0.472837] [G loss: 0.128487] [ema: 0.999667] 
[Epoch 38/93] [Batch 400/539] [D loss: 0.490549] [G loss: 0.134234] [ema: 0.999668] 
[Epoch 38/93] [Batch 500/539] [D loss: 0.530442] [G loss: 0.128709] [ema: 0.999670] 
[Epoch 39/93] [Batch 0/539] [D loss: 0.524773] [G loss: 0.147931] [ema: 0.999670] 
[Epoch 39/93] [Batch 100/539] [D loss: 0.523137] [G loss: 0.104277] [ema: 0.999672] 
[Epoch 39/93] [Batch 200/539] [D loss: 0.492534] [G loss: 0.124179] [ema: 0.999673] 
[Epoch 39/93] [Batch 300/539] [D loss: 0.494853] [G loss: 0.151294] [ema: 0.999675] 
[Epoch 39/93] [Batch 400/539] [D loss: 0.518322] [G loss: 0.139353] [ema: 0.999676] 
[Epoch 39/93] [Batch 500/539] [D loss: 0.528999] [G loss: 0.134852] [ema: 0.999678] 
[Epoch 40/93] [Batch 0/539] [D loss: 0.488692] [G loss: 0.143400] [ema: 0.999679] 
[Epoch 40/93] [Batch 100/539] [D loss: 0.521597] [G loss: 0.122209] [ema: 0.999680] 
[Epoch 40/93] [Batch 200/539] [D loss: 0.497063] [G loss: 0.125283] [ema: 0.999682] 
[Epoch 40/93] [Batch 300/539] [D loss: 0.491023] [G loss: 0.130892] [ema: 0.999683] 
[Epoch 40/93] [Batch 400/539] [D loss: 0.518067] [G loss: 0.121577] [ema: 0.999684] 
[Epoch 40/93] [Batch 500/539] [D loss: 0.529929] [G loss: 0.140819] [ema: 0.999686] 
[Epoch 41/93] [Batch 0/539] [D loss: 0.525905] [G loss: 0.123574] [ema: 0.999686] 
[Epoch 41/93] [Batch 100/539] [D loss: 0.493339] [G loss: 0.114865] [ema: 0.999688] 
[Epoch 41/93] [Batch 200/539] [D loss: 0.504625] [G loss: 0.139856] [ema: 0.999689] 
[Epoch 41/93] [Batch 300/539] [D loss: 0.520609] [G loss: 0.126954] [ema: 0.999691] 
[Epoch 41/93] [Batch 400/539] [D loss: 0.501385] [G loss: 0.142469] [ema: 0.999692] 
[Epoch 41/93] [Batch 500/539] [D loss: 0.522558] [G loss: 0.114567] [ema: 0.999693] 
[Epoch 42/93] [Batch 0/539] [D loss: 0.486122] [G loss: 0.152224] [ema: 0.999694] 
[Epoch 42/93] [Batch 100/539] [D loss: 0.529123] [G loss: 0.138059] [ema: 0.999695] 
[Epoch 42/93] [Batch 200/539] [D loss: 0.497620] [G loss: 0.145775] [ema: 0.999697] 
[Epoch 42/93] [Batch 300/539] [D loss: 0.480605] [G loss: 0.133265] [ema: 0.999698] 
[Epoch 42/93] [Batch 400/539] [D loss: 0.479839] [G loss: 0.127700] [ema: 0.999699] 
[Epoch 42/93] [Batch 500/539] [D loss: 0.515982] [G loss: 0.128551] [ema: 0.999700] 
[Epoch 43/93] [Batch 0/539] [D loss: 0.493241] [G loss: 0.123251] [ema: 0.999701] 
[Epoch 43/93] [Batch 100/539] [D loss: 0.508875] [G loss: 0.124736] [ema: 0.999702] 
[Epoch 43/93] [Batch 200/539] [D loss: 0.532330] [G loss: 0.147118] [ema: 0.999704] 
[Epoch 43/93] [Batch 300/539] [D loss: 0.537683] [G loss: 0.122483] [ema: 0.999705] 
[Epoch 43/93] [Batch 400/539] [D loss: 0.478916] [G loss: 0.135618] [ema: 0.999706] 
[Epoch 43/93] [Batch 500/539] [D loss: 0.520909] [G loss: 0.126264] [ema: 0.999707] 
[Epoch 44/93] [Batch 0/539] [D loss: 0.556236] [G loss: 0.131868] [ema: 0.999708] 
[Epoch 44/93] [Batch 100/539] [D loss: 0.526671] [G loss: 0.130923] [ema: 0.999709] 
[Epoch 44/93] [Batch 200/539] [D loss: 0.561172] [G loss: 0.121006] [ema: 0.999710] 
[Epoch 44/93] [Batch 300/539] [D loss: 0.538303] [G loss: 0.114165] [ema: 0.999711] 
[Epoch 44/93] [Batch 400/539] [D loss: 0.550674] [G loss: 0.119217] [ema: 0.999713] 
[Epoch 44/93] [Batch 500/539] [D loss: 0.526426] [G loss: 0.135467] [ema: 0.999714] 
[Epoch 45/93] [Batch 0/539] [D loss: 0.586077] [G loss: 0.137012] [ema: 0.999714] 
[Epoch 45/93] [Batch 100/539] [D loss: 0.521742] [G loss: 0.115918] [ema: 0.999715] 
[Epoch 45/93] [Batch 200/539] [D loss: 0.519433] [G loss: 0.110510] [ema: 0.999717] 
[Epoch 45/93] [Batch 300/539] [D loss: 0.533217] [G loss: 0.134872] [ema: 0.999718] 
[Epoch 45/93] [Batch 400/539] [D loss: 0.515051] [G loss: 0.134341] [ema: 0.999719] 
[Epoch 45/93] [Batch 500/539] [D loss: 0.483774] [G loss: 0.120778] [ema: 0.999720] 
[Epoch 46/93] [Batch 0/539] [D loss: 0.513074] [G loss: 0.137837] [ema: 0.999720] 
[Epoch 46/93] [Batch 100/539] [D loss: 0.532250] [G loss: 0.139341] [ema: 0.999722] 
[Epoch 46/93] [Batch 200/539] [D loss: 0.500060] [G loss: 0.140768] [ema: 0.999723] 
[Epoch 46/93] [Batch 300/539] [D loss: 0.567715] [G loss: 0.127652] [ema: 0.999724] 
[Epoch 46/93] [Batch 400/539] [D loss: 0.456770] [G loss: 0.136165] [ema: 0.999725] 
[Epoch 46/93] [Batch 500/539] [D loss: 0.491132] [G loss: 0.141782] [ema: 0.999726] 
[Epoch 47/93] [Batch 0/539] [D loss: 0.497538] [G loss: 0.145773] [ema: 0.999726] 
[Epoch 47/93] [Batch 100/539] [D loss: 0.530129] [G loss: 0.133112] [ema: 0.999727] 
[Epoch 47/93] [Batch 200/539] [D loss: 0.515416] [G loss: 0.129313] [ema: 0.999729] 
[Epoch 47/93] [Batch 300/539] [D loss: 0.536243] [G loss: 0.130927] [ema: 0.999730] 
[Epoch 47/93] [Batch 400/539] [D loss: 0.542218] [G loss: 0.117692] [ema: 0.999731] 
[Epoch 47/93] [Batch 500/539] [D loss: 0.497354] [G loss: 0.141618] [ema: 0.999732] 



Saving checkpoint 3 in logs/daghar_50000_60_100/sit_50000_D_60_2024_10_23_19_34_15/Model



[Epoch 48/93] [Batch 0/539] [D loss: 0.512222] [G loss: 0.144022] [ema: 0.999732] 
[Epoch 48/93] [Batch 100/539] [D loss: 0.535745] [G loss: 0.131245] [ema: 0.999733] 
[Epoch 48/93] [Batch 200/539] [D loss: 0.496931] [G loss: 0.119979] [ema: 0.999734] 
[Epoch 48/93] [Batch 300/539] [D loss: 0.540167] [G loss: 0.106768] [ema: 0.999735] 
[Epoch 48/93] [Batch 400/539] [D loss: 0.565396] [G loss: 0.122033] [ema: 0.999736] 
[Epoch 48/93] [Batch 500/539] [D loss: 0.550460] [G loss: 0.106729] [ema: 0.999737] 
[Epoch 49/93] [Batch 0/539] [D loss: 0.524955] [G loss: 0.121967] [ema: 0.999738] 
[Epoch 49/93] [Batch 100/539] [D loss: 0.555680] [G loss: 0.123053] [ema: 0.999739] 
[Epoch 49/93] [Batch 200/539] [D loss: 0.531540] [G loss: 0.114115] [ema: 0.999740] 
[Epoch 49/93] [Batch 300/539] [D loss: 0.522799] [G loss: 0.123955] [ema: 0.999741] 
[Epoch 49/93] [Batch 400/539] [D loss: 0.496245] [G loss: 0.129698] [ema: 0.999742] 
[Epoch 49/93] [Batch 500/539] [D loss: 0.533192] [G loss: 0.110376] [ema: 0.999742] 
[Epoch 50/93] [Batch 0/539] [D loss: 0.562851] [G loss: 0.128437] [ema: 0.999743] 
[Epoch 50/93] [Batch 100/539] [D loss: 0.541874] [G loss: 0.124494] [ema: 0.999744] 
[Epoch 50/93] [Batch 200/539] [D loss: 0.493856] [G loss: 0.122676] [ema: 0.999745] 
[Epoch 50/93] [Batch 300/539] [D loss: 0.483345] [G loss: 0.140576] [ema: 0.999746] 
[Epoch 50/93] [Batch 400/539] [D loss: 0.499081] [G loss: 0.123949] [ema: 0.999747] 
[Epoch 50/93] [Batch 500/539] [D loss: 0.521668] [G loss: 0.121904] [ema: 0.999748] 
[Epoch 51/93] [Batch 0/539] [D loss: 0.480750] [G loss: 0.125455] [ema: 0.999748] 
[Epoch 51/93] [Batch 100/539] [D loss: 0.494547] [G loss: 0.145279] [ema: 0.999749] 
[Epoch 51/93] [Batch 200/539] [D loss: 0.551419] [G loss: 0.133749] [ema: 0.999750] 
[Epoch 51/93] [Batch 300/539] [D loss: 0.543083] [G loss: 0.123325] [ema: 0.999751] 
[Epoch 51/93] [Batch 400/539] [D loss: 0.485672] [G loss: 0.104079] [ema: 0.999751] 
[Epoch 51/93] [Batch 500/539] [D loss: 0.532590] [G loss: 0.143715] [ema: 0.999752] 
[Epoch 52/93] [Batch 0/539] [D loss: 0.529624] [G loss: 0.134781] [ema: 0.999753] 
[Epoch 52/93] [Batch 100/539] [D loss: 0.539833] [G loss: 0.141052] [ema: 0.999754] 
[Epoch 52/93] [Batch 200/539] [D loss: 0.505461] [G loss: 0.132534] [ema: 0.999754] 
[Epoch 52/93] [Batch 300/539] [D loss: 0.542781] [G loss: 0.137637] [ema: 0.999755] 
[Epoch 52/93] [Batch 400/539] [D loss: 0.502215] [G loss: 0.122796] [ema: 0.999756] 
[Epoch 52/93] [Batch 500/539] [D loss: 0.504382] [G loss: 0.130468] [ema: 0.999757] 
[Epoch 53/93] [Batch 0/539] [D loss: 0.528777] [G loss: 0.129215] [ema: 0.999757] 
[Epoch 53/93] [Batch 100/539] [D loss: 0.549915] [G loss: 0.130562] [ema: 0.999758] 
[Epoch 53/93] [Batch 200/539] [D loss: 0.520777] [G loss: 0.111541] [ema: 0.999759] 
[Epoch 53/93] [Batch 300/539] [D loss: 0.532671] [G loss: 0.119852] [ema: 0.999760] 
[Epoch 53/93] [Batch 400/539] [D loss: 0.543336] [G loss: 0.139788] [ema: 0.999761] 
[Epoch 53/93] [Batch 500/539] [D loss: 0.491451] [G loss: 0.131429] [ema: 0.999762] 
[Epoch 54/93] [Batch 0/539] [D loss: 0.525724] [G loss: 0.140487] [ema: 0.999762] 
[Epoch 54/93] [Batch 100/539] [D loss: 0.516010] [G loss: 0.122407] [ema: 0.999763] 
[Epoch 54/93] [Batch 200/539] [D loss: 0.515594] [G loss: 0.137009] [ema: 0.999764] 
[Epoch 54/93] [Batch 300/539] [D loss: 0.538818] [G loss: 0.138014] [ema: 0.999764] 
[Epoch 54/93] [Batch 400/539] [D loss: 0.573979] [G loss: 0.128256] [ema: 0.999765] 
[Epoch 54/93] [Batch 500/539] [D loss: 0.513017] [G loss: 0.143893] [ema: 0.999766] 
[Epoch 55/93] [Batch 0/539] [D loss: 0.574603] [G loss: 0.142014] [ema: 0.999766] 
[Epoch 55/93] [Batch 100/539] [D loss: 0.506366] [G loss: 0.118000] [ema: 0.999767] 
[Epoch 55/93] [Batch 200/539] [D loss: 0.493554] [G loss: 0.150701] [ema: 0.999768] 
[Epoch 55/93] [Batch 300/539] [D loss: 0.474652] [G loss: 0.125842] [ema: 0.999769] 
[Epoch 55/93] [Batch 400/539] [D loss: 0.504301] [G loss: 0.123721] [ema: 0.999769] 
[Epoch 55/93] [Batch 500/539] [D loss: 0.471169] [G loss: 0.126642] [ema: 0.999770] 
[Epoch 56/93] [Batch 0/539] [D loss: 0.511716] [G loss: 0.131839] [ema: 0.999770] 
[Epoch 56/93] [Batch 100/539] [D loss: 0.504489] [G loss: 0.110605] [ema: 0.999771] 
[Epoch 56/93] [Batch 200/539] [D loss: 0.514884] [G loss: 0.107136] [ema: 0.999772] 
[Epoch 56/93] [Batch 300/539] [D loss: 0.527908] [G loss: 0.113927] [ema: 0.999773] 
[Epoch 56/93] [Batch 400/539] [D loss: 0.535931] [G loss: 0.133419] [ema: 0.999773] 
[Epoch 56/93] [Batch 500/539] [D loss: 0.505858] [G loss: 0.117571] [ema: 0.999774] 
[Epoch 57/93] [Batch 0/539] [D loss: 0.510089] [G loss: 0.136512] [ema: 0.999774] 
[Epoch 57/93] [Batch 100/539] [D loss: 0.519120] [G loss: 0.125607] [ema: 0.999775] 
[Epoch 57/93] [Batch 200/539] [D loss: 0.462235] [G loss: 0.133593] [ema: 0.999776] 
[Epoch 57/93] [Batch 300/539] [D loss: 0.520138] [G loss: 0.120168] [ema: 0.999777] 
[Epoch 57/93] [Batch 400/539] [D loss: 0.513271] [G loss: 0.129244] [ema: 0.999777] 
[Epoch 57/93] [Batch 500/539] [D loss: 0.555564] [G loss: 0.113007] [ema: 0.999778] 
[Epoch 58/93] [Batch 0/539] [D loss: 0.536336] [G loss: 0.117412] [ema: 0.999778] 
[Epoch 58/93] [Batch 100/539] [D loss: 0.512353] [G loss: 0.130777] [ema: 0.999779] 
[Epoch 58/93] [Batch 200/539] [D loss: 0.539240] [G loss: 0.111789] [ema: 0.999780] 
[Epoch 58/93] [Batch 300/539] [D loss: 0.554409] [G loss: 0.120228] [ema: 0.999780] 
[Epoch 58/93] [Batch 400/539] [D loss: 0.537130] [G loss: 0.103389] [ema: 0.999781] 
[Epoch 58/93] [Batch 500/539] [D loss: 0.565800] [G loss: 0.107607] [ema: 0.999782] 
[Epoch 59/93] [Batch 0/539] [D loss: 0.549014] [G loss: 0.113405] [ema: 0.999782] 
[Epoch 59/93] [Batch 100/539] [D loss: 0.534247] [G loss: 0.117845] [ema: 0.999783] 
[Epoch 59/93] [Batch 200/539] [D loss: 0.509977] [G loss: 0.111237] [ema: 0.999783] 
[Epoch 59/93] [Batch 300/539] [D loss: 0.518839] [G loss: 0.144957] [ema: 0.999784] 
[Epoch 59/93] [Batch 400/539] [D loss: 0.562569] [G loss: 0.110850] [ema: 0.999785] 
[Epoch 59/93] [Batch 500/539] [D loss: 0.537538] [G loss: 0.113219] [ema: 0.999785] 
[Epoch 60/93] [Batch 0/539] [D loss: 0.509028] [G loss: 0.140816] [ema: 0.999786] 
[Epoch 60/93] [Batch 100/539] [D loss: 0.532084] [G loss: 0.115359] [ema: 0.999786] 
[Epoch 60/93] [Batch 200/539] [D loss: 0.527001] [G loss: 0.122287] [ema: 0.999787] 
[Epoch 60/93] [Batch 300/539] [D loss: 0.515427] [G loss: 0.113616] [ema: 0.999788] 
[Epoch 60/93] [Batch 400/539] [D loss: 0.549345] [G loss: 0.118653] [ema: 0.999788] 
[Epoch 60/93] [Batch 500/539] [D loss: 0.536896] [G loss: 0.131383] [ema: 0.999789] 
[Epoch 61/93] [Batch 0/539] [D loss: 0.552705] [G loss: 0.119733] [ema: 0.999789] 
[Epoch 61/93] [Batch 100/539] [D loss: 0.521453] [G loss: 0.117110] [ema: 0.999790] 
[Epoch 61/93] [Batch 200/539] [D loss: 0.538103] [G loss: 0.119165] [ema: 0.999790] 
[Epoch 61/93] [Batch 300/539] [D loss: 0.546086] [G loss: 0.115106] [ema: 0.999791] 
[Epoch 61/93] [Batch 400/539] [D loss: 0.535669] [G loss: 0.126007] [ema: 0.999792] 
[Epoch 61/93] [Batch 500/539] [D loss: 0.520285] [G loss: 0.130442] [ema: 0.999792] 
[Epoch 62/93] [Batch 0/539] [D loss: 0.538396] [G loss: 0.115564] [ema: 0.999793] 
[Epoch 62/93] [Batch 100/539] [D loss: 0.528696] [G loss: 0.110221] [ema: 0.999793] 
[Epoch 62/93] [Batch 200/539] [D loss: 0.549789] [G loss: 0.119630] [ema: 0.999794] 
[Epoch 62/93] [Batch 300/539] [D loss: 0.518820] [G loss: 0.121483] [ema: 0.999794] 
[Epoch 62/93] [Batch 400/539] [D loss: 0.549301] [G loss: 0.113764] [ema: 0.999795] 
[Epoch 62/93] [Batch 500/539] [D loss: 0.519787] [G loss: 0.109227] [ema: 0.999796] 
[Epoch 63/93] [Batch 0/539] [D loss: 0.553493] [G loss: 0.138954] [ema: 0.999796] 
[Epoch 63/93] [Batch 100/539] [D loss: 0.529135] [G loss: 0.118002] [ema: 0.999796] 
[Epoch 63/93] [Batch 200/539] [D loss: 0.549377] [G loss: 0.113532] [ema: 0.999797] 
[Epoch 63/93] [Batch 300/539] [D loss: 0.513122] [G loss: 0.123763] [ema: 0.999798] 
[Epoch 63/93] [Batch 400/539] [D loss: 0.532787] [G loss: 0.117585] [ema: 0.999798] 
[Epoch 63/93] [Batch 500/539] [D loss: 0.543273] [G loss: 0.121924] [ema: 0.999799] 
[Epoch 64/93] [Batch 0/539] [D loss: 0.518638] [G loss: 0.128697] [ema: 0.999799] 
[Epoch 64/93] [Batch 100/539] [D loss: 0.565411] [G loss: 0.101547] [ema: 0.999800] 
[Epoch 64/93] [Batch 200/539] [D loss: 0.522949] [G loss: 0.117343] [ema: 0.999800] 
[Epoch 64/93] [Batch 300/539] [D loss: 0.558779] [G loss: 0.131484] [ema: 0.999801] 
[Epoch 64/93] [Batch 400/539] [D loss: 0.548103] [G loss: 0.128342] [ema: 0.999801] 
[Epoch 64/93] [Batch 500/539] [D loss: 0.558992] [G loss: 0.121853] [ema: 0.999802] 
[Epoch 65/93] [Batch 0/539] [D loss: 0.528087] [G loss: 0.102245] [ema: 0.999802] 
[Epoch 65/93] [Batch 100/539] [D loss: 0.558168] [G loss: 0.119346] [ema: 0.999803] 
[Epoch 65/93] [Batch 200/539] [D loss: 0.536274] [G loss: 0.109819] [ema: 0.999803] 
[Epoch 65/93] [Batch 300/539] [D loss: 0.514034] [G loss: 0.128444] [ema: 0.999804] 
[Epoch 65/93] [Batch 400/539] [D loss: 0.532492] [G loss: 0.120046] [ema: 0.999804] 
[Epoch 65/93] [Batch 500/539] [D loss: 0.561018] [G loss: 0.114728] [ema: 0.999805] 
[Epoch 66/93] [Batch 0/539] [D loss: 0.536566] [G loss: 0.119930] [ema: 0.999805] 
[Epoch 66/93] [Batch 100/539] [D loss: 0.559175] [G loss: 0.131462] [ema: 0.999806] 
[Epoch 66/93] [Batch 200/539] [D loss: 0.546364] [G loss: 0.125742] [ema: 0.999806] 
[Epoch 66/93] [Batch 300/539] [D loss: 0.565710] [G loss: 0.116736] [ema: 0.999807] 
[Epoch 66/93] [Batch 400/539] [D loss: 0.542168] [G loss: 0.119014] [ema: 0.999807] 
[Epoch 66/93] [Batch 500/539] [D loss: 0.535200] [G loss: 0.123020] [ema: 0.999808] 
[Epoch 67/93] [Batch 0/539] [D loss: 0.516398] [G loss: 0.126951] [ema: 0.999808] 
[Epoch 67/93] [Batch 100/539] [D loss: 0.541171] [G loss: 0.114436] [ema: 0.999809] 
[Epoch 67/93] [Batch 200/539] [D loss: 0.534674] [G loss: 0.119612] [ema: 0.999809] 
[Epoch 67/93] [Batch 300/539] [D loss: 0.529729] [G loss: 0.103196] [ema: 0.999810] 
[Epoch 67/93] [Batch 400/539] [D loss: 0.546870] [G loss: 0.117757] [ema: 0.999810] 
[Epoch 67/93] [Batch 500/539] [D loss: 0.552918] [G loss: 0.113656] [ema: 0.999811] 
[Epoch 68/93] [Batch 0/539] [D loss: 0.555174] [G loss: 0.131849] [ema: 0.999811] 
[Epoch 68/93] [Batch 100/539] [D loss: 0.546516] [G loss: 0.112579] [ema: 0.999811] 
[Epoch 68/93] [Batch 200/539] [D loss: 0.563582] [G loss: 0.114272] [ema: 0.999812] 
[Epoch 68/93] [Batch 300/539] [D loss: 0.528313] [G loss: 0.124115] [ema: 0.999812] 
[Epoch 68/93] [Batch 400/539] [D loss: 0.558174] [G loss: 0.121979] [ema: 0.999813] 
[Epoch 68/93] [Batch 500/539] [D loss: 0.527294] [G loss: 0.124265] [ema: 0.999813] 
[Epoch 69/93] [Batch 0/539] [D loss: 0.542315] [G loss: 0.122781] [ema: 0.999814] 
[Epoch 69/93] [Batch 100/539] [D loss: 0.514133] [G loss: 0.118739] [ema: 0.999814] 
[Epoch 69/93] [Batch 200/539] [D loss: 0.590126] [G loss: 0.109521] [ema: 0.999815] 
[Epoch 69/93] [Batch 300/539] [D loss: 0.543921] [G loss: 0.124064] [ema: 0.999815] 
[Epoch 69/93] [Batch 400/539] [D loss: 0.533081] [G loss: 0.129520] [ema: 0.999816] 
[Epoch 69/93] [Batch 500/539] [D loss: 0.531405] [G loss: 0.123492] [ema: 0.999816] 
[Epoch 70/93] [Batch 0/539] [D loss: 0.510573] [G loss: 0.129789] [ema: 0.999816] 
[Epoch 70/93] [Batch 100/539] [D loss: 0.530612] [G loss: 0.126656] [ema: 0.999817] 
[Epoch 70/93] [Batch 200/539] [D loss: 0.490241] [G loss: 0.130285] [ema: 0.999817] 
[Epoch 70/93] [Batch 300/539] [D loss: 0.510205] [G loss: 0.129264] [ema: 0.999818] 
[Epoch 70/93] [Batch 400/539] [D loss: 0.504253] [G loss: 0.117169] [ema: 0.999818] 
[Epoch 70/93] [Batch 500/539] [D loss: 0.512471] [G loss: 0.142158] [ema: 0.999819] 
[Epoch 71/93] [Batch 0/539] [D loss: 0.542068] [G loss: 0.120761] [ema: 0.999819] 
[Epoch 71/93] [Batch 100/539] [D loss: 0.544298] [G loss: 0.144344] [ema: 0.999819] 
[Epoch 71/93] [Batch 200/539] [D loss: 0.572665] [G loss: 0.115699] [ema: 0.999820] 
[Epoch 71/93] [Batch 300/539] [D loss: 0.553985] [G loss: 0.124164] [ema: 0.999820] 
[Epoch 71/93] [Batch 400/539] [D loss: 0.506035] [G loss: 0.111764] [ema: 0.999821] 
[Epoch 71/93] [Batch 500/539] [D loss: 0.563694] [G loss: 0.137486] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_50000_60_100/sit_50000_D_60_2024_10_23_19_34_15/Model



[Epoch 72/93] [Batch 0/539] [D loss: 0.527888] [G loss: 0.151013] [ema: 0.999821] 
[Epoch 72/93] [Batch 100/539] [D loss: 0.545272] [G loss: 0.122779] [ema: 0.999822] 
[Epoch 72/93] [Batch 200/539] [D loss: 0.554875] [G loss: 0.129625] [ema: 0.999822] 
[Epoch 72/93] [Batch 300/539] [D loss: 0.497340] [G loss: 0.121546] [ema: 0.999823] 
[Epoch 72/93] [Batch 400/539] [D loss: 0.513044] [G loss: 0.115003] [ema: 0.999823] 
[Epoch 72/93] [Batch 500/539] [D loss: 0.517240] [G loss: 0.131561] [ema: 0.999824] 
[Epoch 73/93] [Batch 0/539] [D loss: 0.545659] [G loss: 0.128889] [ema: 0.999824] 
[Epoch 73/93] [Batch 100/539] [D loss: 0.543485] [G loss: 0.125587] [ema: 0.999824] 
[Epoch 73/93] [Batch 200/539] [D loss: 0.545292] [G loss: 0.115616] [ema: 0.999825] 
[Epoch 73/93] [Batch 300/539] [D loss: 0.533359] [G loss: 0.113158] [ema: 0.999825] 
[Epoch 73/93] [Batch 400/539] [D loss: 0.522726] [G loss: 0.134863] [ema: 0.999826] 
[Epoch 73/93] [Batch 500/539] [D loss: 0.511519] [G loss: 0.116688] [ema: 0.999826] 
[Epoch 74/93] [Batch 0/539] [D loss: 0.509275] [G loss: 0.131229] [ema: 0.999826] 
[Epoch 74/93] [Batch 100/539] [D loss: 0.524224] [G loss: 0.103760] [ema: 0.999827] 
[Epoch 74/93] [Batch 200/539] [D loss: 0.524965] [G loss: 0.127800] [ema: 0.999827] 
[Epoch 74/93] [Batch 300/539] [D loss: 0.499750] [G loss: 0.117005] [ema: 0.999828] 
[Epoch 74/93] [Batch 400/539] [D loss: 0.575269] [G loss: 0.130375] [ema: 0.999828] 
[Epoch 74/93] [Batch 500/539] [D loss: 0.531916] [G loss: 0.123019] [ema: 0.999828] 
[Epoch 75/93] [Batch 0/539] [D loss: 0.559253] [G loss: 0.115511] [ema: 0.999829] 
[Epoch 75/93] [Batch 100/539] [D loss: 0.554368] [G loss: 0.126521] [ema: 0.999829] 
[Epoch 75/93] [Batch 200/539] [D loss: 0.537217] [G loss: 0.105929] [ema: 0.999829] 
[Epoch 75/93] [Batch 300/539] [D loss: 0.524898] [G loss: 0.113546] [ema: 0.999830] 
[Epoch 75/93] [Batch 400/539] [D loss: 0.510230] [G loss: 0.114402] [ema: 0.999830] 
[Epoch 75/93] [Batch 500/539] [D loss: 0.522903] [G loss: 0.111276] [ema: 0.999831] 
[Epoch 76/93] [Batch 0/539] [D loss: 0.548445] [G loss: 0.121563] [ema: 0.999831] 
[Epoch 76/93] [Batch 100/539] [D loss: 0.548268] [G loss: 0.114464] [ema: 0.999831] 
[Epoch 76/93] [Batch 200/539] [D loss: 0.504705] [G loss: 0.120191] [ema: 0.999832] 
[Epoch 76/93] [Batch 300/539] [D loss: 0.456704] [G loss: 0.175229] [ema: 0.999832] 
[Epoch 76/93] [Batch 400/539] [D loss: 0.535517] [G loss: 0.121693] [ema: 0.999832] 
[Epoch 76/93] [Batch 500/539] [D loss: 0.542110] [G loss: 0.122208] [ema: 0.999833] 
[Epoch 77/93] [Batch 0/539] [D loss: 0.505394] [G loss: 0.114889] [ema: 0.999833] 
[Epoch 77/93] [Batch 100/539] [D loss: 0.575827] [G loss: 0.128320] [ema: 0.999833] 
[Epoch 77/93] [Batch 200/539] [D loss: 0.519800] [G loss: 0.123329] [ema: 0.999834] 
[Epoch 77/93] [Batch 300/539] [D loss: 0.537975] [G loss: 0.120049] [ema: 0.999834] 
[Epoch 77/93] [Batch 400/539] [D loss: 0.540238] [G loss: 0.112673] [ema: 0.999835] 
[Epoch 77/93] [Batch 500/539] [D loss: 0.509956] [G loss: 0.129687] [ema: 0.999835] 
[Epoch 78/93] [Batch 0/539] [D loss: 0.511265] [G loss: 0.126005] [ema: 0.999835] 
[Epoch 78/93] [Batch 100/539] [D loss: 0.551060] [G loss: 0.130589] [ema: 0.999836] 
[Epoch 78/93] [Batch 200/539] [D loss: 0.515548] [G loss: 0.130407] [ema: 0.999836] 
[Epoch 78/93] [Batch 300/539] [D loss: 0.555760] [G loss: 0.138605] [ema: 0.999836] 
[Epoch 78/93] [Batch 400/539] [D loss: 0.502282] [G loss: 0.129743] [ema: 0.999837] 
[Epoch 78/93] [Batch 500/539] [D loss: 0.512241] [G loss: 0.122900] [ema: 0.999837] 
[Epoch 79/93] [Batch 0/539] [D loss: 0.539022] [G loss: 0.134389] [ema: 0.999837] 
[Epoch 79/93] [Batch 100/539] [D loss: 0.556009] [G loss: 0.117900] [ema: 0.999838] 
[Epoch 79/93] [Batch 200/539] [D loss: 0.529230] [G loss: 0.117278] [ema: 0.999838] 
[Epoch 79/93] [Batch 300/539] [D loss: 0.491292] [G loss: 0.135291] [ema: 0.999838] 
[Epoch 79/93] [Batch 400/539] [D loss: 0.558577] [G loss: 0.119325] [ema: 0.999839] 
[Epoch 79/93] [Batch 500/539] [D loss: 0.505369] [G loss: 0.129928] [ema: 0.999839] 
[Epoch 80/93] [Batch 0/539] [D loss: 0.589448] [G loss: 0.136925] [ema: 0.999839] 
[Epoch 80/93] [Batch 100/539] [D loss: 0.488874] [G loss: 0.134590] [ema: 0.999840] 
[Epoch 80/93] [Batch 200/539] [D loss: 0.554447] [G loss: 0.124353] [ema: 0.999840] 
[Epoch 80/93] [Batch 300/539] [D loss: 0.530111] [G loss: 0.130692] [ema: 0.999840] 
[Epoch 80/93] [Batch 400/539] [D loss: 0.549119] [G loss: 0.115849] [ema: 0.999841] 
[Epoch 80/93] [Batch 500/539] [D loss: 0.593537] [G loss: 0.126382] [ema: 0.999841] 
[Epoch 81/93] [Batch 0/539] [D loss: 0.502646] [G loss: 0.130986] [ema: 0.999841] 
[Epoch 81/93] [Batch 100/539] [D loss: 0.509041] [G loss: 0.128610] [ema: 0.999842] 
[Epoch 81/93] [Batch 200/539] [D loss: 0.508978] [G loss: 0.129978] [ema: 0.999842] 
[Epoch 81/93] [Batch 300/539] [D loss: 0.483995] [G loss: 0.137388] [ema: 0.999842] 
[Epoch 81/93] [Batch 400/539] [D loss: 0.532225] [G loss: 0.123449] [ema: 0.999843] 
[Epoch 81/93] [Batch 500/539] [D loss: 0.562416] [G loss: 0.127276] [ema: 0.999843] 
[Epoch 82/93] [Batch 0/539] [D loss: 0.480499] [G loss: 0.144124] [ema: 0.999843] 
[Epoch 82/93] [Batch 100/539] [D loss: 0.552341] [G loss: 0.123104] [ema: 0.999844] 
[Epoch 82/93] [Batch 200/539] [D loss: 0.558305] [G loss: 0.136348] [ema: 0.999844] 
[Epoch 82/93] [Batch 300/539] [D loss: 0.514297] [G loss: 0.117072] [ema: 0.999844] 
[Epoch 82/93] [Batch 400/539] [D loss: 0.541421] [G loss: 0.138516] [ema: 0.999845] 
[Epoch 82/93] [Batch 500/539] [D loss: 0.515322] [G loss: 0.130978] [ema: 0.999845] 
[Epoch 83/93] [Batch 0/539] [D loss: 0.522200] [G loss: 0.132377] [ema: 0.999845] 
[Epoch 83/93] [Batch 100/539] [D loss: 0.514561] [G loss: 0.126888] [ema: 0.999845] 
[Epoch 83/93] [Batch 200/539] [D loss: 0.501377] [G loss: 0.127543] [ema: 0.999846] 
[Epoch 83/93] [Batch 300/539] [D loss: 0.537277] [G loss: 0.126996] [ema: 0.999846] 
[Epoch 83/93] [Batch 400/539] [D loss: 0.527012] [G loss: 0.127919] [ema: 0.999846] 
[Epoch 83/93] [Batch 500/539] [D loss: 0.528101] [G loss: 0.147900] [ema: 0.999847] 
[Epoch 84/93] [Batch 0/539] [D loss: 0.534533] [G loss: 0.145917] [ema: 0.999847] 
[Epoch 84/93] [Batch 100/539] [D loss: 0.512137] [G loss: 0.121738] [ema: 0.999847] 
[Epoch 84/93] [Batch 200/539] [D loss: 0.537097] [G loss: 0.130063] [ema: 0.999848] 
[Epoch 84/93] [Batch 300/539] [D loss: 0.505539] [G loss: 0.143525] [ema: 0.999848] 
[Epoch 84/93] [Batch 400/539] [D loss: 0.493749] [G loss: 0.139012] [ema: 0.999848] 
[Epoch 84/93] [Batch 500/539] [D loss: 0.522229] [G loss: 0.135038] [ema: 0.999849] 
[Epoch 85/93] [Batch 0/539] [D loss: 0.518140] [G loss: 0.129652] [ema: 0.999849] 
[Epoch 85/93] [Batch 100/539] [D loss: 0.499645] [G loss: 0.120855] [ema: 0.999849] 
[Epoch 85/93] [Batch 200/539] [D loss: 0.526764] [G loss: 0.134256] [ema: 0.999849] 
[Epoch 85/93] [Batch 300/539] [D loss: 0.477463] [G loss: 0.110264] [ema: 0.999850] 
[Epoch 85/93] [Batch 400/539] [D loss: 0.552006] [G loss: 0.130197] [ema: 0.999850] 
[Epoch 85/93] [Batch 500/539] [D loss: 0.479243] [G loss: 0.126298] [ema: 0.999850] 
[Epoch 86/93] [Batch 0/539] [D loss: 0.521307] [G loss: 0.124132] [ema: 0.999850] 
[Epoch 86/93] [Batch 100/539] [D loss: 0.529054] [G loss: 0.126621] [ema: 0.999851] 
[Epoch 86/93] [Batch 200/539] [D loss: 0.521212] [G loss: 0.131789] [ema: 0.999851] 
[Epoch 86/93] [Batch 300/539] [D loss: 0.535723] [G loss: 0.114904] [ema: 0.999851] 
[Epoch 86/93] [Batch 400/539] [D loss: 0.516787] [G loss: 0.124722] [ema: 0.999852] 
[Epoch 86/93] [Batch 500/539] [D loss: 0.518381] [G loss: 0.124947] [ema: 0.999852] 
[Epoch 87/93] [Batch 0/539] [D loss: 0.482826] [G loss: 0.127772] [ema: 0.999852] 
[Epoch 87/93] [Batch 100/539] [D loss: 0.482834] [G loss: 0.132151] [ema: 0.999853] 
[Epoch 87/93] [Batch 200/539] [D loss: 0.519950] [G loss: 0.139990] [ema: 0.999853] 
[Epoch 87/93] [Batch 300/539] [D loss: 0.494931] [G loss: 0.126359] [ema: 0.999853] 
[Epoch 87/93] [Batch 400/539] [D loss: 0.485389] [G loss: 0.136035] [ema: 0.999853] 
[Epoch 87/93] [Batch 500/539] [D loss: 0.535086] [G loss: 0.116186] [ema: 0.999854] 
[Epoch 88/93] [Batch 0/539] [D loss: 0.498277] [G loss: 0.133608] [ema: 0.999854] 
[Epoch 88/93] [Batch 100/539] [D loss: 0.497070] [G loss: 0.152227] [ema: 0.999854] 
[Epoch 88/93] [Batch 200/539] [D loss: 0.519042] [G loss: 0.141105] [ema: 0.999854] 
[Epoch 88/93] [Batch 300/539] [D loss: 0.526691] [G loss: 0.146317] [ema: 0.999855] 
[Epoch 88/93] [Batch 400/539] [D loss: 0.505698] [G loss: 0.119978] [ema: 0.999855] 
[Epoch 88/93] [Batch 500/539] [D loss: 0.513935] [G loss: 0.133987] [ema: 0.999855] 
[Epoch 89/93] [Batch 0/539] [D loss: 0.500337] [G loss: 0.126531] [ema: 0.999856] 
[Epoch 89/93] [Batch 100/539] [D loss: 0.474776] [G loss: 0.129923] [ema: 0.999856] 
[Epoch 89/93] [Batch 200/539] [D loss: 0.504033] [G loss: 0.118574] [ema: 0.999856] 
[Epoch 89/93] [Batch 300/539] [D loss: 0.493641] [G loss: 0.108457] [ema: 0.999856] 
[Epoch 89/93] [Batch 400/539] [D loss: 0.528717] [G loss: 0.123024] [ema: 0.999857] 
[Epoch 89/93] [Batch 500/539] [D loss: 0.511173] [G loss: 0.147693] [ema: 0.999857] 
[Epoch 90/93] [Batch 0/539] [D loss: 0.553261] [G loss: 0.148471] [ema: 0.999857] 
[Epoch 90/93] [Batch 100/539] [D loss: 0.516974] [G loss: 0.119343] [ema: 0.999857] 
[Epoch 90/93] [Batch 200/539] [D loss: 0.523613] [G loss: 0.133866] [ema: 0.999858] 
[Epoch 90/93] [Batch 300/539] [D loss: 0.519099] [G loss: 0.136332] [ema: 0.999858] 
[Epoch 90/93] [Batch 400/539] [D loss: 0.493628] [G loss: 0.138798] [ema: 0.999858] 
[Epoch 90/93] [Batch 500/539] [D loss: 0.554485] [G loss: 0.134151] [ema: 0.999859] 
[Epoch 91/93] [Batch 0/539] [D loss: 0.481369] [G loss: 0.140798] [ema: 0.999859] 
[Epoch 91/93] [Batch 100/539] [D loss: 0.543082] [G loss: 0.125967] [ema: 0.999859] 
[Epoch 91/93] [Batch 200/539] [D loss: 0.550824] [G loss: 0.118081] [ema: 0.999859] 
[Epoch 91/93] [Batch 300/539] [D loss: 0.498356] [G loss: 0.125240] [ema: 0.999860] 
[Epoch 91/93] [Batch 400/539] [D loss: 0.526920] [G loss: 0.108668] [ema: 0.999860] 
[Epoch 91/93] [Batch 500/539] [D loss: 0.526247] [G loss: 0.130236] [ema: 0.999860] 
[Epoch 92/93] [Batch 0/539] [D loss: 0.516540] [G loss: 0.138487] [ema: 0.999860] 
[Epoch 92/93] [Batch 100/539] [D loss: 0.514488] [G loss: 0.132134] [ema: 0.999861] 
[Epoch 92/93] [Batch 200/539] [D loss: 0.530104] [G loss: 0.129834] [ema: 0.999861] 
[Epoch 92/93] [Batch 300/539] [D loss: 0.521936] [G loss: 0.117076] [ema: 0.999861] 
[Epoch 92/93] [Batch 400/539] [D loss: 0.492777] [G loss: 0.120296] [ema: 0.999861] 
[Epoch 92/93] [Batch 500/539] [D loss: 0.484441] [G loss: 0.133433] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
stand training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
stand
daghar
return single class data and labels, class is stand
data shape is (8614, 3, 1, 60)
label shape is (8614,)
539
Epochs between checkpoint: 24



Saving checkpoint 1 in logs/daghar_50000_60_100/stand_50000_D_60_2024_10_23_20_08_15/Model



[Epoch 0/93] [Batch 0/539] [D loss: 1.114312] [G loss: 0.314029] [ema: 0.000000] 
[Epoch 0/93] [Batch 100/539] [D loss: 0.556347] [G loss: 0.173354] [ema: 0.933033] 
[Epoch 0/93] [Batch 200/539] [D loss: 0.568100] [G loss: 0.141413] [ema: 0.965936] 
[Epoch 0/93] [Batch 300/539] [D loss: 0.608901] [G loss: 0.120992] [ema: 0.977160] 
[Epoch 0/93] [Batch 400/539] [D loss: 0.582669] [G loss: 0.103059] [ema: 0.982821] 
[Epoch 0/93] [Batch 500/539] [D loss: 0.614130] [G loss: 0.108416] [ema: 0.986233] 
[Epoch 1/93] [Batch 0/539] [D loss: 0.536240] [G loss: 0.152354] [ema: 0.987222] 
[Epoch 1/93] [Batch 100/539] [D loss: 0.518471] [G loss: 0.156118] [ema: 0.989211] 
[Epoch 1/93] [Batch 200/539] [D loss: 0.512308] [G loss: 0.114278] [ema: 0.990664] 
[Epoch 1/93] [Batch 300/539] [D loss: 0.494058] [G loss: 0.123208] [ema: 0.991772] 
[Epoch 1/93] [Batch 400/539] [D loss: 0.529636] [G loss: 0.129002] [ema: 0.992645] 
[Epoch 1/93] [Batch 500/539] [D loss: 0.496534] [G loss: 0.112800] [ema: 0.993351] 
[Epoch 2/93] [Batch 0/539] [D loss: 0.499514] [G loss: 0.163047] [ema: 0.993591] 
[Epoch 2/93] [Batch 100/539] [D loss: 0.587605] [G loss: 0.115612] [ema: 0.994133] 
[Epoch 2/93] [Batch 200/539] [D loss: 0.590445] [G loss: 0.129641] [ema: 0.994591] 
[Epoch 2/93] [Batch 300/539] [D loss: 0.554757] [G loss: 0.120513] [ema: 0.994983] 
[Epoch 2/93] [Batch 400/539] [D loss: 0.549549] [G loss: 0.090558] [ema: 0.995321] 
[Epoch 2/93] [Batch 500/539] [D loss: 0.611834] [G loss: 0.102409] [ema: 0.995617] 
[Epoch 3/93] [Batch 0/539] [D loss: 0.547650] [G loss: 0.122495] [ema: 0.995723] 
[Epoch 3/93] [Batch 100/539] [D loss: 0.605628] [G loss: 0.105387] [ema: 0.995971] 
[Epoch 3/93] [Batch 200/539] [D loss: 0.635405] [G loss: 0.100899] [ema: 0.996192] 
[Epoch 3/93] [Batch 300/539] [D loss: 0.594053] [G loss: 0.110196] [ema: 0.996391] 
[Epoch 3/93] [Batch 400/539] [D loss: 0.605171] [G loss: 0.106241] [ema: 0.996569] 
[Epoch 3/93] [Batch 500/539] [D loss: 0.604361] [G loss: 0.093635] [ema: 0.996731] 
[Epoch 4/93] [Batch 0/539] [D loss: 0.546262] [G loss: 0.128811] [ema: 0.996790] 
[Epoch 4/93] [Batch 100/539] [D loss: 0.510530] [G loss: 0.127077] [ema: 0.996932] 
[Epoch 4/93] [Batch 200/539] [D loss: 0.587748] [G loss: 0.101533] [ema: 0.997062] 
[Epoch 4/93] [Batch 300/539] [D loss: 0.574740] [G loss: 0.120417] [ema: 0.997182] 
[Epoch 4/93] [Batch 400/539] [D loss: 0.578872] [G loss: 0.111241] [ema: 0.997292] 
[Epoch 4/93] [Batch 500/539] [D loss: 0.572202] [G loss: 0.107718] [ema: 0.997394] 
[Epoch 5/93] [Batch 0/539] [D loss: 0.538390] [G loss: 0.118733] [ema: 0.997431] 
[Epoch 5/93] [Batch 100/539] [D loss: 0.553552] [G loss: 0.129412] [ema: 0.997523] 
[Epoch 5/93] [Batch 200/539] [D loss: 0.500465] [G loss: 0.127124] [ema: 0.997609] 
[Epoch 5/93] [Batch 300/539] [D loss: 0.550073] [G loss: 0.125504] [ema: 0.997688] 
[Epoch 5/93] [Batch 400/539] [D loss: 0.534544] [G loss: 0.110026] [ema: 0.997763] 
[Epoch 5/93] [Batch 500/539] [D loss: 0.551162] [G loss: 0.117471] [ema: 0.997833] 
[Epoch 6/93] [Batch 0/539] [D loss: 0.503638] [G loss: 0.136113] [ema: 0.997859] 
[Epoch 6/93] [Batch 100/539] [D loss: 0.527648] [G loss: 0.120535] [ema: 0.997923] 
[Epoch 6/93] [Batch 200/539] [D loss: 0.540224] [G loss: 0.101737] [ema: 0.997984] 
[Epoch 6/93] [Batch 300/539] [D loss: 0.545422] [G loss: 0.111417] [ema: 0.998041] 
[Epoch 6/93] [Batch 400/539] [D loss: 0.570021] [G loss: 0.108137] [ema: 0.998094] 
[Epoch 6/93] [Batch 500/539] [D loss: 0.609976] [G loss: 0.107399] [ema: 0.998145] 
[Epoch 7/93] [Batch 0/539] [D loss: 0.554278] [G loss: 0.118053] [ema: 0.998165] 
[Epoch 7/93] [Batch 100/539] [D loss: 0.541385] [G loss: 0.108711] [ema: 0.998212] 
[Epoch 7/93] [Batch 200/539] [D loss: 0.547914] [G loss: 0.117283] [ema: 0.998257] 
[Epoch 7/93] [Batch 300/539] [D loss: 0.575788] [G loss: 0.105157] [ema: 0.998300] 
[Epoch 7/93] [Batch 400/539] [D loss: 0.522970] [G loss: 0.121231] [ema: 0.998340] 
[Epoch 7/93] [Batch 500/539] [D loss: 0.544017] [G loss: 0.111987] [ema: 0.998379] 
[Epoch 8/93] [Batch 0/539] [D loss: 0.541548] [G loss: 0.119915] [ema: 0.998394] 
[Epoch 8/93] [Batch 100/539] [D loss: 0.507115] [G loss: 0.113597] [ema: 0.998430] 
[Epoch 8/93] [Batch 200/539] [D loss: 0.566775] [G loss: 0.111764] [ema: 0.998465] 
[Epoch 8/93] [Batch 300/539] [D loss: 0.538354] [G loss: 0.127710] [ema: 0.998498] 
[Epoch 8/93] [Batch 400/539] [D loss: 0.532884] [G loss: 0.124821] [ema: 0.998530] 
[Epoch 8/93] [Batch 500/539] [D loss: 0.592043] [G loss: 0.100487] [ema: 0.998561] 
[Epoch 9/93] [Batch 0/539] [D loss: 0.501865] [G loss: 0.142551] [ema: 0.998572] 
[Epoch 9/93] [Batch 100/539] [D loss: 0.533750] [G loss: 0.125850] [ema: 0.998601] 
[Epoch 9/93] [Batch 200/539] [D loss: 0.492837] [G loss: 0.118866] [ema: 0.998629] 
[Epoch 9/93] [Batch 300/539] [D loss: 0.500914] [G loss: 0.140661] [ema: 0.998655] 
[Epoch 9/93] [Batch 400/539] [D loss: 0.545987] [G loss: 0.140361] [ema: 0.998681] 
[Epoch 9/93] [Batch 500/539] [D loss: 0.526850] [G loss: 0.123478] [ema: 0.998705] 
[Epoch 10/93] [Batch 0/539] [D loss: 0.535134] [G loss: 0.137493] [ema: 0.998715] 
[Epoch 10/93] [Batch 100/539] [D loss: 0.522080] [G loss: 0.100252] [ema: 0.998738] 
[Epoch 10/93] [Batch 200/539] [D loss: 0.549366] [G loss: 0.099302] [ema: 0.998761] 
[Epoch 10/93] [Batch 300/539] [D loss: 0.539924] [G loss: 0.121722] [ema: 0.998783] 
[Epoch 10/93] [Batch 400/539] [D loss: 0.474812] [G loss: 0.129630] [ema: 0.998804] 
[Epoch 10/93] [Batch 500/539] [D loss: 0.531053] [G loss: 0.116126] [ema: 0.998824] 
[Epoch 11/93] [Batch 0/539] [D loss: 0.520617] [G loss: 0.130651] [ema: 0.998832] 
[Epoch 11/93] [Batch 100/539] [D loss: 0.471039] [G loss: 0.138172] [ema: 0.998851] 
[Epoch 11/93] [Batch 200/539] [D loss: 0.513507] [G loss: 0.121235] [ema: 0.998870] 
[Epoch 11/93] [Batch 300/539] [D loss: 0.535723] [G loss: 0.134157] [ema: 0.998888] 
[Epoch 11/93] [Batch 400/539] [D loss: 0.452840] [G loss: 0.158030] [ema: 0.998905] 
[Epoch 11/93] [Batch 500/539] [D loss: 0.488804] [G loss: 0.147378] [ema: 0.998922] 
[Epoch 12/93] [Batch 0/539] [D loss: 0.485404] [G loss: 0.165564] [ema: 0.998929] 
[Epoch 12/93] [Batch 100/539] [D loss: 0.490587] [G loss: 0.149558] [ema: 0.998945] 
[Epoch 12/93] [Batch 200/539] [D loss: 0.563974] [G loss: 0.108565] [ema: 0.998961] 
[Epoch 12/93] [Batch 300/539] [D loss: 0.484766] [G loss: 0.139864] [ema: 0.998976] 
[Epoch 12/93] [Batch 400/539] [D loss: 0.525975] [G loss: 0.143643] [ema: 0.998991] 
[Epoch 12/93] [Batch 500/539] [D loss: 0.472179] [G loss: 0.146092] [ema: 0.999006] 
[Epoch 13/93] [Batch 0/539] [D loss: 0.466682] [G loss: 0.153158] [ema: 0.999011] 
[Epoch 13/93] [Batch 100/539] [D loss: 0.395267] [G loss: 0.151273] [ema: 0.999025] 
[Epoch 13/93] [Batch 200/539] [D loss: 0.495086] [G loss: 0.155383] [ema: 0.999039] 
[Epoch 13/93] [Batch 300/539] [D loss: 0.498062] [G loss: 0.132974] [ema: 0.999052] 
[Epoch 13/93] [Batch 400/539] [D loss: 0.453091] [G loss: 0.160967] [ema: 0.999065] 
[Epoch 13/93] [Batch 500/539] [D loss: 0.459881] [G loss: 0.146404] [ema: 0.999077] 
[Epoch 14/93] [Batch 0/539] [D loss: 0.577127] [G loss: 0.147597] [ema: 0.999082] 
[Epoch 14/93] [Batch 100/539] [D loss: 0.539755] [G loss: 0.134139] [ema: 0.999094] 
[Epoch 14/93] [Batch 200/539] [D loss: 0.486402] [G loss: 0.163239] [ema: 0.999106] 
[Epoch 14/93] [Batch 300/539] [D loss: 0.544488] [G loss: 0.146587] [ema: 0.999117] 
[Epoch 14/93] [Batch 400/539] [D loss: 0.453349] [G loss: 0.136305] [ema: 0.999128] 
[Epoch 14/93] [Batch 500/539] [D loss: 0.462193] [G loss: 0.164533] [ema: 0.999139] 
[Epoch 15/93] [Batch 0/539] [D loss: 0.470754] [G loss: 0.161514] [ema: 0.999143] 
[Epoch 15/93] [Batch 100/539] [D loss: 0.448303] [G loss: 0.165364] [ema: 0.999154] 
[Epoch 15/93] [Batch 200/539] [D loss: 0.487643] [G loss: 0.160227] [ema: 0.999164] 
[Epoch 15/93] [Batch 300/539] [D loss: 0.477379] [G loss: 0.136744] [ema: 0.999174] 
[Epoch 15/93] [Batch 400/539] [D loss: 0.507896] [G loss: 0.147951] [ema: 0.999183] 
[Epoch 15/93] [Batch 500/539] [D loss: 0.506370] [G loss: 0.125170] [ema: 0.999193] 
[Epoch 16/93] [Batch 0/539] [D loss: 0.440255] [G loss: 0.137021] [ema: 0.999197] 
[Epoch 16/93] [Batch 100/539] [D loss: 0.457458] [G loss: 0.132895] [ema: 0.999206] 
[Epoch 16/93] [Batch 200/539] [D loss: 0.477403] [G loss: 0.174173] [ema: 0.999215] 
[Epoch 16/93] [Batch 300/539] [D loss: 0.459280] [G loss: 0.151820] [ema: 0.999224] 
[Epoch 16/93] [Batch 400/539] [D loss: 0.471373] [G loss: 0.138734] [ema: 0.999232] 
[Epoch 16/93] [Batch 500/539] [D loss: 0.425660] [G loss: 0.153281] [ema: 0.999241] 
[Epoch 17/93] [Batch 0/539] [D loss: 0.477155] [G loss: 0.168394] [ema: 0.999244] 
[Epoch 17/93] [Batch 100/539] [D loss: 0.471679] [G loss: 0.165020] [ema: 0.999252] 
[Epoch 17/93] [Batch 200/539] [D loss: 0.456021] [G loss: 0.143119] [ema: 0.999260] 
[Epoch 17/93] [Batch 300/539] [D loss: 0.526443] [G loss: 0.181532] [ema: 0.999268] 
[Epoch 17/93] [Batch 400/539] [D loss: 0.444360] [G loss: 0.140120] [ema: 0.999275] 
[Epoch 17/93] [Batch 500/539] [D loss: 0.458090] [G loss: 0.135209] [ema: 0.999283] 
[Epoch 18/93] [Batch 0/539] [D loss: 0.439230] [G loss: 0.126474] [ema: 0.999286] 
[Epoch 18/93] [Batch 100/539] [D loss: 0.467214] [G loss: 0.151018] [ema: 0.999293] 
[Epoch 18/93] [Batch 200/539] [D loss: 0.474203] [G loss: 0.169573] [ema: 0.999300] 
[Epoch 18/93] [Batch 300/539] [D loss: 0.494272] [G loss: 0.127833] [ema: 0.999307] 
[Epoch 18/93] [Batch 400/539] [D loss: 0.489703] [G loss: 0.131341] [ema: 0.999314] 
[Epoch 18/93] [Batch 500/539] [D loss: 0.451725] [G loss: 0.134880] [ema: 0.999321] 
[Epoch 19/93] [Batch 0/539] [D loss: 0.484134] [G loss: 0.157928] [ema: 0.999323] 
[Epoch 19/93] [Batch 100/539] [D loss: 0.534327] [G loss: 0.154823] [ema: 0.999330] 
[Epoch 19/93] [Batch 200/539] [D loss: 0.476022] [G loss: 0.135860] [ema: 0.999336] 
[Epoch 19/93] [Batch 300/539] [D loss: 0.509362] [G loss: 0.188705] [ema: 0.999343] 
[Epoch 19/93] [Batch 400/539] [D loss: 0.414879] [G loss: 0.148284] [ema: 0.999349] 
[Epoch 19/93] [Batch 500/539] [D loss: 0.490202] [G loss: 0.140347] [ema: 0.999355] 
[Epoch 20/93] [Batch 0/539] [D loss: 0.523213] [G loss: 0.149880] [ema: 0.999357] 
[Epoch 20/93] [Batch 100/539] [D loss: 0.514087] [G loss: 0.146445] [ema: 0.999363] 
[Epoch 20/93] [Batch 200/539] [D loss: 0.484207] [G loss: 0.130618] [ema: 0.999369] 
[Epoch 20/93] [Batch 300/539] [D loss: 0.475726] [G loss: 0.137474] [ema: 0.999375] 
[Epoch 20/93] [Batch 400/539] [D loss: 0.492701] [G loss: 0.142908] [ema: 0.999380] 
[Epoch 20/93] [Batch 500/539] [D loss: 0.459951] [G loss: 0.152512] [ema: 0.999386] 
[Epoch 21/93] [Batch 0/539] [D loss: 0.475772] [G loss: 0.142770] [ema: 0.999388] 
[Epoch 21/93] [Batch 100/539] [D loss: 0.486392] [G loss: 0.140107] [ema: 0.999393] 
[Epoch 21/93] [Batch 200/539] [D loss: 0.493757] [G loss: 0.142099] [ema: 0.999398] 
[Epoch 21/93] [Batch 300/539] [D loss: 0.547566] [G loss: 0.139047] [ema: 0.999404] 
[Epoch 21/93] [Batch 400/539] [D loss: 0.437622] [G loss: 0.141552] [ema: 0.999409] 
[Epoch 21/93] [Batch 500/539] [D loss: 0.503646] [G loss: 0.153849] [ema: 0.999414] 
[Epoch 22/93] [Batch 0/539] [D loss: 0.488260] [G loss: 0.125061] [ema: 0.999416] 
[Epoch 22/93] [Batch 100/539] [D loss: 0.497933] [G loss: 0.148398] [ema: 0.999421] 
[Epoch 22/93] [Batch 200/539] [D loss: 0.479382] [G loss: 0.133071] [ema: 0.999425] 
[Epoch 22/93] [Batch 300/539] [D loss: 0.524867] [G loss: 0.131009] [ema: 0.999430] 
[Epoch 22/93] [Batch 400/539] [D loss: 0.453313] [G loss: 0.134249] [ema: 0.999435] 
[Epoch 22/93] [Batch 500/539] [D loss: 0.510686] [G loss: 0.148228] [ema: 0.999439] 
[Epoch 23/93] [Batch 0/539] [D loss: 0.471532] [G loss: 0.163142] [ema: 0.999441] 
[Epoch 23/93] [Batch 100/539] [D loss: 0.500704] [G loss: 0.150774] [ema: 0.999446] 
[Epoch 23/93] [Batch 200/539] [D loss: 0.534595] [G loss: 0.123092] [ema: 0.999450] 
[Epoch 23/93] [Batch 300/539] [D loss: 0.452489] [G loss: 0.148559] [ema: 0.999454] 
[Epoch 23/93] [Batch 400/539] [D loss: 0.497187] [G loss: 0.123950] [ema: 0.999458] 
[Epoch 23/93] [Batch 500/539] [D loss: 0.465291] [G loss: 0.141409] [ema: 0.999463] 



Saving checkpoint 2 in logs/daghar_50000_60_100/stand_50000_D_60_2024_10_23_20_08_15/Model



[Epoch 24/93] [Batch 0/539] [D loss: 0.479573] [G loss: 0.137859] [ema: 0.999464] 
[Epoch 24/93] [Batch 100/539] [D loss: 0.450836] [G loss: 0.149087] [ema: 0.999468] 
[Epoch 24/93] [Batch 200/539] [D loss: 0.516740] [G loss: 0.150558] [ema: 0.999472] 
[Epoch 24/93] [Batch 300/539] [D loss: 0.480635] [G loss: 0.164682] [ema: 0.999476] 
[Epoch 24/93] [Batch 400/539] [D loss: 0.486853] [G loss: 0.154841] [ema: 0.999480] 
[Epoch 24/93] [Batch 500/539] [D loss: 0.509016] [G loss: 0.143407] [ema: 0.999484] 
[Epoch 25/93] [Batch 0/539] [D loss: 0.439278] [G loss: 0.144098] [ema: 0.999486] 
[Epoch 25/93] [Batch 100/539] [D loss: 0.491096] [G loss: 0.129931] [ema: 0.999490] 
[Epoch 25/93] [Batch 200/539] [D loss: 0.495249] [G loss: 0.125007] [ema: 0.999493] 
[Epoch 25/93] [Batch 300/539] [D loss: 0.494668] [G loss: 0.159618] [ema: 0.999497] 
[Epoch 25/93] [Batch 400/539] [D loss: 0.568310] [G loss: 0.146576] [ema: 0.999501] 
[Epoch 25/93] [Batch 500/539] [D loss: 0.416870] [G loss: 0.141039] [ema: 0.999504] 
[Epoch 26/93] [Batch 0/539] [D loss: 0.470726] [G loss: 0.119389] [ema: 0.999506] 
[Epoch 26/93] [Batch 100/539] [D loss: 0.506029] [G loss: 0.141072] [ema: 0.999509] 
[Epoch 26/93] [Batch 200/539] [D loss: 0.505677] [G loss: 0.135302] [ema: 0.999512] 
[Epoch 26/93] [Batch 300/539] [D loss: 0.513122] [G loss: 0.128937] [ema: 0.999516] 
[Epoch 26/93] [Batch 400/539] [D loss: 0.453242] [G loss: 0.160449] [ema: 0.999519] 
[Epoch 26/93] [Batch 500/539] [D loss: 0.554291] [G loss: 0.173627] [ema: 0.999523] 
[Epoch 27/93] [Batch 0/539] [D loss: 0.466647] [G loss: 0.158932] [ema: 0.999524] 
[Epoch 27/93] [Batch 100/539] [D loss: 0.428047] [G loss: 0.148484] [ema: 0.999527] 
[Epoch 27/93] [Batch 200/539] [D loss: 0.495278] [G loss: 0.168146] [ema: 0.999530] 
[Epoch 27/93] [Batch 300/539] [D loss: 0.524999] [G loss: 0.138818] [ema: 0.999533] 
[Epoch 27/93] [Batch 400/539] [D loss: 0.435502] [G loss: 0.158146] [ema: 0.999537] 
[Epoch 27/93] [Batch 500/539] [D loss: 0.455973] [G loss: 0.163275] [ema: 0.999540] 
[Epoch 28/93] [Batch 0/539] [D loss: 0.445964] [G loss: 0.183342] [ema: 0.999541] 
[Epoch 28/93] [Batch 100/539] [D loss: 0.481474] [G loss: 0.171117] [ema: 0.999544] 
[Epoch 28/93] [Batch 200/539] [D loss: 0.461113] [G loss: 0.169751] [ema: 0.999547] 
[Epoch 28/93] [Batch 300/539] [D loss: 0.458836] [G loss: 0.172784] [ema: 0.999550] 
[Epoch 28/93] [Batch 400/539] [D loss: 0.490999] [G loss: 0.137870] [ema: 0.999553] 
[Epoch 28/93] [Batch 500/539] [D loss: 0.487666] [G loss: 0.185170] [ema: 0.999556] 
[Epoch 29/93] [Batch 0/539] [D loss: 0.523283] [G loss: 0.187458] [ema: 0.999557] 
[Epoch 29/93] [Batch 100/539] [D loss: 0.517969] [G loss: 0.138285] [ema: 0.999559] 
[Epoch 29/93] [Batch 200/539] [D loss: 0.536111] [G loss: 0.138208] [ema: 0.999562] 
[Epoch 29/93] [Batch 300/539] [D loss: 0.481073] [G loss: 0.171536] [ema: 0.999565] 
[Epoch 29/93] [Batch 400/539] [D loss: 0.464544] [G loss: 0.151027] [ema: 0.999568] 
[Epoch 29/93] [Batch 500/539] [D loss: 0.433114] [G loss: 0.157235] [ema: 0.999570] 
[Epoch 30/93] [Batch 0/539] [D loss: 0.503405] [G loss: 0.156401] [ema: 0.999571] 
[Epoch 30/93] [Batch 100/539] [D loss: 0.472241] [G loss: 0.136270] [ema: 0.999574] 
[Epoch 30/93] [Batch 200/539] [D loss: 0.527822] [G loss: 0.127349] [ema: 0.999577] 
[Epoch 30/93] [Batch 300/539] [D loss: 0.533692] [G loss: 0.156117] [ema: 0.999579] 
[Epoch 30/93] [Batch 400/539] [D loss: 0.486399] [G loss: 0.144586] [ema: 0.999582] 
[Epoch 30/93] [Batch 500/539] [D loss: 0.482014] [G loss: 0.140866] [ema: 0.999584] 
[Epoch 31/93] [Batch 0/539] [D loss: 0.517374] [G loss: 0.132683] [ema: 0.999585] 
[Epoch 31/93] [Batch 100/539] [D loss: 0.461549] [G loss: 0.148600] [ema: 0.999588] 
[Epoch 31/93] [Batch 200/539] [D loss: 0.468955] [G loss: 0.148182] [ema: 0.999590] 
[Epoch 31/93] [Batch 300/539] [D loss: 0.521508] [G loss: 0.140854] [ema: 0.999593] 
[Epoch 31/93] [Batch 400/539] [D loss: 0.531945] [G loss: 0.135474] [ema: 0.999595] 
[Epoch 31/93] [Batch 500/539] [D loss: 0.464262] [G loss: 0.123222] [ema: 0.999597] 
[Epoch 32/93] [Batch 0/539] [D loss: 0.442554] [G loss: 0.149739] [ema: 0.999598] 
[Epoch 32/93] [Batch 100/539] [D loss: 0.500954] [G loss: 0.133610] [ema: 0.999601] 
[Epoch 32/93] [Batch 200/539] [D loss: 0.479243] [G loss: 0.139052] [ema: 0.999603] 
[Epoch 32/93] [Batch 300/539] [D loss: 0.519351] [G loss: 0.136792] [ema: 0.999605] 
[Epoch 32/93] [Batch 400/539] [D loss: 0.498108] [G loss: 0.114167] [ema: 0.999607] 
[Epoch 32/93] [Batch 500/539] [D loss: 0.542878] [G loss: 0.154713] [ema: 0.999610] 
[Epoch 33/93] [Batch 0/539] [D loss: 0.532478] [G loss: 0.131933] [ema: 0.999610] 
[Epoch 33/93] [Batch 100/539] [D loss: 0.506674] [G loss: 0.139747] [ema: 0.999613] 
[Epoch 33/93] [Batch 200/539] [D loss: 0.534223] [G loss: 0.136942] [ema: 0.999615] 
[Epoch 33/93] [Batch 300/539] [D loss: 0.537181] [G loss: 0.138638] [ema: 0.999617] 
[Epoch 33/93] [Batch 400/539] [D loss: 0.514332] [G loss: 0.125772] [ema: 0.999619] 
[Epoch 33/93] [Batch 500/539] [D loss: 0.516047] [G loss: 0.140784] [ema: 0.999621] 
[Epoch 34/93] [Batch 0/539] [D loss: 0.502864] [G loss: 0.139971] [ema: 0.999622] 
[Epoch 34/93] [Batch 100/539] [D loss: 0.500253] [G loss: 0.131071] [ema: 0.999624] 
[Epoch 34/93] [Batch 200/539] [D loss: 0.520426] [G loss: 0.109165] [ema: 0.999626] 
[Epoch 34/93] [Batch 300/539] [D loss: 0.501564] [G loss: 0.129951] [ema: 0.999628] 
[Epoch 34/93] [Batch 400/539] [D loss: 0.521147] [G loss: 0.124053] [ema: 0.999630] 
[Epoch 34/93] [Batch 500/539] [D loss: 0.490907] [G loss: 0.121343] [ema: 0.999632] 
[Epoch 35/93] [Batch 0/539] [D loss: 0.547655] [G loss: 0.132303] [ema: 0.999633] 
[Epoch 35/93] [Batch 100/539] [D loss: 0.519212] [G loss: 0.144393] [ema: 0.999635] 
[Epoch 35/93] [Batch 200/539] [D loss: 0.468668] [G loss: 0.133027] [ema: 0.999636] 
[Epoch 35/93] [Batch 300/539] [D loss: 0.468056] [G loss: 0.132258] [ema: 0.999638] 
[Epoch 35/93] [Batch 400/539] [D loss: 0.504741] [G loss: 0.120253] [ema: 0.999640] 
[Epoch 35/93] [Batch 500/539] [D loss: 0.531386] [G loss: 0.120414] [ema: 0.999642] 
[Epoch 36/93] [Batch 0/539] [D loss: 0.568595] [G loss: 0.120074] [ema: 0.999643] 
[Epoch 36/93] [Batch 100/539] [D loss: 0.545559] [G loss: 0.135351] [ema: 0.999645] 
[Epoch 36/93] [Batch 200/539] [D loss: 0.559566] [G loss: 0.128453] [ema: 0.999646] 
[Epoch 36/93] [Batch 300/539] [D loss: 0.494484] [G loss: 0.115112] [ema: 0.999648] 
[Epoch 36/93] [Batch 400/539] [D loss: 0.511274] [G loss: 0.130687] [ema: 0.999650] 
[Epoch 36/93] [Batch 500/539] [D loss: 0.505516] [G loss: 0.136781] [ema: 0.999652] 
[Epoch 37/93] [Batch 0/539] [D loss: 0.532629] [G loss: 0.122092] [ema: 0.999652] 
[Epoch 37/93] [Batch 100/539] [D loss: 0.503941] [G loss: 0.124793] [ema: 0.999654] 
[Epoch 37/93] [Batch 200/539] [D loss: 0.475615] [G loss: 0.133442] [ema: 0.999656] 
[Epoch 37/93] [Batch 300/539] [D loss: 0.515269] [G loss: 0.128286] [ema: 0.999658] 
[Epoch 37/93] [Batch 400/539] [D loss: 0.523877] [G loss: 0.138785] [ema: 0.999659] 
[Epoch 37/93] [Batch 500/539] [D loss: 0.468934] [G loss: 0.148966] [ema: 0.999661] 
[Epoch 38/93] [Batch 0/539] [D loss: 0.437708] [G loss: 0.163668] [ema: 0.999662] 
[Epoch 38/93] [Batch 100/539] [D loss: 0.484354] [G loss: 0.141713] [ema: 0.999663] 
[Epoch 38/93] [Batch 200/539] [D loss: 0.467520] [G loss: 0.161367] [ema: 0.999665] 
[Epoch 38/93] [Batch 300/539] [D loss: 0.487682] [G loss: 0.171583] [ema: 0.999667] 
[Epoch 38/93] [Batch 400/539] [D loss: 0.476559] [G loss: 0.129781] [ema: 0.999668] 
[Epoch 38/93] [Batch 500/539] [D loss: 0.432760] [G loss: 0.176429] [ema: 0.999670] 
[Epoch 39/93] [Batch 0/539] [D loss: 0.510702] [G loss: 0.130217] [ema: 0.999670] 
[Epoch 39/93] [Batch 100/539] [D loss: 0.491402] [G loss: 0.177055] [ema: 0.999672] 
[Epoch 39/93] [Batch 200/539] [D loss: 0.456529] [G loss: 0.137424] [ema: 0.999673] 
[Epoch 39/93] [Batch 300/539] [D loss: 0.479601] [G loss: 0.135003] [ema: 0.999675] 
[Epoch 39/93] [Batch 400/539] [D loss: 0.497594] [G loss: 0.142374] [ema: 0.999676] 
[Epoch 39/93] [Batch 500/539] [D loss: 0.481696] [G loss: 0.138905] [ema: 0.999678] 
[Epoch 40/93] [Batch 0/539] [D loss: 0.440885] [G loss: 0.146407] [ema: 0.999679] 
[Epoch 40/93] [Batch 100/539] [D loss: 0.562687] [G loss: 0.139294] [ema: 0.999680] 
[Epoch 40/93] [Batch 200/539] [D loss: 0.539317] [G loss: 0.136819] [ema: 0.999682] 
[Epoch 40/93] [Batch 300/539] [D loss: 0.553246] [G loss: 0.112546] [ema: 0.999683] 
[Epoch 40/93] [Batch 400/539] [D loss: 0.559933] [G loss: 0.115285] [ema: 0.999684] 
[Epoch 40/93] [Batch 500/539] [D loss: 0.519703] [G loss: 0.119781] [ema: 0.999686] 
[Epoch 41/93] [Batch 0/539] [D loss: 0.554434] [G loss: 0.141201] [ema: 0.999686] 
[Epoch 41/93] [Batch 100/539] [D loss: 0.490451] [G loss: 0.136621] [ema: 0.999688] 
[Epoch 41/93] [Batch 200/539] [D loss: 0.601003] [G loss: 0.115829] [ema: 0.999689] 
[Epoch 41/93] [Batch 300/539] [D loss: 0.508132] [G loss: 0.132105] [ema: 0.999691] 
[Epoch 41/93] [Batch 400/539] [D loss: 0.570435] [G loss: 0.121964] [ema: 0.999692] 
[Epoch 41/93] [Batch 500/539] [D loss: 0.473926] [G loss: 0.155832] [ema: 0.999693] 
[Epoch 42/93] [Batch 0/539] [D loss: 0.448550] [G loss: 0.136787] [ema: 0.999694] 
[Epoch 42/93] [Batch 100/539] [D loss: 0.502324] [G loss: 0.111861] [ema: 0.999695] 
[Epoch 42/93] [Batch 200/539] [D loss: 0.494405] [G loss: 0.116418] [ema: 0.999697] 
[Epoch 42/93] [Batch 300/539] [D loss: 0.540453] [G loss: 0.128161] [ema: 0.999698] 
[Epoch 42/93] [Batch 400/539] [D loss: 0.559563] [G loss: 0.121310] [ema: 0.999699] 
[Epoch 42/93] [Batch 500/539] [D loss: 0.533134] [G loss: 0.110009] [ema: 0.999700] 
[Epoch 43/93] [Batch 0/539] [D loss: 0.537217] [G loss: 0.141530] [ema: 0.999701] 
[Epoch 43/93] [Batch 100/539] [D loss: 0.540458] [G loss: 0.134254] [ema: 0.999702] 
[Epoch 43/93] [Batch 200/539] [D loss: 0.531388] [G loss: 0.132840] [ema: 0.999704] 
[Epoch 43/93] [Batch 300/539] [D loss: 0.517695] [G loss: 0.118587] [ema: 0.999705] 
[Epoch 43/93] [Batch 400/539] [D loss: 0.498399] [G loss: 0.141297] [ema: 0.999706] 
[Epoch 43/93] [Batch 500/539] [D loss: 0.505573] [G loss: 0.129583] [ema: 0.999707] 
[Epoch 44/93] [Batch 0/539] [D loss: 0.515034] [G loss: 0.148016] [ema: 0.999708] 
[Epoch 44/93] [Batch 100/539] [D loss: 0.538856] [G loss: 0.135044] [ema: 0.999709] 
[Epoch 44/93] [Batch 200/539] [D loss: 0.555270] [G loss: 0.113839] [ema: 0.999710] 
[Epoch 44/93] [Batch 300/539] [D loss: 0.561107] [G loss: 0.131542] [ema: 0.999711] 
[Epoch 44/93] [Batch 400/539] [D loss: 0.556857] [G loss: 0.127523] [ema: 0.999713] 
[Epoch 44/93] [Batch 500/539] [D loss: 0.516715] [G loss: 0.112169] [ema: 0.999714] 
[Epoch 45/93] [Batch 0/539] [D loss: 0.513613] [G loss: 0.136288] [ema: 0.999714] 
[Epoch 45/93] [Batch 100/539] [D loss: 0.491564] [G loss: 0.126768] [ema: 0.999715] 
[Epoch 45/93] [Batch 200/539] [D loss: 0.487163] [G loss: 0.152523] [ema: 0.999717] 
[Epoch 45/93] [Batch 300/539] [D loss: 0.511618] [G loss: 0.119172] [ema: 0.999718] 
[Epoch 45/93] [Batch 400/539] [D loss: 0.560257] [G loss: 0.126098] [ema: 0.999719] 
[Epoch 45/93] [Batch 500/539] [D loss: 0.523637] [G loss: 0.117812] [ema: 0.999720] 
[Epoch 46/93] [Batch 0/539] [D loss: 0.492219] [G loss: 0.130250] [ema: 0.999720] 
[Epoch 46/93] [Batch 100/539] [D loss: 0.548575] [G loss: 0.132254] [ema: 0.999722] 
[Epoch 46/93] [Batch 200/539] [D loss: 0.534123] [G loss: 0.128619] [ema: 0.999723] 
[Epoch 46/93] [Batch 300/539] [D loss: 0.513245] [G loss: 0.123235] [ema: 0.999724] 
[Epoch 46/93] [Batch 400/539] [D loss: 0.525765] [G loss: 0.096191] [ema: 0.999725] 
[Epoch 46/93] [Batch 500/539] [D loss: 0.553691] [G loss: 0.137679] [ema: 0.999726] 
[Epoch 47/93] [Batch 0/539] [D loss: 0.526465] [G loss: 0.120607] [ema: 0.999726] 
[Epoch 47/93] [Batch 100/539] [D loss: 0.533174] [G loss: 0.128070] [ema: 0.999727] 
[Epoch 47/93] [Batch 200/539] [D loss: 0.526766] [G loss: 0.113368] [ema: 0.999729] 
[Epoch 47/93] [Batch 300/539] [D loss: 0.548655] [G loss: 0.120296] [ema: 0.999730] 
[Epoch 47/93] [Batch 400/539] [D loss: 0.490485] [G loss: 0.135914] [ema: 0.999731] 
[Epoch 47/93] [Batch 500/539] [D loss: 0.547387] [G loss: 0.105319] [ema: 0.999732] 



Saving checkpoint 3 in logs/daghar_50000_60_100/stand_50000_D_60_2024_10_23_20_08_15/Model



[Epoch 48/93] [Batch 0/539] [D loss: 0.494584] [G loss: 0.147454] [ema: 0.999732] 
[Epoch 48/93] [Batch 100/539] [D loss: 0.521084] [G loss: 0.144872] [ema: 0.999733] 
[Epoch 48/93] [Batch 200/539] [D loss: 0.530489] [G loss: 0.134072] [ema: 0.999734] 
[Epoch 48/93] [Batch 300/539] [D loss: 0.518320] [G loss: 0.137603] [ema: 0.999735] 
[Epoch 48/93] [Batch 400/539] [D loss: 0.510618] [G loss: 0.123101] [ema: 0.999736] 
[Epoch 48/93] [Batch 500/539] [D loss: 0.549145] [G loss: 0.109012] [ema: 0.999737] 
[Epoch 49/93] [Batch 0/539] [D loss: 0.534637] [G loss: 0.124513] [ema: 0.999738] 
[Epoch 49/93] [Batch 100/539] [D loss: 0.517701] [G loss: 0.133524] [ema: 0.999739] 
[Epoch 49/93] [Batch 200/539] [D loss: 0.482452] [G loss: 0.122323] [ema: 0.999740] 
[Epoch 49/93] [Batch 300/539] [D loss: 0.540182] [G loss: 0.122500] [ema: 0.999741] 
[Epoch 49/93] [Batch 400/539] [D loss: 0.498142] [G loss: 0.137805] [ema: 0.999742] 
[Epoch 49/93] [Batch 500/539] [D loss: 0.510884] [G loss: 0.126728] [ema: 0.999742] 
[Epoch 50/93] [Batch 0/539] [D loss: 0.542983] [G loss: 0.151190] [ema: 0.999743] 
[Epoch 50/93] [Batch 100/539] [D loss: 0.509599] [G loss: 0.123640] [ema: 0.999744] 
[Epoch 50/93] [Batch 200/539] [D loss: 0.502145] [G loss: 0.127916] [ema: 0.999745] 
[Epoch 50/93] [Batch 300/539] [D loss: 0.521554] [G loss: 0.123804] [ema: 0.999746] 
[Epoch 50/93] [Batch 400/539] [D loss: 0.505572] [G loss: 0.134623] [ema: 0.999747] 
[Epoch 50/93] [Batch 500/539] [D loss: 0.555800] [G loss: 0.119040] [ema: 0.999748] 
[Epoch 51/93] [Batch 0/539] [D loss: 0.535177] [G loss: 0.099966] [ema: 0.999748] 
[Epoch 51/93] [Batch 100/539] [D loss: 0.553160] [G loss: 0.131138] [ema: 0.999749] 
[Epoch 51/93] [Batch 200/539] [D loss: 0.515264] [G loss: 0.127722] [ema: 0.999750] 
[Epoch 51/93] [Batch 300/539] [D loss: 0.510331] [G loss: 0.128466] [ema: 0.999751] 
[Epoch 51/93] [Batch 400/539] [D loss: 0.531609] [G loss: 0.122242] [ema: 0.999751] 
[Epoch 51/93] [Batch 500/539] [D loss: 0.482642] [G loss: 0.133118] [ema: 0.999752] 
[Epoch 52/93] [Batch 0/539] [D loss: 0.479497] [G loss: 0.178395] [ema: 0.999753] 
[Epoch 52/93] [Batch 100/539] [D loss: 0.566806] [G loss: 0.098091] [ema: 0.999754] 
[Epoch 52/93] [Batch 200/539] [D loss: 0.563361] [G loss: 0.131250] [ema: 0.999754] 
[Epoch 52/93] [Batch 300/539] [D loss: 0.536590] [G loss: 0.120911] [ema: 0.999755] 
[Epoch 52/93] [Batch 400/539] [D loss: 0.506221] [G loss: 0.121795] [ema: 0.999756] 
[Epoch 52/93] [Batch 500/539] [D loss: 0.527364] [G loss: 0.104800] [ema: 0.999757] 
[Epoch 53/93] [Batch 0/539] [D loss: 0.525283] [G loss: 0.101791] [ema: 0.999757] 
[Epoch 53/93] [Batch 100/539] [D loss: 0.565912] [G loss: 0.120850] [ema: 0.999758] 
[Epoch 53/93] [Batch 200/539] [D loss: 0.500326] [G loss: 0.116585] [ema: 0.999759] 
[Epoch 53/93] [Batch 300/539] [D loss: 0.509603] [G loss: 0.153616] [ema: 0.999760] 
[Epoch 53/93] [Batch 400/539] [D loss: 0.471813] [G loss: 0.134695] [ema: 0.999761] 
[Epoch 53/93] [Batch 500/539] [D loss: 0.506148] [G loss: 0.147908] [ema: 0.999762] 
[Epoch 54/93] [Batch 0/539] [D loss: 0.533181] [G loss: 0.125554] [ema: 0.999762] 
[Epoch 54/93] [Batch 100/539] [D loss: 0.519607] [G loss: 0.127031] [ema: 0.999763] 
[Epoch 54/93] [Batch 200/539] [D loss: 0.567001] [G loss: 0.143758] [ema: 0.999764] 
[Epoch 54/93] [Batch 300/539] [D loss: 0.526391] [G loss: 0.111294] [ema: 0.999764] 
[Epoch 54/93] [Batch 400/539] [D loss: 0.533455] [G loss: 0.129059] [ema: 0.999765] 
[Epoch 54/93] [Batch 500/539] [D loss: 0.552173] [G loss: 0.111295] [ema: 0.999766] 
[Epoch 55/93] [Batch 0/539] [D loss: 0.546062] [G loss: 0.121903] [ema: 0.999766] 
[Epoch 55/93] [Batch 100/539] [D loss: 0.522996] [G loss: 0.130313] [ema: 0.999767] 
[Epoch 55/93] [Batch 200/539] [D loss: 0.513629] [G loss: 0.121539] [ema: 0.999768] 
[Epoch 55/93] [Batch 300/539] [D loss: 0.491399] [G loss: 0.128081] [ema: 0.999769] 
[Epoch 55/93] [Batch 400/539] [D loss: 0.513038] [G loss: 0.120340] [ema: 0.999769] 
[Epoch 55/93] [Batch 500/539] [D loss: 0.526352] [G loss: 0.123943] [ema: 0.999770] 
[Epoch 56/93] [Batch 0/539] [D loss: 0.559955] [G loss: 0.138747] [ema: 0.999770] 
[Epoch 56/93] [Batch 100/539] [D loss: 0.491494] [G loss: 0.134551] [ema: 0.999771] 
[Epoch 56/93] [Batch 200/539] [D loss: 0.509149] [G loss: 0.107007] [ema: 0.999772] 
[Epoch 56/93] [Batch 300/539] [D loss: 0.459653] [G loss: 0.125338] [ema: 0.999773] 
[Epoch 56/93] [Batch 400/539] [D loss: 0.549807] [G loss: 0.116774] [ema: 0.999773] 
[Epoch 56/93] [Batch 500/539] [D loss: 0.513231] [G loss: 0.130805] [ema: 0.999774] 
[Epoch 57/93] [Batch 0/539] [D loss: 0.519256] [G loss: 0.131575] [ema: 0.999774] 
[Epoch 57/93] [Batch 100/539] [D loss: 0.509779] [G loss: 0.126544] [ema: 0.999775] 
[Epoch 57/93] [Batch 200/539] [D loss: 0.523949] [G loss: 0.146307] [ema: 0.999776] 
[Epoch 57/93] [Batch 300/539] [D loss: 0.504011] [G loss: 0.140092] [ema: 0.999777] 
[Epoch 57/93] [Batch 400/539] [D loss: 0.527504] [G loss: 0.130526] [ema: 0.999777] 
[Epoch 57/93] [Batch 500/539] [D loss: 0.528900] [G loss: 0.127309] [ema: 0.999778] 
[Epoch 58/93] [Batch 0/539] [D loss: 0.482947] [G loss: 0.115168] [ema: 0.999778] 
[Epoch 58/93] [Batch 100/539] [D loss: 0.532183] [G loss: 0.141625] [ema: 0.999779] 
[Epoch 58/93] [Batch 200/539] [D loss: 0.591344] [G loss: 0.129346] [ema: 0.999780] 
[Epoch 58/93] [Batch 300/539] [D loss: 0.535700] [G loss: 0.112440] [ema: 0.999780] 
[Epoch 58/93] [Batch 400/539] [D loss: 0.529493] [G loss: 0.116067] [ema: 0.999781] 
[Epoch 58/93] [Batch 500/539] [D loss: 0.553246] [G loss: 0.110839] [ema: 0.999782] 
[Epoch 59/93] [Batch 0/539] [D loss: 0.534447] [G loss: 0.121710] [ema: 0.999782] 
[Epoch 59/93] [Batch 100/539] [D loss: 0.513385] [G loss: 0.111727] [ema: 0.999783] 
[Epoch 59/93] [Batch 200/539] [D loss: 0.525628] [G loss: 0.129495] [ema: 0.999783] 
[Epoch 59/93] [Batch 300/539] [D loss: 0.541586] [G loss: 0.113231] [ema: 0.999784] 
[Epoch 59/93] [Batch 400/539] [D loss: 0.580121] [G loss: 0.137235] [ema: 0.999785] 
[Epoch 59/93] [Batch 500/539] [D loss: 0.520765] [G loss: 0.109766] [ema: 0.999785] 
[Epoch 60/93] [Batch 0/539] [D loss: 0.486971] [G loss: 0.131427] [ema: 0.999786] 
[Epoch 60/93] [Batch 100/539] [D loss: 0.531011] [G loss: 0.125094] [ema: 0.999786] 
[Epoch 60/93] [Batch 200/539] [D loss: 0.499438] [G loss: 0.122666] [ema: 0.999787] 
[Epoch 60/93] [Batch 300/539] [D loss: 0.530155] [G loss: 0.130000] [ema: 0.999788] 
[Epoch 60/93] [Batch 400/539] [D loss: 0.506207] [G loss: 0.118876] [ema: 0.999788] 
[Epoch 60/93] [Batch 500/539] [D loss: 0.538646] [G loss: 0.113286] [ema: 0.999789] 
[Epoch 61/93] [Batch 0/539] [D loss: 0.534323] [G loss: 0.149472] [ema: 0.999789] 
[Epoch 61/93] [Batch 100/539] [D loss: 0.480067] [G loss: 0.135629] [ema: 0.999790] 
[Epoch 61/93] [Batch 200/539] [D loss: 0.537805] [G loss: 0.125928] [ema: 0.999790] 
[Epoch 61/93] [Batch 300/539] [D loss: 0.535133] [G loss: 0.130844] [ema: 0.999791] 
[Epoch 61/93] [Batch 400/539] [D loss: 0.539747] [G loss: 0.113590] [ema: 0.999792] 
[Epoch 61/93] [Batch 500/539] [D loss: 0.491381] [G loss: 0.123940] [ema: 0.999792] 
[Epoch 62/93] [Batch 0/539] [D loss: 0.571873] [G loss: 0.134082] [ema: 0.999793] 
[Epoch 62/93] [Batch 100/539] [D loss: 0.556643] [G loss: 0.128863] [ema: 0.999793] 
[Epoch 62/93] [Batch 200/539] [D loss: 0.532833] [G loss: 0.115619] [ema: 0.999794] 
[Epoch 62/93] [Batch 300/539] [D loss: 0.531930] [G loss: 0.115779] [ema: 0.999794] 
[Epoch 62/93] [Batch 400/539] [D loss: 0.530072] [G loss: 0.121131] [ema: 0.999795] 
[Epoch 62/93] [Batch 500/539] [D loss: 0.523716] [G loss: 0.144558] [ema: 0.999796] 
[Epoch 63/93] [Batch 0/539] [D loss: 0.513023] [G loss: 0.113146] [ema: 0.999796] 
[Epoch 63/93] [Batch 100/539] [D loss: 0.516143] [G loss: 0.137173] [ema: 0.999796] 
[Epoch 63/93] [Batch 200/539] [D loss: 0.508110] [G loss: 0.132434] [ema: 0.999797] 
[Epoch 63/93] [Batch 300/539] [D loss: 0.517662] [G loss: 0.136517] [ema: 0.999798] 
[Epoch 63/93] [Batch 400/539] [D loss: 0.542404] [G loss: 0.122481] [ema: 0.999798] 
[Epoch 63/93] [Batch 500/539] [D loss: 0.546570] [G loss: 0.133526] [ema: 0.999799] 
[Epoch 64/93] [Batch 0/539] [D loss: 0.521851] [G loss: 0.128688] [ema: 0.999799] 
[Epoch 64/93] [Batch 100/539] [D loss: 0.517305] [G loss: 0.111441] [ema: 0.999800] 
[Epoch 64/93] [Batch 200/539] [D loss: 0.537122] [G loss: 0.129506] [ema: 0.999800] 
[Epoch 64/93] [Batch 300/539] [D loss: 0.523603] [G loss: 0.134037] [ema: 0.999801] 
[Epoch 64/93] [Batch 400/539] [D loss: 0.501871] [G loss: 0.119253] [ema: 0.999801] 
[Epoch 64/93] [Batch 500/539] [D loss: 0.536263] [G loss: 0.154451] [ema: 0.999802] 
[Epoch 65/93] [Batch 0/539] [D loss: 0.522889] [G loss: 0.127680] [ema: 0.999802] 
[Epoch 65/93] [Batch 100/539] [D loss: 0.487268] [G loss: 0.137875] [ema: 0.999803] 
[Epoch 65/93] [Batch 200/539] [D loss: 0.535590] [G loss: 0.121958] [ema: 0.999803] 
[Epoch 65/93] [Batch 300/539] [D loss: 0.490660] [G loss: 0.127800] [ema: 0.999804] 
[Epoch 65/93] [Batch 400/539] [D loss: 0.553899] [G loss: 0.121190] [ema: 0.999804] 
[Epoch 65/93] [Batch 500/539] [D loss: 0.528807] [G loss: 0.118271] [ema: 0.999805] 
[Epoch 66/93] [Batch 0/539] [D loss: 0.502007] [G loss: 0.134276] [ema: 0.999805] 
[Epoch 66/93] [Batch 100/539] [D loss: 0.529764] [G loss: 0.122025] [ema: 0.999806] 
[Epoch 66/93] [Batch 200/539] [D loss: 0.517309] [G loss: 0.124485] [ema: 0.999806] 
[Epoch 66/93] [Batch 300/539] [D loss: 0.526515] [G loss: 0.127516] [ema: 0.999807] 
[Epoch 66/93] [Batch 400/539] [D loss: 0.505833] [G loss: 0.105185] [ema: 0.999807] 
[Epoch 66/93] [Batch 500/539] [D loss: 0.531421] [G loss: 0.107115] [ema: 0.999808] 
[Epoch 67/93] [Batch 0/539] [D loss: 0.507719] [G loss: 0.123161] [ema: 0.999808] 
[Epoch 67/93] [Batch 100/539] [D loss: 0.555807] [G loss: 0.137209] [ema: 0.999809] 
[Epoch 67/93] [Batch 200/539] [D loss: 0.573315] [G loss: 0.116166] [ema: 0.999809] 
[Epoch 67/93] [Batch 300/539] [D loss: 0.476045] [G loss: 0.104743] [ema: 0.999810] 
[Epoch 67/93] [Batch 400/539] [D loss: 0.501799] [G loss: 0.123601] [ema: 0.999810] 
[Epoch 67/93] [Batch 500/539] [D loss: 0.538780] [G loss: 0.113719] [ema: 0.999811] 
[Epoch 68/93] [Batch 0/539] [D loss: 0.490518] [G loss: 0.143141] [ema: 0.999811] 
[Epoch 68/93] [Batch 100/539] [D loss: 0.522170] [G loss: 0.116219] [ema: 0.999811] 
[Epoch 68/93] [Batch 200/539] [D loss: 0.556011] [G loss: 0.130541] [ema: 0.999812] 
[Epoch 68/93] [Batch 300/539] [D loss: 0.482684] [G loss: 0.126170] [ema: 0.999812] 
[Epoch 68/93] [Batch 400/539] [D loss: 0.508153] [G loss: 0.124641] [ema: 0.999813] 
[Epoch 68/93] [Batch 500/539] [D loss: 0.529089] [G loss: 0.125489] [ema: 0.999813] 
[Epoch 69/93] [Batch 0/539] [D loss: 0.523779] [G loss: 0.129745] [ema: 0.999814] 
[Epoch 69/93] [Batch 100/539] [D loss: 0.544909] [G loss: 0.121117] [ema: 0.999814] 
[Epoch 69/93] [Batch 200/539] [D loss: 0.515210] [G loss: 0.130754] [ema: 0.999815] 
[Epoch 69/93] [Batch 300/539] [D loss: 0.518587] [G loss: 0.121477] [ema: 0.999815] 
[Epoch 69/93] [Batch 400/539] [D loss: 0.501401] [G loss: 0.121589] [ema: 0.999816] 
[Epoch 69/93] [Batch 500/539] [D loss: 0.562786] [G loss: 0.127309] [ema: 0.999816] 
[Epoch 70/93] [Batch 0/539] [D loss: 0.514217] [G loss: 0.124013] [ema: 0.999816] 
[Epoch 70/93] [Batch 100/539] [D loss: 0.562328] [G loss: 0.115161] [ema: 0.999817] 
[Epoch 70/93] [Batch 200/539] [D loss: 0.540001] [G loss: 0.128642] [ema: 0.999817] 
[Epoch 70/93] [Batch 300/539] [D loss: 0.524400] [G loss: 0.117784] [ema: 0.999818] 
[Epoch 70/93] [Batch 400/539] [D loss: 0.504594] [G loss: 0.138796] [ema: 0.999818] 
[Epoch 70/93] [Batch 500/539] [D loss: 0.489097] [G loss: 0.114502] [ema: 0.999819] 
[Epoch 71/93] [Batch 0/539] [D loss: 0.545613] [G loss: 0.119760] [ema: 0.999819] 
[Epoch 71/93] [Batch 100/539] [D loss: 0.529654] [G loss: 0.130937] [ema: 0.999819] 
[Epoch 71/93] [Batch 200/539] [D loss: 0.514253] [G loss: 0.113489] [ema: 0.999820] 
[Epoch 71/93] [Batch 300/539] [D loss: 0.537909] [G loss: 0.150568] [ema: 0.999820] 
[Epoch 71/93] [Batch 400/539] [D loss: 0.556052] [G loss: 0.120090] [ema: 0.999821] 
[Epoch 71/93] [Batch 500/539] [D loss: 0.568349] [G loss: 0.107890] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_50000_60_100/stand_50000_D_60_2024_10_23_20_08_15/Model



[Epoch 72/93] [Batch 0/539] [D loss: 0.517798] [G loss: 0.115534] [ema: 0.999821] 
[Epoch 72/93] [Batch 100/539] [D loss: 0.530913] [G loss: 0.129411] [ema: 0.999822] 
[Epoch 72/93] [Batch 200/539] [D loss: 0.535586] [G loss: 0.113811] [ema: 0.999822] 
[Epoch 72/93] [Batch 300/539] [D loss: 0.535805] [G loss: 0.124121] [ema: 0.999823] 
[Epoch 72/93] [Batch 400/539] [D loss: 0.551526] [G loss: 0.137613] [ema: 0.999823] 
[Epoch 72/93] [Batch 500/539] [D loss: 0.516125] [G loss: 0.117949] [ema: 0.999824] 
[Epoch 73/93] [Batch 0/539] [D loss: 0.553721] [G loss: 0.123340] [ema: 0.999824] 
[Epoch 73/93] [Batch 100/539] [D loss: 0.512177] [G loss: 0.129944] [ema: 0.999824] 
[Epoch 73/93] [Batch 200/539] [D loss: 0.529240] [G loss: 0.121583] [ema: 0.999825] 
[Epoch 73/93] [Batch 300/539] [D loss: 0.524835] [G loss: 0.118279] [ema: 0.999825] 
[Epoch 73/93] [Batch 400/539] [D loss: 0.536353] [G loss: 0.108682] [ema: 0.999826] 
[Epoch 73/93] [Batch 500/539] [D loss: 0.491510] [G loss: 0.116519] [ema: 0.999826] 
[Epoch 74/93] [Batch 0/539] [D loss: 0.517705] [G loss: 0.120607] [ema: 0.999826] 
[Epoch 74/93] [Batch 100/539] [D loss: 0.500554] [G loss: 0.131113] [ema: 0.999827] 
[Epoch 74/93] [Batch 200/539] [D loss: 0.540888] [G loss: 0.133933] [ema: 0.999827] 
[Epoch 74/93] [Batch 300/539] [D loss: 0.523507] [G loss: 0.125249] [ema: 0.999828] 
[Epoch 74/93] [Batch 400/539] [D loss: 0.517681] [G loss: 0.125083] [ema: 0.999828] 
[Epoch 74/93] [Batch 500/539] [D loss: 0.542903] [G loss: 0.126779] [ema: 0.999828] 
[Epoch 75/93] [Batch 0/539] [D loss: 0.552241] [G loss: 0.132420] [ema: 0.999829] 
[Epoch 75/93] [Batch 100/539] [D loss: 0.516272] [G loss: 0.128244] [ema: 0.999829] 
[Epoch 75/93] [Batch 200/539] [D loss: 0.523357] [G loss: 0.134600] [ema: 0.999829] 
[Epoch 75/93] [Batch 300/539] [D loss: 0.551441] [G loss: 0.120230] [ema: 0.999830] 
[Epoch 75/93] [Batch 400/539] [D loss: 0.514456] [G loss: 0.109109] [ema: 0.999830] 
[Epoch 75/93] [Batch 500/539] [D loss: 0.546987] [G loss: 0.144658] [ema: 0.999831] 
[Epoch 76/93] [Batch 0/539] [D loss: 0.510058] [G loss: 0.136245] [ema: 0.999831] 
[Epoch 76/93] [Batch 100/539] [D loss: 0.536535] [G loss: 0.127012] [ema: 0.999831] 
[Epoch 76/93] [Batch 200/539] [D loss: 0.470703] [G loss: 0.119669] [ema: 0.999832] 
[Epoch 76/93] [Batch 300/539] [D loss: 0.542366] [G loss: 0.116158] [ema: 0.999832] 
[Epoch 76/93] [Batch 400/539] [D loss: 0.478408] [G loss: 0.132927] [ema: 0.999832] 
[Epoch 76/93] [Batch 500/539] [D loss: 0.483265] [G loss: 0.130468] [ema: 0.999833] 
[Epoch 77/93] [Batch 0/539] [D loss: 0.556209] [G loss: 0.119654] [ema: 0.999833] 
[Epoch 77/93] [Batch 100/539] [D loss: 0.524313] [G loss: 0.128666] [ema: 0.999833] 
[Epoch 77/93] [Batch 200/539] [D loss: 0.587724] [G loss: 0.128622] [ema: 0.999834] 
[Epoch 77/93] [Batch 300/539] [D loss: 0.502674] [G loss: 0.111423] [ema: 0.999834] 
[Epoch 77/93] [Batch 400/539] [D loss: 0.540862] [G loss: 0.109087] [ema: 0.999835] 
[Epoch 77/93] [Batch 500/539] [D loss: 0.489498] [G loss: 0.123492] [ema: 0.999835] 
[Epoch 78/93] [Batch 0/539] [D loss: 0.507002] [G loss: 0.119881] [ema: 0.999835] 
[Epoch 78/93] [Batch 100/539] [D loss: 0.547023] [G loss: 0.120084] [ema: 0.999836] 
[Epoch 78/93] [Batch 200/539] [D loss: 0.539847] [G loss: 0.121518] [ema: 0.999836] 
[Epoch 78/93] [Batch 300/539] [D loss: 0.561654] [G loss: 0.128983] [ema: 0.999836] 
[Epoch 78/93] [Batch 400/539] [D loss: 0.518704] [G loss: 0.126263] [ema: 0.999837] 
[Epoch 78/93] [Batch 500/539] [D loss: 0.553944] [G loss: 0.119658] [ema: 0.999837] 
[Epoch 79/93] [Batch 0/539] [D loss: 0.507788] [G loss: 0.108664] [ema: 0.999837] 
[Epoch 79/93] [Batch 100/539] [D loss: 0.547884] [G loss: 0.125212] [ema: 0.999838] 
[Epoch 79/93] [Batch 200/539] [D loss: 0.559222] [G loss: 0.103759] [ema: 0.999838] 
[Epoch 79/93] [Batch 300/539] [D loss: 0.562732] [G loss: 0.123724] [ema: 0.999838] 
[Epoch 79/93] [Batch 400/539] [D loss: 0.526981] [G loss: 0.121169] [ema: 0.999839] 
[Epoch 79/93] [Batch 500/539] [D loss: 0.549913] [G loss: 0.121486] [ema: 0.999839] 
[Epoch 80/93] [Batch 0/539] [D loss: 0.538158] [G loss: 0.133058] [ema: 0.999839] 
[Epoch 80/93] [Batch 100/539] [D loss: 0.525532] [G loss: 0.120783] [ema: 0.999840] 
[Epoch 80/93] [Batch 200/539] [D loss: 0.554736] [G loss: 0.122671] [ema: 0.999840] 
[Epoch 80/93] [Batch 300/539] [D loss: 0.537710] [G loss: 0.114604] [ema: 0.999840] 
[Epoch 80/93] [Batch 400/539] [D loss: 0.520193] [G loss: 0.130136] [ema: 0.999841] 
[Epoch 80/93] [Batch 500/539] [D loss: 0.490421] [G loss: 0.113081] [ema: 0.999841] 
[Epoch 81/93] [Batch 0/539] [D loss: 0.555302] [G loss: 0.124247] [ema: 0.999841] 
[Epoch 81/93] [Batch 100/539] [D loss: 0.569942] [G loss: 0.124264] [ema: 0.999842] 
[Epoch 81/93] [Batch 200/539] [D loss: 0.532924] [G loss: 0.134223] [ema: 0.999842] 
[Epoch 81/93] [Batch 300/539] [D loss: 0.513250] [G loss: 0.141163] [ema: 0.999842] 
[Epoch 81/93] [Batch 400/539] [D loss: 0.557642] [G loss: 0.121070] [ema: 0.999843] 
[Epoch 81/93] [Batch 500/539] [D loss: 0.535469] [G loss: 0.124087] [ema: 0.999843] 
[Epoch 82/93] [Batch 0/539] [D loss: 0.538922] [G loss: 0.126040] [ema: 0.999843] 
[Epoch 82/93] [Batch 100/539] [D loss: 0.544889] [G loss: 0.120609] [ema: 0.999844] 
[Epoch 82/93] [Batch 200/539] [D loss: 0.513011] [G loss: 0.119559] [ema: 0.999844] 
[Epoch 82/93] [Batch 300/539] [D loss: 0.533046] [G loss: 0.122856] [ema: 0.999844] 
[Epoch 82/93] [Batch 400/539] [D loss: 0.529701] [G loss: 0.127318] [ema: 0.999845] 
[Epoch 82/93] [Batch 500/539] [D loss: 0.501716] [G loss: 0.118637] [ema: 0.999845] 
[Epoch 83/93] [Batch 0/539] [D loss: 0.452483] [G loss: 0.137786] [ema: 0.999845] 
[Epoch 83/93] [Batch 100/539] [D loss: 0.532136] [G loss: 0.128144] [ema: 0.999845] 
[Epoch 83/93] [Batch 200/539] [D loss: 0.541162] [G loss: 0.110615] [ema: 0.999846] 
[Epoch 83/93] [Batch 300/539] [D loss: 0.524706] [G loss: 0.110050] [ema: 0.999846] 
[Epoch 83/93] [Batch 400/539] [D loss: 0.512446] [G loss: 0.117129] [ema: 0.999846] 
[Epoch 83/93] [Batch 500/539] [D loss: 0.556378] [G loss: 0.129539] [ema: 0.999847] 
[Epoch 84/93] [Batch 0/539] [D loss: 0.552982] [G loss: 0.122660] [ema: 0.999847] 
[Epoch 84/93] [Batch 100/539] [D loss: 0.531476] [G loss: 0.123583] [ema: 0.999847] 
[Epoch 84/93] [Batch 200/539] [D loss: 0.570311] [G loss: 0.112048] [ema: 0.999848] 
[Epoch 84/93] [Batch 300/539] [D loss: 0.515799] [G loss: 0.130869] [ema: 0.999848] 
[Epoch 84/93] [Batch 400/539] [D loss: 0.556928] [G loss: 0.106672] [ema: 0.999848] 
[Epoch 84/93] [Batch 500/539] [D loss: 0.525958] [G loss: 0.108956] [ema: 0.999849] 
[Epoch 85/93] [Batch 0/539] [D loss: 0.541879] [G loss: 0.115901] [ema: 0.999849] 
[Epoch 85/93] [Batch 100/539] [D loss: 0.545960] [G loss: 0.108070] [ema: 0.999849] 
[Epoch 85/93] [Batch 200/539] [D loss: 0.590629] [G loss: 0.129214] [ema: 0.999849] 
[Epoch 85/93] [Batch 300/539] [D loss: 0.541418] [G loss: 0.108495] [ema: 0.999850] 
[Epoch 85/93] [Batch 400/539] [D loss: 0.522269] [G loss: 0.105476] [ema: 0.999850] 
[Epoch 85/93] [Batch 500/539] [D loss: 0.524565] [G loss: 0.115786] [ema: 0.999850] 
[Epoch 86/93] [Batch 0/539] [D loss: 0.536010] [G loss: 0.124030] [ema: 0.999850] 
[Epoch 86/93] [Batch 100/539] [D loss: 0.553894] [G loss: 0.122436] [ema: 0.999851] 
[Epoch 86/93] [Batch 200/539] [D loss: 0.540720] [G loss: 0.127704] [ema: 0.999851] 
[Epoch 86/93] [Batch 300/539] [D loss: 0.562074] [G loss: 0.125898] [ema: 0.999851] 
[Epoch 86/93] [Batch 400/539] [D loss: 0.530167] [G loss: 0.122776] [ema: 0.999852] 
[Epoch 86/93] [Batch 500/539] [D loss: 0.532038] [G loss: 0.108666] [ema: 0.999852] 
[Epoch 87/93] [Batch 0/539] [D loss: 0.511657] [G loss: 0.133710] [ema: 0.999852] 
[Epoch 87/93] [Batch 100/539] [D loss: 0.501259] [G loss: 0.147289] [ema: 0.999853] 
[Epoch 87/93] [Batch 200/539] [D loss: 0.535756] [G loss: 0.106948] [ema: 0.999853] 
[Epoch 87/93] [Batch 300/539] [D loss: 0.542026] [G loss: 0.105307] [ema: 0.999853] 
[Epoch 87/93] [Batch 400/539] [D loss: 0.547869] [G loss: 0.113976] [ema: 0.999853] 
[Epoch 87/93] [Batch 500/539] [D loss: 0.513981] [G loss: 0.121572] [ema: 0.999854] 
[Epoch 88/93] [Batch 0/539] [D loss: 0.531160] [G loss: 0.123879] [ema: 0.999854] 
[Epoch 88/93] [Batch 100/539] [D loss: 0.534663] [G loss: 0.110824] [ema: 0.999854] 
[Epoch 88/93] [Batch 200/539] [D loss: 0.542324] [G loss: 0.129595] [ema: 0.999854] 
[Epoch 88/93] [Batch 300/539] [D loss: 0.530778] [G loss: 0.125918] [ema: 0.999855] 
[Epoch 88/93] [Batch 400/539] [D loss: 0.485595] [G loss: 0.118050] [ema: 0.999855] 
[Epoch 88/93] [Batch 500/539] [D loss: 0.501528] [G loss: 0.128926] [ema: 0.999855] 
[Epoch 89/93] [Batch 0/539] [D loss: 0.487146] [G loss: 0.132194] [ema: 0.999856] 
[Epoch 89/93] [Batch 100/539] [D loss: 0.546430] [G loss: 0.123092] [ema: 0.999856] 
[Epoch 89/93] [Batch 200/539] [D loss: 0.532255] [G loss: 0.121208] [ema: 0.999856] 
[Epoch 89/93] [Batch 300/539] [D loss: 0.564649] [G loss: 0.128010] [ema: 0.999856] 
[Epoch 89/93] [Batch 400/539] [D loss: 0.554261] [G loss: 0.115126] [ema: 0.999857] 
[Epoch 89/93] [Batch 500/539] [D loss: 0.514624] [G loss: 0.133972] [ema: 0.999857] 
[Epoch 90/93] [Batch 0/539] [D loss: 0.516091] [G loss: 0.135470] [ema: 0.999857] 
[Epoch 90/93] [Batch 100/539] [D loss: 0.557787] [G loss: 0.114450] [ema: 0.999857] 
[Epoch 90/93] [Batch 200/539] [D loss: 0.530265] [G loss: 0.139194] [ema: 0.999858] 
[Epoch 90/93] [Batch 300/539] [D loss: 0.496254] [G loss: 0.121947] [ema: 0.999858] 
[Epoch 90/93] [Batch 400/539] [D loss: 0.539563] [G loss: 0.114744] [ema: 0.999858] 
[Epoch 90/93] [Batch 500/539] [D loss: 0.540990] [G loss: 0.111456] [ema: 0.999859] 
[Epoch 91/93] [Batch 0/539] [D loss: 0.524899] [G loss: 0.120054] [ema: 0.999859] 
[Epoch 91/93] [Batch 100/539] [D loss: 0.536839] [G loss: 0.107229] [ema: 0.999859] 
[Epoch 91/93] [Batch 200/539] [D loss: 0.524605] [G loss: 0.125001] [ema: 0.999859] 
[Epoch 91/93] [Batch 300/539] [D loss: 0.547472] [G loss: 0.096523] [ema: 0.999860] 
[Epoch 91/93] [Batch 400/539] [D loss: 0.529943] [G loss: 0.113293] [ema: 0.999860] 
[Epoch 91/93] [Batch 500/539] [D loss: 0.518716] [G loss: 0.108865] [ema: 0.999860] 
[Epoch 92/93] [Batch 0/539] [D loss: 0.518015] [G loss: 0.114327] [ema: 0.999860] 
[Epoch 92/93] [Batch 100/539] [D loss: 0.534343] [G loss: 0.127342] [ema: 0.999861] 
[Epoch 92/93] [Batch 200/539] [D loss: 0.512262] [G loss: 0.116541] [ema: 0.999861] 
[Epoch 92/93] [Batch 300/539] [D loss: 0.516428] [G loss: 0.120162] [ema: 0.999861] 
[Epoch 92/93] [Batch 400/539] [D loss: 0.535367] [G loss: 0.113111] [ema: 0.999861] 
[Epoch 92/93] [Batch 500/539] [D loss: 0.565771] [G loss: 0.112525] [ema: 0.999862] 
