
 Starting training
Total of classes being trained: 6

['MotionSense_DAGHAR_Multiclass.csv', 'RealWorld_thigh_DAGHAR_Multiclass.csv', 'WISDM_DAGHAR_Multiclass.csv', 'UCI_DAGHAR_Multiclass.csv', 'RealWorld_waist_DAGHAR_Multiclass.csv', 'KuHar_DAGHAR_Multiclass.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
MotionSense_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
MotionSense_DAGHAR_Multiclass
daghar
return single class data and labels, class is MotionSense_DAGHAR_Multiclass
data shape is (7116, 6, 1, 30)
label shape is (7116,)
445
Epochs between checkpoint: 29



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_30_100/MotionSense_DAGHAR_Multiclass_50000_D_30_2024_10_28_18_44_21/Model



[Epoch 0/113] [Batch 0/445] [D loss: 4.937519] [G loss: 2.520982] [ema: 0.000000] 
[Epoch 0/113] [Batch 100/445] [D loss: 0.399313] [G loss: 0.264790] [ema: 0.933033] 
[Epoch 0/113] [Batch 200/445] [D loss: 0.581387] [G loss: 0.129970] [ema: 0.965936] 
[Epoch 0/113] [Batch 300/445] [D loss: 0.404699] [G loss: 0.192282] [ema: 0.977160] 
[Epoch 0/113] [Batch 400/445] [D loss: 0.469857] [G loss: 0.101457] [ema: 0.982821] 
[Epoch 1/113] [Batch 0/445] [D loss: 0.520126] [G loss: 0.131386] [ema: 0.984544] 
[Epoch 1/113] [Batch 100/445] [D loss: 0.489981] [G loss: 0.158447] [ema: 0.987362] 
[Epoch 1/113] [Batch 200/445] [D loss: 0.413714] [G loss: 0.164485] [ema: 0.989311] 
[Epoch 1/113] [Batch 300/445] [D loss: 0.472857] [G loss: 0.178385] [ema: 0.990739] 
[Epoch 1/113] [Batch 400/445] [D loss: 0.453423] [G loss: 0.163389] [ema: 0.991831] 
[Epoch 2/113] [Batch 0/445] [D loss: 0.416363] [G loss: 0.165575] [ema: 0.992242] 
[Epoch 2/113] [Batch 100/445] [D loss: 0.397013] [G loss: 0.153304] [ema: 0.993023] 
[Epoch 2/113] [Batch 200/445] [D loss: 0.453147] [G loss: 0.158970] [ema: 0.993661] 
[Epoch 2/113] [Batch 300/445] [D loss: 0.425116] [G loss: 0.160899] [ema: 0.994192] 
[Epoch 2/113] [Batch 400/445] [D loss: 0.407045] [G loss: 0.196176] [ema: 0.994641] 
[Epoch 3/113] [Batch 0/445] [D loss: 0.430939] [G loss: 0.179054] [ema: 0.994821] 
[Epoch 3/113] [Batch 100/445] [D loss: 0.472678] [G loss: 0.165153] [ema: 0.995181] 
[Epoch 3/113] [Batch 200/445] [D loss: 0.452344] [G loss: 0.169966] [ema: 0.995495] 
[Epoch 3/113] [Batch 300/445] [D loss: 0.433450] [G loss: 0.189413] [ema: 0.995770] 
[Epoch 3/113] [Batch 400/445] [D loss: 0.437065] [G loss: 0.175748] [ema: 0.996013] 
[Epoch 4/113] [Batch 0/445] [D loss: 0.412552] [G loss: 0.173294] [ema: 0.996113] 
[Epoch 4/113] [Batch 100/445] [D loss: 0.446037] [G loss: 0.155163] [ema: 0.996320] 
[Epoch 4/113] [Batch 200/445] [D loss: 0.420427] [G loss: 0.151533] [ema: 0.996505] 
[Epoch 4/113] [Batch 300/445] [D loss: 0.351259] [G loss: 0.216866] [ema: 0.996673] 
[Epoch 4/113] [Batch 400/445] [D loss: 0.328657] [G loss: 0.205613] [ema: 0.996825] 
[Epoch 5/113] [Batch 0/445] [D loss: 0.372210] [G loss: 0.194584] [ema: 0.996890] 
[Epoch 5/113] [Batch 100/445] [D loss: 0.443301] [G loss: 0.203276] [ema: 0.997023] 
[Epoch 5/113] [Batch 200/445] [D loss: 0.425625] [G loss: 0.165262] [ema: 0.997146] 
[Epoch 5/113] [Batch 300/445] [D loss: 0.360746] [G loss: 0.194964] [ema: 0.997259] 
[Epoch 5/113] [Batch 400/445] [D loss: 0.304262] [G loss: 0.199551] [ema: 0.997363] 
[Epoch 6/113] [Batch 0/445] [D loss: 0.344417] [G loss: 0.199580] [ema: 0.997407] 
[Epoch 6/113] [Batch 100/445] [D loss: 0.347841] [G loss: 0.208458] [ema: 0.997501] 
[Epoch 6/113] [Batch 200/445] [D loss: 0.291916] [G loss: 0.221513] [ema: 0.997588] 
[Epoch 6/113] [Batch 300/445] [D loss: 0.354541] [G loss: 0.249086] [ema: 0.997669] 
[Epoch 6/113] [Batch 400/445] [D loss: 0.377809] [G loss: 0.253374] [ema: 0.997745] 
[Epoch 7/113] [Batch 0/445] [D loss: 0.352811] [G loss: 0.195841] [ema: 0.997777] 
[Epoch 7/113] [Batch 100/445] [D loss: 0.350116] [G loss: 0.209289] [ema: 0.997846] 
[Epoch 7/113] [Batch 200/445] [D loss: 0.377480] [G loss: 0.190250] [ema: 0.997911] 
[Epoch 7/113] [Batch 300/445] [D loss: 0.399995] [G loss: 0.161759] [ema: 0.997972] 
[Epoch 7/113] [Batch 400/445] [D loss: 0.380440] [G loss: 0.181130] [ema: 0.998030] 
[Epoch 8/113] [Batch 0/445] [D loss: 0.479538] [G loss: 0.203548] [ema: 0.998055] 
[Epoch 8/113] [Batch 100/445] [D loss: 0.447504] [G loss: 0.174301] [ema: 0.998108] 
[Epoch 8/113] [Batch 200/445] [D loss: 0.427055] [G loss: 0.174552] [ema: 0.998158] 
[Epoch 8/113] [Batch 300/445] [D loss: 0.394323] [G loss: 0.144420] [ema: 0.998206] 
[Epoch 8/113] [Batch 400/445] [D loss: 0.343591] [G loss: 0.185876] [ema: 0.998251] 
[Epoch 9/113] [Batch 0/445] [D loss: 0.364828] [G loss: 0.177454] [ema: 0.998271] 
[Epoch 9/113] [Batch 100/445] [D loss: 0.355652] [G loss: 0.193725] [ema: 0.998313] 
[Epoch 9/113] [Batch 200/445] [D loss: 0.296070] [G loss: 0.171189] [ema: 0.998353] 
[Epoch 9/113] [Batch 300/445] [D loss: 0.325338] [G loss: 0.181209] [ema: 0.998391] 
[Epoch 9/113] [Batch 400/445] [D loss: 0.410362] [G loss: 0.201462] [ema: 0.998428] 
[Epoch 10/113] [Batch 0/445] [D loss: 0.397774] [G loss: 0.239055] [ema: 0.998444] 
[Epoch 10/113] [Batch 100/445] [D loss: 0.416537] [G loss: 0.205287] [ema: 0.998478] 
[Epoch 10/113] [Batch 200/445] [D loss: 0.344205] [G loss: 0.172447] [ema: 0.998510] 
[Epoch 10/113] [Batch 300/445] [D loss: 0.387362] [G loss: 0.216377] [ema: 0.998542] 
[Epoch 10/113] [Batch 400/445] [D loss: 0.306759] [G loss: 0.231196] [ema: 0.998572] 
[Epoch 11/113] [Batch 0/445] [D loss: 0.359627] [G loss: 0.224146] [ema: 0.998585] 
[Epoch 11/113] [Batch 100/445] [D loss: 0.353802] [G loss: 0.236042] [ema: 0.998613] 
[Epoch 11/113] [Batch 200/445] [D loss: 0.363281] [G loss: 0.170369] [ema: 0.998640] 
[Epoch 11/113] [Batch 300/445] [D loss: 0.393162] [G loss: 0.172079] [ema: 0.998667] 
[Epoch 11/113] [Batch 400/445] [D loss: 0.355200] [G loss: 0.170018] [ema: 0.998692] 
[Epoch 12/113] [Batch 0/445] [D loss: 0.367552] [G loss: 0.231283] [ema: 0.998703] 
[Epoch 12/113] [Batch 100/445] [D loss: 0.403855] [G loss: 0.200949] [ema: 0.998727] 
[Epoch 12/113] [Batch 200/445] [D loss: 0.351973] [G loss: 0.208910] [ema: 0.998750] 
[Epoch 12/113] [Batch 300/445] [D loss: 0.379273] [G loss: 0.243368] [ema: 0.998772] 
[Epoch 12/113] [Batch 400/445] [D loss: 0.334678] [G loss: 0.226501] [ema: 0.998793] 
[Epoch 13/113] [Batch 0/445] [D loss: 0.357855] [G loss: 0.226895] [ema: 0.998803] 
[Epoch 13/113] [Batch 100/445] [D loss: 0.399611] [G loss: 0.243853] [ema: 0.998823] 
[Epoch 13/113] [Batch 200/445] [D loss: 0.379555] [G loss: 0.236682] [ema: 0.998843] 
[Epoch 13/113] [Batch 300/445] [D loss: 0.347562] [G loss: 0.183708] [ema: 0.998862] 
[Epoch 13/113] [Batch 400/445] [D loss: 0.375560] [G loss: 0.198951] [ema: 0.998880] 
[Epoch 14/113] [Batch 0/445] [D loss: 0.339709] [G loss: 0.197229] [ema: 0.998888] 
[Epoch 14/113] [Batch 100/445] [D loss: 0.413848] [G loss: 0.193094] [ema: 0.998906] 
[Epoch 14/113] [Batch 200/445] [D loss: 0.323391] [G loss: 0.223717] [ema: 0.998923] 
[Epoch 14/113] [Batch 300/445] [D loss: 0.308082] [G loss: 0.240471] [ema: 0.998939] 
[Epoch 14/113] [Batch 400/445] [D loss: 0.374383] [G loss: 0.200445] [ema: 0.998955] 
[Epoch 15/113] [Batch 0/445] [D loss: 0.347614] [G loss: 0.187375] [ema: 0.998962] 
[Epoch 15/113] [Batch 100/445] [D loss: 0.343276] [G loss: 0.220503] [ema: 0.998977] 
[Epoch 15/113] [Batch 200/445] [D loss: 0.333520] [G loss: 0.221117] [ema: 0.998992] 
[Epoch 15/113] [Batch 300/445] [D loss: 0.379781] [G loss: 0.243740] [ema: 0.999007] 
[Epoch 15/113] [Batch 400/445] [D loss: 0.349010] [G loss: 0.223871] [ema: 0.999021] 
[Epoch 16/113] [Batch 0/445] [D loss: 0.311488] [G loss: 0.218297] [ema: 0.999027] 
[Epoch 16/113] [Batch 100/445] [D loss: 0.309760] [G loss: 0.212539] [ema: 0.999040] 
[Epoch 16/113] [Batch 200/445] [D loss: 0.274845] [G loss: 0.232826] [ema: 0.999054] 
[Epoch 16/113] [Batch 300/445] [D loss: 0.283118] [G loss: 0.242155] [ema: 0.999066] 
[Epoch 16/113] [Batch 400/445] [D loss: 0.315495] [G loss: 0.201701] [ema: 0.999079] 
[Epoch 17/113] [Batch 0/445] [D loss: 0.290970] [G loss: 0.232990] [ema: 0.999084] 
[Epoch 17/113] [Batch 100/445] [D loss: 0.304589] [G loss: 0.208334] [ema: 0.999096] 
[Epoch 17/113] [Batch 200/445] [D loss: 0.322603] [G loss: 0.195351] [ema: 0.999108] 
[Epoch 17/113] [Batch 300/445] [D loss: 0.297131] [G loss: 0.231971] [ema: 0.999119] 
[Epoch 17/113] [Batch 400/445] [D loss: 0.326454] [G loss: 0.214481] [ema: 0.999130] 
[Epoch 18/113] [Batch 0/445] [D loss: 0.355379] [G loss: 0.221733] [ema: 0.999135] 
[Epoch 18/113] [Batch 100/445] [D loss: 0.308387] [G loss: 0.251727] [ema: 0.999146] 
[Epoch 18/113] [Batch 200/445] [D loss: 0.293190] [G loss: 0.235007] [ema: 0.999156] 
[Epoch 18/113] [Batch 300/445] [D loss: 0.333729] [G loss: 0.230712] [ema: 0.999166] 
[Epoch 18/113] [Batch 400/445] [D loss: 0.322511] [G loss: 0.214105] [ema: 0.999176] 
[Epoch 19/113] [Batch 0/445] [D loss: 0.300359] [G loss: 0.213710] [ema: 0.999181] 
[Epoch 19/113] [Batch 100/445] [D loss: 0.354262] [G loss: 0.220978] [ema: 0.999190] 
[Epoch 19/113] [Batch 200/445] [D loss: 0.312887] [G loss: 0.227783] [ema: 0.999199] 
[Epoch 19/113] [Batch 300/445] [D loss: 0.389228] [G loss: 0.198515] [ema: 0.999209] 
[Epoch 19/113] [Batch 400/445] [D loss: 0.320853] [G loss: 0.230225] [ema: 0.999218] 
[Epoch 20/113] [Batch 0/445] [D loss: 0.312101] [G loss: 0.225669] [ema: 0.999221] 
[Epoch 20/113] [Batch 100/445] [D loss: 0.346998] [G loss: 0.186043] [ema: 0.999230] 
[Epoch 20/113] [Batch 200/445] [D loss: 0.322128] [G loss: 0.224295] [ema: 0.999239] 
[Epoch 20/113] [Batch 300/445] [D loss: 0.325656] [G loss: 0.213763] [ema: 0.999247] 
[Epoch 20/113] [Batch 400/445] [D loss: 0.313697] [G loss: 0.224591] [ema: 0.999255] 
[Epoch 21/113] [Batch 0/445] [D loss: 0.336767] [G loss: 0.225785] [ema: 0.999259] 
[Epoch 21/113] [Batch 100/445] [D loss: 0.295422] [G loss: 0.233724] [ema: 0.999266] 
[Epoch 21/113] [Batch 200/445] [D loss: 0.318788] [G loss: 0.208351] [ema: 0.999274] 
[Epoch 21/113] [Batch 300/445] [D loss: 0.322316] [G loss: 0.238281] [ema: 0.999282] 
[Epoch 21/113] [Batch 400/445] [D loss: 0.303799] [G loss: 0.219488] [ema: 0.999289] 
[Epoch 22/113] [Batch 0/445] [D loss: 0.308121] [G loss: 0.243234] [ema: 0.999292] 
[Epoch 22/113] [Batch 100/445] [D loss: 0.308600] [G loss: 0.204970] [ema: 0.999299] 
[Epoch 22/113] [Batch 200/445] [D loss: 0.331123] [G loss: 0.224249] [ema: 0.999306] 
[Epoch 22/113] [Batch 300/445] [D loss: 0.275176] [G loss: 0.246736] [ema: 0.999313] 
[Epoch 22/113] [Batch 400/445] [D loss: 0.299260] [G loss: 0.252826] [ema: 0.999320] 
[Epoch 23/113] [Batch 0/445] [D loss: 0.329841] [G loss: 0.223017] [ema: 0.999323] 
[Epoch 23/113] [Batch 100/445] [D loss: 0.315158] [G loss: 0.233297] [ema: 0.999330] 
[Epoch 23/113] [Batch 200/445] [D loss: 0.310503] [G loss: 0.205341] [ema: 0.999336] 
[Epoch 23/113] [Batch 300/445] [D loss: 0.260080] [G loss: 0.233995] [ema: 0.999342] 
[Epoch 23/113] [Batch 400/445] [D loss: 0.334771] [G loss: 0.230096] [ema: 0.999348] 
[Epoch 24/113] [Batch 0/445] [D loss: 0.286614] [G loss: 0.211968] [ema: 0.999351] 
[Epoch 24/113] [Batch 100/445] [D loss: 0.308334] [G loss: 0.218808] [ema: 0.999357] 
[Epoch 24/113] [Batch 200/445] [D loss: 0.377013] [G loss: 0.224443] [ema: 0.999363] 
[Epoch 24/113] [Batch 300/445] [D loss: 0.326212] [G loss: 0.245705] [ema: 0.999369] 
[Epoch 24/113] [Batch 400/445] [D loss: 0.294707] [G loss: 0.233864] [ema: 0.999375] 
[Epoch 25/113] [Batch 0/445] [D loss: 0.340521] [G loss: 0.234559] [ema: 0.999377] 
[Epoch 25/113] [Batch 100/445] [D loss: 0.386524] [G loss: 0.193518] [ema: 0.999383] 
[Epoch 25/113] [Batch 200/445] [D loss: 0.350268] [G loss: 0.220240] [ema: 0.999388] 
[Epoch 25/113] [Batch 300/445] [D loss: 0.400932] [G loss: 0.216648] [ema: 0.999393] 
[Epoch 25/113] [Batch 400/445] [D loss: 0.376648] [G loss: 0.199300] [ema: 0.999399] 
[Epoch 26/113] [Batch 0/445] [D loss: 0.367697] [G loss: 0.177682] [ema: 0.999401] 
[Epoch 26/113] [Batch 100/445] [D loss: 0.330002] [G loss: 0.223696] [ema: 0.999406] 
[Epoch 26/113] [Batch 200/445] [D loss: 0.325722] [G loss: 0.216789] [ema: 0.999411] 
[Epoch 26/113] [Batch 300/445] [D loss: 0.375821] [G loss: 0.210160] [ema: 0.999416] 
[Epoch 26/113] [Batch 400/445] [D loss: 0.318203] [G loss: 0.200381] [ema: 0.999421] 
[Epoch 27/113] [Batch 0/445] [D loss: 0.323533] [G loss: 0.207476] [ema: 0.999423] 
[Epoch 27/113] [Batch 100/445] [D loss: 0.328921] [G loss: 0.224126] [ema: 0.999428] 
[Epoch 27/113] [Batch 200/445] [D loss: 0.359555] [G loss: 0.196889] [ema: 0.999433] 
[Epoch 27/113] [Batch 300/445] [D loss: 0.337989] [G loss: 0.194210] [ema: 0.999437] 
[Epoch 27/113] [Batch 400/445] [D loss: 0.304101] [G loss: 0.213025] [ema: 0.999442] 
[Epoch 28/113] [Batch 0/445] [D loss: 0.325510] [G loss: 0.172523] [ema: 0.999444] 
[Epoch 28/113] [Batch 100/445] [D loss: 0.308689] [G loss: 0.224064] [ema: 0.999448] 
[Epoch 28/113] [Batch 200/445] [D loss: 0.360088] [G loss: 0.215149] [ema: 0.999453] 
[Epoch 28/113] [Batch 300/445] [D loss: 0.291344] [G loss: 0.247285] [ema: 0.999457] 
[Epoch 28/113] [Batch 400/445] [D loss: 0.379448] [G loss: 0.203813] [ema: 0.999461] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_30_100/MotionSense_DAGHAR_Multiclass_50000_D_30_2024_10_28_18_44_21/Model



[Epoch 29/113] [Batch 0/445] [D loss: 0.341132] [G loss: 0.185764] [ema: 0.999463] 
[Epoch 29/113] [Batch 100/445] [D loss: 0.317307] [G loss: 0.235719] [ema: 0.999467] 
[Epoch 29/113] [Batch 200/445] [D loss: 0.335561] [G loss: 0.213379] [ema: 0.999471] 
[Epoch 29/113] [Batch 300/445] [D loss: 0.316440] [G loss: 0.206311] [ema: 0.999475] 
[Epoch 29/113] [Batch 400/445] [D loss: 0.296037] [G loss: 0.205108] [ema: 0.999479] 
[Epoch 30/113] [Batch 0/445] [D loss: 0.288096] [G loss: 0.244136] [ema: 0.999481] 
[Epoch 30/113] [Batch 100/445] [D loss: 0.295070] [G loss: 0.221238] [ema: 0.999485] 
[Epoch 30/113] [Batch 200/445] [D loss: 0.292339] [G loss: 0.227614] [ema: 0.999489] 
[Epoch 30/113] [Batch 300/445] [D loss: 0.282886] [G loss: 0.243375] [ema: 0.999492] 
[Epoch 30/113] [Batch 400/445] [D loss: 0.288326] [G loss: 0.258696] [ema: 0.999496] 
[Epoch 31/113] [Batch 0/445] [D loss: 0.284998] [G loss: 0.233039] [ema: 0.999498] 
[Epoch 31/113] [Batch 100/445] [D loss: 0.310023] [G loss: 0.258804] [ema: 0.999501] 
[Epoch 31/113] [Batch 200/445] [D loss: 0.308429] [G loss: 0.210162] [ema: 0.999505] 
[Epoch 31/113] [Batch 300/445] [D loss: 0.312708] [G loss: 0.227349] [ema: 0.999508] 
[Epoch 31/113] [Batch 400/445] [D loss: 0.312207] [G loss: 0.203799] [ema: 0.999512] 
[Epoch 32/113] [Batch 0/445] [D loss: 0.398439] [G loss: 0.206333] [ema: 0.999513] 
[Epoch 32/113] [Batch 100/445] [D loss: 0.295611] [G loss: 0.208210] [ema: 0.999517] 
[Epoch 32/113] [Batch 200/445] [D loss: 0.364612] [G loss: 0.229598] [ema: 0.999520] 
[Epoch 32/113] [Batch 300/445] [D loss: 0.325897] [G loss: 0.190004] [ema: 0.999523] 
[Epoch 32/113] [Batch 400/445] [D loss: 0.341235] [G loss: 0.201255] [ema: 0.999527] 
[Epoch 33/113] [Batch 0/445] [D loss: 0.351143] [G loss: 0.199481] [ema: 0.999528] 
[Epoch 33/113] [Batch 100/445] [D loss: 0.342456] [G loss: 0.194943] [ema: 0.999531] 
[Epoch 33/113] [Batch 200/445] [D loss: 0.385019] [G loss: 0.170573] [ema: 0.999534] 
[Epoch 33/113] [Batch 300/445] [D loss: 0.357432] [G loss: 0.183544] [ema: 0.999538] 
[Epoch 33/113] [Batch 400/445] [D loss: 0.355749] [G loss: 0.207143] [ema: 0.999541] 
[Epoch 34/113] [Batch 0/445] [D loss: 0.407441] [G loss: 0.191262] [ema: 0.999542] 
[Epoch 34/113] [Batch 100/445] [D loss: 0.350293] [G loss: 0.171972] [ema: 0.999545] 
[Epoch 34/113] [Batch 200/445] [D loss: 0.329875] [G loss: 0.207432] [ema: 0.999548] 
[Epoch 34/113] [Batch 300/445] [D loss: 0.302288] [G loss: 0.217646] [ema: 0.999551] 
[Epoch 34/113] [Batch 400/445] [D loss: 0.380105] [G loss: 0.191867] [ema: 0.999554] 
[Epoch 35/113] [Batch 0/445] [D loss: 0.370059] [G loss: 0.205993] [ema: 0.999555] 
[Epoch 35/113] [Batch 100/445] [D loss: 0.292420] [G loss: 0.211117] [ema: 0.999558] 
[Epoch 35/113] [Batch 200/445] [D loss: 0.441306] [G loss: 0.216598] [ema: 0.999561] 
[Epoch 35/113] [Batch 300/445] [D loss: 0.344027] [G loss: 0.184957] [ema: 0.999563] 
[Epoch 35/113] [Batch 400/445] [D loss: 0.303541] [G loss: 0.236937] [ema: 0.999566] 
[Epoch 36/113] [Batch 0/445] [D loss: 0.294522] [G loss: 0.249172] [ema: 0.999567] 
[Epoch 36/113] [Batch 100/445] [D loss: 0.337014] [G loss: 0.225696] [ema: 0.999570] 
[Epoch 36/113] [Batch 200/445] [D loss: 0.337923] [G loss: 0.201513] [ema: 0.999573] 
[Epoch 36/113] [Batch 300/445] [D loss: 0.329886] [G loss: 0.208570] [ema: 0.999575] 
[Epoch 36/113] [Batch 400/445] [D loss: 0.365919] [G loss: 0.183107] [ema: 0.999578] 
[Epoch 37/113] [Batch 0/445] [D loss: 0.369595] [G loss: 0.235542] [ema: 0.999579] 
[Epoch 37/113] [Batch 100/445] [D loss: 0.344908] [G loss: 0.209683] [ema: 0.999582] 
[Epoch 37/113] [Batch 200/445] [D loss: 0.344642] [G loss: 0.204172] [ema: 0.999584] 
[Epoch 37/113] [Batch 300/445] [D loss: 0.297020] [G loss: 0.225288] [ema: 0.999587] 
[Epoch 37/113] [Batch 400/445] [D loss: 0.299889] [G loss: 0.203976] [ema: 0.999589] 
[Epoch 38/113] [Batch 0/445] [D loss: 0.309422] [G loss: 0.234037] [ema: 0.999590] 
[Epoch 38/113] [Batch 100/445] [D loss: 0.285247] [G loss: 0.239788] [ema: 0.999593] 
[Epoch 38/113] [Batch 200/445] [D loss: 0.357422] [G loss: 0.227834] [ema: 0.999595] 
[Epoch 38/113] [Batch 300/445] [D loss: 0.287982] [G loss: 0.224424] [ema: 0.999597] 
[Epoch 38/113] [Batch 400/445] [D loss: 0.353047] [G loss: 0.218860] [ema: 0.999600] 
[Epoch 39/113] [Batch 0/445] [D loss: 0.254429] [G loss: 0.243057] [ema: 0.999601] 
[Epoch 39/113] [Batch 100/445] [D loss: 0.292750] [G loss: 0.223978] [ema: 0.999603] 
[Epoch 39/113] [Batch 200/445] [D loss: 0.306449] [G loss: 0.202943] [ema: 0.999605] 
[Epoch 39/113] [Batch 300/445] [D loss: 0.296820] [G loss: 0.197123] [ema: 0.999607] 
[Epoch 39/113] [Batch 400/445] [D loss: 0.311002] [G loss: 0.223469] [ema: 0.999610] 
[Epoch 40/113] [Batch 0/445] [D loss: 0.346767] [G loss: 0.208598] [ema: 0.999611] 
[Epoch 40/113] [Batch 100/445] [D loss: 0.321632] [G loss: 0.208696] [ema: 0.999613] 
[Epoch 40/113] [Batch 200/445] [D loss: 0.404185] [G loss: 0.204579] [ema: 0.999615] 
[Epoch 40/113] [Batch 300/445] [D loss: 0.435553] [G loss: 0.208567] [ema: 0.999617] 
[Epoch 40/113] [Batch 400/445] [D loss: 0.385479] [G loss: 0.187226] [ema: 0.999619] 
[Epoch 41/113] [Batch 0/445] [D loss: 0.373699] [G loss: 0.200095] [ema: 0.999620] 
[Epoch 41/113] [Batch 100/445] [D loss: 0.333530] [G loss: 0.205942] [ema: 0.999622] 
[Epoch 41/113] [Batch 200/445] [D loss: 0.389497] [G loss: 0.194885] [ema: 0.999624] 
[Epoch 41/113] [Batch 300/445] [D loss: 0.363608] [G loss: 0.190741] [ema: 0.999626] 
[Epoch 41/113] [Batch 400/445] [D loss: 0.415160] [G loss: 0.178620] [ema: 0.999628] 
[Epoch 42/113] [Batch 0/445] [D loss: 0.338758] [G loss: 0.214980] [ema: 0.999629] 
[Epoch 42/113] [Batch 100/445] [D loss: 0.380341] [G loss: 0.198290] [ema: 0.999631] 
[Epoch 42/113] [Batch 200/445] [D loss: 0.297598] [G loss: 0.219713] [ema: 0.999633] 
[Epoch 42/113] [Batch 300/445] [D loss: 0.340520] [G loss: 0.202353] [ema: 0.999635] 
[Epoch 42/113] [Batch 400/445] [D loss: 0.347100] [G loss: 0.207796] [ema: 0.999637] 
[Epoch 43/113] [Batch 0/445] [D loss: 0.290831] [G loss: 0.213194] [ema: 0.999638] 
[Epoch 43/113] [Batch 100/445] [D loss: 0.367719] [G loss: 0.231802] [ema: 0.999640] 
[Epoch 43/113] [Batch 200/445] [D loss: 0.355470] [G loss: 0.206557] [ema: 0.999642] 
[Epoch 43/113] [Batch 300/445] [D loss: 0.385199] [G loss: 0.196858] [ema: 0.999643] 
[Epoch 43/113] [Batch 400/445] [D loss: 0.319407] [G loss: 0.193004] [ema: 0.999645] 
[Epoch 44/113] [Batch 0/445] [D loss: 0.373903] [G loss: 0.221399] [ema: 0.999646] 
[Epoch 44/113] [Batch 100/445] [D loss: 0.330336] [G loss: 0.208659] [ema: 0.999648] 
[Epoch 44/113] [Batch 200/445] [D loss: 0.328321] [G loss: 0.230226] [ema: 0.999650] 
[Epoch 44/113] [Batch 300/445] [D loss: 0.360620] [G loss: 0.219706] [ema: 0.999651] 
[Epoch 44/113] [Batch 400/445] [D loss: 0.362783] [G loss: 0.210251] [ema: 0.999653] 
[Epoch 45/113] [Batch 0/445] [D loss: 0.356380] [G loss: 0.205618] [ema: 0.999654] 
[Epoch 45/113] [Batch 100/445] [D loss: 0.346822] [G loss: 0.208326] [ema: 0.999656] 
[Epoch 45/113] [Batch 200/445] [D loss: 0.338087] [G loss: 0.192656] [ema: 0.999657] 
[Epoch 45/113] [Batch 300/445] [D loss: 0.337860] [G loss: 0.214427] [ema: 0.999659] 
[Epoch 45/113] [Batch 400/445] [D loss: 0.318102] [G loss: 0.211973] [ema: 0.999661] 
[Epoch 46/113] [Batch 0/445] [D loss: 0.329157] [G loss: 0.208136] [ema: 0.999661] 
[Epoch 46/113] [Batch 100/445] [D loss: 0.324532] [G loss: 0.235473] [ema: 0.999663] 
[Epoch 46/113] [Batch 200/445] [D loss: 0.368112] [G loss: 0.204031] [ema: 0.999665] 
[Epoch 46/113] [Batch 300/445] [D loss: 0.372985] [G loss: 0.194892] [ema: 0.999666] 
[Epoch 46/113] [Batch 400/445] [D loss: 0.360106] [G loss: 0.179715] [ema: 0.999668] 
[Epoch 47/113] [Batch 0/445] [D loss: 0.343268] [G loss: 0.192111] [ema: 0.999669] 
[Epoch 47/113] [Batch 100/445] [D loss: 0.318933] [G loss: 0.220627] [ema: 0.999670] 
[Epoch 47/113] [Batch 200/445] [D loss: 0.333873] [G loss: 0.214936] [ema: 0.999672] 
[Epoch 47/113] [Batch 300/445] [D loss: 0.402621] [G loss: 0.206773] [ema: 0.999673] 
[Epoch 47/113] [Batch 400/445] [D loss: 0.299975] [G loss: 0.204039] [ema: 0.999675] 
[Epoch 48/113] [Batch 0/445] [D loss: 0.330481] [G loss: 0.229954] [ema: 0.999676] 
[Epoch 48/113] [Batch 100/445] [D loss: 0.352409] [G loss: 0.186739] [ema: 0.999677] 
[Epoch 48/113] [Batch 200/445] [D loss: 0.363079] [G loss: 0.197346] [ema: 0.999679] 
[Epoch 48/113] [Batch 300/445] [D loss: 0.394206] [G loss: 0.211157] [ema: 0.999680] 
[Epoch 48/113] [Batch 400/445] [D loss: 0.375139] [G loss: 0.176688] [ema: 0.999682] 
[Epoch 49/113] [Batch 0/445] [D loss: 0.327701] [G loss: 0.185868] [ema: 0.999682] 
[Epoch 49/113] [Batch 100/445] [D loss: 0.384019] [G loss: 0.169447] [ema: 0.999684] 
[Epoch 49/113] [Batch 200/445] [D loss: 0.312860] [G loss: 0.225069] [ema: 0.999685] 
[Epoch 49/113] [Batch 300/445] [D loss: 0.391120] [G loss: 0.203605] [ema: 0.999686] 
[Epoch 49/113] [Batch 400/445] [D loss: 0.413215] [G loss: 0.177801] [ema: 0.999688] 
[Epoch 50/113] [Batch 0/445] [D loss: 0.319934] [G loss: 0.195912] [ema: 0.999689] 
[Epoch 50/113] [Batch 100/445] [D loss: 0.356131] [G loss: 0.192692] [ema: 0.999690] 
[Epoch 50/113] [Batch 200/445] [D loss: 0.396181] [G loss: 0.178801] [ema: 0.999691] 
[Epoch 50/113] [Batch 300/445] [D loss: 0.353843] [G loss: 0.202363] [ema: 0.999693] 
[Epoch 50/113] [Batch 400/445] [D loss: 0.330698] [G loss: 0.219610] [ema: 0.999694] 
[Epoch 51/113] [Batch 0/445] [D loss: 0.347935] [G loss: 0.214981] [ema: 0.999695] 
[Epoch 51/113] [Batch 100/445] [D loss: 0.441018] [G loss: 0.183885] [ema: 0.999696] 
[Epoch 51/113] [Batch 200/445] [D loss: 0.322161] [G loss: 0.204801] [ema: 0.999697] 
[Epoch 51/113] [Batch 300/445] [D loss: 0.430428] [G loss: 0.217019] [ema: 0.999699] 
[Epoch 51/113] [Batch 400/445] [D loss: 0.308438] [G loss: 0.186587] [ema: 0.999700] 
[Epoch 52/113] [Batch 0/445] [D loss: 0.352089] [G loss: 0.181391] [ema: 0.999700] 
[Epoch 52/113] [Batch 100/445] [D loss: 0.347998] [G loss: 0.168530] [ema: 0.999702] 
[Epoch 52/113] [Batch 200/445] [D loss: 0.364991] [G loss: 0.229620] [ema: 0.999703] 
[Epoch 52/113] [Batch 300/445] [D loss: 0.356898] [G loss: 0.174910] [ema: 0.999704] 
[Epoch 52/113] [Batch 400/445] [D loss: 0.321778] [G loss: 0.181933] [ema: 0.999706] 
[Epoch 53/113] [Batch 0/445] [D loss: 0.359022] [G loss: 0.210389] [ema: 0.999706] 
[Epoch 53/113] [Batch 100/445] [D loss: 0.314812] [G loss: 0.234975] [ema: 0.999707] 
[Epoch 53/113] [Batch 200/445] [D loss: 0.402458] [G loss: 0.193333] [ema: 0.999709] 
[Epoch 53/113] [Batch 300/445] [D loss: 0.370159] [G loss: 0.199272] [ema: 0.999710] 
[Epoch 53/113] [Batch 400/445] [D loss: 0.418191] [G loss: 0.210594] [ema: 0.999711] 
[Epoch 54/113] [Batch 0/445] [D loss: 0.308996] [G loss: 0.206846] [ema: 0.999712] 
[Epoch 54/113] [Batch 100/445] [D loss: 0.341260] [G loss: 0.188812] [ema: 0.999713] 
[Epoch 54/113] [Batch 200/445] [D loss: 0.373306] [G loss: 0.182283] [ema: 0.999714] 
[Epoch 54/113] [Batch 300/445] [D loss: 0.327297] [G loss: 0.192484] [ema: 0.999715] 
[Epoch 54/113] [Batch 400/445] [D loss: 0.413704] [G loss: 0.212467] [ema: 0.999716] 
[Epoch 55/113] [Batch 0/445] [D loss: 0.322093] [G loss: 0.216728] [ema: 0.999717] 
[Epoch 55/113] [Batch 100/445] [D loss: 0.295548] [G loss: 0.197219] [ema: 0.999718] 
[Epoch 55/113] [Batch 200/445] [D loss: 0.376871] [G loss: 0.164595] [ema: 0.999719] 
[Epoch 55/113] [Batch 300/445] [D loss: 0.347586] [G loss: 0.204303] [ema: 0.999720] 
[Epoch 55/113] [Batch 400/445] [D loss: 0.390521] [G loss: 0.186352] [ema: 0.999721] 
[Epoch 56/113] [Batch 0/445] [D loss: 0.356866] [G loss: 0.204804] [ema: 0.999722] 
[Epoch 56/113] [Batch 100/445] [D loss: 0.367540] [G loss: 0.198285] [ema: 0.999723] 
[Epoch 56/113] [Batch 200/445] [D loss: 0.343539] [G loss: 0.204556] [ema: 0.999724] 
[Epoch 56/113] [Batch 300/445] [D loss: 0.355592] [G loss: 0.184987] [ema: 0.999725] 
[Epoch 56/113] [Batch 400/445] [D loss: 0.331396] [G loss: 0.203454] [ema: 0.999726] 
[Epoch 57/113] [Batch 0/445] [D loss: 0.324122] [G loss: 0.202477] [ema: 0.999727] 
[Epoch 57/113] [Batch 100/445] [D loss: 0.398886] [G loss: 0.216343] [ema: 0.999728] 
[Epoch 57/113] [Batch 200/445] [D loss: 0.392225] [G loss: 0.203853] [ema: 0.999729] 
[Epoch 57/113] [Batch 300/445] [D loss: 0.383696] [G loss: 0.167657] [ema: 0.999730] 
[Epoch 57/113] [Batch 400/445] [D loss: 0.367466] [G loss: 0.176108] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_30_100/MotionSense_DAGHAR_Multiclass_50000_D_30_2024_10_28_18_44_21/Model



[Epoch 58/113] [Batch 0/445] [D loss: 0.346951] [G loss: 0.199170] [ema: 0.999731] 
[Epoch 58/113] [Batch 100/445] [D loss: 0.315616] [G loss: 0.216726] [ema: 0.999733] 
[Epoch 58/113] [Batch 200/445] [D loss: 0.418600] [G loss: 0.207012] [ema: 0.999734] 
[Epoch 58/113] [Batch 300/445] [D loss: 0.378197] [G loss: 0.186728] [ema: 0.999735] 
[Epoch 58/113] [Batch 400/445] [D loss: 0.371000] [G loss: 0.185669] [ema: 0.999736] 
[Epoch 59/113] [Batch 0/445] [D loss: 0.339375] [G loss: 0.215086] [ema: 0.999736] 
[Epoch 59/113] [Batch 100/445] [D loss: 0.333672] [G loss: 0.212927] [ema: 0.999737] 
[Epoch 59/113] [Batch 200/445] [D loss: 0.323119] [G loss: 0.182927] [ema: 0.999738] 
[Epoch 59/113] [Batch 300/445] [D loss: 0.332699] [G loss: 0.193264] [ema: 0.999739] 
[Epoch 59/113] [Batch 400/445] [D loss: 0.387541] [G loss: 0.162865] [ema: 0.999740] 
[Epoch 60/113] [Batch 0/445] [D loss: 0.405668] [G loss: 0.195800] [ema: 0.999740] 
[Epoch 60/113] [Batch 100/445] [D loss: 0.369817] [G loss: 0.176820] [ema: 0.999741] 
[Epoch 60/113] [Batch 200/445] [D loss: 0.394288] [G loss: 0.187559] [ema: 0.999742] 
[Epoch 60/113] [Batch 300/445] [D loss: 0.370382] [G loss: 0.184098] [ema: 0.999743] 
[Epoch 60/113] [Batch 400/445] [D loss: 0.340076] [G loss: 0.194168] [ema: 0.999744] 
[Epoch 61/113] [Batch 0/445] [D loss: 0.361574] [G loss: 0.184263] [ema: 0.999745] 
[Epoch 61/113] [Batch 100/445] [D loss: 0.372156] [G loss: 0.227468] [ema: 0.999746] 
[Epoch 61/113] [Batch 200/445] [D loss: 0.393553] [G loss: 0.206651] [ema: 0.999747] 
[Epoch 61/113] [Batch 300/445] [D loss: 0.348240] [G loss: 0.199036] [ema: 0.999747] 
[Epoch 61/113] [Batch 400/445] [D loss: 0.369288] [G loss: 0.203015] [ema: 0.999748] 
[Epoch 62/113] [Batch 0/445] [D loss: 0.377918] [G loss: 0.196535] [ema: 0.999749] 
[Epoch 62/113] [Batch 100/445] [D loss: 0.351004] [G loss: 0.216426] [ema: 0.999750] 
[Epoch 62/113] [Batch 200/445] [D loss: 0.345873] [G loss: 0.211122] [ema: 0.999751] 
[Epoch 62/113] [Batch 300/445] [D loss: 0.386217] [G loss: 0.169768] [ema: 0.999752] 
[Epoch 62/113] [Batch 400/445] [D loss: 0.354226] [G loss: 0.197049] [ema: 0.999752] 
[Epoch 63/113] [Batch 0/445] [D loss: 0.455086] [G loss: 0.183357] [ema: 0.999753] 
[Epoch 63/113] [Batch 100/445] [D loss: 0.394566] [G loss: 0.199126] [ema: 0.999754] 
[Epoch 63/113] [Batch 200/445] [D loss: 0.378209] [G loss: 0.185128] [ema: 0.999755] 
[Epoch 63/113] [Batch 300/445] [D loss: 0.349694] [G loss: 0.216186] [ema: 0.999755] 
[Epoch 63/113] [Batch 400/445] [D loss: 0.332659] [G loss: 0.197442] [ema: 0.999756] 
[Epoch 64/113] [Batch 0/445] [D loss: 0.330669] [G loss: 0.174169] [ema: 0.999757] 
[Epoch 64/113] [Batch 100/445] [D loss: 0.381605] [G loss: 0.185475] [ema: 0.999758] 
[Epoch 64/113] [Batch 200/445] [D loss: 0.382402] [G loss: 0.174127] [ema: 0.999758] 
[Epoch 64/113] [Batch 300/445] [D loss: 0.391160] [G loss: 0.191490] [ema: 0.999759] 
[Epoch 64/113] [Batch 400/445] [D loss: 0.376197] [G loss: 0.201399] [ema: 0.999760] 
[Epoch 65/113] [Batch 0/445] [D loss: 0.357920] [G loss: 0.181893] [ema: 0.999760] 
[Epoch 65/113] [Batch 100/445] [D loss: 0.362047] [G loss: 0.188467] [ema: 0.999761] 
[Epoch 65/113] [Batch 200/445] [D loss: 0.348720] [G loss: 0.171548] [ema: 0.999762] 
[Epoch 65/113] [Batch 300/445] [D loss: 0.402242] [G loss: 0.194528] [ema: 0.999763] 
[Epoch 65/113] [Batch 400/445] [D loss: 0.398638] [G loss: 0.172242] [ema: 0.999764] 
[Epoch 66/113] [Batch 0/445] [D loss: 0.338517] [G loss: 0.182267] [ema: 0.999764] 
[Epoch 66/113] [Batch 100/445] [D loss: 0.355185] [G loss: 0.187789] [ema: 0.999765] 
[Epoch 66/113] [Batch 200/445] [D loss: 0.378916] [G loss: 0.188577] [ema: 0.999766] 
[Epoch 66/113] [Batch 300/445] [D loss: 0.362855] [G loss: 0.194685] [ema: 0.999766] 
[Epoch 66/113] [Batch 400/445] [D loss: 0.403701] [G loss: 0.193429] [ema: 0.999767] 
[Epoch 67/113] [Batch 0/445] [D loss: 0.376243] [G loss: 0.195069] [ema: 0.999768] 
[Epoch 67/113] [Batch 100/445] [D loss: 0.364572] [G loss: 0.210351] [ema: 0.999768] 
[Epoch 67/113] [Batch 200/445] [D loss: 0.305167] [G loss: 0.218982] [ema: 0.999769] 
[Epoch 67/113] [Batch 300/445] [D loss: 0.382272] [G loss: 0.177729] [ema: 0.999770] 
[Epoch 67/113] [Batch 400/445] [D loss: 0.350716] [G loss: 0.209667] [ema: 0.999771] 
[Epoch 68/113] [Batch 0/445] [D loss: 0.347584] [G loss: 0.189189] [ema: 0.999771] 
[Epoch 68/113] [Batch 100/445] [D loss: 0.385765] [G loss: 0.200000] [ema: 0.999772] 
[Epoch 68/113] [Batch 200/445] [D loss: 0.335992] [G loss: 0.187664] [ema: 0.999772] 
[Epoch 68/113] [Batch 300/445] [D loss: 0.416695] [G loss: 0.201101] [ema: 0.999773] 
[Epoch 68/113] [Batch 400/445] [D loss: 0.337770] [G loss: 0.221898] [ema: 0.999774] 
[Epoch 69/113] [Batch 0/445] [D loss: 0.352151] [G loss: 0.219273] [ema: 0.999774] 
[Epoch 69/113] [Batch 100/445] [D loss: 0.349360] [G loss: 0.179579] [ema: 0.999775] 
[Epoch 69/113] [Batch 200/445] [D loss: 0.362256] [G loss: 0.167413] [ema: 0.999776] 
[Epoch 69/113] [Batch 300/445] [D loss: 0.382548] [G loss: 0.184582] [ema: 0.999776] 
[Epoch 69/113] [Batch 400/445] [D loss: 0.372070] [G loss: 0.189623] [ema: 0.999777] 
[Epoch 70/113] [Batch 0/445] [D loss: 0.356017] [G loss: 0.197438] [ema: 0.999778] 
[Epoch 70/113] [Batch 100/445] [D loss: 0.422221] [G loss: 0.228497] [ema: 0.999778] 
[Epoch 70/113] [Batch 200/445] [D loss: 0.397106] [G loss: 0.152554] [ema: 0.999779] 
[Epoch 70/113] [Batch 300/445] [D loss: 0.338392] [G loss: 0.207187] [ema: 0.999780] 
[Epoch 70/113] [Batch 400/445] [D loss: 0.395635] [G loss: 0.220865] [ema: 0.999780] 
[Epoch 71/113] [Batch 0/445] [D loss: 0.376133] [G loss: 0.204816] [ema: 0.999781] 
[Epoch 71/113] [Batch 100/445] [D loss: 0.355480] [G loss: 0.216359] [ema: 0.999781] 
[Epoch 71/113] [Batch 200/445] [D loss: 0.445998] [G loss: 0.190923] [ema: 0.999782] 
[Epoch 71/113] [Batch 300/445] [D loss: 0.448754] [G loss: 0.201483] [ema: 0.999783] 
[Epoch 71/113] [Batch 400/445] [D loss: 0.377526] [G loss: 0.163052] [ema: 0.999783] 
[Epoch 72/113] [Batch 0/445] [D loss: 0.366134] [G loss: 0.201105] [ema: 0.999784] 
[Epoch 72/113] [Batch 100/445] [D loss: 0.305290] [G loss: 0.193245] [ema: 0.999784] 
[Epoch 72/113] [Batch 200/445] [D loss: 0.383337] [G loss: 0.208927] [ema: 0.999785] 
[Epoch 72/113] [Batch 300/445] [D loss: 0.396633] [G loss: 0.193319] [ema: 0.999786] 
[Epoch 72/113] [Batch 400/445] [D loss: 0.347733] [G loss: 0.187272] [ema: 0.999786] 
[Epoch 73/113] [Batch 0/445] [D loss: 0.421001] [G loss: 0.183034] [ema: 0.999787] 
[Epoch 73/113] [Batch 100/445] [D loss: 0.336074] [G loss: 0.202388] [ema: 0.999787] 
[Epoch 73/113] [Batch 200/445] [D loss: 0.314543] [G loss: 0.225144] [ema: 0.999788] 
[Epoch 73/113] [Batch 300/445] [D loss: 0.379103] [G loss: 0.188383] [ema: 0.999789] 
[Epoch 73/113] [Batch 400/445] [D loss: 0.449971] [G loss: 0.191281] [ema: 0.999789] 
[Epoch 74/113] [Batch 0/445] [D loss: 0.327823] [G loss: 0.209126] [ema: 0.999790] 
[Epoch 74/113] [Batch 100/445] [D loss: 0.407775] [G loss: 0.185048] [ema: 0.999790] 
[Epoch 74/113] [Batch 200/445] [D loss: 0.404883] [G loss: 0.162080] [ema: 0.999791] 
[Epoch 74/113] [Batch 300/445] [D loss: 0.360902] [G loss: 0.197577] [ema: 0.999791] 
[Epoch 74/113] [Batch 400/445] [D loss: 0.346341] [G loss: 0.219718] [ema: 0.999792] 
[Epoch 75/113] [Batch 0/445] [D loss: 0.432052] [G loss: 0.210184] [ema: 0.999792] 
[Epoch 75/113] [Batch 100/445] [D loss: 0.378742] [G loss: 0.216325] [ema: 0.999793] 
[Epoch 75/113] [Batch 200/445] [D loss: 0.354622] [G loss: 0.175742] [ema: 0.999794] 
[Epoch 75/113] [Batch 300/445] [D loss: 0.439980] [G loss: 0.180038] [ema: 0.999794] 
[Epoch 75/113] [Batch 400/445] [D loss: 0.401288] [G loss: 0.211069] [ema: 0.999795] 
[Epoch 76/113] [Batch 0/445] [D loss: 0.425562] [G loss: 0.220649] [ema: 0.999795] 
[Epoch 76/113] [Batch 100/445] [D loss: 0.385531] [G loss: 0.215216] [ema: 0.999796] 
[Epoch 76/113] [Batch 200/445] [D loss: 0.408712] [G loss: 0.207895] [ema: 0.999796] 
[Epoch 76/113] [Batch 300/445] [D loss: 0.348342] [G loss: 0.222076] [ema: 0.999797] 
[Epoch 76/113] [Batch 400/445] [D loss: 0.514282] [G loss: 0.212672] [ema: 0.999797] 
[Epoch 77/113] [Batch 0/445] [D loss: 0.326514] [G loss: 0.238178] [ema: 0.999798] 
[Epoch 77/113] [Batch 100/445] [D loss: 0.305006] [G loss: 0.191659] [ema: 0.999798] 
[Epoch 77/113] [Batch 200/445] [D loss: 0.341310] [G loss: 0.206196] [ema: 0.999799] 
[Epoch 77/113] [Batch 300/445] [D loss: 0.358637] [G loss: 0.201570] [ema: 0.999799] 
[Epoch 77/113] [Batch 400/445] [D loss: 0.348420] [G loss: 0.188278] [ema: 0.999800] 
[Epoch 78/113] [Batch 0/445] [D loss: 0.340902] [G loss: 0.202085] [ema: 0.999800] 
[Epoch 78/113] [Batch 100/445] [D loss: 0.363376] [G loss: 0.221175] [ema: 0.999801] 
[Epoch 78/113] [Batch 200/445] [D loss: 0.377558] [G loss: 0.203288] [ema: 0.999801] 
[Epoch 78/113] [Batch 300/445] [D loss: 0.310774] [G loss: 0.188132] [ema: 0.999802] 
[Epoch 78/113] [Batch 400/445] [D loss: 0.343239] [G loss: 0.207947] [ema: 0.999803] 
[Epoch 79/113] [Batch 0/445] [D loss: 0.326038] [G loss: 0.210569] [ema: 0.999803] 
[Epoch 79/113] [Batch 100/445] [D loss: 0.383703] [G loss: 0.197710] [ema: 0.999803] 
[Epoch 79/113] [Batch 200/445] [D loss: 0.281108] [G loss: 0.230878] [ema: 0.999804] 
[Epoch 79/113] [Batch 300/445] [D loss: 0.398464] [G loss: 0.191537] [ema: 0.999805] 
[Epoch 79/113] [Batch 400/445] [D loss: 0.483567] [G loss: 0.224977] [ema: 0.999805] 
[Epoch 80/113] [Batch 0/445] [D loss: 0.379169] [G loss: 0.230802] [ema: 0.999805] 
[Epoch 80/113] [Batch 100/445] [D loss: 0.301604] [G loss: 0.206125] [ema: 0.999806] 
[Epoch 80/113] [Batch 200/445] [D loss: 0.387106] [G loss: 0.201861] [ema: 0.999806] 
[Epoch 80/113] [Batch 300/445] [D loss: 0.358096] [G loss: 0.189620] [ema: 0.999807] 
[Epoch 80/113] [Batch 400/445] [D loss: 0.318367] [G loss: 0.195451] [ema: 0.999807] 
[Epoch 81/113] [Batch 0/445] [D loss: 0.458705] [G loss: 0.213493] [ema: 0.999808] 
[Epoch 81/113] [Batch 100/445] [D loss: 0.342100] [G loss: 0.197049] [ema: 0.999808] 
[Epoch 81/113] [Batch 200/445] [D loss: 0.324826] [G loss: 0.200592] [ema: 0.999809] 
[Epoch 81/113] [Batch 300/445] [D loss: 0.325481] [G loss: 0.217664] [ema: 0.999809] 
[Epoch 81/113] [Batch 400/445] [D loss: 0.343808] [G loss: 0.209331] [ema: 0.999810] 
[Epoch 82/113] [Batch 0/445] [D loss: 0.383241] [G loss: 0.194286] [ema: 0.999810] 
[Epoch 82/113] [Batch 100/445] [D loss: 0.445846] [G loss: 0.206113] [ema: 0.999811] 
[Epoch 82/113] [Batch 200/445] [D loss: 0.305376] [G loss: 0.226892] [ema: 0.999811] 
[Epoch 82/113] [Batch 300/445] [D loss: 0.392337] [G loss: 0.178952] [ema: 0.999812] 
[Epoch 82/113] [Batch 400/445] [D loss: 0.330788] [G loss: 0.183625] [ema: 0.999812] 
[Epoch 83/113] [Batch 0/445] [D loss: 0.388719] [G loss: 0.235700] [ema: 0.999812] 
[Epoch 83/113] [Batch 100/445] [D loss: 0.342140] [G loss: 0.177773] [ema: 0.999813] 
[Epoch 83/113] [Batch 200/445] [D loss: 0.366253] [G loss: 0.210004] [ema: 0.999813] 
[Epoch 83/113] [Batch 300/445] [D loss: 0.404511] [G loss: 0.205346] [ema: 0.999814] 
[Epoch 83/113] [Batch 400/445] [D loss: 0.359329] [G loss: 0.203316] [ema: 0.999814] 
[Epoch 84/113] [Batch 0/445] [D loss: 0.341749] [G loss: 0.204969] [ema: 0.999815] 
[Epoch 84/113] [Batch 100/445] [D loss: 0.294456] [G loss: 0.219220] [ema: 0.999815] 
[Epoch 84/113] [Batch 200/445] [D loss: 0.367563] [G loss: 0.177725] [ema: 0.999816] 
[Epoch 84/113] [Batch 300/445] [D loss: 0.388540] [G loss: 0.185943] [ema: 0.999816] 
[Epoch 84/113] [Batch 400/445] [D loss: 0.373578] [G loss: 0.195191] [ema: 0.999817] 
[Epoch 85/113] [Batch 0/445] [D loss: 0.384612] [G loss: 0.210513] [ema: 0.999817] 
[Epoch 85/113] [Batch 100/445] [D loss: 0.343777] [G loss: 0.223491] [ema: 0.999817] 
[Epoch 85/113] [Batch 200/445] [D loss: 0.323866] [G loss: 0.202729] [ema: 0.999818] 
[Epoch 85/113] [Batch 300/445] [D loss: 0.306574] [G loss: 0.195310] [ema: 0.999818] 
[Epoch 85/113] [Batch 400/445] [D loss: 0.363179] [G loss: 0.196864] [ema: 0.999819] 
[Epoch 86/113] [Batch 0/445] [D loss: 0.423515] [G loss: 0.202597] [ema: 0.999819] 
[Epoch 86/113] [Batch 100/445] [D loss: 0.338456] [G loss: 0.218327] [ema: 0.999819] 
[Epoch 86/113] [Batch 200/445] [D loss: 0.377434] [G loss: 0.189425] [ema: 0.999820] 
[Epoch 86/113] [Batch 300/445] [D loss: 0.361362] [G loss: 0.188826] [ema: 0.999820] 
[Epoch 86/113] [Batch 400/445] [D loss: 0.363035] [G loss: 0.203705] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_30_100/MotionSense_DAGHAR_Multiclass_50000_D_30_2024_10_28_18_44_21/Model



[Epoch 87/113] [Batch 0/445] [D loss: 0.375668] [G loss: 0.210685] [ema: 0.999821] 
[Epoch 87/113] [Batch 100/445] [D loss: 0.332636] [G loss: 0.177758] [ema: 0.999821] 
[Epoch 87/113] [Batch 200/445] [D loss: 0.418171] [G loss: 0.195226] [ema: 0.999822] 
[Epoch 87/113] [Batch 300/445] [D loss: 0.368111] [G loss: 0.169535] [ema: 0.999822] 
[Epoch 87/113] [Batch 400/445] [D loss: 0.303070] [G loss: 0.192790] [ema: 0.999823] 
[Epoch 88/113] [Batch 0/445] [D loss: 0.382453] [G loss: 0.196818] [ema: 0.999823] 
[Epoch 88/113] [Batch 100/445] [D loss: 0.348577] [G loss: 0.210216] [ema: 0.999823] 
[Epoch 88/113] [Batch 200/445] [D loss: 0.365371] [G loss: 0.194608] [ema: 0.999824] 
[Epoch 88/113] [Batch 300/445] [D loss: 0.369321] [G loss: 0.199988] [ema: 0.999824] 
[Epoch 88/113] [Batch 400/445] [D loss: 0.350017] [G loss: 0.205185] [ema: 0.999825] 
[Epoch 89/113] [Batch 0/445] [D loss: 0.342035] [G loss: 0.193825] [ema: 0.999825] 
[Epoch 89/113] [Batch 100/445] [D loss: 0.363709] [G loss: 0.207700] [ema: 0.999825] 
[Epoch 89/113] [Batch 200/445] [D loss: 0.345483] [G loss: 0.178703] [ema: 0.999826] 
[Epoch 89/113] [Batch 300/445] [D loss: 0.372410] [G loss: 0.202356] [ema: 0.999826] 
[Epoch 89/113] [Batch 400/445] [D loss: 0.330930] [G loss: 0.211827] [ema: 0.999827] 
[Epoch 90/113] [Batch 0/445] [D loss: 0.423210] [G loss: 0.200668] [ema: 0.999827] 
[Epoch 90/113] [Batch 100/445] [D loss: 0.329535] [G loss: 0.192419] [ema: 0.999827] 
[Epoch 90/113] [Batch 200/445] [D loss: 0.349325] [G loss: 0.194302] [ema: 0.999828] 
[Epoch 90/113] [Batch 300/445] [D loss: 0.376499] [G loss: 0.206896] [ema: 0.999828] 
[Epoch 90/113] [Batch 400/445] [D loss: 0.365394] [G loss: 0.196229] [ema: 0.999829] 
[Epoch 91/113] [Batch 0/445] [D loss: 0.351237] [G loss: 0.205186] [ema: 0.999829] 
[Epoch 91/113] [Batch 100/445] [D loss: 0.373828] [G loss: 0.200063] [ema: 0.999829] 
[Epoch 91/113] [Batch 200/445] [D loss: 0.338528] [G loss: 0.220032] [ema: 0.999830] 
[Epoch 91/113] [Batch 300/445] [D loss: 0.425857] [G loss: 0.195078] [ema: 0.999830] 
[Epoch 91/113] [Batch 400/445] [D loss: 0.346383] [G loss: 0.211218] [ema: 0.999831] 
[Epoch 92/113] [Batch 0/445] [D loss: 0.358557] [G loss: 0.206932] [ema: 0.999831] 
[Epoch 92/113] [Batch 100/445] [D loss: 0.334382] [G loss: 0.195017] [ema: 0.999831] 
[Epoch 92/113] [Batch 200/445] [D loss: 0.305103] [G loss: 0.198090] [ema: 0.999832] 
[Epoch 92/113] [Batch 300/445] [D loss: 0.380105] [G loss: 0.218134] [ema: 0.999832] 
[Epoch 92/113] [Batch 400/445] [D loss: 0.348055] [G loss: 0.204131] [ema: 0.999832] 
[Epoch 93/113] [Batch 0/445] [D loss: 0.365047] [G loss: 0.178114] [ema: 0.999833] 
[Epoch 93/113] [Batch 100/445] [D loss: 0.368940] [G loss: 0.205144] [ema: 0.999833] 
[Epoch 93/113] [Batch 200/445] [D loss: 0.330104] [G loss: 0.209339] [ema: 0.999833] 
[Epoch 93/113] [Batch 300/445] [D loss: 0.413967] [G loss: 0.173321] [ema: 0.999834] 
[Epoch 93/113] [Batch 400/445] [D loss: 0.356637] [G loss: 0.215361] [ema: 0.999834] 
[Epoch 94/113] [Batch 0/445] [D loss: 0.346933] [G loss: 0.220339] [ema: 0.999834] 
[Epoch 94/113] [Batch 100/445] [D loss: 0.307984] [G loss: 0.156370] [ema: 0.999835] 
[Epoch 94/113] [Batch 200/445] [D loss: 0.403073] [G loss: 0.217240] [ema: 0.999835] 
[Epoch 94/113] [Batch 300/445] [D loss: 0.432985] [G loss: 0.168674] [ema: 0.999835] 
[Epoch 94/113] [Batch 400/445] [D loss: 0.303209] [G loss: 0.227414] [ema: 0.999836] 
[Epoch 95/113] [Batch 0/445] [D loss: 0.385058] [G loss: 0.206145] [ema: 0.999836] 
[Epoch 95/113] [Batch 100/445] [D loss: 0.305362] [G loss: 0.178286] [ema: 0.999836] 
[Epoch 95/113] [Batch 200/445] [D loss: 0.352974] [G loss: 0.211442] [ema: 0.999837] 
[Epoch 95/113] [Batch 300/445] [D loss: 0.399992] [G loss: 0.192122] [ema: 0.999837] 
[Epoch 95/113] [Batch 400/445] [D loss: 0.317166] [G loss: 0.219065] [ema: 0.999838] 
[Epoch 96/113] [Batch 0/445] [D loss: 0.280370] [G loss: 0.213211] [ema: 0.999838] 
[Epoch 96/113] [Batch 100/445] [D loss: 0.367094] [G loss: 0.215305] [ema: 0.999838] 
[Epoch 96/113] [Batch 200/445] [D loss: 0.383133] [G loss: 0.200661] [ema: 0.999839] 
[Epoch 96/113] [Batch 300/445] [D loss: 0.371316] [G loss: 0.203077] [ema: 0.999839] 
[Epoch 96/113] [Batch 400/445] [D loss: 0.417456] [G loss: 0.207856] [ema: 0.999839] 
[Epoch 97/113] [Batch 0/445] [D loss: 0.351250] [G loss: 0.218847] [ema: 0.999839] 
[Epoch 97/113] [Batch 100/445] [D loss: 0.367401] [G loss: 0.232899] [ema: 0.999840] 
[Epoch 97/113] [Batch 200/445] [D loss: 0.361176] [G loss: 0.207375] [ema: 0.999840] 
[Epoch 97/113] [Batch 300/445] [D loss: 0.338945] [G loss: 0.191429] [ema: 0.999841] 
[Epoch 97/113] [Batch 400/445] [D loss: 0.309317] [G loss: 0.192863] [ema: 0.999841] 
[Epoch 98/113] [Batch 0/445] [D loss: 0.357758] [G loss: 0.203928] [ema: 0.999841] 
[Epoch 98/113] [Batch 100/445] [D loss: 0.336926] [G loss: 0.188344] [ema: 0.999841] 
[Epoch 98/113] [Batch 200/445] [D loss: 0.332817] [G loss: 0.210986] [ema: 0.999842] 
[Epoch 98/113] [Batch 300/445] [D loss: 0.308497] [G loss: 0.189636] [ema: 0.999842] 
[Epoch 98/113] [Batch 400/445] [D loss: 0.374709] [G loss: 0.212805] [ema: 0.999843] 
[Epoch 99/113] [Batch 0/445] [D loss: 0.356708] [G loss: 0.211949] [ema: 0.999843] 
[Epoch 99/113] [Batch 100/445] [D loss: 0.328577] [G loss: 0.217344] [ema: 0.999843] 
[Epoch 99/113] [Batch 200/445] [D loss: 0.376520] [G loss: 0.200268] [ema: 0.999843] 
[Epoch 99/113] [Batch 300/445] [D loss: 0.391180] [G loss: 0.202445] [ema: 0.999844] 
[Epoch 99/113] [Batch 400/445] [D loss: 0.365426] [G loss: 0.204227] [ema: 0.999844] 
[Epoch 100/113] [Batch 0/445] [D loss: 0.399989] [G loss: 0.218272] [ema: 0.999844] 
[Epoch 100/113] [Batch 100/445] [D loss: 0.372510] [G loss: 0.211262] [ema: 0.999845] 
[Epoch 100/113] [Batch 200/445] [D loss: 0.378368] [G loss: 0.219576] [ema: 0.999845] 
[Epoch 100/113] [Batch 300/445] [D loss: 0.306970] [G loss: 0.205935] [ema: 0.999845] 
[Epoch 100/113] [Batch 400/445] [D loss: 0.435121] [G loss: 0.209896] [ema: 0.999846] 
[Epoch 101/113] [Batch 0/445] [D loss: 0.349940] [G loss: 0.177207] [ema: 0.999846] 
[Epoch 101/113] [Batch 100/445] [D loss: 0.338698] [G loss: 0.199073] [ema: 0.999846] 
[Epoch 101/113] [Batch 200/445] [D loss: 0.367519] [G loss: 0.218851] [ema: 0.999846] 
[Epoch 101/113] [Batch 300/445] [D loss: 0.458770] [G loss: 0.198283] [ema: 0.999847] 
[Epoch 101/113] [Batch 400/445] [D loss: 0.361093] [G loss: 0.192032] [ema: 0.999847] 
[Epoch 102/113] [Batch 0/445] [D loss: 0.299202] [G loss: 0.212413] [ema: 0.999847] 
[Epoch 102/113] [Batch 100/445] [D loss: 0.313429] [G loss: 0.232861] [ema: 0.999848] 
[Epoch 102/113] [Batch 200/445] [D loss: 0.350025] [G loss: 0.226456] [ema: 0.999848] 
[Epoch 102/113] [Batch 300/445] [D loss: 0.330724] [G loss: 0.198313] [ema: 0.999848] 
[Epoch 102/113] [Batch 400/445] [D loss: 0.352415] [G loss: 0.201753] [ema: 0.999849] 
[Epoch 103/113] [Batch 0/445] [D loss: 0.357220] [G loss: 0.231585] [ema: 0.999849] 
[Epoch 103/113] [Batch 100/445] [D loss: 0.396425] [G loss: 0.192059] [ema: 0.999849] 
[Epoch 103/113] [Batch 200/445] [D loss: 0.335970] [G loss: 0.196738] [ema: 0.999849] 
[Epoch 103/113] [Batch 300/445] [D loss: 0.339266] [G loss: 0.217408] [ema: 0.999850] 
[Epoch 103/113] [Batch 400/445] [D loss: 0.342360] [G loss: 0.208088] [ema: 0.999850] 
[Epoch 104/113] [Batch 0/445] [D loss: 0.360512] [G loss: 0.194938] [ema: 0.999850] 
[Epoch 104/113] [Batch 100/445] [D loss: 0.398189] [G loss: 0.205237] [ema: 0.999851] 
[Epoch 104/113] [Batch 200/445] [D loss: 0.312173] [G loss: 0.202919] [ema: 0.999851] 
[Epoch 104/113] [Batch 300/445] [D loss: 0.359819] [G loss: 0.194351] [ema: 0.999851] 
[Epoch 104/113] [Batch 400/445] [D loss: 0.296218] [G loss: 0.210691] [ema: 0.999852] 
[Epoch 105/113] [Batch 0/445] [D loss: 0.346660] [G loss: 0.208109] [ema: 0.999852] 
[Epoch 105/113] [Batch 100/445] [D loss: 0.345825] [G loss: 0.207459] [ema: 0.999852] 
[Epoch 105/113] [Batch 200/445] [D loss: 0.367147] [G loss: 0.190103] [ema: 0.999852] 
[Epoch 105/113] [Batch 300/445] [D loss: 0.301920] [G loss: 0.223009] [ema: 0.999853] 
[Epoch 105/113] [Batch 400/445] [D loss: 0.300860] [G loss: 0.197763] [ema: 0.999853] 
[Epoch 106/113] [Batch 0/445] [D loss: 0.361338] [G loss: 0.208617] [ema: 0.999853] 
[Epoch 106/113] [Batch 100/445] [D loss: 0.333950] [G loss: 0.189164] [ema: 0.999853] 
[Epoch 106/113] [Batch 200/445] [D loss: 0.361803] [G loss: 0.204704] [ema: 0.999854] 
[Epoch 106/113] [Batch 300/445] [D loss: 0.355057] [G loss: 0.188144] [ema: 0.999854] 
[Epoch 106/113] [Batch 400/445] [D loss: 0.322072] [G loss: 0.209565] [ema: 0.999854] 
[Epoch 107/113] [Batch 0/445] [D loss: 0.322435] [G loss: 0.212281] [ema: 0.999854] 
[Epoch 107/113] [Batch 100/445] [D loss: 0.321809] [G loss: 0.216546] [ema: 0.999855] 
[Epoch 107/113] [Batch 200/445] [D loss: 0.398984] [G loss: 0.214067] [ema: 0.999855] 
[Epoch 107/113] [Batch 300/445] [D loss: 0.310136] [G loss: 0.213059] [ema: 0.999855] 
[Epoch 107/113] [Batch 400/445] [D loss: 0.354491] [G loss: 0.210938] [ema: 0.999856] 
[Epoch 108/113] [Batch 0/445] [D loss: 0.311550] [G loss: 0.211143] [ema: 0.999856] 
[Epoch 108/113] [Batch 100/445] [D loss: 0.318432] [G loss: 0.238032] [ema: 0.999856] 
[Epoch 108/113] [Batch 200/445] [D loss: 0.375803] [G loss: 0.227634] [ema: 0.999856] 
[Epoch 108/113] [Batch 300/445] [D loss: 0.302375] [G loss: 0.215246] [ema: 0.999857] 
[Epoch 108/113] [Batch 400/445] [D loss: 0.336527] [G loss: 0.183930] [ema: 0.999857] 
[Epoch 109/113] [Batch 0/445] [D loss: 0.402642] [G loss: 0.180066] [ema: 0.999857] 
[Epoch 109/113] [Batch 100/445] [D loss: 0.351342] [G loss: 0.223867] [ema: 0.999857] 
[Epoch 109/113] [Batch 200/445] [D loss: 0.381980] [G loss: 0.205250] [ema: 0.999858] 
[Epoch 109/113] [Batch 300/445] [D loss: 0.350085] [G loss: 0.165902] [ema: 0.999858] 
[Epoch 109/113] [Batch 400/445] [D loss: 0.339393] [G loss: 0.197085] [ema: 0.999858] 
[Epoch 110/113] [Batch 0/445] [D loss: 0.403255] [G loss: 0.203021] [ema: 0.999858] 
[Epoch 110/113] [Batch 100/445] [D loss: 0.356031] [G loss: 0.216168] [ema: 0.999859] 
[Epoch 110/113] [Batch 200/445] [D loss: 0.340686] [G loss: 0.187211] [ema: 0.999859] 
[Epoch 110/113] [Batch 300/445] [D loss: 0.327062] [G loss: 0.235022] [ema: 0.999859] 
[Epoch 110/113] [Batch 400/445] [D loss: 0.329828] [G loss: 0.229153] [ema: 0.999860] 
[Epoch 111/113] [Batch 0/445] [D loss: 0.359622] [G loss: 0.213726] [ema: 0.999860] 
[Epoch 111/113] [Batch 100/445] [D loss: 0.313702] [G loss: 0.180430] [ema: 0.999860] 
[Epoch 111/113] [Batch 200/445] [D loss: 0.342085] [G loss: 0.237545] [ema: 0.999860] 
[Epoch 111/113] [Batch 300/445] [D loss: 0.368500] [G loss: 0.194042] [ema: 0.999861] 
[Epoch 111/113] [Batch 400/445] [D loss: 0.324027] [G loss: 0.193982] [ema: 0.999861] 
[Epoch 112/113] [Batch 0/445] [D loss: 0.339135] [G loss: 0.238228] [ema: 0.999861] 
[Epoch 112/113] [Batch 100/445] [D loss: 0.348200] [G loss: 0.205648] [ema: 0.999861] 
[Epoch 112/113] [Batch 200/445] [D loss: 0.352366] [G loss: 0.206650] [ema: 0.999861] 
[Epoch 112/113] [Batch 300/445] [D loss: 0.370366] [G loss: 0.195687] [ema: 0.999862] 
[Epoch 112/113] [Batch 400/445] [D loss: 0.387390] [G loss: 0.215024] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
RealWorld_thigh_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
RealWorld_thigh_DAGHAR_Multiclass
daghar
return single class data and labels, class is RealWorld_thigh_DAGHAR_Multiclass
data shape is (20676, 6, 1, 30)
label shape is (20676,)
1293
Epochs between checkpoint: 10



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_30_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_30_2024_10_28_19_18_32/Model



[Epoch 0/39] [Batch 0/1293] [D loss: 4.389705] [G loss: 2.637331] [ema: 0.000000] 
[Epoch 0/39] [Batch 100/1293] [D loss: 0.393874] [G loss: 0.284863] [ema: 0.933033] 
[Epoch 0/39] [Batch 200/1293] [D loss: 0.572951] [G loss: 0.136963] [ema: 0.965936] 
[Epoch 0/39] [Batch 300/1293] [D loss: 0.456392] [G loss: 0.178491] [ema: 0.977160] 
[Epoch 0/39] [Batch 400/1293] [D loss: 0.465861] [G loss: 0.107235] [ema: 0.982821] 
[Epoch 0/39] [Batch 500/1293] [D loss: 0.431404] [G loss: 0.193099] [ema: 0.986233] 
[Epoch 0/39] [Batch 600/1293] [D loss: 0.455327] [G loss: 0.145969] [ema: 0.988514] 
[Epoch 0/39] [Batch 700/1293] [D loss: 0.457546] [G loss: 0.185497] [ema: 0.990147] 
[Epoch 0/39] [Batch 800/1293] [D loss: 0.431877] [G loss: 0.198625] [ema: 0.991373] 
[Epoch 0/39] [Batch 900/1293] [D loss: 0.481413] [G loss: 0.156846] [ema: 0.992328] 
[Epoch 0/39] [Batch 1000/1293] [D loss: 0.479285] [G loss: 0.161564] [ema: 0.993092] 
[Epoch 0/39] [Batch 1100/1293] [D loss: 0.405241] [G loss: 0.150558] [ema: 0.993718] 
[Epoch 0/39] [Batch 1200/1293] [D loss: 0.458616] [G loss: 0.168979] [ema: 0.994240] 
[Epoch 1/39] [Batch 0/1293] [D loss: 0.498631] [G loss: 0.167535] [ema: 0.994654] 
[Epoch 1/39] [Batch 100/1293] [D loss: 0.439546] [G loss: 0.146526] [ema: 0.995036] 
[Epoch 1/39] [Batch 200/1293] [D loss: 0.399405] [G loss: 0.156797] [ema: 0.995368] 
[Epoch 1/39] [Batch 300/1293] [D loss: 0.446364] [G loss: 0.143528] [ema: 0.995658] 
[Epoch 1/39] [Batch 400/1293] [D loss: 0.406760] [G loss: 0.162079] [ema: 0.995914] 
[Epoch 1/39] [Batch 500/1293] [D loss: 0.486157] [G loss: 0.169779] [ema: 0.996142] 
[Epoch 1/39] [Batch 600/1293] [D loss: 0.403544] [G loss: 0.186121] [ema: 0.996345] 
[Epoch 1/39] [Batch 700/1293] [D loss: 0.433798] [G loss: 0.165435] [ema: 0.996528] 
[Epoch 1/39] [Batch 800/1293] [D loss: 0.454506] [G loss: 0.162941] [ema: 0.996694] 
[Epoch 1/39] [Batch 900/1293] [D loss: 0.454747] [G loss: 0.173907] [ema: 0.996844] 
[Epoch 1/39] [Batch 1000/1293] [D loss: 0.386225] [G loss: 0.182554] [ema: 0.996982] 
[Epoch 1/39] [Batch 1100/1293] [D loss: 0.356498] [G loss: 0.222333] [ema: 0.997108] 
[Epoch 1/39] [Batch 1200/1293] [D loss: 0.404773] [G loss: 0.173077] [ema: 0.997223] 
[Epoch 2/39] [Batch 0/1293] [D loss: 0.410179] [G loss: 0.146329] [ema: 0.997323] 
[Epoch 2/39] [Batch 100/1293] [D loss: 0.419559] [G loss: 0.184036] [ema: 0.997423] 
[Epoch 2/39] [Batch 200/1293] [D loss: 0.471139] [G loss: 0.224244] [ema: 0.997515] 
[Epoch 2/39] [Batch 300/1293] [D loss: 0.375338] [G loss: 0.177165] [ema: 0.997601] 
[Epoch 2/39] [Batch 400/1293] [D loss: 0.377967] [G loss: 0.184558] [ema: 0.997681] 
[Epoch 2/39] [Batch 500/1293] [D loss: 0.442528] [G loss: 0.203921] [ema: 0.997756] 
[Epoch 2/39] [Batch 600/1293] [D loss: 0.402537] [G loss: 0.185614] [ema: 0.997827] 
[Epoch 2/39] [Batch 700/1293] [D loss: 0.402776] [G loss: 0.177605] [ema: 0.997893] 
[Epoch 2/39] [Batch 800/1293] [D loss: 0.390925] [G loss: 0.188178] [ema: 0.997955] 
[Epoch 2/39] [Batch 900/1293] [D loss: 0.360014] [G loss: 0.190170] [ema: 0.998014] 
[Epoch 2/39] [Batch 1000/1293] [D loss: 0.412912] [G loss: 0.192079] [ema: 0.998069] 
[Epoch 2/39] [Batch 1100/1293] [D loss: 0.395843] [G loss: 0.183624] [ema: 0.998121] 
[Epoch 2/39] [Batch 1200/1293] [D loss: 0.401244] [G loss: 0.186590] [ema: 0.998171] 
[Epoch 3/39] [Batch 0/1293] [D loss: 0.395111] [G loss: 0.189702] [ema: 0.998215] 
[Epoch 3/39] [Batch 100/1293] [D loss: 0.357381] [G loss: 0.182087] [ema: 0.998260] 
[Epoch 3/39] [Batch 200/1293] [D loss: 0.405090] [G loss: 0.167954] [ema: 0.998302] 
[Epoch 3/39] [Batch 300/1293] [D loss: 0.403410] [G loss: 0.198591] [ema: 0.998343] 
[Epoch 3/39] [Batch 400/1293] [D loss: 0.402841] [G loss: 0.181356] [ema: 0.998381] 
[Epoch 3/39] [Batch 500/1293] [D loss: 0.449661] [G loss: 0.169994] [ema: 0.998418] 
[Epoch 3/39] [Batch 600/1293] [D loss: 0.478980] [G loss: 0.175914] [ema: 0.998454] 
[Epoch 3/39] [Batch 700/1293] [D loss: 0.370495] [G loss: 0.191216] [ema: 0.998487] 
[Epoch 3/39] [Batch 800/1293] [D loss: 0.399919] [G loss: 0.178344] [ema: 0.998520] 
[Epoch 3/39] [Batch 900/1293] [D loss: 0.379463] [G loss: 0.200001] [ema: 0.998551] 
[Epoch 3/39] [Batch 1000/1293] [D loss: 0.401618] [G loss: 0.156083] [ema: 0.998580] 
[Epoch 3/39] [Batch 1100/1293] [D loss: 0.401302] [G loss: 0.170931] [ema: 0.998609] 
[Epoch 3/39] [Batch 1200/1293] [D loss: 0.415130] [G loss: 0.182218] [ema: 0.998636] 
[Epoch 4/39] [Batch 0/1293] [D loss: 0.405007] [G loss: 0.184540] [ema: 0.998661] 
[Epoch 4/39] [Batch 100/1293] [D loss: 0.383358] [G loss: 0.173633] [ema: 0.998686] 
[Epoch 4/39] [Batch 200/1293] [D loss: 0.393492] [G loss: 0.178399] [ema: 0.998711] 
[Epoch 4/39] [Batch 300/1293] [D loss: 0.395432] [G loss: 0.171049] [ema: 0.998734] 
[Epoch 4/39] [Batch 400/1293] [D loss: 0.383656] [G loss: 0.180945] [ema: 0.998757] 
[Epoch 4/39] [Batch 500/1293] [D loss: 0.401834] [G loss: 0.183408] [ema: 0.998779] 
[Epoch 4/39] [Batch 600/1293] [D loss: 0.411338] [G loss: 0.187906] [ema: 0.998800] 
[Epoch 4/39] [Batch 700/1293] [D loss: 0.433556] [G loss: 0.177483] [ema: 0.998820] 
[Epoch 4/39] [Batch 800/1293] [D loss: 0.387936] [G loss: 0.192327] [ema: 0.998840] 
[Epoch 4/39] [Batch 900/1293] [D loss: 0.416830] [G loss: 0.155560] [ema: 0.998859] 
[Epoch 4/39] [Batch 1000/1293] [D loss: 0.358343] [G loss: 0.181054] [ema: 0.998878] 
[Epoch 4/39] [Batch 1100/1293] [D loss: 0.379657] [G loss: 0.187824] [ema: 0.998895] 
[Epoch 4/39] [Batch 1200/1293] [D loss: 0.379430] [G loss: 0.177079] [ema: 0.998913] 
[Epoch 5/39] [Batch 0/1293] [D loss: 0.396780] [G loss: 0.190919] [ema: 0.998928] 
[Epoch 5/39] [Batch 100/1293] [D loss: 0.398045] [G loss: 0.161651] [ema: 0.998945] 
[Epoch 5/39] [Batch 200/1293] [D loss: 0.402299] [G loss: 0.187151] [ema: 0.998961] 
[Epoch 5/39] [Batch 300/1293] [D loss: 0.422867] [G loss: 0.176223] [ema: 0.998976] 
[Epoch 5/39] [Batch 400/1293] [D loss: 0.390661] [G loss: 0.190922] [ema: 0.998991] 
[Epoch 5/39] [Batch 500/1293] [D loss: 0.388056] [G loss: 0.187625] [ema: 0.999005] 
[Epoch 5/39] [Batch 600/1293] [D loss: 0.424014] [G loss: 0.168822] [ema: 0.999019] 
[Epoch 5/39] [Batch 700/1293] [D loss: 0.355169] [G loss: 0.186432] [ema: 0.999033] 
[Epoch 5/39] [Batch 800/1293] [D loss: 0.413973] [G loss: 0.172292] [ema: 0.999046] 
[Epoch 5/39] [Batch 900/1293] [D loss: 0.396382] [G loss: 0.179814] [ema: 0.999059] 
[Epoch 5/39] [Batch 1000/1293] [D loss: 0.421242] [G loss: 0.181602] [ema: 0.999072] 
[Epoch 5/39] [Batch 1100/1293] [D loss: 0.407347] [G loss: 0.167894] [ema: 0.999084] 
[Epoch 5/39] [Batch 1200/1293] [D loss: 0.381120] [G loss: 0.143738] [ema: 0.999096] 
[Epoch 6/39] [Batch 0/1293] [D loss: 0.388375] [G loss: 0.191557] [ema: 0.999107] 
[Epoch 6/39] [Batch 100/1293] [D loss: 0.400756] [G loss: 0.197982] [ema: 0.999118] 
[Epoch 6/39] [Batch 200/1293] [D loss: 0.430845] [G loss: 0.166495] [ema: 0.999129] 
[Epoch 6/39] [Batch 300/1293] [D loss: 0.380684] [G loss: 0.169693] [ema: 0.999140] 
[Epoch 6/39] [Batch 400/1293] [D loss: 0.354497] [G loss: 0.189132] [ema: 0.999151] 
[Epoch 6/39] [Batch 500/1293] [D loss: 0.401959] [G loss: 0.198353] [ema: 0.999161] 
[Epoch 6/39] [Batch 600/1293] [D loss: 0.371199] [G loss: 0.189147] [ema: 0.999171] 
[Epoch 6/39] [Batch 700/1293] [D loss: 0.397179] [G loss: 0.175214] [ema: 0.999181] 
[Epoch 6/39] [Batch 800/1293] [D loss: 0.400510] [G loss: 0.179821] [ema: 0.999190] 
[Epoch 6/39] [Batch 900/1293] [D loss: 0.396899] [G loss: 0.163331] [ema: 0.999200] 
[Epoch 6/39] [Batch 1000/1293] [D loss: 0.371864] [G loss: 0.189845] [ema: 0.999209] 
[Epoch 6/39] [Batch 1100/1293] [D loss: 0.405683] [G loss: 0.191785] [ema: 0.999218] 
[Epoch 6/39] [Batch 1200/1293] [D loss: 0.438786] [G loss: 0.153907] [ema: 0.999227] 
[Epoch 7/39] [Batch 0/1293] [D loss: 0.420140] [G loss: 0.192427] [ema: 0.999234] 
[Epoch 7/39] [Batch 100/1293] [D loss: 0.382421] [G loss: 0.180544] [ema: 0.999243] 
[Epoch 7/39] [Batch 200/1293] [D loss: 0.410535] [G loss: 0.180707] [ema: 0.999251] 
[Epoch 7/39] [Batch 300/1293] [D loss: 0.404950] [G loss: 0.166548] [ema: 0.999259] 
[Epoch 7/39] [Batch 400/1293] [D loss: 0.397040] [G loss: 0.175744] [ema: 0.999267] 
[Epoch 7/39] [Batch 500/1293] [D loss: 0.407589] [G loss: 0.170480] [ema: 0.999275] 
[Epoch 7/39] [Batch 600/1293] [D loss: 0.390344] [G loss: 0.180902] [ema: 0.999282] 
[Epoch 7/39] [Batch 700/1293] [D loss: 0.401521] [G loss: 0.181042] [ema: 0.999289] 
[Epoch 7/39] [Batch 800/1293] [D loss: 0.437365] [G loss: 0.184432] [ema: 0.999297] 
[Epoch 7/39] [Batch 900/1293] [D loss: 0.417077] [G loss: 0.177577] [ema: 0.999304] 
[Epoch 7/39] [Batch 1000/1293] [D loss: 0.426220] [G loss: 0.178213] [ema: 0.999311] 
[Epoch 7/39] [Batch 1100/1293] [D loss: 0.433286] [G loss: 0.187362] [ema: 0.999317] 
[Epoch 7/39] [Batch 1200/1293] [D loss: 0.358417] [G loss: 0.175913] [ema: 0.999324] 
[Epoch 8/39] [Batch 0/1293] [D loss: 0.423408] [G loss: 0.185921] [ema: 0.999330] 
[Epoch 8/39] [Batch 100/1293] [D loss: 0.359423] [G loss: 0.193830] [ema: 0.999337] 
[Epoch 8/39] [Batch 200/1293] [D loss: 0.370963] [G loss: 0.196442] [ema: 0.999343] 
[Epoch 8/39] [Batch 300/1293] [D loss: 0.367961] [G loss: 0.179391] [ema: 0.999349] 
[Epoch 8/39] [Batch 400/1293] [D loss: 0.415814] [G loss: 0.172500] [ema: 0.999355] 
[Epoch 8/39] [Batch 500/1293] [D loss: 0.384333] [G loss: 0.183270] [ema: 0.999361] 
[Epoch 8/39] [Batch 600/1293] [D loss: 0.382142] [G loss: 0.179139] [ema: 0.999367] 
[Epoch 8/39] [Batch 700/1293] [D loss: 0.380747] [G loss: 0.178931] [ema: 0.999373] 
[Epoch 8/39] [Batch 800/1293] [D loss: 0.389993] [G loss: 0.185664] [ema: 0.999378] 
[Epoch 8/39] [Batch 900/1293] [D loss: 0.404599] [G loss: 0.175090] [ema: 0.999384] 
[Epoch 8/39] [Batch 1000/1293] [D loss: 0.419296] [G loss: 0.179413] [ema: 0.999389] 
[Epoch 8/39] [Batch 1100/1293] [D loss: 0.384619] [G loss: 0.182793] [ema: 0.999394] 
[Epoch 8/39] [Batch 1200/1293] [D loss: 0.389341] [G loss: 0.174231] [ema: 0.999400] 
[Epoch 9/39] [Batch 0/1293] [D loss: 0.375577] [G loss: 0.191720] [ema: 0.999405] 
[Epoch 9/39] [Batch 100/1293] [D loss: 0.434709] [G loss: 0.181995] [ema: 0.999410] 
[Epoch 9/39] [Batch 200/1293] [D loss: 0.398044] [G loss: 0.185184] [ema: 0.999415] 
[Epoch 9/39] [Batch 300/1293] [D loss: 0.423621] [G loss: 0.175624] [ema: 0.999419] 
[Epoch 9/39] [Batch 400/1293] [D loss: 0.388009] [G loss: 0.173044] [ema: 0.999424] 
[Epoch 9/39] [Batch 500/1293] [D loss: 0.387339] [G loss: 0.197094] [ema: 0.999429] 
[Epoch 9/39] [Batch 600/1293] [D loss: 0.418738] [G loss: 0.180753] [ema: 0.999434] 
[Epoch 9/39] [Batch 700/1293] [D loss: 0.376115] [G loss: 0.184056] [ema: 0.999438] 
[Epoch 9/39] [Batch 800/1293] [D loss: 0.389809] [G loss: 0.186229] [ema: 0.999443] 
[Epoch 9/39] [Batch 900/1293] [D loss: 0.372609] [G loss: 0.180909] [ema: 0.999447] 
[Epoch 9/39] [Batch 1000/1293] [D loss: 0.402258] [G loss: 0.175352] [ema: 0.999452] 
[Epoch 9/39] [Batch 1100/1293] [D loss: 0.384981] [G loss: 0.177428] [ema: 0.999456] 
[Epoch 9/39] [Batch 1200/1293] [D loss: 0.441144] [G loss: 0.180958] [ema: 0.999460] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_30_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_30_2024_10_28_19_18_32/Model



[Epoch 10/39] [Batch 0/1293] [D loss: 0.390999] [G loss: 0.166917] [ema: 0.999464] 
[Epoch 10/39] [Batch 100/1293] [D loss: 0.437208] [G loss: 0.181730] [ema: 0.999468] 
[Epoch 10/39] [Batch 200/1293] [D loss: 0.400322] [G loss: 0.182954] [ema: 0.999472] 
[Epoch 10/39] [Batch 300/1293] [D loss: 0.392267] [G loss: 0.185355] [ema: 0.999476] 
[Epoch 10/39] [Batch 400/1293] [D loss: 0.394198] [G loss: 0.179501] [ema: 0.999480] 
[Epoch 10/39] [Batch 500/1293] [D loss: 0.399765] [G loss: 0.185700] [ema: 0.999484] 
[Epoch 10/39] [Batch 600/1293] [D loss: 0.369336] [G loss: 0.180894] [ema: 0.999488] 
[Epoch 10/39] [Batch 700/1293] [D loss: 0.394464] [G loss: 0.189560] [ema: 0.999492] 
[Epoch 10/39] [Batch 800/1293] [D loss: 0.432424] [G loss: 0.169230] [ema: 0.999495] 
[Epoch 10/39] [Batch 900/1293] [D loss: 0.406774] [G loss: 0.190348] [ema: 0.999499] 
[Epoch 10/39] [Batch 1000/1293] [D loss: 0.367535] [G loss: 0.172975] [ema: 0.999503] 
[Epoch 10/39] [Batch 1100/1293] [D loss: 0.372134] [G loss: 0.168520] [ema: 0.999506] 
[Epoch 10/39] [Batch 1200/1293] [D loss: 0.418964] [G loss: 0.181968] [ema: 0.999510] 
[Epoch 11/39] [Batch 0/1293] [D loss: 0.396342] [G loss: 0.195979] [ema: 0.999513] 
[Epoch 11/39] [Batch 100/1293] [D loss: 0.387752] [G loss: 0.178269] [ema: 0.999516] 
[Epoch 11/39] [Batch 200/1293] [D loss: 0.406962] [G loss: 0.168510] [ema: 0.999520] 
[Epoch 11/39] [Batch 300/1293] [D loss: 0.422756] [G loss: 0.169322] [ema: 0.999523] 
[Epoch 11/39] [Batch 400/1293] [D loss: 0.346617] [G loss: 0.188822] [ema: 0.999526] 
[Epoch 11/39] [Batch 500/1293] [D loss: 0.379979] [G loss: 0.192529] [ema: 0.999529] 
[Epoch 11/39] [Batch 600/1293] [D loss: 0.357195] [G loss: 0.190518] [ema: 0.999532] 
[Epoch 11/39] [Batch 700/1293] [D loss: 0.376114] [G loss: 0.182787] [ema: 0.999536] 
[Epoch 11/39] [Batch 800/1293] [D loss: 0.362408] [G loss: 0.180798] [ema: 0.999539] 
[Epoch 11/39] [Batch 900/1293] [D loss: 0.452139] [G loss: 0.185317] [ema: 0.999542] 
[Epoch 11/39] [Batch 1000/1293] [D loss: 0.370251] [G loss: 0.192274] [ema: 0.999545] 
[Epoch 11/39] [Batch 1100/1293] [D loss: 0.370189] [G loss: 0.181537] [ema: 0.999548] 
[Epoch 11/39] [Batch 1200/1293] [D loss: 0.382002] [G loss: 0.179197] [ema: 0.999551] 
[Epoch 12/39] [Batch 0/1293] [D loss: 0.353396] [G loss: 0.182661] [ema: 0.999553] 
[Epoch 12/39] [Batch 100/1293] [D loss: 0.403304] [G loss: 0.171413] [ema: 0.999556] 
[Epoch 12/39] [Batch 200/1293] [D loss: 0.368488] [G loss: 0.184278] [ema: 0.999559] 
[Epoch 12/39] [Batch 300/1293] [D loss: 0.419748] [G loss: 0.184965] [ema: 0.999562] 
[Epoch 12/39] [Batch 400/1293] [D loss: 0.356491] [G loss: 0.202684] [ema: 0.999565] 
[Epoch 12/39] [Batch 500/1293] [D loss: 0.375287] [G loss: 0.179768] [ema: 0.999567] 
[Epoch 12/39] [Batch 600/1293] [D loss: 0.401027] [G loss: 0.182594] [ema: 0.999570] 
[Epoch 12/39] [Batch 700/1293] [D loss: 0.387972] [G loss: 0.185527] [ema: 0.999573] 
[Epoch 12/39] [Batch 800/1293] [D loss: 0.391816] [G loss: 0.190005] [ema: 0.999575] 
[Epoch 12/39] [Batch 900/1293] [D loss: 0.375945] [G loss: 0.198448] [ema: 0.999578] 
[Epoch 12/39] [Batch 1000/1293] [D loss: 0.419696] [G loss: 0.178428] [ema: 0.999580] 
[Epoch 12/39] [Batch 1100/1293] [D loss: 0.386031] [G loss: 0.169227] [ema: 0.999583] 
[Epoch 12/39] [Batch 1200/1293] [D loss: 0.374716] [G loss: 0.177269] [ema: 0.999585] 
[Epoch 13/39] [Batch 0/1293] [D loss: 0.381814] [G loss: 0.200677] [ema: 0.999588] 
[Epoch 13/39] [Batch 100/1293] [D loss: 0.347433] [G loss: 0.193710] [ema: 0.999590] 
[Epoch 13/39] [Batch 200/1293] [D loss: 0.448968] [G loss: 0.201124] [ema: 0.999593] 
[Epoch 13/39] [Batch 300/1293] [D loss: 0.372700] [G loss: 0.191001] [ema: 0.999595] 
[Epoch 13/39] [Batch 400/1293] [D loss: 0.313779] [G loss: 0.183831] [ema: 0.999597] 
[Epoch 13/39] [Batch 500/1293] [D loss: 0.437072] [G loss: 0.170615] [ema: 0.999600] 
[Epoch 13/39] [Batch 600/1293] [D loss: 0.337511] [G loss: 0.194262] [ema: 0.999602] 
[Epoch 13/39] [Batch 700/1293] [D loss: 0.392252] [G loss: 0.178880] [ema: 0.999604] 
[Epoch 13/39] [Batch 800/1293] [D loss: 0.376211] [G loss: 0.196288] [ema: 0.999606] 
[Epoch 13/39] [Batch 900/1293] [D loss: 0.384263] [G loss: 0.186367] [ema: 0.999609] 
[Epoch 13/39] [Batch 1000/1293] [D loss: 0.385438] [G loss: 0.176166] [ema: 0.999611] 
[Epoch 13/39] [Batch 1100/1293] [D loss: 0.397421] [G loss: 0.171954] [ema: 0.999613] 
[Epoch 13/39] [Batch 1200/1293] [D loss: 0.376384] [G loss: 0.178697] [ema: 0.999615] 
[Epoch 14/39] [Batch 0/1293] [D loss: 0.415387] [G loss: 0.197672] [ema: 0.999617] 
[Epoch 14/39] [Batch 100/1293] [D loss: 0.387681] [G loss: 0.173801] [ema: 0.999619] 
[Epoch 14/39] [Batch 200/1293] [D loss: 0.376474] [G loss: 0.171918] [ema: 0.999621] 
[Epoch 14/39] [Batch 300/1293] [D loss: 0.386795] [G loss: 0.199891] [ema: 0.999623] 
[Epoch 14/39] [Batch 400/1293] [D loss: 0.407314] [G loss: 0.173693] [ema: 0.999625] 
[Epoch 14/39] [Batch 500/1293] [D loss: 0.383537] [G loss: 0.183436] [ema: 0.999627] 
[Epoch 14/39] [Batch 600/1293] [D loss: 0.397242] [G loss: 0.189071] [ema: 0.999629] 
[Epoch 14/39] [Batch 700/1293] [D loss: 0.420236] [G loss: 0.188991] [ema: 0.999631] 
[Epoch 14/39] [Batch 800/1293] [D loss: 0.405970] [G loss: 0.180265] [ema: 0.999633] 
[Epoch 14/39] [Batch 900/1293] [D loss: 0.462095] [G loss: 0.187396] [ema: 0.999635] 
[Epoch 14/39] [Batch 1000/1293] [D loss: 0.401447] [G loss: 0.173539] [ema: 0.999637] 
[Epoch 14/39] [Batch 1100/1293] [D loss: 0.387971] [G loss: 0.175662] [ema: 0.999639] 
[Epoch 14/39] [Batch 1200/1293] [D loss: 0.352385] [G loss: 0.182622] [ema: 0.999641] 
[Epoch 15/39] [Batch 0/1293] [D loss: 0.405280] [G loss: 0.190182] [ema: 0.999643] 
[Epoch 15/39] [Batch 100/1293] [D loss: 0.398937] [G loss: 0.193885] [ema: 0.999645] 
[Epoch 15/39] [Batch 200/1293] [D loss: 0.374779] [G loss: 0.170228] [ema: 0.999646] 
[Epoch 15/39] [Batch 300/1293] [D loss: 0.370487] [G loss: 0.184618] [ema: 0.999648] 
[Epoch 15/39] [Batch 400/1293] [D loss: 0.386557] [G loss: 0.183289] [ema: 0.999650] 
[Epoch 15/39] [Batch 500/1293] [D loss: 0.379513] [G loss: 0.201343] [ema: 0.999652] 
[Epoch 15/39] [Batch 600/1293] [D loss: 0.417758] [G loss: 0.189919] [ema: 0.999653] 
[Epoch 15/39] [Batch 700/1293] [D loss: 0.365021] [G loss: 0.187259] [ema: 0.999655] 
[Epoch 15/39] [Batch 800/1293] [D loss: 0.390343] [G loss: 0.176764] [ema: 0.999657] 
[Epoch 15/39] [Batch 900/1293] [D loss: 0.423835] [G loss: 0.179017] [ema: 0.999659] 
[Epoch 15/39] [Batch 1000/1293] [D loss: 0.429833] [G loss: 0.187164] [ema: 0.999660] 
[Epoch 15/39] [Batch 1100/1293] [D loss: 0.369585] [G loss: 0.190050] [ema: 0.999662] 
[Epoch 15/39] [Batch 1200/1293] [D loss: 0.454204] [G loss: 0.190495] [ema: 0.999663] 
[Epoch 16/39] [Batch 0/1293] [D loss: 0.392018] [G loss: 0.201590] [ema: 0.999665] 
[Epoch 16/39] [Batch 100/1293] [D loss: 0.386379] [G loss: 0.180587] [ema: 0.999667] 
[Epoch 16/39] [Batch 200/1293] [D loss: 0.402200] [G loss: 0.188939] [ema: 0.999668] 
[Epoch 16/39] [Batch 300/1293] [D loss: 0.376307] [G loss: 0.176955] [ema: 0.999670] 
[Epoch 16/39] [Batch 400/1293] [D loss: 0.385863] [G loss: 0.183850] [ema: 0.999671] 
[Epoch 16/39] [Batch 500/1293] [D loss: 0.384584] [G loss: 0.174213] [ema: 0.999673] 
[Epoch 16/39] [Batch 600/1293] [D loss: 0.375626] [G loss: 0.179473] [ema: 0.999674] 
[Epoch 16/39] [Batch 700/1293] [D loss: 0.360726] [G loss: 0.173997] [ema: 0.999676] 
[Epoch 16/39] [Batch 800/1293] [D loss: 0.351343] [G loss: 0.184931] [ema: 0.999677] 
[Epoch 16/39] [Batch 900/1293] [D loss: 0.410716] [G loss: 0.176123] [ema: 0.999679] 
[Epoch 16/39] [Batch 1000/1293] [D loss: 0.390265] [G loss: 0.182731] [ema: 0.999680] 
[Epoch 16/39] [Batch 1100/1293] [D loss: 0.379434] [G loss: 0.191059] [ema: 0.999682] 
[Epoch 16/39] [Batch 1200/1293] [D loss: 0.379431] [G loss: 0.174933] [ema: 0.999683] 
[Epoch 17/39] [Batch 0/1293] [D loss: 0.377094] [G loss: 0.192733] [ema: 0.999685] 
[Epoch 17/39] [Batch 100/1293] [D loss: 0.423471] [G loss: 0.174618] [ema: 0.999686] 
[Epoch 17/39] [Batch 200/1293] [D loss: 0.385038] [G loss: 0.186617] [ema: 0.999688] 
[Epoch 17/39] [Batch 300/1293] [D loss: 0.363389] [G loss: 0.191015] [ema: 0.999689] 
[Epoch 17/39] [Batch 400/1293] [D loss: 0.351278] [G loss: 0.185122] [ema: 0.999690] 
[Epoch 17/39] [Batch 500/1293] [D loss: 0.394458] [G loss: 0.176065] [ema: 0.999692] 
[Epoch 17/39] [Batch 600/1293] [D loss: 0.364107] [G loss: 0.174655] [ema: 0.999693] 
[Epoch 17/39] [Batch 700/1293] [D loss: 0.389120] [G loss: 0.200788] [ema: 0.999694] 
[Epoch 17/39] [Batch 800/1293] [D loss: 0.379485] [G loss: 0.191144] [ema: 0.999696] 
[Epoch 17/39] [Batch 900/1293] [D loss: 0.398099] [G loss: 0.176681] [ema: 0.999697] 
[Epoch 17/39] [Batch 1000/1293] [D loss: 0.437487] [G loss: 0.181537] [ema: 0.999698] 
[Epoch 17/39] [Batch 1100/1293] [D loss: 0.341011] [G loss: 0.191529] [ema: 0.999700] 
[Epoch 17/39] [Batch 1200/1293] [D loss: 0.370449] [G loss: 0.178093] [ema: 0.999701] 
[Epoch 18/39] [Batch 0/1293] [D loss: 0.325998] [G loss: 0.205365] [ema: 0.999702] 
[Epoch 18/39] [Batch 100/1293] [D loss: 0.393549] [G loss: 0.169246] [ema: 0.999703] 
[Epoch 18/39] [Batch 200/1293] [D loss: 0.398008] [G loss: 0.196488] [ema: 0.999705] 
[Epoch 18/39] [Batch 300/1293] [D loss: 0.402508] [G loss: 0.194866] [ema: 0.999706] 
[Epoch 18/39] [Batch 400/1293] [D loss: 0.373778] [G loss: 0.195535] [ema: 0.999707] 
[Epoch 18/39] [Batch 500/1293] [D loss: 0.384877] [G loss: 0.180514] [ema: 0.999708] 
[Epoch 18/39] [Batch 600/1293] [D loss: 0.374088] [G loss: 0.195045] [ema: 0.999710] 
[Epoch 18/39] [Batch 700/1293] [D loss: 0.372064] [G loss: 0.181761] [ema: 0.999711] 
[Epoch 18/39] [Batch 800/1293] [D loss: 0.389322] [G loss: 0.195534] [ema: 0.999712] 
[Epoch 18/39] [Batch 900/1293] [D loss: 0.379846] [G loss: 0.195076] [ema: 0.999713] 
[Epoch 18/39] [Batch 1000/1293] [D loss: 0.418827] [G loss: 0.181199] [ema: 0.999714] 
[Epoch 18/39] [Batch 1100/1293] [D loss: 0.399914] [G loss: 0.163038] [ema: 0.999716] 
[Epoch 18/39] [Batch 1200/1293] [D loss: 0.395809] [G loss: 0.176730] [ema: 0.999717] 
[Epoch 19/39] [Batch 0/1293] [D loss: 0.347281] [G loss: 0.188460] [ema: 0.999718] 
[Epoch 19/39] [Batch 100/1293] [D loss: 0.372922] [G loss: 0.190369] [ema: 0.999719] 
[Epoch 19/39] [Batch 200/1293] [D loss: 0.344538] [G loss: 0.197193] [ema: 0.999720] 
[Epoch 19/39] [Batch 300/1293] [D loss: 0.377156] [G loss: 0.179987] [ema: 0.999721] 
[Epoch 19/39] [Batch 400/1293] [D loss: 0.392545] [G loss: 0.184687] [ema: 0.999722] 
[Epoch 19/39] [Batch 500/1293] [D loss: 0.390148] [G loss: 0.191369] [ema: 0.999724] 
[Epoch 19/39] [Batch 600/1293] [D loss: 0.377595] [G loss: 0.179838] [ema: 0.999725] 
[Epoch 19/39] [Batch 700/1293] [D loss: 0.393471] [G loss: 0.179285] [ema: 0.999726] 
[Epoch 19/39] [Batch 800/1293] [D loss: 0.363634] [G loss: 0.178787] [ema: 0.999727] 
[Epoch 19/39] [Batch 900/1293] [D loss: 0.416300] [G loss: 0.174460] [ema: 0.999728] 
[Epoch 19/39] [Batch 1000/1293] [D loss: 0.358970] [G loss: 0.176109] [ema: 0.999729] 
[Epoch 19/39] [Batch 1100/1293] [D loss: 0.390414] [G loss: 0.174168] [ema: 0.999730] 
[Epoch 19/39] [Batch 1200/1293] [D loss: 0.401494] [G loss: 0.181698] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_30_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_30_2024_10_28_19_18_32/Model



[Epoch 20/39] [Batch 0/1293] [D loss: 0.376860] [G loss: 0.180996] [ema: 0.999732] 
[Epoch 20/39] [Batch 100/1293] [D loss: 0.392126] [G loss: 0.186425] [ema: 0.999733] 
[Epoch 20/39] [Batch 200/1293] [D loss: 0.359915] [G loss: 0.177545] [ema: 0.999734] 
[Epoch 20/39] [Batch 300/1293] [D loss: 0.417872] [G loss: 0.187421] [ema: 0.999735] 
[Epoch 20/39] [Batch 400/1293] [D loss: 0.383285] [G loss: 0.174596] [ema: 0.999736] 
[Epoch 20/39] [Batch 500/1293] [D loss: 0.420090] [G loss: 0.182937] [ema: 0.999737] 
[Epoch 20/39] [Batch 600/1293] [D loss: 0.376426] [G loss: 0.201588] [ema: 0.999738] 
[Epoch 20/39] [Batch 700/1293] [D loss: 0.390179] [G loss: 0.186340] [ema: 0.999739] 
[Epoch 20/39] [Batch 800/1293] [D loss: 0.385299] [G loss: 0.191256] [ema: 0.999740] 
[Epoch 20/39] [Batch 900/1293] [D loss: 0.382105] [G loss: 0.176993] [ema: 0.999741] 
[Epoch 20/39] [Batch 1000/1293] [D loss: 0.397020] [G loss: 0.176212] [ema: 0.999742] 
[Epoch 20/39] [Batch 1100/1293] [D loss: 0.395054] [G loss: 0.188234] [ema: 0.999743] 
[Epoch 20/39] [Batch 1200/1293] [D loss: 0.364649] [G loss: 0.171682] [ema: 0.999744] 
[Epoch 21/39] [Batch 0/1293] [D loss: 0.378722] [G loss: 0.180942] [ema: 0.999745] 
[Epoch 21/39] [Batch 100/1293] [D loss: 0.413687] [G loss: 0.171328] [ema: 0.999746] 
[Epoch 21/39] [Batch 200/1293] [D loss: 0.431949] [G loss: 0.163250] [ema: 0.999747] 
[Epoch 21/39] [Batch 300/1293] [D loss: 0.384489] [G loss: 0.189022] [ema: 0.999748] 
[Epoch 21/39] [Batch 400/1293] [D loss: 0.357126] [G loss: 0.180472] [ema: 0.999748] 
[Epoch 21/39] [Batch 500/1293] [D loss: 0.395322] [G loss: 0.182928] [ema: 0.999749] 
[Epoch 21/39] [Batch 600/1293] [D loss: 0.388161] [G loss: 0.194282] [ema: 0.999750] 
[Epoch 21/39] [Batch 700/1293] [D loss: 0.363863] [G loss: 0.186630] [ema: 0.999751] 
[Epoch 21/39] [Batch 800/1293] [D loss: 0.409622] [G loss: 0.191262] [ema: 0.999752] 
[Epoch 21/39] [Batch 900/1293] [D loss: 0.419937] [G loss: 0.185240] [ema: 0.999753] 
[Epoch 21/39] [Batch 1000/1293] [D loss: 0.428299] [G loss: 0.190018] [ema: 0.999754] 
[Epoch 21/39] [Batch 1100/1293] [D loss: 0.404190] [G loss: 0.191583] [ema: 0.999755] 
[Epoch 21/39] [Batch 1200/1293] [D loss: 0.385297] [G loss: 0.186735] [ema: 0.999756] 
[Epoch 22/39] [Batch 0/1293] [D loss: 0.397045] [G loss: 0.196662] [ema: 0.999756] 
[Epoch 22/39] [Batch 100/1293] [D loss: 0.337196] [G loss: 0.188037] [ema: 0.999757] 
[Epoch 22/39] [Batch 200/1293] [D loss: 0.388929] [G loss: 0.187341] [ema: 0.999758] 
[Epoch 22/39] [Batch 300/1293] [D loss: 0.403199] [G loss: 0.176844] [ema: 0.999759] 
[Epoch 22/39] [Batch 400/1293] [D loss: 0.387133] [G loss: 0.186212] [ema: 0.999760] 
[Epoch 22/39] [Batch 500/1293] [D loss: 0.418559] [G loss: 0.189224] [ema: 0.999761] 
[Epoch 22/39] [Batch 600/1293] [D loss: 0.409666] [G loss: 0.177105] [ema: 0.999761] 
[Epoch 22/39] [Batch 700/1293] [D loss: 0.372401] [G loss: 0.175157] [ema: 0.999762] 
[Epoch 22/39] [Batch 800/1293] [D loss: 0.386750] [G loss: 0.174510] [ema: 0.999763] 
[Epoch 22/39] [Batch 900/1293] [D loss: 0.378927] [G loss: 0.183045] [ema: 0.999764] 
[Epoch 22/39] [Batch 1000/1293] [D loss: 0.385830] [G loss: 0.175566] [ema: 0.999765] 
[Epoch 22/39] [Batch 1100/1293] [D loss: 0.397455] [G loss: 0.185772] [ema: 0.999765] 
[Epoch 22/39] [Batch 1200/1293] [D loss: 0.388098] [G loss: 0.187181] [ema: 0.999766] 
[Epoch 23/39] [Batch 0/1293] [D loss: 0.396247] [G loss: 0.191753] [ema: 0.999767] 
[Epoch 23/39] [Batch 100/1293] [D loss: 0.364695] [G loss: 0.186500] [ema: 0.999768] 
[Epoch 23/39] [Batch 200/1293] [D loss: 0.402582] [G loss: 0.188945] [ema: 0.999769] 
[Epoch 23/39] [Batch 300/1293] [D loss: 0.372343] [G loss: 0.166576] [ema: 0.999769] 
[Epoch 23/39] [Batch 400/1293] [D loss: 0.458696] [G loss: 0.178484] [ema: 0.999770] 
[Epoch 23/39] [Batch 500/1293] [D loss: 0.399004] [G loss: 0.182388] [ema: 0.999771] 
[Epoch 23/39] [Batch 600/1293] [D loss: 0.388168] [G loss: 0.192386] [ema: 0.999772] 
[Epoch 23/39] [Batch 700/1293] [D loss: 0.396222] [G loss: 0.177301] [ema: 0.999772] 
[Epoch 23/39] [Batch 800/1293] [D loss: 0.358511] [G loss: 0.181423] [ema: 0.999773] 
[Epoch 23/39] [Batch 900/1293] [D loss: 0.413279] [G loss: 0.168550] [ema: 0.999774] 
[Epoch 23/39] [Batch 1000/1293] [D loss: 0.357431] [G loss: 0.207664] [ema: 0.999775] 
[Epoch 23/39] [Batch 1100/1293] [D loss: 0.397261] [G loss: 0.188040] [ema: 0.999775] 
[Epoch 23/39] [Batch 1200/1293] [D loss: 0.386618] [G loss: 0.198607] [ema: 0.999776] 
[Epoch 24/39] [Batch 0/1293] [D loss: 0.369653] [G loss: 0.183789] [ema: 0.999777] 
[Epoch 24/39] [Batch 100/1293] [D loss: 0.392739] [G loss: 0.177220] [ema: 0.999777] 
[Epoch 24/39] [Batch 200/1293] [D loss: 0.341664] [G loss: 0.201997] [ema: 0.999778] 
[Epoch 24/39] [Batch 300/1293] [D loss: 0.372233] [G loss: 0.197786] [ema: 0.999779] 
[Epoch 24/39] [Batch 400/1293] [D loss: 0.417340] [G loss: 0.173336] [ema: 0.999780] 
[Epoch 24/39] [Batch 500/1293] [D loss: 0.393141] [G loss: 0.192592] [ema: 0.999780] 
[Epoch 24/39] [Batch 600/1293] [D loss: 0.387841] [G loss: 0.180157] [ema: 0.999781] 
[Epoch 24/39] [Batch 700/1293] [D loss: 0.425008] [G loss: 0.184731] [ema: 0.999782] 
[Epoch 24/39] [Batch 800/1293] [D loss: 0.366805] [G loss: 0.198318] [ema: 0.999782] 
[Epoch 24/39] [Batch 900/1293] [D loss: 0.350773] [G loss: 0.204265] [ema: 0.999783] 
[Epoch 24/39] [Batch 1000/1293] [D loss: 0.350107] [G loss: 0.191667] [ema: 0.999784] 
[Epoch 24/39] [Batch 1100/1293] [D loss: 0.406571] [G loss: 0.183450] [ema: 0.999784] 
[Epoch 24/39] [Batch 1200/1293] [D loss: 0.370026] [G loss: 0.185496] [ema: 0.999785] 
[Epoch 25/39] [Batch 0/1293] [D loss: 0.362506] [G loss: 0.197200] [ema: 0.999786] 
[Epoch 25/39] [Batch 100/1293] [D loss: 0.365047] [G loss: 0.183365] [ema: 0.999786] 
[Epoch 25/39] [Batch 200/1293] [D loss: 0.396105] [G loss: 0.179406] [ema: 0.999787] 
[Epoch 25/39] [Batch 300/1293] [D loss: 0.389181] [G loss: 0.196036] [ema: 0.999788] 
[Epoch 25/39] [Batch 400/1293] [D loss: 0.389085] [G loss: 0.195301] [ema: 0.999788] 
[Epoch 25/39] [Batch 500/1293] [D loss: 0.396470] [G loss: 0.194266] [ema: 0.999789] 
[Epoch 25/39] [Batch 600/1293] [D loss: 0.384077] [G loss: 0.189191] [ema: 0.999789] 
[Epoch 25/39] [Batch 700/1293] [D loss: 0.422775] [G loss: 0.179164] [ema: 0.999790] 
[Epoch 25/39] [Batch 800/1293] [D loss: 0.385419] [G loss: 0.182387] [ema: 0.999791] 
[Epoch 25/39] [Batch 900/1293] [D loss: 0.365709] [G loss: 0.157599] [ema: 0.999791] 
[Epoch 25/39] [Batch 1000/1293] [D loss: 0.376581] [G loss: 0.195086] [ema: 0.999792] 
[Epoch 25/39] [Batch 1100/1293] [D loss: 0.366905] [G loss: 0.209083] [ema: 0.999793] 
[Epoch 25/39] [Batch 1200/1293] [D loss: 0.397668] [G loss: 0.198519] [ema: 0.999793] 
[Epoch 26/39] [Batch 0/1293] [D loss: 0.377923] [G loss: 0.183476] [ema: 0.999794] 
[Epoch 26/39] [Batch 100/1293] [D loss: 0.434705] [G loss: 0.195332] [ema: 0.999794] 
[Epoch 26/39] [Batch 200/1293] [D loss: 0.387535] [G loss: 0.176201] [ema: 0.999795] 
[Epoch 26/39] [Batch 300/1293] [D loss: 0.349717] [G loss: 0.177852] [ema: 0.999796] 
[Epoch 26/39] [Batch 400/1293] [D loss: 0.376409] [G loss: 0.174955] [ema: 0.999796] 
[Epoch 26/39] [Batch 500/1293] [D loss: 0.396297] [G loss: 0.177777] [ema: 0.999797] 
[Epoch 26/39] [Batch 600/1293] [D loss: 0.379843] [G loss: 0.183463] [ema: 0.999797] 
[Epoch 26/39] [Batch 700/1293] [D loss: 0.431567] [G loss: 0.170911] [ema: 0.999798] 
[Epoch 26/39] [Batch 800/1293] [D loss: 0.457376] [G loss: 0.179361] [ema: 0.999799] 
[Epoch 26/39] [Batch 900/1293] [D loss: 0.382956] [G loss: 0.184755] [ema: 0.999799] 
[Epoch 26/39] [Batch 1000/1293] [D loss: 0.370656] [G loss: 0.188685] [ema: 0.999800] 
[Epoch 26/39] [Batch 1100/1293] [D loss: 0.396947] [G loss: 0.174461] [ema: 0.999800] 
[Epoch 26/39] [Batch 1200/1293] [D loss: 0.396585] [G loss: 0.178050] [ema: 0.999801] 
[Epoch 27/39] [Batch 0/1293] [D loss: 0.360095] [G loss: 0.194485] [ema: 0.999801] 
[Epoch 27/39] [Batch 100/1293] [D loss: 0.404531] [G loss: 0.171908] [ema: 0.999802] 
[Epoch 27/39] [Batch 200/1293] [D loss: 0.396376] [G loss: 0.190233] [ema: 0.999803] 
[Epoch 27/39] [Batch 300/1293] [D loss: 0.393679] [G loss: 0.182503] [ema: 0.999803] 
[Epoch 27/39] [Batch 400/1293] [D loss: 0.378678] [G loss: 0.172272] [ema: 0.999804] 
[Epoch 27/39] [Batch 500/1293] [D loss: 0.358699] [G loss: 0.200331] [ema: 0.999804] 
[Epoch 27/39] [Batch 600/1293] [D loss: 0.372500] [G loss: 0.186364] [ema: 0.999805] 
[Epoch 27/39] [Batch 700/1293] [D loss: 0.392770] [G loss: 0.197576] [ema: 0.999805] 
[Epoch 27/39] [Batch 800/1293] [D loss: 0.416614] [G loss: 0.176076] [ema: 0.999806] 
[Epoch 27/39] [Batch 900/1293] [D loss: 0.437079] [G loss: 0.190865] [ema: 0.999806] 
[Epoch 27/39] [Batch 1000/1293] [D loss: 0.363607] [G loss: 0.168739] [ema: 0.999807] 
[Epoch 27/39] [Batch 1100/1293] [D loss: 0.444112] [G loss: 0.186850] [ema: 0.999808] 
[Epoch 27/39] [Batch 1200/1293] [D loss: 0.411074] [G loss: 0.183400] [ema: 0.999808] 
[Epoch 28/39] [Batch 0/1293] [D loss: 0.388196] [G loss: 0.194746] [ema: 0.999809] 
[Epoch 28/39] [Batch 100/1293] [D loss: 0.364778] [G loss: 0.183695] [ema: 0.999809] 
[Epoch 28/39] [Batch 200/1293] [D loss: 0.440311] [G loss: 0.186874] [ema: 0.999810] 
[Epoch 28/39] [Batch 300/1293] [D loss: 0.365983] [G loss: 0.189469] [ema: 0.999810] 
[Epoch 28/39] [Batch 400/1293] [D loss: 0.421192] [G loss: 0.185821] [ema: 0.999811] 
[Epoch 28/39] [Batch 500/1293] [D loss: 0.415706] [G loss: 0.190557] [ema: 0.999811] 
[Epoch 28/39] [Batch 600/1293] [D loss: 0.396220] [G loss: 0.173771] [ema: 0.999812] 
[Epoch 28/39] [Batch 700/1293] [D loss: 0.375711] [G loss: 0.177649] [ema: 0.999812] 
[Epoch 28/39] [Batch 800/1293] [D loss: 0.352023] [G loss: 0.185626] [ema: 0.999813] 
[Epoch 28/39] [Batch 900/1293] [D loss: 0.375702] [G loss: 0.180420] [ema: 0.999813] 
[Epoch 28/39] [Batch 1000/1293] [D loss: 0.347902] [G loss: 0.196992] [ema: 0.999814] 
[Epoch 28/39] [Batch 1100/1293] [D loss: 0.385561] [G loss: 0.181495] [ema: 0.999814] 
[Epoch 28/39] [Batch 1200/1293] [D loss: 0.383206] [G loss: 0.184565] [ema: 0.999815] 
[Epoch 29/39] [Batch 0/1293] [D loss: 0.436153] [G loss: 0.165719] [ema: 0.999815] 
[Epoch 29/39] [Batch 100/1293] [D loss: 0.407880] [G loss: 0.190718] [ema: 0.999816] 
[Epoch 29/39] [Batch 200/1293] [D loss: 0.395242] [G loss: 0.176180] [ema: 0.999816] 
[Epoch 29/39] [Batch 300/1293] [D loss: 0.377713] [G loss: 0.175887] [ema: 0.999817] 
[Epoch 29/39] [Batch 400/1293] [D loss: 0.382336] [G loss: 0.191524] [ema: 0.999817] 
[Epoch 29/39] [Batch 500/1293] [D loss: 0.382997] [G loss: 0.180963] [ema: 0.999818] 
[Epoch 29/39] [Batch 600/1293] [D loss: 0.422459] [G loss: 0.177931] [ema: 0.999818] 
[Epoch 29/39] [Batch 700/1293] [D loss: 0.359823] [G loss: 0.184390] [ema: 0.999819] 
[Epoch 29/39] [Batch 800/1293] [D loss: 0.399477] [G loss: 0.179463] [ema: 0.999819] 
[Epoch 29/39] [Batch 900/1293] [D loss: 0.383873] [G loss: 0.189442] [ema: 0.999819] 
[Epoch 29/39] [Batch 1000/1293] [D loss: 0.360190] [G loss: 0.175602] [ema: 0.999820] 
[Epoch 29/39] [Batch 1100/1293] [D loss: 0.410910] [G loss: 0.206452] [ema: 0.999820] 
[Epoch 29/39] [Batch 1200/1293] [D loss: 0.344785] [G loss: 0.187617] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_30_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_30_2024_10_28_19_18_32/Model



[Epoch 30/39] [Batch 0/1293] [D loss: 0.427124] [G loss: 0.179045] [ema: 0.999821] 
[Epoch 30/39] [Batch 100/1293] [D loss: 0.369358] [G loss: 0.186637] [ema: 0.999822] 
[Epoch 30/39] [Batch 200/1293] [D loss: 0.390256] [G loss: 0.180983] [ema: 0.999822] 
[Epoch 30/39] [Batch 300/1293] [D loss: 0.374458] [G loss: 0.176333] [ema: 0.999823] 
[Epoch 30/39] [Batch 400/1293] [D loss: 0.435223] [G loss: 0.184197] [ema: 0.999823] 
[Epoch 30/39] [Batch 500/1293] [D loss: 0.357267] [G loss: 0.195539] [ema: 0.999824] 
[Epoch 30/39] [Batch 600/1293] [D loss: 0.435668] [G loss: 0.185173] [ema: 0.999824] 
[Epoch 30/39] [Batch 700/1293] [D loss: 0.365222] [G loss: 0.187925] [ema: 0.999824] 
[Epoch 30/39] [Batch 800/1293] [D loss: 0.421066] [G loss: 0.172774] [ema: 0.999825] 
[Epoch 30/39] [Batch 900/1293] [D loss: 0.390858] [G loss: 0.188894] [ema: 0.999825] 
[Epoch 30/39] [Batch 1000/1293] [D loss: 0.372643] [G loss: 0.178983] [ema: 0.999826] 
[Epoch 30/39] [Batch 1100/1293] [D loss: 0.380318] [G loss: 0.187008] [ema: 0.999826] 
[Epoch 30/39] [Batch 1200/1293] [D loss: 0.388765] [G loss: 0.197774] [ema: 0.999827] 
[Epoch 31/39] [Batch 0/1293] [D loss: 0.450889] [G loss: 0.171592] [ema: 0.999827] 
[Epoch 31/39] [Batch 100/1293] [D loss: 0.413558] [G loss: 0.165495] [ema: 0.999828] 
[Epoch 31/39] [Batch 200/1293] [D loss: 0.392001] [G loss: 0.197219] [ema: 0.999828] 
[Epoch 31/39] [Batch 300/1293] [D loss: 0.416942] [G loss: 0.175699] [ema: 0.999828] 
[Epoch 31/39] [Batch 400/1293] [D loss: 0.459863] [G loss: 0.185961] [ema: 0.999829] 
[Epoch 31/39] [Batch 500/1293] [D loss: 0.406822] [G loss: 0.176936] [ema: 0.999829] 
[Epoch 31/39] [Batch 600/1293] [D loss: 0.419011] [G loss: 0.182435] [ema: 0.999830] 
[Epoch 31/39] [Batch 700/1293] [D loss: 0.347624] [G loss: 0.184658] [ema: 0.999830] 
[Epoch 31/39] [Batch 800/1293] [D loss: 0.387345] [G loss: 0.171686] [ema: 0.999830] 
[Epoch 31/39] [Batch 900/1293] [D loss: 0.410454] [G loss: 0.194478] [ema: 0.999831] 
[Epoch 31/39] [Batch 1000/1293] [D loss: 0.376089] [G loss: 0.185119] [ema: 0.999831] 
[Epoch 31/39] [Batch 1100/1293] [D loss: 0.396364] [G loss: 0.184717] [ema: 0.999832] 
[Epoch 31/39] [Batch 1200/1293] [D loss: 0.378540] [G loss: 0.179399] [ema: 0.999832] 
[Epoch 32/39] [Batch 0/1293] [D loss: 0.367901] [G loss: 0.178343] [ema: 0.999832] 
[Epoch 32/39] [Batch 100/1293] [D loss: 0.385432] [G loss: 0.175571] [ema: 0.999833] 
[Epoch 32/39] [Batch 200/1293] [D loss: 0.398616] [G loss: 0.184242] [ema: 0.999833] 
[Epoch 32/39] [Batch 300/1293] [D loss: 0.359018] [G loss: 0.188344] [ema: 0.999834] 
[Epoch 32/39] [Batch 400/1293] [D loss: 0.424195] [G loss: 0.180692] [ema: 0.999834] 
[Epoch 32/39] [Batch 500/1293] [D loss: 0.471335] [G loss: 0.172245] [ema: 0.999834] 
[Epoch 32/39] [Batch 600/1293] [D loss: 0.373963] [G loss: 0.190481] [ema: 0.999835] 
[Epoch 32/39] [Batch 700/1293] [D loss: 0.402868] [G loss: 0.173699] [ema: 0.999835] 
[Epoch 32/39] [Batch 800/1293] [D loss: 0.379220] [G loss: 0.195010] [ema: 0.999836] 
[Epoch 32/39] [Batch 900/1293] [D loss: 0.400836] [G loss: 0.173269] [ema: 0.999836] 
[Epoch 32/39] [Batch 1000/1293] [D loss: 0.440646] [G loss: 0.172820] [ema: 0.999836] 
[Epoch 32/39] [Batch 1100/1293] [D loss: 0.417697] [G loss: 0.184230] [ema: 0.999837] 
[Epoch 32/39] [Batch 1200/1293] [D loss: 0.411332] [G loss: 0.171759] [ema: 0.999837] 
[Epoch 33/39] [Batch 0/1293] [D loss: 0.447873] [G loss: 0.178216] [ema: 0.999838] 
[Epoch 33/39] [Batch 100/1293] [D loss: 0.344958] [G loss: 0.184262] [ema: 0.999838] 
[Epoch 33/39] [Batch 200/1293] [D loss: 0.362124] [G loss: 0.189857] [ema: 0.999838] 
[Epoch 33/39] [Batch 300/1293] [D loss: 0.367119] [G loss: 0.187365] [ema: 0.999839] 
[Epoch 33/39] [Batch 400/1293] [D loss: 0.431746] [G loss: 0.169382] [ema: 0.999839] 
[Epoch 33/39] [Batch 500/1293] [D loss: 0.462180] [G loss: 0.163498] [ema: 0.999839] 
[Epoch 33/39] [Batch 600/1293] [D loss: 0.408490] [G loss: 0.176968] [ema: 0.999840] 
[Epoch 33/39] [Batch 700/1293] [D loss: 0.398111] [G loss: 0.196313] [ema: 0.999840] 
[Epoch 33/39] [Batch 800/1293] [D loss: 0.408059] [G loss: 0.157895] [ema: 0.999841] 
[Epoch 33/39] [Batch 900/1293] [D loss: 0.391187] [G loss: 0.178466] [ema: 0.999841] 
[Epoch 33/39] [Batch 1000/1293] [D loss: 0.418443] [G loss: 0.181848] [ema: 0.999841] 
[Epoch 33/39] [Batch 1100/1293] [D loss: 0.392065] [G loss: 0.179423] [ema: 0.999842] 
[Epoch 33/39] [Batch 1200/1293] [D loss: 0.449837] [G loss: 0.184463] [ema: 0.999842] 
[Epoch 34/39] [Batch 0/1293] [D loss: 0.373200] [G loss: 0.187851] [ema: 0.999842] 
[Epoch 34/39] [Batch 100/1293] [D loss: 0.379174] [G loss: 0.181425] [ema: 0.999843] 
[Epoch 34/39] [Batch 200/1293] [D loss: 0.383051] [G loss: 0.178768] [ema: 0.999843] 
[Epoch 34/39] [Batch 300/1293] [D loss: 0.394304] [G loss: 0.187295] [ema: 0.999843] 
[Epoch 34/39] [Batch 400/1293] [D loss: 0.398396] [G loss: 0.183531] [ema: 0.999844] 
[Epoch 34/39] [Batch 500/1293] [D loss: 0.405845] [G loss: 0.190254] [ema: 0.999844] 
[Epoch 34/39] [Batch 600/1293] [D loss: 0.410522] [G loss: 0.191609] [ema: 0.999844] 
[Epoch 34/39] [Batch 700/1293] [D loss: 0.414306] [G loss: 0.168016] [ema: 0.999845] 
[Epoch 34/39] [Batch 800/1293] [D loss: 0.378831] [G loss: 0.173450] [ema: 0.999845] 
[Epoch 34/39] [Batch 900/1293] [D loss: 0.401643] [G loss: 0.183718] [ema: 0.999846] 
[Epoch 34/39] [Batch 1000/1293] [D loss: 0.426572] [G loss: 0.182724] [ema: 0.999846] 
[Epoch 34/39] [Batch 1100/1293] [D loss: 0.395735] [G loss: 0.177376] [ema: 0.999846] 
[Epoch 34/39] [Batch 1200/1293] [D loss: 0.365051] [G loss: 0.156455] [ema: 0.999847] 
[Epoch 35/39] [Batch 0/1293] [D loss: 0.429959] [G loss: 0.175643] [ema: 0.999847] 
[Epoch 35/39] [Batch 100/1293] [D loss: 0.396080] [G loss: 0.189701] [ema: 0.999847] 
[Epoch 35/39] [Batch 200/1293] [D loss: 0.384341] [G loss: 0.173059] [ema: 0.999848] 
[Epoch 35/39] [Batch 300/1293] [D loss: 0.381037] [G loss: 0.189886] [ema: 0.999848] 
[Epoch 35/39] [Batch 400/1293] [D loss: 0.360657] [G loss: 0.194759] [ema: 0.999848] 
[Epoch 35/39] [Batch 500/1293] [D loss: 0.381232] [G loss: 0.192063] [ema: 0.999849] 
[Epoch 35/39] [Batch 600/1293] [D loss: 0.398950] [G loss: 0.186164] [ema: 0.999849] 
[Epoch 35/39] [Batch 700/1293] [D loss: 0.362947] [G loss: 0.173721] [ema: 0.999849] 
[Epoch 35/39] [Batch 800/1293] [D loss: 0.432814] [G loss: 0.177894] [ema: 0.999850] 
[Epoch 35/39] [Batch 900/1293] [D loss: 0.381859] [G loss: 0.178327] [ema: 0.999850] 
[Epoch 35/39] [Batch 1000/1293] [D loss: 0.387075] [G loss: 0.165399] [ema: 0.999850] 
[Epoch 35/39] [Batch 1100/1293] [D loss: 0.396958] [G loss: 0.165564] [ema: 0.999850] 
[Epoch 35/39] [Batch 1200/1293] [D loss: 0.369755] [G loss: 0.190211] [ema: 0.999851] 
[Epoch 36/39] [Batch 0/1293] [D loss: 0.412499] [G loss: 0.183729] [ema: 0.999851] 
[Epoch 36/39] [Batch 100/1293] [D loss: 0.386926] [G loss: 0.179039] [ema: 0.999851] 
[Epoch 36/39] [Batch 200/1293] [D loss: 0.402954] [G loss: 0.173707] [ema: 0.999852] 
[Epoch 36/39] [Batch 300/1293] [D loss: 0.383435] [G loss: 0.185603] [ema: 0.999852] 
[Epoch 36/39] [Batch 400/1293] [D loss: 0.375193] [G loss: 0.172331] [ema: 0.999852] 
[Epoch 36/39] [Batch 500/1293] [D loss: 0.439313] [G loss: 0.187442] [ema: 0.999853] 
[Epoch 36/39] [Batch 600/1293] [D loss: 0.407238] [G loss: 0.182514] [ema: 0.999853] 
[Epoch 36/39] [Batch 700/1293] [D loss: 0.365258] [G loss: 0.170893] [ema: 0.999853] 
[Epoch 36/39] [Batch 800/1293] [D loss: 0.387109] [G loss: 0.170942] [ema: 0.999854] 
[Epoch 36/39] [Batch 900/1293] [D loss: 0.419257] [G loss: 0.179967] [ema: 0.999854] 
[Epoch 36/39] [Batch 1000/1293] [D loss: 0.395185] [G loss: 0.187031] [ema: 0.999854] 
[Epoch 36/39] [Batch 1100/1293] [D loss: 0.413073] [G loss: 0.180487] [ema: 0.999855] 
[Epoch 36/39] [Batch 1200/1293] [D loss: 0.423751] [G loss: 0.175191] [ema: 0.999855] 
[Epoch 37/39] [Batch 0/1293] [D loss: 0.358801] [G loss: 0.189444] [ema: 0.999855] 
[Epoch 37/39] [Batch 100/1293] [D loss: 0.376101] [G loss: 0.174856] [ema: 0.999855] 
[Epoch 37/39] [Batch 200/1293] [D loss: 0.420186] [G loss: 0.174726] [ema: 0.999856] 
[Epoch 37/39] [Batch 300/1293] [D loss: 0.379142] [G loss: 0.180868] [ema: 0.999856] 
[Epoch 37/39] [Batch 400/1293] [D loss: 0.394001] [G loss: 0.186829] [ema: 0.999856] 
[Epoch 37/39] [Batch 500/1293] [D loss: 0.404576] [G loss: 0.177746] [ema: 0.999857] 
[Epoch 37/39] [Batch 600/1293] [D loss: 0.356841] [G loss: 0.170788] [ema: 0.999857] 
[Epoch 37/39] [Batch 700/1293] [D loss: 0.400950] [G loss: 0.189587] [ema: 0.999857] 
[Epoch 37/39] [Batch 800/1293] [D loss: 0.417345] [G loss: 0.162447] [ema: 0.999858] 
[Epoch 37/39] [Batch 900/1293] [D loss: 0.395633] [G loss: 0.176729] [ema: 0.999858] 
[Epoch 37/39] [Batch 1000/1293] [D loss: 0.414839] [G loss: 0.173205] [ema: 0.999858] 
[Epoch 37/39] [Batch 1100/1293] [D loss: 0.396743] [G loss: 0.173883] [ema: 0.999858] 
[Epoch 37/39] [Batch 1200/1293] [D loss: 0.363327] [G loss: 0.171918] [ema: 0.999859] 
[Epoch 38/39] [Batch 0/1293] [D loss: 0.402745] [G loss: 0.171425] [ema: 0.999859] 
[Epoch 38/39] [Batch 100/1293] [D loss: 0.403922] [G loss: 0.170523] [ema: 0.999859] 
[Epoch 38/39] [Batch 200/1293] [D loss: 0.413228] [G loss: 0.183067] [ema: 0.999860] 
[Epoch 38/39] [Batch 300/1293] [D loss: 0.365819] [G loss: 0.184739] [ema: 0.999860] 
[Epoch 38/39] [Batch 400/1293] [D loss: 0.407932] [G loss: 0.175415] [ema: 0.999860] 
[Epoch 38/39] [Batch 500/1293] [D loss: 0.392643] [G loss: 0.184930] [ema: 0.999860] 
[Epoch 38/39] [Batch 600/1293] [D loss: 0.418205] [G loss: 0.176893] [ema: 0.999861] 
[Epoch 38/39] [Batch 700/1293] [D loss: 0.353967] [G loss: 0.192542] [ema: 0.999861] 
[Epoch 38/39] [Batch 800/1293] [D loss: 0.447291] [G loss: 0.172089] [ema: 0.999861] 
[Epoch 38/39] [Batch 900/1293] [D loss: 0.392794] [G loss: 0.183851] [ema: 0.999861] 
[Epoch 38/39] [Batch 1000/1293] [D loss: 0.414118] [G loss: 0.187822] [ema: 0.999862] 
[Epoch 38/39] [Batch 1100/1293] [D loss: 0.432296] [G loss: 0.174267] [ema: 0.999862] 
[Epoch 38/39] [Batch 1200/1293] [D loss: 0.457468] [G loss: 0.170376] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
WISDM_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
WISDM_DAGHAR_Multiclass
daghar
return single class data and labels, class is WISDM_DAGHAR_Multiclass
data shape is (17496, 6, 1, 30)
label shape is (17496,)
1094
Epochs between checkpoint: 12



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_30_100/WISDM_DAGHAR_Multiclass_50000_D_30_2024_10_28_19_52_19/Model



[Epoch 0/46] [Batch 0/1094] [D loss: 4.301488] [G loss: 2.521659] [ema: 0.000000] 
[Epoch 0/46] [Batch 100/1094] [D loss: 0.368050] [G loss: 0.282640] [ema: 0.933033] 
[Epoch 0/46] [Batch 200/1094] [D loss: 0.563634] [G loss: 0.137189] [ema: 0.965936] 
[Epoch 0/46] [Batch 300/1094] [D loss: 0.490304] [G loss: 0.195918] [ema: 0.977160] 
[Epoch 0/46] [Batch 400/1094] [D loss: 0.485648] [G loss: 0.097098] [ema: 0.982821] 
[Epoch 0/46] [Batch 500/1094] [D loss: 0.435828] [G loss: 0.168016] [ema: 0.986233] 
[Epoch 0/46] [Batch 600/1094] [D loss: 0.445461] [G loss: 0.151456] [ema: 0.988514] 
[Epoch 0/46] [Batch 700/1094] [D loss: 0.484976] [G loss: 0.153088] [ema: 0.990147] 
[Epoch 0/46] [Batch 800/1094] [D loss: 0.464951] [G loss: 0.181561] [ema: 0.991373] 
[Epoch 0/46] [Batch 900/1094] [D loss: 0.464636] [G loss: 0.132370] [ema: 0.992328] 
[Epoch 0/46] [Batch 1000/1094] [D loss: 0.547504] [G loss: 0.140151] [ema: 0.993092] 
[Epoch 1/46] [Batch 0/1094] [D loss: 0.468323] [G loss: 0.171052] [ema: 0.993684] 
[Epoch 1/46] [Batch 100/1094] [D loss: 0.483879] [G loss: 0.128533] [ema: 0.994212] 
[Epoch 1/46] [Batch 200/1094] [D loss: 0.470896] [G loss: 0.165542] [ema: 0.994658] 
[Epoch 1/46] [Batch 300/1094] [D loss: 0.487743] [G loss: 0.145491] [ema: 0.995040] 
[Epoch 1/46] [Batch 400/1094] [D loss: 0.472507] [G loss: 0.160326] [ema: 0.995371] 
[Epoch 1/46] [Batch 500/1094] [D loss: 0.441615] [G loss: 0.147473] [ema: 0.995661] 
[Epoch 1/46] [Batch 600/1094] [D loss: 0.453298] [G loss: 0.152217] [ema: 0.995917] 
[Epoch 1/46] [Batch 700/1094] [D loss: 0.491591] [G loss: 0.157907] [ema: 0.996144] 
[Epoch 1/46] [Batch 800/1094] [D loss: 0.478675] [G loss: 0.161028] [ema: 0.996347] 
[Epoch 1/46] [Batch 900/1094] [D loss: 0.436272] [G loss: 0.132023] [ema: 0.996530] 
[Epoch 1/46] [Batch 1000/1094] [D loss: 0.471791] [G loss: 0.146854] [ema: 0.996695] 
[Epoch 2/46] [Batch 0/1094] [D loss: 0.431213] [G loss: 0.148118] [ema: 0.996837] 
[Epoch 2/46] [Batch 100/1094] [D loss: 0.424376] [G loss: 0.126351] [ema: 0.996975] 
[Epoch 2/46] [Batch 200/1094] [D loss: 0.402941] [G loss: 0.174641] [ema: 0.997102] 
[Epoch 2/46] [Batch 300/1094] [D loss: 0.425824] [G loss: 0.173524] [ema: 0.997218] 
[Epoch 2/46] [Batch 400/1094] [D loss: 0.463178] [G loss: 0.170497] [ema: 0.997325] 
[Epoch 2/46] [Batch 500/1094] [D loss: 0.386835] [G loss: 0.199071] [ema: 0.997425] 
[Epoch 2/46] [Batch 600/1094] [D loss: 0.428737] [G loss: 0.151970] [ema: 0.997517] 
[Epoch 2/46] [Batch 700/1094] [D loss: 0.443456] [G loss: 0.163148] [ema: 0.997603] 
[Epoch 2/46] [Batch 800/1094] [D loss: 0.396606] [G loss: 0.186156] [ema: 0.997683] 
[Epoch 2/46] [Batch 900/1094] [D loss: 0.399319] [G loss: 0.172764] [ema: 0.997758] 
[Epoch 2/46] [Batch 1000/1094] [D loss: 0.462729] [G loss: 0.147392] [ema: 0.997828] 
[Epoch 3/46] [Batch 0/1094] [D loss: 0.425859] [G loss: 0.171562] [ema: 0.997890] 
[Epoch 3/46] [Batch 100/1094] [D loss: 0.438242] [G loss: 0.159485] [ema: 0.997953] 
[Epoch 3/46] [Batch 200/1094] [D loss: 0.364385] [G loss: 0.168829] [ema: 0.998011] 
[Epoch 3/46] [Batch 300/1094] [D loss: 0.448563] [G loss: 0.191058] [ema: 0.998067] 
[Epoch 3/46] [Batch 400/1094] [D loss: 0.475229] [G loss: 0.143816] [ema: 0.998119] 
[Epoch 3/46] [Batch 500/1094] [D loss: 0.428949] [G loss: 0.158894] [ema: 0.998169] 
[Epoch 3/46] [Batch 600/1094] [D loss: 0.403601] [G loss: 0.173776] [ema: 0.998216] 
[Epoch 3/46] [Batch 700/1094] [D loss: 0.401298] [G loss: 0.166975] [ema: 0.998261] 
[Epoch 3/46] [Batch 800/1094] [D loss: 0.413834] [G loss: 0.173614] [ema: 0.998303] 
[Epoch 3/46] [Batch 900/1094] [D loss: 0.394469] [G loss: 0.179818] [ema: 0.998344] 
[Epoch 3/46] [Batch 1000/1094] [D loss: 0.431601] [G loss: 0.167437] [ema: 0.998383] 
[Epoch 4/46] [Batch 0/1094] [D loss: 0.399417] [G loss: 0.164050] [ema: 0.998417] 
[Epoch 4/46] [Batch 100/1094] [D loss: 0.436539] [G loss: 0.154906] [ema: 0.998453] 
[Epoch 4/46] [Batch 200/1094] [D loss: 0.471981] [G loss: 0.178092] [ema: 0.998486] 
[Epoch 4/46] [Batch 300/1094] [D loss: 0.468486] [G loss: 0.163589] [ema: 0.998519] 
[Epoch 4/46] [Batch 400/1094] [D loss: 0.464447] [G loss: 0.165929] [ema: 0.998550] 
[Epoch 4/46] [Batch 500/1094] [D loss: 0.431657] [G loss: 0.152329] [ema: 0.998579] 
[Epoch 4/46] [Batch 600/1094] [D loss: 0.395980] [G loss: 0.165800] [ema: 0.998608] 
[Epoch 4/46] [Batch 700/1094] [D loss: 0.406894] [G loss: 0.166534] [ema: 0.998635] 
[Epoch 4/46] [Batch 800/1094] [D loss: 0.461597] [G loss: 0.169946] [ema: 0.998662] 
[Epoch 4/46] [Batch 900/1094] [D loss: 0.436088] [G loss: 0.157230] [ema: 0.998687] 
[Epoch 4/46] [Batch 1000/1094] [D loss: 0.447504] [G loss: 0.173667] [ema: 0.998711] 
[Epoch 5/46] [Batch 0/1094] [D loss: 0.387972] [G loss: 0.160118] [ema: 0.998734] 
[Epoch 5/46] [Batch 100/1094] [D loss: 0.459089] [G loss: 0.149542] [ema: 0.998756] 
[Epoch 5/46] [Batch 200/1094] [D loss: 0.430360] [G loss: 0.167729] [ema: 0.998778] 
[Epoch 5/46] [Batch 300/1094] [D loss: 0.453339] [G loss: 0.150137] [ema: 0.998799] 
[Epoch 5/46] [Batch 400/1094] [D loss: 0.433265] [G loss: 0.165679] [ema: 0.998820] 
[Epoch 5/46] [Batch 500/1094] [D loss: 0.435864] [G loss: 0.167979] [ema: 0.998840] 
[Epoch 5/46] [Batch 600/1094] [D loss: 0.431315] [G loss: 0.159278] [ema: 0.998859] 
[Epoch 5/46] [Batch 700/1094] [D loss: 0.465185] [G loss: 0.161631] [ema: 0.998877] 
[Epoch 5/46] [Batch 800/1094] [D loss: 0.376383] [G loss: 0.174069] [ema: 0.998895] 
[Epoch 5/46] [Batch 900/1094] [D loss: 0.414334] [G loss: 0.191346] [ema: 0.998912] 
[Epoch 5/46] [Batch 1000/1094] [D loss: 0.459250] [G loss: 0.174053] [ema: 0.998929] 
[Epoch 6/46] [Batch 0/1094] [D loss: 0.389588] [G loss: 0.174648] [ema: 0.998945] 
[Epoch 6/46] [Batch 100/1094] [D loss: 0.429672] [G loss: 0.168577] [ema: 0.998960] 
[Epoch 6/46] [Batch 200/1094] [D loss: 0.453465] [G loss: 0.173830] [ema: 0.998976] 
[Epoch 6/46] [Batch 300/1094] [D loss: 0.461241] [G loss: 0.168330] [ema: 0.998991] 
[Epoch 6/46] [Batch 400/1094] [D loss: 0.431535] [G loss: 0.168865] [ema: 0.999005] 
[Epoch 6/46] [Batch 500/1094] [D loss: 0.433448] [G loss: 0.151588] [ema: 0.999019] 
[Epoch 6/46] [Batch 600/1094] [D loss: 0.425277] [G loss: 0.183984] [ema: 0.999033] 
[Epoch 6/46] [Batch 700/1094] [D loss: 0.439817] [G loss: 0.173284] [ema: 0.999046] 
[Epoch 6/46] [Batch 800/1094] [D loss: 0.387784] [G loss: 0.166022] [ema: 0.999059] 
[Epoch 6/46] [Batch 900/1094] [D loss: 0.402119] [G loss: 0.163492] [ema: 0.999072] 
[Epoch 6/46] [Batch 1000/1094] [D loss: 0.441436] [G loss: 0.165133] [ema: 0.999084] 
[Epoch 7/46] [Batch 0/1094] [D loss: 0.438241] [G loss: 0.160870] [ema: 0.999095] 
[Epoch 7/46] [Batch 100/1094] [D loss: 0.427552] [G loss: 0.164250] [ema: 0.999107] 
[Epoch 7/46] [Batch 200/1094] [D loss: 0.404420] [G loss: 0.191413] [ema: 0.999118] 
[Epoch 7/46] [Batch 300/1094] [D loss: 0.431159] [G loss: 0.169685] [ema: 0.999129] 
[Epoch 7/46] [Batch 400/1094] [D loss: 0.364900] [G loss: 0.177914] [ema: 0.999140] 
[Epoch 7/46] [Batch 500/1094] [D loss: 0.428033] [G loss: 0.180133] [ema: 0.999151] 
[Epoch 7/46] [Batch 600/1094] [D loss: 0.456823] [G loss: 0.167857] [ema: 0.999161] 
[Epoch 7/46] [Batch 700/1094] [D loss: 0.437779] [G loss: 0.184018] [ema: 0.999171] 
[Epoch 7/46] [Batch 800/1094] [D loss: 0.448290] [G loss: 0.168324] [ema: 0.999181] 
[Epoch 7/46] [Batch 900/1094] [D loss: 0.428969] [G loss: 0.160887] [ema: 0.999190] 
[Epoch 7/46] [Batch 1000/1094] [D loss: 0.415972] [G loss: 0.163640] [ema: 0.999200] 
[Epoch 8/46] [Batch 0/1094] [D loss: 0.421636] [G loss: 0.177799] [ema: 0.999208] 
[Epoch 8/46] [Batch 100/1094] [D loss: 0.366285] [G loss: 0.180047] [ema: 0.999217] 
[Epoch 8/46] [Batch 200/1094] [D loss: 0.437298] [G loss: 0.162192] [ema: 0.999226] 
[Epoch 8/46] [Batch 300/1094] [D loss: 0.443578] [G loss: 0.151394] [ema: 0.999235] 
[Epoch 8/46] [Batch 400/1094] [D loss: 0.395475] [G loss: 0.173035] [ema: 0.999243] 
[Epoch 8/46] [Batch 500/1094] [D loss: 0.427943] [G loss: 0.180212] [ema: 0.999251] 
[Epoch 8/46] [Batch 600/1094] [D loss: 0.412882] [G loss: 0.184166] [ema: 0.999259] 
[Epoch 8/46] [Batch 700/1094] [D loss: 0.392369] [G loss: 0.165771] [ema: 0.999267] 
[Epoch 8/46] [Batch 800/1094] [D loss: 0.387046] [G loss: 0.173765] [ema: 0.999275] 
[Epoch 8/46] [Batch 900/1094] [D loss: 0.420217] [G loss: 0.176759] [ema: 0.999282] 
[Epoch 8/46] [Batch 1000/1094] [D loss: 0.406005] [G loss: 0.182756] [ema: 0.999289] 
[Epoch 9/46] [Batch 0/1094] [D loss: 0.383872] [G loss: 0.183699] [ema: 0.999296] 
[Epoch 9/46] [Batch 100/1094] [D loss: 0.394858] [G loss: 0.162326] [ema: 0.999303] 
[Epoch 9/46] [Batch 200/1094] [D loss: 0.397831] [G loss: 0.161150] [ema: 0.999310] 
[Epoch 9/46] [Batch 300/1094] [D loss: 0.439707] [G loss: 0.180682] [ema: 0.999317] 
[Epoch 9/46] [Batch 400/1094] [D loss: 0.432793] [G loss: 0.174762] [ema: 0.999324] 
[Epoch 9/46] [Batch 500/1094] [D loss: 0.376681] [G loss: 0.163862] [ema: 0.999330] 
[Epoch 9/46] [Batch 600/1094] [D loss: 0.417744] [G loss: 0.170405] [ema: 0.999337] 
[Epoch 9/46] [Batch 700/1094] [D loss: 0.445944] [G loss: 0.164999] [ema: 0.999343] 
[Epoch 9/46] [Batch 800/1094] [D loss: 0.411236] [G loss: 0.178525] [ema: 0.999349] 
[Epoch 9/46] [Batch 900/1094] [D loss: 0.411430] [G loss: 0.182428] [ema: 0.999355] 
[Epoch 9/46] [Batch 1000/1094] [D loss: 0.461293] [G loss: 0.164881] [ema: 0.999361] 
[Epoch 10/46] [Batch 0/1094] [D loss: 0.404807] [G loss: 0.175593] [ema: 0.999367] 
[Epoch 10/46] [Batch 100/1094] [D loss: 0.425635] [G loss: 0.148603] [ema: 0.999372] 
[Epoch 10/46] [Batch 200/1094] [D loss: 0.417342] [G loss: 0.191191] [ema: 0.999378] 
[Epoch 10/46] [Batch 300/1094] [D loss: 0.411093] [G loss: 0.175588] [ema: 0.999384] 
[Epoch 10/46] [Batch 400/1094] [D loss: 0.455286] [G loss: 0.164789] [ema: 0.999389] 
[Epoch 10/46] [Batch 500/1094] [D loss: 0.384838] [G loss: 0.184224] [ema: 0.999394] 
[Epoch 10/46] [Batch 600/1094] [D loss: 0.382552] [G loss: 0.181478] [ema: 0.999400] 
[Epoch 10/46] [Batch 700/1094] [D loss: 0.392928] [G loss: 0.176136] [ema: 0.999405] 
[Epoch 10/46] [Batch 800/1094] [D loss: 0.398213] [G loss: 0.179394] [ema: 0.999410] 
[Epoch 10/46] [Batch 900/1094] [D loss: 0.460038] [G loss: 0.178553] [ema: 0.999415] 
[Epoch 10/46] [Batch 1000/1094] [D loss: 0.389973] [G loss: 0.192345] [ema: 0.999420] 
[Epoch 11/46] [Batch 0/1094] [D loss: 0.432238] [G loss: 0.177346] [ema: 0.999424] 
[Epoch 11/46] [Batch 100/1094] [D loss: 0.436612] [G loss: 0.192730] [ema: 0.999429] 
[Epoch 11/46] [Batch 200/1094] [D loss: 0.390821] [G loss: 0.199143] [ema: 0.999434] 
[Epoch 11/46] [Batch 300/1094] [D loss: 0.405142] [G loss: 0.203369] [ema: 0.999438] 
[Epoch 11/46] [Batch 400/1094] [D loss: 0.365604] [G loss: 0.181986] [ema: 0.999443] 
[Epoch 11/46] [Batch 500/1094] [D loss: 0.454284] [G loss: 0.143830] [ema: 0.999447] 
[Epoch 11/46] [Batch 600/1094] [D loss: 0.466825] [G loss: 0.174907] [ema: 0.999452] 
[Epoch 11/46] [Batch 700/1094] [D loss: 0.385974] [G loss: 0.171665] [ema: 0.999456] 
[Epoch 11/46] [Batch 800/1094] [D loss: 0.424267] [G loss: 0.186817] [ema: 0.999460] 
[Epoch 11/46] [Batch 900/1094] [D loss: 0.463538] [G loss: 0.176645] [ema: 0.999464] 
[Epoch 11/46] [Batch 1000/1094] [D loss: 0.374492] [G loss: 0.184461] [ema: 0.999468] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_30_100/WISDM_DAGHAR_Multiclass_50000_D_30_2024_10_28_19_52_19/Model



[Epoch 12/46] [Batch 0/1094] [D loss: 0.420221] [G loss: 0.182680] [ema: 0.999472] 
[Epoch 12/46] [Batch 100/1094] [D loss: 0.403627] [G loss: 0.178895] [ema: 0.999476] 
[Epoch 12/46] [Batch 200/1094] [D loss: 0.397623] [G loss: 0.188069] [ema: 0.999480] 
[Epoch 12/46] [Batch 300/1094] [D loss: 0.416667] [G loss: 0.171120] [ema: 0.999484] 
[Epoch 12/46] [Batch 400/1094] [D loss: 0.423758] [G loss: 0.189437] [ema: 0.999488] 
[Epoch 12/46] [Batch 500/1094] [D loss: 0.431295] [G loss: 0.164207] [ema: 0.999492] 
[Epoch 12/46] [Batch 600/1094] [D loss: 0.434278] [G loss: 0.176495] [ema: 0.999495] 
[Epoch 12/46] [Batch 700/1094] [D loss: 0.410426] [G loss: 0.184294] [ema: 0.999499] 
[Epoch 12/46] [Batch 800/1094] [D loss: 0.401988] [G loss: 0.182393] [ema: 0.999502] 
[Epoch 12/46] [Batch 900/1094] [D loss: 0.391157] [G loss: 0.191678] [ema: 0.999506] 
[Epoch 12/46] [Batch 1000/1094] [D loss: 0.389531] [G loss: 0.177238] [ema: 0.999510] 
[Epoch 13/46] [Batch 0/1094] [D loss: 0.408620] [G loss: 0.178357] [ema: 0.999513] 
[Epoch 13/46] [Batch 100/1094] [D loss: 0.398180] [G loss: 0.168257] [ema: 0.999516] 
[Epoch 13/46] [Batch 200/1094] [D loss: 0.435477] [G loss: 0.188896] [ema: 0.999519] 
[Epoch 13/46] [Batch 300/1094] [D loss: 0.436881] [G loss: 0.175334] [ema: 0.999523] 
[Epoch 13/46] [Batch 400/1094] [D loss: 0.437502] [G loss: 0.187442] [ema: 0.999526] 
[Epoch 13/46] [Batch 500/1094] [D loss: 0.389972] [G loss: 0.175808] [ema: 0.999529] 
[Epoch 13/46] [Batch 600/1094] [D loss: 0.434394] [G loss: 0.149050] [ema: 0.999532] 
[Epoch 13/46] [Batch 700/1094] [D loss: 0.333301] [G loss: 0.197613] [ema: 0.999536] 
[Epoch 13/46] [Batch 800/1094] [D loss: 0.441942] [G loss: 0.177725] [ema: 0.999539] 
[Epoch 13/46] [Batch 900/1094] [D loss: 0.367787] [G loss: 0.176580] [ema: 0.999542] 
[Epoch 13/46] [Batch 1000/1094] [D loss: 0.414801] [G loss: 0.183828] [ema: 0.999545] 
[Epoch 14/46] [Batch 0/1094] [D loss: 0.402630] [G loss: 0.194678] [ema: 0.999548] 
[Epoch 14/46] [Batch 100/1094] [D loss: 0.364427] [G loss: 0.176114] [ema: 0.999550] 
[Epoch 14/46] [Batch 200/1094] [D loss: 0.380219] [G loss: 0.175597] [ema: 0.999553] 
[Epoch 14/46] [Batch 300/1094] [D loss: 0.449027] [G loss: 0.163618] [ema: 0.999556] 
[Epoch 14/46] [Batch 400/1094] [D loss: 0.462662] [G loss: 0.170895] [ema: 0.999559] 
[Epoch 14/46] [Batch 500/1094] [D loss: 0.438154] [G loss: 0.177979] [ema: 0.999562] 
[Epoch 14/46] [Batch 600/1094] [D loss: 0.443251] [G loss: 0.168910] [ema: 0.999565] 
[Epoch 14/46] [Batch 700/1094] [D loss: 0.394099] [G loss: 0.168153] [ema: 0.999567] 
[Epoch 14/46] [Batch 800/1094] [D loss: 0.383458] [G loss: 0.173224] [ema: 0.999570] 
[Epoch 14/46] [Batch 900/1094] [D loss: 0.378353] [G loss: 0.163782] [ema: 0.999573] 
[Epoch 14/46] [Batch 1000/1094] [D loss: 0.420450] [G loss: 0.167216] [ema: 0.999575] 
[Epoch 15/46] [Batch 0/1094] [D loss: 0.393268] [G loss: 0.172701] [ema: 0.999578] 
[Epoch 15/46] [Batch 100/1094] [D loss: 0.414917] [G loss: 0.164214] [ema: 0.999580] 
[Epoch 15/46] [Batch 200/1094] [D loss: 0.449987] [G loss: 0.171417] [ema: 0.999583] 
[Epoch 15/46] [Batch 300/1094] [D loss: 0.383757] [G loss: 0.190049] [ema: 0.999585] 
[Epoch 15/46] [Batch 400/1094] [D loss: 0.390824] [G loss: 0.179296] [ema: 0.999588] 
[Epoch 15/46] [Batch 500/1094] [D loss: 0.414375] [G loss: 0.181880] [ema: 0.999590] 
[Epoch 15/46] [Batch 600/1094] [D loss: 0.401225] [G loss: 0.166118] [ema: 0.999593] 
[Epoch 15/46] [Batch 700/1094] [D loss: 0.409848] [G loss: 0.172415] [ema: 0.999595] 
[Epoch 15/46] [Batch 800/1094] [D loss: 0.395929] [G loss: 0.170464] [ema: 0.999597] 
[Epoch 15/46] [Batch 900/1094] [D loss: 0.413277] [G loss: 0.176545] [ema: 0.999600] 
[Epoch 15/46] [Batch 1000/1094] [D loss: 0.427252] [G loss: 0.153618] [ema: 0.999602] 
[Epoch 16/46] [Batch 0/1094] [D loss: 0.450068] [G loss: 0.186702] [ema: 0.999604] 
[Epoch 16/46] [Batch 100/1094] [D loss: 0.359013] [G loss: 0.186738] [ema: 0.999606] 
[Epoch 16/46] [Batch 200/1094] [D loss: 0.354885] [G loss: 0.183638] [ema: 0.999609] 
[Epoch 16/46] [Batch 300/1094] [D loss: 0.408823] [G loss: 0.188805] [ema: 0.999611] 
[Epoch 16/46] [Batch 400/1094] [D loss: 0.399022] [G loss: 0.172417] [ema: 0.999613] 
[Epoch 16/46] [Batch 500/1094] [D loss: 0.396538] [G loss: 0.176178] [ema: 0.999615] 
[Epoch 16/46] [Batch 600/1094] [D loss: 0.436252] [G loss: 0.159636] [ema: 0.999617] 
[Epoch 16/46] [Batch 700/1094] [D loss: 0.406749] [G loss: 0.174275] [ema: 0.999619] 
[Epoch 16/46] [Batch 800/1094] [D loss: 0.352747] [G loss: 0.181981] [ema: 0.999621] 
[Epoch 16/46] [Batch 900/1094] [D loss: 0.409498] [G loss: 0.205714] [ema: 0.999623] 
[Epoch 16/46] [Batch 1000/1094] [D loss: 0.386883] [G loss: 0.210877] [ema: 0.999625] 
[Epoch 17/46] [Batch 0/1094] [D loss: 0.406010] [G loss: 0.181532] [ema: 0.999627] 
[Epoch 17/46] [Batch 100/1094] [D loss: 0.414884] [G loss: 0.180157] [ema: 0.999629] 
[Epoch 17/46] [Batch 200/1094] [D loss: 0.440331] [G loss: 0.175579] [ema: 0.999631] 
[Epoch 17/46] [Batch 300/1094] [D loss: 0.358907] [G loss: 0.184645] [ema: 0.999633] 
[Epoch 17/46] [Batch 400/1094] [D loss: 0.391318] [G loss: 0.191369] [ema: 0.999635] 
[Epoch 17/46] [Batch 500/1094] [D loss: 0.374050] [G loss: 0.180782] [ema: 0.999637] 
[Epoch 17/46] [Batch 600/1094] [D loss: 0.363790] [G loss: 0.178650] [ema: 0.999639] 
[Epoch 17/46] [Batch 700/1094] [D loss: 0.421259] [G loss: 0.157122] [ema: 0.999641] 
[Epoch 17/46] [Batch 800/1094] [D loss: 0.361264] [G loss: 0.185221] [ema: 0.999643] 
[Epoch 17/46] [Batch 900/1094] [D loss: 0.422482] [G loss: 0.186241] [ema: 0.999645] 
[Epoch 17/46] [Batch 1000/1094] [D loss: 0.370960] [G loss: 0.185739] [ema: 0.999646] 
[Epoch 18/46] [Batch 0/1094] [D loss: 0.385063] [G loss: 0.186973] [ema: 0.999648] 
[Epoch 18/46] [Batch 100/1094] [D loss: 0.358627] [G loss: 0.184097] [ema: 0.999650] 
[Epoch 18/46] [Batch 200/1094] [D loss: 0.407351] [G loss: 0.178898] [ema: 0.999652] 
[Epoch 18/46] [Batch 300/1094] [D loss: 0.411410] [G loss: 0.185024] [ema: 0.999653] 
[Epoch 18/46] [Batch 400/1094] [D loss: 0.408101] [G loss: 0.203670] [ema: 0.999655] 
[Epoch 18/46] [Batch 500/1094] [D loss: 0.456646] [G loss: 0.173716] [ema: 0.999657] 
[Epoch 18/46] [Batch 600/1094] [D loss: 0.387901] [G loss: 0.182294] [ema: 0.999658] 
[Epoch 18/46] [Batch 700/1094] [D loss: 0.381286] [G loss: 0.186380] [ema: 0.999660] 
[Epoch 18/46] [Batch 800/1094] [D loss: 0.388053] [G loss: 0.165535] [ema: 0.999662] 
[Epoch 18/46] [Batch 900/1094] [D loss: 0.353594] [G loss: 0.167482] [ema: 0.999663] 
[Epoch 18/46] [Batch 1000/1094] [D loss: 0.391387] [G loss: 0.185487] [ema: 0.999665] 
[Epoch 19/46] [Batch 0/1094] [D loss: 0.392425] [G loss: 0.181361] [ema: 0.999667] 
[Epoch 19/46] [Batch 100/1094] [D loss: 0.404325] [G loss: 0.171976] [ema: 0.999668] 
[Epoch 19/46] [Batch 200/1094] [D loss: 0.351166] [G loss: 0.199672] [ema: 0.999670] 
[Epoch 19/46] [Batch 300/1094] [D loss: 0.439692] [G loss: 0.193718] [ema: 0.999671] 
[Epoch 19/46] [Batch 400/1094] [D loss: 0.436295] [G loss: 0.200191] [ema: 0.999673] 
[Epoch 19/46] [Batch 500/1094] [D loss: 0.357172] [G loss: 0.184199] [ema: 0.999674] 
[Epoch 19/46] [Batch 600/1094] [D loss: 0.380380] [G loss: 0.167050] [ema: 0.999676] 
[Epoch 19/46] [Batch 700/1094] [D loss: 0.401388] [G loss: 0.189823] [ema: 0.999677] 
[Epoch 19/46] [Batch 800/1094] [D loss: 0.387683] [G loss: 0.180417] [ema: 0.999679] 
[Epoch 19/46] [Batch 900/1094] [D loss: 0.387792] [G loss: 0.185829] [ema: 0.999680] 
[Epoch 19/46] [Batch 1000/1094] [D loss: 0.364526] [G loss: 0.216312] [ema: 0.999682] 
[Epoch 20/46] [Batch 0/1094] [D loss: 0.415835] [G loss: 0.162870] [ema: 0.999683] 
[Epoch 20/46] [Batch 100/1094] [D loss: 0.375827] [G loss: 0.170359] [ema: 0.999685] 
[Epoch 20/46] [Batch 200/1094] [D loss: 0.358387] [G loss: 0.186707] [ema: 0.999686] 
[Epoch 20/46] [Batch 300/1094] [D loss: 0.376780] [G loss: 0.184649] [ema: 0.999688] 
[Epoch 20/46] [Batch 400/1094] [D loss: 0.383055] [G loss: 0.177194] [ema: 0.999689] 
[Epoch 20/46] [Batch 500/1094] [D loss: 0.379983] [G loss: 0.170494] [ema: 0.999690] 
[Epoch 20/46] [Batch 600/1094] [D loss: 0.386597] [G loss: 0.195094] [ema: 0.999692] 
[Epoch 20/46] [Batch 700/1094] [D loss: 0.416053] [G loss: 0.170074] [ema: 0.999693] 
[Epoch 20/46] [Batch 800/1094] [D loss: 0.364853] [G loss: 0.196124] [ema: 0.999694] 
[Epoch 20/46] [Batch 900/1094] [D loss: 0.416402] [G loss: 0.182042] [ema: 0.999696] 
[Epoch 20/46] [Batch 1000/1094] [D loss: 0.319115] [G loss: 0.198020] [ema: 0.999697] 
[Epoch 21/46] [Batch 0/1094] [D loss: 0.387215] [G loss: 0.191576] [ema: 0.999698] 
[Epoch 21/46] [Batch 100/1094] [D loss: 0.372982] [G loss: 0.191353] [ema: 0.999700] 
[Epoch 21/46] [Batch 200/1094] [D loss: 0.492458] [G loss: 0.179449] [ema: 0.999701] 
[Epoch 21/46] [Batch 300/1094] [D loss: 0.297928] [G loss: 0.238478] [ema: 0.999702] 
[Epoch 21/46] [Batch 400/1094] [D loss: 0.431410] [G loss: 0.162793] [ema: 0.999703] 
[Epoch 21/46] [Batch 500/1094] [D loss: 0.506260] [G loss: 0.145429] [ema: 0.999705] 
[Epoch 21/46] [Batch 600/1094] [D loss: 0.416928] [G loss: 0.165162] [ema: 0.999706] 
[Epoch 21/46] [Batch 700/1094] [D loss: 0.397619] [G loss: 0.182329] [ema: 0.999707] 
[Epoch 21/46] [Batch 800/1094] [D loss: 0.373715] [G loss: 0.148929] [ema: 0.999708] 
[Epoch 21/46] [Batch 900/1094] [D loss: 0.406808] [G loss: 0.190352] [ema: 0.999710] 
[Epoch 21/46] [Batch 1000/1094] [D loss: 0.427473] [G loss: 0.179622] [ema: 0.999711] 
[Epoch 22/46] [Batch 0/1094] [D loss: 0.393057] [G loss: 0.181625] [ema: 0.999712] 
[Epoch 22/46] [Batch 100/1094] [D loss: 0.407134] [G loss: 0.183051] [ema: 0.999713] 
[Epoch 22/46] [Batch 200/1094] [D loss: 0.410651] [G loss: 0.180766] [ema: 0.999714] 
[Epoch 22/46] [Batch 300/1094] [D loss: 0.409085] [G loss: 0.185130] [ema: 0.999716] 
[Epoch 22/46] [Batch 400/1094] [D loss: 0.402193] [G loss: 0.163761] [ema: 0.999717] 
[Epoch 22/46] [Batch 500/1094] [D loss: 0.366576] [G loss: 0.184580] [ema: 0.999718] 
[Epoch 22/46] [Batch 600/1094] [D loss: 0.389618] [G loss: 0.177696] [ema: 0.999719] 
[Epoch 22/46] [Batch 700/1094] [D loss: 0.370439] [G loss: 0.186863] [ema: 0.999720] 
[Epoch 22/46] [Batch 800/1094] [D loss: 0.352703] [G loss: 0.178089] [ema: 0.999721] 
[Epoch 22/46] [Batch 900/1094] [D loss: 0.373851] [G loss: 0.192824] [ema: 0.999722] 
[Epoch 22/46] [Batch 1000/1094] [D loss: 0.416503] [G loss: 0.200101] [ema: 0.999724] 
[Epoch 23/46] [Batch 0/1094] [D loss: 0.409204] [G loss: 0.193743] [ema: 0.999725] 
[Epoch 23/46] [Batch 100/1094] [D loss: 0.423118] [G loss: 0.203536] [ema: 0.999726] 
[Epoch 23/46] [Batch 200/1094] [D loss: 0.382595] [G loss: 0.205150] [ema: 0.999727] 
[Epoch 23/46] [Batch 300/1094] [D loss: 0.395666] [G loss: 0.183682] [ema: 0.999728] 
[Epoch 23/46] [Batch 400/1094] [D loss: 0.406987] [G loss: 0.178731] [ema: 0.999729] 
[Epoch 23/46] [Batch 500/1094] [D loss: 0.384109] [G loss: 0.182528] [ema: 0.999730] 
[Epoch 23/46] [Batch 600/1094] [D loss: 0.433397] [G loss: 0.194172] [ema: 0.999731] 
[Epoch 23/46] [Batch 700/1094] [D loss: 0.342168] [G loss: 0.189888] [ema: 0.999732] 
[Epoch 23/46] [Batch 800/1094] [D loss: 0.367740] [G loss: 0.188571] [ema: 0.999733] 
[Epoch 23/46] [Batch 900/1094] [D loss: 0.413287] [G loss: 0.182179] [ema: 0.999734] 
[Epoch 23/46] [Batch 1000/1094] [D loss: 0.387135] [G loss: 0.204891] [ema: 0.999735] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_30_100/WISDM_DAGHAR_Multiclass_50000_D_30_2024_10_28_19_52_19/Model



[Epoch 24/46] [Batch 0/1094] [D loss: 0.423043] [G loss: 0.167818] [ema: 0.999736] 
[Epoch 24/46] [Batch 100/1094] [D loss: 0.393947] [G loss: 0.181718] [ema: 0.999737] 
[Epoch 24/46] [Batch 200/1094] [D loss: 0.372510] [G loss: 0.212472] [ema: 0.999738] 
[Epoch 24/46] [Batch 300/1094] [D loss: 0.393138] [G loss: 0.182012] [ema: 0.999739] 
[Epoch 24/46] [Batch 400/1094] [D loss: 0.392191] [G loss: 0.149825] [ema: 0.999740] 
[Epoch 24/46] [Batch 500/1094] [D loss: 0.426786] [G loss: 0.184140] [ema: 0.999741] 
[Epoch 24/46] [Batch 600/1094] [D loss: 0.352235] [G loss: 0.198997] [ema: 0.999742] 
[Epoch 24/46] [Batch 700/1094] [D loss: 0.369593] [G loss: 0.174320] [ema: 0.999743] 
[Epoch 24/46] [Batch 800/1094] [D loss: 0.388631] [G loss: 0.184905] [ema: 0.999744] 
[Epoch 24/46] [Batch 900/1094] [D loss: 0.400759] [G loss: 0.145249] [ema: 0.999745] 
[Epoch 24/46] [Batch 1000/1094] [D loss: 0.339160] [G loss: 0.203507] [ema: 0.999746] 
[Epoch 25/46] [Batch 0/1094] [D loss: 0.408298] [G loss: 0.169812] [ema: 0.999747] 
[Epoch 25/46] [Batch 100/1094] [D loss: 0.362733] [G loss: 0.206635] [ema: 0.999748] 
[Epoch 25/46] [Batch 200/1094] [D loss: 0.373911] [G loss: 0.190854] [ema: 0.999748] 
[Epoch 25/46] [Batch 300/1094] [D loss: 0.391087] [G loss: 0.178331] [ema: 0.999749] 
[Epoch 25/46] [Batch 400/1094] [D loss: 0.357987] [G loss: 0.202370] [ema: 0.999750] 
[Epoch 25/46] [Batch 500/1094] [D loss: 0.393649] [G loss: 0.201583] [ema: 0.999751] 
[Epoch 25/46] [Batch 600/1094] [D loss: 0.397554] [G loss: 0.179773] [ema: 0.999752] 
[Epoch 25/46] [Batch 700/1094] [D loss: 0.365641] [G loss: 0.195341] [ema: 0.999753] 
[Epoch 25/46] [Batch 800/1094] [D loss: 0.395286] [G loss: 0.200306] [ema: 0.999754] 
[Epoch 25/46] [Batch 900/1094] [D loss: 0.346610] [G loss: 0.202037] [ema: 0.999755] 
[Epoch 25/46] [Batch 1000/1094] [D loss: 0.359849] [G loss: 0.201086] [ema: 0.999756] 
[Epoch 26/46] [Batch 0/1094] [D loss: 0.370336] [G loss: 0.182217] [ema: 0.999756] 
[Epoch 26/46] [Batch 100/1094] [D loss: 0.368858] [G loss: 0.187793] [ema: 0.999757] 
[Epoch 26/46] [Batch 200/1094] [D loss: 0.371900] [G loss: 0.186876] [ema: 0.999758] 
[Epoch 26/46] [Batch 300/1094] [D loss: 0.331115] [G loss: 0.197481] [ema: 0.999759] 
[Epoch 26/46] [Batch 400/1094] [D loss: 0.365649] [G loss: 0.209762] [ema: 0.999760] 
[Epoch 26/46] [Batch 500/1094] [D loss: 0.418283] [G loss: 0.173139] [ema: 0.999761] 
[Epoch 26/46] [Batch 600/1094] [D loss: 0.332250] [G loss: 0.204853] [ema: 0.999761] 
[Epoch 26/46] [Batch 700/1094] [D loss: 0.396279] [G loss: 0.186830] [ema: 0.999762] 
[Epoch 26/46] [Batch 800/1094] [D loss: 0.365693] [G loss: 0.195967] [ema: 0.999763] 
[Epoch 26/46] [Batch 900/1094] [D loss: 0.333190] [G loss: 0.186432] [ema: 0.999764] 
[Epoch 26/46] [Batch 1000/1094] [D loss: 0.394816] [G loss: 0.174826] [ema: 0.999765] 
[Epoch 27/46] [Batch 0/1094] [D loss: 0.430015] [G loss: 0.206600] [ema: 0.999765] 
[Epoch 27/46] [Batch 100/1094] [D loss: 0.366802] [G loss: 0.199951] [ema: 0.999766] 
[Epoch 27/46] [Batch 200/1094] [D loss: 0.394189] [G loss: 0.184787] [ema: 0.999767] 
[Epoch 27/46] [Batch 300/1094] [D loss: 0.374941] [G loss: 0.220608] [ema: 0.999768] 
[Epoch 27/46] [Batch 400/1094] [D loss: 0.425671] [G loss: 0.181230] [ema: 0.999768] 
[Epoch 27/46] [Batch 500/1094] [D loss: 0.263452] [G loss: 0.219176] [ema: 0.999769] 
[Epoch 27/46] [Batch 600/1094] [D loss: 0.496666] [G loss: 0.135275] [ema: 0.999770] 
[Epoch 27/46] [Batch 700/1094] [D loss: 0.391987] [G loss: 0.185567] [ema: 0.999771] 
[Epoch 27/46] [Batch 800/1094] [D loss: 0.418153] [G loss: 0.175987] [ema: 0.999772] 
[Epoch 27/46] [Batch 900/1094] [D loss: 0.415524] [G loss: 0.157641] [ema: 0.999772] 
[Epoch 27/46] [Batch 1000/1094] [D loss: 0.410502] [G loss: 0.178081] [ema: 0.999773] 
[Epoch 28/46] [Batch 0/1094] [D loss: 0.372236] [G loss: 0.199629] [ema: 0.999774] 
[Epoch 28/46] [Batch 100/1094] [D loss: 0.374098] [G loss: 0.187609] [ema: 0.999774] 
[Epoch 28/46] [Batch 200/1094] [D loss: 0.382577] [G loss: 0.202656] [ema: 0.999775] 
[Epoch 28/46] [Batch 300/1094] [D loss: 0.359234] [G loss: 0.190763] [ema: 0.999776] 
[Epoch 28/46] [Batch 400/1094] [D loss: 0.383114] [G loss: 0.178879] [ema: 0.999777] 
[Epoch 28/46] [Batch 500/1094] [D loss: 0.350093] [G loss: 0.182799] [ema: 0.999777] 
[Epoch 28/46] [Batch 600/1094] [D loss: 0.353353] [G loss: 0.202146] [ema: 0.999778] 
[Epoch 28/46] [Batch 700/1094] [D loss: 0.361520] [G loss: 0.215061] [ema: 0.999779] 
[Epoch 28/46] [Batch 800/1094] [D loss: 0.357441] [G loss: 0.184695] [ema: 0.999780] 
[Epoch 28/46] [Batch 900/1094] [D loss: 0.443493] [G loss: 0.196606] [ema: 0.999780] 
[Epoch 28/46] [Batch 1000/1094] [D loss: 0.338698] [G loss: 0.188414] [ema: 0.999781] 
[Epoch 29/46] [Batch 0/1094] [D loss: 0.361651] [G loss: 0.213483] [ema: 0.999782] 
[Epoch 29/46] [Batch 100/1094] [D loss: 0.340598] [G loss: 0.198369] [ema: 0.999782] 
[Epoch 29/46] [Batch 200/1094] [D loss: 0.380409] [G loss: 0.184909] [ema: 0.999783] 
[Epoch 29/46] [Batch 300/1094] [D loss: 0.400491] [G loss: 0.193156] [ema: 0.999784] 
[Epoch 29/46] [Batch 400/1094] [D loss: 0.362064] [G loss: 0.174656] [ema: 0.999784] 
[Epoch 29/46] [Batch 500/1094] [D loss: 0.412309] [G loss: 0.188926] [ema: 0.999785] 
[Epoch 29/46] [Batch 600/1094] [D loss: 0.352027] [G loss: 0.171233] [ema: 0.999786] 
[Epoch 29/46] [Batch 700/1094] [D loss: 0.391603] [G loss: 0.199387] [ema: 0.999786] 
[Epoch 29/46] [Batch 800/1094] [D loss: 0.387659] [G loss: 0.205776] [ema: 0.999787] 
[Epoch 29/46] [Batch 900/1094] [D loss: 0.386715] [G loss: 0.205982] [ema: 0.999788] 
[Epoch 29/46] [Batch 1000/1094] [D loss: 0.323099] [G loss: 0.174208] [ema: 0.999788] 
[Epoch 30/46] [Batch 0/1094] [D loss: 0.354867] [G loss: 0.211188] [ema: 0.999789] 
[Epoch 30/46] [Batch 100/1094] [D loss: 0.430581] [G loss: 0.213059] [ema: 0.999789] 
[Epoch 30/46] [Batch 200/1094] [D loss: 0.329579] [G loss: 0.217956] [ema: 0.999790] 
[Epoch 30/46] [Batch 300/1094] [D loss: 0.412836] [G loss: 0.174996] [ema: 0.999791] 
[Epoch 30/46] [Batch 400/1094] [D loss: 0.352709] [G loss: 0.214240] [ema: 0.999791] 
[Epoch 30/46] [Batch 500/1094] [D loss: 0.346682] [G loss: 0.195227] [ema: 0.999792] 
[Epoch 30/46] [Batch 600/1094] [D loss: 0.359251] [G loss: 0.199844] [ema: 0.999793] 
[Epoch 30/46] [Batch 700/1094] [D loss: 0.397199] [G loss: 0.190787] [ema: 0.999793] 
[Epoch 30/46] [Batch 800/1094] [D loss: 0.365276] [G loss: 0.186532] [ema: 0.999794] 
[Epoch 30/46] [Batch 900/1094] [D loss: 0.350539] [G loss: 0.185227] [ema: 0.999794] 
[Epoch 30/46] [Batch 1000/1094] [D loss: 0.414149] [G loss: 0.188447] [ema: 0.999795] 
[Epoch 31/46] [Batch 0/1094] [D loss: 0.375739] [G loss: 0.180747] [ema: 0.999796] 
[Epoch 31/46] [Batch 100/1094] [D loss: 0.348901] [G loss: 0.194312] [ema: 0.999796] 
[Epoch 31/46] [Batch 200/1094] [D loss: 0.359955] [G loss: 0.201702] [ema: 0.999797] 
[Epoch 31/46] [Batch 300/1094] [D loss: 0.351196] [G loss: 0.206734] [ema: 0.999797] 
[Epoch 31/46] [Batch 400/1094] [D loss: 0.370061] [G loss: 0.197628] [ema: 0.999798] 
[Epoch 31/46] [Batch 500/1094] [D loss: 0.355578] [G loss: 0.169874] [ema: 0.999799] 
[Epoch 31/46] [Batch 600/1094] [D loss: 0.312388] [G loss: 0.214559] [ema: 0.999799] 
[Epoch 31/46] [Batch 700/1094] [D loss: 0.359518] [G loss: 0.216714] [ema: 0.999800] 
[Epoch 31/46] [Batch 800/1094] [D loss: 0.384504] [G loss: 0.155977] [ema: 0.999800] 
[Epoch 31/46] [Batch 900/1094] [D loss: 0.356730] [G loss: 0.198192] [ema: 0.999801] 
[Epoch 31/46] [Batch 1000/1094] [D loss: 0.345842] [G loss: 0.203036] [ema: 0.999801] 
[Epoch 32/46] [Batch 0/1094] [D loss: 0.319782] [G loss: 0.206469] [ema: 0.999802] 
[Epoch 32/46] [Batch 100/1094] [D loss: 0.405648] [G loss: 0.191359] [ema: 0.999803] 
[Epoch 32/46] [Batch 200/1094] [D loss: 0.297669] [G loss: 0.203177] [ema: 0.999803] 
[Epoch 32/46] [Batch 300/1094] [D loss: 0.377429] [G loss: 0.202954] [ema: 0.999804] 
[Epoch 32/46] [Batch 400/1094] [D loss: 0.384255] [G loss: 0.186762] [ema: 0.999804] 
[Epoch 32/46] [Batch 500/1094] [D loss: 0.381361] [G loss: 0.197523] [ema: 0.999805] 
[Epoch 32/46] [Batch 600/1094] [D loss: 0.346585] [G loss: 0.177328] [ema: 0.999805] 
[Epoch 32/46] [Batch 700/1094] [D loss: 0.312360] [G loss: 0.202546] [ema: 0.999806] 
[Epoch 32/46] [Batch 800/1094] [D loss: 0.430811] [G loss: 0.188986] [ema: 0.999806] 
[Epoch 32/46] [Batch 900/1094] [D loss: 0.377264] [G loss: 0.198236] [ema: 0.999807] 
[Epoch 32/46] [Batch 1000/1094] [D loss: 0.381152] [G loss: 0.199666] [ema: 0.999808] 
[Epoch 33/46] [Batch 0/1094] [D loss: 0.361832] [G loss: 0.215386] [ema: 0.999808] 
[Epoch 33/46] [Batch 100/1094] [D loss: 0.336159] [G loss: 0.193812] [ema: 0.999809] 
[Epoch 33/46] [Batch 200/1094] [D loss: 0.372167] [G loss: 0.206666] [ema: 0.999809] 
[Epoch 33/46] [Batch 300/1094] [D loss: 0.361409] [G loss: 0.201522] [ema: 0.999810] 
[Epoch 33/46] [Batch 400/1094] [D loss: 0.420822] [G loss: 0.176990] [ema: 0.999810] 
[Epoch 33/46] [Batch 500/1094] [D loss: 0.390085] [G loss: 0.216892] [ema: 0.999811] 
[Epoch 33/46] [Batch 600/1094] [D loss: 0.350886] [G loss: 0.183517] [ema: 0.999811] 
[Epoch 33/46] [Batch 700/1094] [D loss: 0.357247] [G loss: 0.190219] [ema: 0.999812] 
[Epoch 33/46] [Batch 800/1094] [D loss: 0.354749] [G loss: 0.189264] [ema: 0.999812] 
[Epoch 33/46] [Batch 900/1094] [D loss: 0.348961] [G loss: 0.207218] [ema: 0.999813] 
[Epoch 33/46] [Batch 1000/1094] [D loss: 0.327497] [G loss: 0.209331] [ema: 0.999813] 
[Epoch 34/46] [Batch 0/1094] [D loss: 0.351776] [G loss: 0.212041] [ema: 0.999814] 
[Epoch 34/46] [Batch 100/1094] [D loss: 0.368483] [G loss: 0.202482] [ema: 0.999814] 
[Epoch 34/46] [Batch 200/1094] [D loss: 0.307087] [G loss: 0.192015] [ema: 0.999815] 
[Epoch 34/46] [Batch 300/1094] [D loss: 0.348896] [G loss: 0.196518] [ema: 0.999815] 
[Epoch 34/46] [Batch 400/1094] [D loss: 0.367365] [G loss: 0.197335] [ema: 0.999816] 
[Epoch 34/46] [Batch 500/1094] [D loss: 0.338232] [G loss: 0.195171] [ema: 0.999816] 
[Epoch 34/46] [Batch 600/1094] [D loss: 0.372808] [G loss: 0.203422] [ema: 0.999817] 
[Epoch 34/46] [Batch 700/1094] [D loss: 0.370536] [G loss: 0.207447] [ema: 0.999817] 
[Epoch 34/46] [Batch 800/1094] [D loss: 0.358921] [G loss: 0.172902] [ema: 0.999818] 
[Epoch 34/46] [Batch 900/1094] [D loss: 0.379958] [G loss: 0.198281] [ema: 0.999818] 
[Epoch 34/46] [Batch 1000/1094] [D loss: 0.364431] [G loss: 0.183900] [ema: 0.999819] 
[Epoch 35/46] [Batch 0/1094] [D loss: 0.377858] [G loss: 0.204493] [ema: 0.999819] 
[Epoch 35/46] [Batch 100/1094] [D loss: 0.430335] [G loss: 0.186555] [ema: 0.999819] 
[Epoch 35/46] [Batch 200/1094] [D loss: 0.340914] [G loss: 0.191954] [ema: 0.999820] 
[Epoch 35/46] [Batch 300/1094] [D loss: 0.311572] [G loss: 0.192061] [ema: 0.999820] 
[Epoch 35/46] [Batch 400/1094] [D loss: 0.374557] [G loss: 0.198675] [ema: 0.999821] 
[Epoch 35/46] [Batch 500/1094] [D loss: 0.332471] [G loss: 0.196838] [ema: 0.999821] 
[Epoch 35/46] [Batch 600/1094] [D loss: 0.370776] [G loss: 0.189486] [ema: 0.999822] 
[Epoch 35/46] [Batch 700/1094] [D loss: 0.395961] [G loss: 0.196037] [ema: 0.999822] 
[Epoch 35/46] [Batch 800/1094] [D loss: 0.364460] [G loss: 0.201525] [ema: 0.999823] 
[Epoch 35/46] [Batch 900/1094] [D loss: 0.386018] [G loss: 0.195246] [ema: 0.999823] 
[Epoch 35/46] [Batch 1000/1094] [D loss: 0.357631] [G loss: 0.189057] [ema: 0.999824] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_30_100/WISDM_DAGHAR_Multiclass_50000_D_30_2024_10_28_19_52_19/Model



[Epoch 36/46] [Batch 0/1094] [D loss: 0.367965] [G loss: 0.212178] [ema: 0.999824] 
[Epoch 36/46] [Batch 100/1094] [D loss: 0.322668] [G loss: 0.211172] [ema: 0.999824] 
[Epoch 36/46] [Batch 200/1094] [D loss: 0.336348] [G loss: 0.216109] [ema: 0.999825] 
[Epoch 36/46] [Batch 300/1094] [D loss: 0.337113] [G loss: 0.208897] [ema: 0.999825] 
[Epoch 36/46] [Batch 400/1094] [D loss: 0.434062] [G loss: 0.183702] [ema: 0.999826] 
[Epoch 36/46] [Batch 500/1094] [D loss: 0.340391] [G loss: 0.193294] [ema: 0.999826] 
[Epoch 36/46] [Batch 600/1094] [D loss: 0.355275] [G loss: 0.197653] [ema: 0.999827] 
[Epoch 36/46] [Batch 700/1094] [D loss: 0.363162] [G loss: 0.197964] [ema: 0.999827] 
[Epoch 36/46] [Batch 800/1094] [D loss: 0.408547] [G loss: 0.188220] [ema: 0.999828] 
[Epoch 36/46] [Batch 900/1094] [D loss: 0.353238] [G loss: 0.179778] [ema: 0.999828] 
[Epoch 36/46] [Batch 1000/1094] [D loss: 0.326539] [G loss: 0.208108] [ema: 0.999828] 
[Epoch 37/46] [Batch 0/1094] [D loss: 0.411540] [G loss: 0.173916] [ema: 0.999829] 
[Epoch 37/46] [Batch 100/1094] [D loss: 0.390881] [G loss: 0.166006] [ema: 0.999829] 
[Epoch 37/46] [Batch 200/1094] [D loss: 0.371453] [G loss: 0.227470] [ema: 0.999830] 
[Epoch 37/46] [Batch 300/1094] [D loss: 0.410744] [G loss: 0.203698] [ema: 0.999830] 
[Epoch 37/46] [Batch 400/1094] [D loss: 0.358479] [G loss: 0.206840] [ema: 0.999830] 
[Epoch 37/46] [Batch 500/1094] [D loss: 0.371364] [G loss: 0.195815] [ema: 0.999831] 
[Epoch 37/46] [Batch 600/1094] [D loss: 0.369759] [G loss: 0.214742] [ema: 0.999831] 
[Epoch 37/46] [Batch 700/1094] [D loss: 0.342535] [G loss: 0.217724] [ema: 0.999832] 
[Epoch 37/46] [Batch 800/1094] [D loss: 0.352670] [G loss: 0.191977] [ema: 0.999832] 
[Epoch 37/46] [Batch 900/1094] [D loss: 0.351345] [G loss: 0.211977] [ema: 0.999832] 
[Epoch 37/46] [Batch 1000/1094] [D loss: 0.282568] [G loss: 0.225998] [ema: 0.999833] 
[Epoch 38/46] [Batch 0/1094] [D loss: 0.304506] [G loss: 0.231869] [ema: 0.999833] 
[Epoch 38/46] [Batch 100/1094] [D loss: 0.403192] [G loss: 0.187749] [ema: 0.999834] 
[Epoch 38/46] [Batch 200/1094] [D loss: 0.356657] [G loss: 0.192851] [ema: 0.999834] 
[Epoch 38/46] [Batch 300/1094] [D loss: 0.352706] [G loss: 0.182855] [ema: 0.999834] 
[Epoch 38/46] [Batch 400/1094] [D loss: 0.389353] [G loss: 0.200279] [ema: 0.999835] 
[Epoch 38/46] [Batch 500/1094] [D loss: 0.365396] [G loss: 0.181984] [ema: 0.999835] 
[Epoch 38/46] [Batch 600/1094] [D loss: 0.327145] [G loss: 0.216109] [ema: 0.999836] 
[Epoch 38/46] [Batch 700/1094] [D loss: 0.372966] [G loss: 0.186481] [ema: 0.999836] 
[Epoch 38/46] [Batch 800/1094] [D loss: 0.391659] [G loss: 0.210907] [ema: 0.999836] 
[Epoch 38/46] [Batch 900/1094] [D loss: 0.332748] [G loss: 0.182417] [ema: 0.999837] 
[Epoch 38/46] [Batch 1000/1094] [D loss: 0.399810] [G loss: 0.203042] [ema: 0.999837] 
[Epoch 39/46] [Batch 0/1094] [D loss: 0.392315] [G loss: 0.165685] [ema: 0.999838] 
[Epoch 39/46] [Batch 100/1094] [D loss: 0.355747] [G loss: 0.218236] [ema: 0.999838] 
[Epoch 39/46] [Batch 200/1094] [D loss: 0.393635] [G loss: 0.196263] [ema: 0.999838] 
[Epoch 39/46] [Batch 300/1094] [D loss: 0.347437] [G loss: 0.191965] [ema: 0.999839] 
[Epoch 39/46] [Batch 400/1094] [D loss: 0.390000] [G loss: 0.200884] [ema: 0.999839] 
[Epoch 39/46] [Batch 500/1094] [D loss: 0.395609] [G loss: 0.198937] [ema: 0.999839] 
[Epoch 39/46] [Batch 600/1094] [D loss: 0.342145] [G loss: 0.200466] [ema: 0.999840] 
[Epoch 39/46] [Batch 700/1094] [D loss: 0.375999] [G loss: 0.190531] [ema: 0.999840] 
[Epoch 39/46] [Batch 800/1094] [D loss: 0.392252] [G loss: 0.196991] [ema: 0.999841] 
[Epoch 39/46] [Batch 900/1094] [D loss: 0.355287] [G loss: 0.169913] [ema: 0.999841] 
[Epoch 39/46] [Batch 1000/1094] [D loss: 0.347468] [G loss: 0.214305] [ema: 0.999841] 
[Epoch 40/46] [Batch 0/1094] [D loss: 0.339607] [G loss: 0.216928] [ema: 0.999842] 
[Epoch 40/46] [Batch 100/1094] [D loss: 0.318559] [G loss: 0.178902] [ema: 0.999842] 
[Epoch 40/46] [Batch 200/1094] [D loss: 0.359476] [G loss: 0.203509] [ema: 0.999842] 
[Epoch 40/46] [Batch 300/1094] [D loss: 0.378959] [G loss: 0.195796] [ema: 0.999843] 
[Epoch 40/46] [Batch 400/1094] [D loss: 0.350134] [G loss: 0.179044] [ema: 0.999843] 
[Epoch 40/46] [Batch 500/1094] [D loss: 0.320555] [G loss: 0.189185] [ema: 0.999843] 
[Epoch 40/46] [Batch 600/1094] [D loss: 0.356412] [G loss: 0.218108] [ema: 0.999844] 
[Epoch 40/46] [Batch 700/1094] [D loss: 0.345722] [G loss: 0.200442] [ema: 0.999844] 
[Epoch 40/46] [Batch 800/1094] [D loss: 0.353517] [G loss: 0.208713] [ema: 0.999844] 
[Epoch 40/46] [Batch 900/1094] [D loss: 0.323292] [G loss: 0.207713] [ema: 0.999845] 
[Epoch 40/46] [Batch 1000/1094] [D loss: 0.376527] [G loss: 0.192960] [ema: 0.999845] 
[Epoch 41/46] [Batch 0/1094] [D loss: 0.367597] [G loss: 0.197249] [ema: 0.999845] 
[Epoch 41/46] [Batch 100/1094] [D loss: 0.377810] [G loss: 0.213489] [ema: 0.999846] 
[Epoch 41/46] [Batch 200/1094] [D loss: 0.347191] [G loss: 0.189603] [ema: 0.999846] 
[Epoch 41/46] [Batch 300/1094] [D loss: 0.357161] [G loss: 0.203235] [ema: 0.999847] 
[Epoch 41/46] [Batch 400/1094] [D loss: 0.351843] [G loss: 0.198481] [ema: 0.999847] 
[Epoch 41/46] [Batch 500/1094] [D loss: 0.359303] [G loss: 0.193841] [ema: 0.999847] 
[Epoch 41/46] [Batch 600/1094] [D loss: 0.432192] [G loss: 0.203702] [ema: 0.999848] 
[Epoch 41/46] [Batch 700/1094] [D loss: 0.329490] [G loss: 0.179420] [ema: 0.999848] 
[Epoch 41/46] [Batch 800/1094] [D loss: 0.408532] [G loss: 0.209668] [ema: 0.999848] 
[Epoch 41/46] [Batch 900/1094] [D loss: 0.346893] [G loss: 0.204166] [ema: 0.999849] 
[Epoch 41/46] [Batch 1000/1094] [D loss: 0.295668] [G loss: 0.210037] [ema: 0.999849] 
[Epoch 42/46] [Batch 0/1094] [D loss: 0.423805] [G loss: 0.185786] [ema: 0.999849] 
[Epoch 42/46] [Batch 100/1094] [D loss: 0.368200] [G loss: 0.209372] [ema: 0.999849] 
[Epoch 42/46] [Batch 200/1094] [D loss: 0.363698] [G loss: 0.178862] [ema: 0.999850] 
[Epoch 42/46] [Batch 300/1094] [D loss: 0.358050] [G loss: 0.209891] [ema: 0.999850] 
[Epoch 42/46] [Batch 400/1094] [D loss: 0.300088] [G loss: 0.213485] [ema: 0.999850] 
[Epoch 42/46] [Batch 500/1094] [D loss: 0.345383] [G loss: 0.195532] [ema: 0.999851] 
[Epoch 42/46] [Batch 600/1094] [D loss: 0.384847] [G loss: 0.172168] [ema: 0.999851] 
[Epoch 42/46] [Batch 700/1094] [D loss: 0.315635] [G loss: 0.197427] [ema: 0.999851] 
[Epoch 42/46] [Batch 800/1094] [D loss: 0.356172] [G loss: 0.194285] [ema: 0.999852] 
[Epoch 42/46] [Batch 900/1094] [D loss: 0.421937] [G loss: 0.189228] [ema: 0.999852] 
[Epoch 42/46] [Batch 1000/1094] [D loss: 0.322911] [G loss: 0.186056] [ema: 0.999852] 
[Epoch 43/46] [Batch 0/1094] [D loss: 0.405662] [G loss: 0.199742] [ema: 0.999853] 
[Epoch 43/46] [Batch 100/1094] [D loss: 0.353054] [G loss: 0.199920] [ema: 0.999853] 
[Epoch 43/46] [Batch 200/1094] [D loss: 0.371040] [G loss: 0.185343] [ema: 0.999853] 
[Epoch 43/46] [Batch 300/1094] [D loss: 0.317264] [G loss: 0.207357] [ema: 0.999854] 
[Epoch 43/46] [Batch 400/1094] [D loss: 0.327946] [G loss: 0.194847] [ema: 0.999854] 
[Epoch 43/46] [Batch 500/1094] [D loss: 0.378210] [G loss: 0.194494] [ema: 0.999854] 
[Epoch 43/46] [Batch 600/1094] [D loss: 0.343655] [G loss: 0.193602] [ema: 0.999855] 
[Epoch 43/46] [Batch 700/1094] [D loss: 0.389277] [G loss: 0.167806] [ema: 0.999855] 
[Epoch 43/46] [Batch 800/1094] [D loss: 0.325914] [G loss: 0.203756] [ema: 0.999855] 
[Epoch 43/46] [Batch 900/1094] [D loss: 0.402537] [G loss: 0.187529] [ema: 0.999855] 
[Epoch 43/46] [Batch 1000/1094] [D loss: 0.341259] [G loss: 0.197698] [ema: 0.999856] 
[Epoch 44/46] [Batch 0/1094] [D loss: 0.375414] [G loss: 0.201335] [ema: 0.999856] 
[Epoch 44/46] [Batch 100/1094] [D loss: 0.364386] [G loss: 0.209199] [ema: 0.999856] 
[Epoch 44/46] [Batch 200/1094] [D loss: 0.353492] [G loss: 0.179565] [ema: 0.999857] 
[Epoch 44/46] [Batch 300/1094] [D loss: 0.312502] [G loss: 0.214657] [ema: 0.999857] 
[Epoch 44/46] [Batch 400/1094] [D loss: 0.370726] [G loss: 0.183903] [ema: 0.999857] 
[Epoch 44/46] [Batch 500/1094] [D loss: 0.329627] [G loss: 0.203764] [ema: 0.999857] 
[Epoch 44/46] [Batch 600/1094] [D loss: 0.367720] [G loss: 0.177402] [ema: 0.999858] 
[Epoch 44/46] [Batch 700/1094] [D loss: 0.324578] [G loss: 0.197163] [ema: 0.999858] 
[Epoch 44/46] [Batch 800/1094] [D loss: 0.368456] [G loss: 0.206685] [ema: 0.999858] 
[Epoch 44/46] [Batch 900/1094] [D loss: 0.413827] [G loss: 0.181353] [ema: 0.999859] 
[Epoch 44/46] [Batch 1000/1094] [D loss: 0.338937] [G loss: 0.199189] [ema: 0.999859] 
[Epoch 45/46] [Batch 0/1094] [D loss: 0.350082] [G loss: 0.196106] [ema: 0.999859] 
[Epoch 45/46] [Batch 100/1094] [D loss: 0.292171] [G loss: 0.235218] [ema: 0.999859] 
[Epoch 45/46] [Batch 200/1094] [D loss: 0.382263] [G loss: 0.168756] [ema: 0.999860] 
[Epoch 45/46] [Batch 300/1094] [D loss: 0.416030] [G loss: 0.175703] [ema: 0.999860] 
[Epoch 45/46] [Batch 400/1094] [D loss: 0.407985] [G loss: 0.192454] [ema: 0.999860] 
[Epoch 45/46] [Batch 500/1094] [D loss: 0.376813] [G loss: 0.200561] [ema: 0.999861] 
[Epoch 45/46] [Batch 600/1094] [D loss: 0.350997] [G loss: 0.187462] [ema: 0.999861] 
[Epoch 45/46] [Batch 700/1094] [D loss: 0.357515] [G loss: 0.174738] [ema: 0.999861] 
[Epoch 45/46] [Batch 800/1094] [D loss: 0.329949] [G loss: 0.214615] [ema: 0.999861] 
[Epoch 45/46] [Batch 900/1094] [D loss: 0.358633] [G loss: 0.226230] [ema: 0.999862] 
[Epoch 45/46] [Batch 1000/1094] [D loss: 0.431006] [G loss: 0.179266] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
UCI_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
UCI_DAGHAR_Multiclass
daghar
return single class data and labels, class is UCI_DAGHAR_Multiclass
data shape is (4840, 6, 1, 30)
label shape is (4840,)
303
Epochs between checkpoint: 42



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_28_20_26_40/Model



[Epoch 0/166] [Batch 0/303] [D loss: 4.764844] [G loss: 2.397161] [ema: 0.000000] 
[Epoch 0/166] [Batch 100/303] [D loss: 0.384039] [G loss: 0.242568] [ema: 0.933033] 
[Epoch 0/166] [Batch 200/303] [D loss: 0.512957] [G loss: 0.138049] [ema: 0.965936] 
[Epoch 0/166] [Batch 300/303] [D loss: 0.571986] [G loss: 0.117227] [ema: 0.977160] 
[Epoch 1/166] [Batch 0/303] [D loss: 0.583701] [G loss: 0.107536] [ema: 0.977384] 
[Epoch 1/166] [Batch 100/303] [D loss: 0.563091] [G loss: 0.122358] [ema: 0.982947] 
[Epoch 1/166] [Batch 200/303] [D loss: 0.483184] [G loss: 0.132271] [ema: 0.986314] 
[Epoch 1/166] [Batch 300/303] [D loss: 0.476370] [G loss: 0.155538] [ema: 0.988571] 
[Epoch 2/166] [Batch 0/303] [D loss: 0.426261] [G loss: 0.167595] [ema: 0.988627] 
[Epoch 2/166] [Batch 100/303] [D loss: 0.447805] [G loss: 0.169659] [ema: 0.990230] 
[Epoch 2/166] [Batch 200/303] [D loss: 0.533815] [G loss: 0.158404] [ema: 0.991437] 
[Epoch 2/166] [Batch 300/303] [D loss: 0.422471] [G loss: 0.157244] [ema: 0.992379] 
[Epoch 3/166] [Batch 0/303] [D loss: 0.454726] [G loss: 0.164053] [ema: 0.992404] 
[Epoch 3/166] [Batch 100/303] [D loss: 0.424914] [G loss: 0.186754] [ema: 0.993154] 
[Epoch 3/166] [Batch 200/303] [D loss: 0.448603] [G loss: 0.168769] [ema: 0.993769] 
[Epoch 3/166] [Batch 300/303] [D loss: 0.434483] [G loss: 0.173754] [ema: 0.994283] 
[Epoch 4/166] [Batch 0/303] [D loss: 0.448968] [G loss: 0.157424] [ema: 0.994297] 
[Epoch 4/166] [Batch 100/303] [D loss: 0.473908] [G loss: 0.158959] [ema: 0.994731] 
[Epoch 4/166] [Batch 200/303] [D loss: 0.485215] [G loss: 0.170660] [ema: 0.995103] 
[Epoch 4/166] [Batch 300/303] [D loss: 0.450947] [G loss: 0.139036] [ema: 0.995426] 
[Epoch 5/166] [Batch 0/303] [D loss: 0.444919] [G loss: 0.146748] [ema: 0.995435] 
[Epoch 5/166] [Batch 100/303] [D loss: 0.466943] [G loss: 0.160526] [ema: 0.995717] 
[Epoch 5/166] [Batch 200/303] [D loss: 0.493441] [G loss: 0.134165] [ema: 0.995966] 
[Epoch 5/166] [Batch 300/303] [D loss: 0.471742] [G loss: 0.165815] [ema: 0.996188] 
[Epoch 6/166] [Batch 0/303] [D loss: 0.469599] [G loss: 0.155692] [ema: 0.996195] 
[Epoch 6/166] [Batch 100/303] [D loss: 0.469226] [G loss: 0.170988] [ema: 0.996393] 
[Epoch 6/166] [Batch 200/303] [D loss: 0.472549] [G loss: 0.151182] [ema: 0.996571] 
[Epoch 6/166] [Batch 300/303] [D loss: 0.452886] [G loss: 0.156244] [ema: 0.996733] 
[Epoch 7/166] [Batch 0/303] [D loss: 0.388178] [G loss: 0.150889] [ema: 0.996737] 
[Epoch 7/166] [Batch 100/303] [D loss: 0.424637] [G loss: 0.195354] [ema: 0.996884] 
[Epoch 7/166] [Batch 200/303] [D loss: 0.389866] [G loss: 0.199931] [ema: 0.997018] 
[Epoch 7/166] [Batch 300/303] [D loss: 0.432193] [G loss: 0.166862] [ema: 0.997141] 
[Epoch 8/166] [Batch 0/303] [D loss: 0.409158] [G loss: 0.144636] [ema: 0.997145] 
[Epoch 8/166] [Batch 100/303] [D loss: 0.394646] [G loss: 0.188277] [ema: 0.997258] 
[Epoch 8/166] [Batch 200/303] [D loss: 0.377094] [G loss: 0.190886] [ema: 0.997362] 
[Epoch 8/166] [Batch 300/303] [D loss: 0.413265] [G loss: 0.189164] [ema: 0.997459] 
[Epoch 9/166] [Batch 0/303] [D loss: 0.339540] [G loss: 0.221495] [ema: 0.997461] 
[Epoch 9/166] [Batch 100/303] [D loss: 0.341047] [G loss: 0.235788] [ema: 0.997551] 
[Epoch 9/166] [Batch 200/303] [D loss: 0.363316] [G loss: 0.160865] [ema: 0.997635] 
[Epoch 9/166] [Batch 300/303] [D loss: 0.371582] [G loss: 0.227781] [ema: 0.997713] 
[Epoch 10/166] [Batch 0/303] [D loss: 0.304503] [G loss: 0.199747] [ema: 0.997715] 
[Epoch 10/166] [Batch 100/303] [D loss: 0.353745] [G loss: 0.179044] [ema: 0.997788] 
[Epoch 10/166] [Batch 200/303] [D loss: 0.341845] [G loss: 0.178952] [ema: 0.997856] 
[Epoch 10/166] [Batch 300/303] [D loss: 0.419977] [G loss: 0.200058] [ema: 0.997921] 
[Epoch 11/166] [Batch 0/303] [D loss: 0.342508] [G loss: 0.211657] [ema: 0.997923] 
[Epoch 11/166] [Batch 100/303] [D loss: 0.402685] [G loss: 0.187288] [ema: 0.997983] 
[Epoch 11/166] [Batch 200/303] [D loss: 0.413321] [G loss: 0.200661] [ema: 0.998040] 
[Epoch 11/166] [Batch 300/303] [D loss: 0.387252] [G loss: 0.188606] [ema: 0.998094] 
[Epoch 12/166] [Batch 0/303] [D loss: 0.371321] [G loss: 0.185934] [ema: 0.998095] 
[Epoch 12/166] [Batch 100/303] [D loss: 0.338174] [G loss: 0.163592] [ema: 0.998146] 
[Epoch 12/166] [Batch 200/303] [D loss: 0.416369] [G loss: 0.154820] [ema: 0.998195] 
[Epoch 12/166] [Batch 300/303] [D loss: 0.409226] [G loss: 0.197868] [ema: 0.998241] 
[Epoch 13/166] [Batch 0/303] [D loss: 0.464657] [G loss: 0.187453] [ema: 0.998242] 
[Epoch 13/166] [Batch 100/303] [D loss: 0.426247] [G loss: 0.168020] [ema: 0.998285] 
[Epoch 13/166] [Batch 200/303] [D loss: 0.395983] [G loss: 0.192940] [ema: 0.998327] 
[Epoch 13/166] [Batch 300/303] [D loss: 0.414406] [G loss: 0.170668] [ema: 0.998366] 
[Epoch 14/166] [Batch 0/303] [D loss: 0.382940] [G loss: 0.180690] [ema: 0.998367] 
[Epoch 14/166] [Batch 100/303] [D loss: 0.394248] [G loss: 0.173482] [ema: 0.998405] 
[Epoch 14/166] [Batch 200/303] [D loss: 0.395117] [G loss: 0.183800] [ema: 0.998441] 
[Epoch 14/166] [Batch 300/303] [D loss: 0.370700] [G loss: 0.192383] [ema: 0.998475] 
[Epoch 15/166] [Batch 0/303] [D loss: 0.398056] [G loss: 0.190727] [ema: 0.998476] 
[Epoch 15/166] [Batch 100/303] [D loss: 0.422818] [G loss: 0.175795] [ema: 0.998509] 
[Epoch 15/166] [Batch 200/303] [D loss: 0.368839] [G loss: 0.183515] [ema: 0.998540] 
[Epoch 15/166] [Batch 300/303] [D loss: 0.422179] [G loss: 0.170317] [ema: 0.998570] 
[Epoch 16/166] [Batch 0/303] [D loss: 0.407460] [G loss: 0.171709] [ema: 0.998571] 
[Epoch 16/166] [Batch 100/303] [D loss: 0.359441] [G loss: 0.173662] [ema: 0.998600] 
[Epoch 16/166] [Batch 200/303] [D loss: 0.398582] [G loss: 0.194862] [ema: 0.998628] 
[Epoch 16/166] [Batch 300/303] [D loss: 0.436273] [G loss: 0.159760] [ema: 0.998654] 
[Epoch 17/166] [Batch 0/303] [D loss: 0.431442] [G loss: 0.167612] [ema: 0.998655] 
[Epoch 17/166] [Batch 100/303] [D loss: 0.410285] [G loss: 0.166448] [ema: 0.998681] 
[Epoch 17/166] [Batch 200/303] [D loss: 0.396813] [G loss: 0.179802] [ema: 0.998705] 
[Epoch 17/166] [Batch 300/303] [D loss: 0.434030] [G loss: 0.167603] [ema: 0.998729] 
[Epoch 18/166] [Batch 0/303] [D loss: 0.465187] [G loss: 0.168075] [ema: 0.998730] 
[Epoch 18/166] [Batch 100/303] [D loss: 0.424613] [G loss: 0.175868] [ema: 0.998753] 
[Epoch 18/166] [Batch 200/303] [D loss: 0.426577] [G loss: 0.190601] [ema: 0.998775] 
[Epoch 18/166] [Batch 300/303] [D loss: 0.419098] [G loss: 0.178118] [ema: 0.998796] 
[Epoch 19/166] [Batch 0/303] [D loss: 0.377882] [G loss: 0.182558] [ema: 0.998797] 
[Epoch 19/166] [Batch 100/303] [D loss: 0.379043] [G loss: 0.192418] [ema: 0.998817] 
[Epoch 19/166] [Batch 200/303] [D loss: 0.433356] [G loss: 0.174597] [ema: 0.998837] 
[Epoch 19/166] [Batch 300/303] [D loss: 0.428255] [G loss: 0.181149] [ema: 0.998856] 
[Epoch 20/166] [Batch 0/303] [D loss: 0.418264] [G loss: 0.179742] [ema: 0.998857] 
[Epoch 20/166] [Batch 100/303] [D loss: 0.402434] [G loss: 0.169620] [ema: 0.998875] 
[Epoch 20/166] [Batch 200/303] [D loss: 0.396021] [G loss: 0.173117] [ema: 0.998893] 
[Epoch 20/166] [Batch 300/303] [D loss: 0.410188] [G loss: 0.171304] [ema: 0.998911] 
[Epoch 21/166] [Batch 0/303] [D loss: 0.405154] [G loss: 0.180342] [ema: 0.998911] 
[Epoch 21/166] [Batch 100/303] [D loss: 0.386279] [G loss: 0.184151] [ema: 0.998928] 
[Epoch 21/166] [Batch 200/303] [D loss: 0.445211] [G loss: 0.163860] [ema: 0.998944] 
[Epoch 21/166] [Batch 300/303] [D loss: 0.367159] [G loss: 0.182782] [ema: 0.998960] 
[Epoch 22/166] [Batch 0/303] [D loss: 0.405156] [G loss: 0.192409] [ema: 0.998961] 
[Epoch 22/166] [Batch 100/303] [D loss: 0.401785] [G loss: 0.183211] [ema: 0.998976] 
[Epoch 22/166] [Batch 200/303] [D loss: 0.431091] [G loss: 0.175405] [ema: 0.998991] 
[Epoch 22/166] [Batch 300/303] [D loss: 0.389278] [G loss: 0.195535] [ema: 0.999005] 
[Epoch 23/166] [Batch 0/303] [D loss: 0.386111] [G loss: 0.192411] [ema: 0.999006] 
[Epoch 23/166] [Batch 100/303] [D loss: 0.442254] [G loss: 0.183691] [ema: 0.999020] 
[Epoch 23/166] [Batch 200/303] [D loss: 0.401136] [G loss: 0.186820] [ema: 0.999034] 
[Epoch 23/166] [Batch 300/303] [D loss: 0.452969] [G loss: 0.186876] [ema: 0.999047] 
[Epoch 24/166] [Batch 0/303] [D loss: 0.393569] [G loss: 0.183049] [ema: 0.999047] 
[Epoch 24/166] [Batch 100/303] [D loss: 0.386161] [G loss: 0.187173] [ema: 0.999060] 
[Epoch 24/166] [Batch 200/303] [D loss: 0.420450] [G loss: 0.171006] [ema: 0.999073] 
[Epoch 24/166] [Batch 300/303] [D loss: 0.382035] [G loss: 0.178894] [ema: 0.999085] 
[Epoch 25/166] [Batch 0/303] [D loss: 0.385203] [G loss: 0.186232] [ema: 0.999085] 
[Epoch 25/166] [Batch 100/303] [D loss: 0.395068] [G loss: 0.169269] [ema: 0.999097] 
[Epoch 25/166] [Batch 200/303] [D loss: 0.357093] [G loss: 0.187368] [ema: 0.999109] 
[Epoch 25/166] [Batch 300/303] [D loss: 0.410953] [G loss: 0.141375] [ema: 0.999120] 
[Epoch 26/166] [Batch 0/303] [D loss: 0.416169] [G loss: 0.222956] [ema: 0.999121] 
[Epoch 26/166] [Batch 100/303] [D loss: 0.447542] [G loss: 0.172479] [ema: 0.999132] 
[Epoch 26/166] [Batch 200/303] [D loss: 0.383527] [G loss: 0.177059] [ema: 0.999142] 
[Epoch 26/166] [Batch 300/303] [D loss: 0.402401] [G loss: 0.193554] [ema: 0.999153] 
[Epoch 27/166] [Batch 0/303] [D loss: 0.419954] [G loss: 0.199606] [ema: 0.999153] 
[Epoch 27/166] [Batch 100/303] [D loss: 0.415267] [G loss: 0.189252] [ema: 0.999163] 
[Epoch 27/166] [Batch 200/303] [D loss: 0.423007] [G loss: 0.180671] [ema: 0.999173] 
[Epoch 27/166] [Batch 300/303] [D loss: 0.377498] [G loss: 0.178804] [ema: 0.999183] 
[Epoch 28/166] [Batch 0/303] [D loss: 0.447147] [G loss: 0.172125] [ema: 0.999183] 
[Epoch 28/166] [Batch 100/303] [D loss: 0.402193] [G loss: 0.183700] [ema: 0.999193] 
[Epoch 28/166] [Batch 200/303] [D loss: 0.412075] [G loss: 0.186140] [ema: 0.999202] 
[Epoch 28/166] [Batch 300/303] [D loss: 0.398585] [G loss: 0.177247] [ema: 0.999211] 
[Epoch 29/166] [Batch 0/303] [D loss: 0.422868] [G loss: 0.173496] [ema: 0.999211] 
[Epoch 29/166] [Batch 100/303] [D loss: 0.391614] [G loss: 0.180560] [ema: 0.999220] 
[Epoch 29/166] [Batch 200/303] [D loss: 0.397009] [G loss: 0.171090] [ema: 0.999229] 
[Epoch 29/166] [Batch 300/303] [D loss: 0.393524] [G loss: 0.171985] [ema: 0.999238] 
[Epoch 30/166] [Batch 0/303] [D loss: 0.426625] [G loss: 0.179158] [ema: 0.999238] 
[Epoch 30/166] [Batch 100/303] [D loss: 0.354875] [G loss: 0.188037] [ema: 0.999246] 
[Epoch 30/166] [Batch 200/303] [D loss: 0.395445] [G loss: 0.201849] [ema: 0.999254] 
[Epoch 30/166] [Batch 300/303] [D loss: 0.442859] [G loss: 0.162731] [ema: 0.999262] 
[Epoch 31/166] [Batch 0/303] [D loss: 0.416150] [G loss: 0.167365] [ema: 0.999262] 
[Epoch 31/166] [Batch 100/303] [D loss: 0.452812] [G loss: 0.178536] [ema: 0.999270] 
[Epoch 31/166] [Batch 200/303] [D loss: 0.388260] [G loss: 0.178531] [ema: 0.999278] 
[Epoch 31/166] [Batch 300/303] [D loss: 0.375605] [G loss: 0.170830] [ema: 0.999285] 
[Epoch 32/166] [Batch 0/303] [D loss: 0.369313] [G loss: 0.182324] [ema: 0.999285] 
[Epoch 32/166] [Batch 100/303] [D loss: 0.378775] [G loss: 0.198079] [ema: 0.999293] 
[Epoch 32/166] [Batch 200/303] [D loss: 0.403280] [G loss: 0.198040] [ema: 0.999300] 
[Epoch 32/166] [Batch 300/303] [D loss: 0.416561] [G loss: 0.185400] [ema: 0.999307] 
[Epoch 33/166] [Batch 0/303] [D loss: 0.383151] [G loss: 0.180239] [ema: 0.999307] 
[Epoch 33/166] [Batch 100/303] [D loss: 0.410072] [G loss: 0.173321] [ema: 0.999314] 
[Epoch 33/166] [Batch 200/303] [D loss: 0.391745] [G loss: 0.180980] [ema: 0.999321] 
[Epoch 33/166] [Batch 300/303] [D loss: 0.375606] [G loss: 0.189540] [ema: 0.999327] 
[Epoch 34/166] [Batch 0/303] [D loss: 0.403310] [G loss: 0.193678] [ema: 0.999327] 
[Epoch 34/166] [Batch 100/303] [D loss: 0.419573] [G loss: 0.173569] [ema: 0.999334] 
[Epoch 34/166] [Batch 200/303] [D loss: 0.386691] [G loss: 0.165094] [ema: 0.999340] 
[Epoch 34/166] [Batch 300/303] [D loss: 0.398924] [G loss: 0.176768] [ema: 0.999346] 
[Epoch 35/166] [Batch 0/303] [D loss: 0.380986] [G loss: 0.170613] [ema: 0.999347] 
[Epoch 35/166] [Batch 100/303] [D loss: 0.375222] [G loss: 0.186009] [ema: 0.999353] 
[Epoch 35/166] [Batch 200/303] [D loss: 0.405235] [G loss: 0.176353] [ema: 0.999359] 
[Epoch 35/166] [Batch 300/303] [D loss: 0.418929] [G loss: 0.183813] [ema: 0.999365] 
[Epoch 36/166] [Batch 0/303] [D loss: 0.392002] [G loss: 0.187746] [ema: 0.999365] 
[Epoch 36/166] [Batch 100/303] [D loss: 0.379296] [G loss: 0.185024] [ema: 0.999371] 
[Epoch 36/166] [Batch 200/303] [D loss: 0.416051] [G loss: 0.188378] [ema: 0.999376] 
[Epoch 36/166] [Batch 300/303] [D loss: 0.419418] [G loss: 0.181594] [ema: 0.999382] 
[Epoch 37/166] [Batch 0/303] [D loss: 0.367672] [G loss: 0.180630] [ema: 0.999382] 
[Epoch 37/166] [Batch 100/303] [D loss: 0.397817] [G loss: 0.185912] [ema: 0.999387] 
[Epoch 37/166] [Batch 200/303] [D loss: 0.428438] [G loss: 0.180566] [ema: 0.999393] 
[Epoch 37/166] [Batch 300/303] [D loss: 0.402375] [G loss: 0.182197] [ema: 0.999398] 
[Epoch 38/166] [Batch 0/303] [D loss: 0.377644] [G loss: 0.193736] [ema: 0.999398] 
[Epoch 38/166] [Batch 100/303] [D loss: 0.373924] [G loss: 0.187665] [ema: 0.999403] 
[Epoch 38/166] [Batch 200/303] [D loss: 0.394194] [G loss: 0.153366] [ema: 0.999408] 
[Epoch 38/166] [Batch 300/303] [D loss: 0.397582] [G loss: 0.169201] [ema: 0.999413] 
[Epoch 39/166] [Batch 0/303] [D loss: 0.381433] [G loss: 0.180411] [ema: 0.999414] 
[Epoch 39/166] [Batch 100/303] [D loss: 0.393236] [G loss: 0.193039] [ema: 0.999419] 
[Epoch 39/166] [Batch 200/303] [D loss: 0.393223] [G loss: 0.191402] [ema: 0.999423] 
[Epoch 39/166] [Batch 300/303] [D loss: 0.372232] [G loss: 0.191437] [ema: 0.999428] 
[Epoch 40/166] [Batch 0/303] [D loss: 0.380803] [G loss: 0.185144] [ema: 0.999428] 
[Epoch 40/166] [Batch 100/303] [D loss: 0.362542] [G loss: 0.184831] [ema: 0.999433] 
[Epoch 40/166] [Batch 200/303] [D loss: 0.391555] [G loss: 0.202287] [ema: 0.999438] 
[Epoch 40/166] [Batch 300/303] [D loss: 0.381959] [G loss: 0.192971] [ema: 0.999442] 
[Epoch 41/166] [Batch 0/303] [D loss: 0.400940] [G loss: 0.178661] [ema: 0.999442] 
[Epoch 41/166] [Batch 100/303] [D loss: 0.413694] [G loss: 0.174675] [ema: 0.999447] 
[Epoch 41/166] [Batch 200/303] [D loss: 0.386190] [G loss: 0.177533] [ema: 0.999451] 
[Epoch 41/166] [Batch 300/303] [D loss: 0.395157] [G loss: 0.186951] [ema: 0.999455] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_28_20_26_40/Model



[Epoch 42/166] [Batch 0/303] [D loss: 0.365075] [G loss: 0.197423] [ema: 0.999455] 
[Epoch 42/166] [Batch 100/303] [D loss: 0.413504] [G loss: 0.173949] [ema: 0.999460] 
[Epoch 42/166] [Batch 200/303] [D loss: 0.380005] [G loss: 0.199183] [ema: 0.999464] 
[Epoch 42/166] [Batch 300/303] [D loss: 0.390849] [G loss: 0.181455] [ema: 0.999468] 
[Epoch 43/166] [Batch 0/303] [D loss: 0.392268] [G loss: 0.194991] [ema: 0.999468] 
[Epoch 43/166] [Batch 100/303] [D loss: 0.411208] [G loss: 0.162224] [ema: 0.999472] 
[Epoch 43/166] [Batch 200/303] [D loss: 0.385804] [G loss: 0.187291] [ema: 0.999476] 
[Epoch 43/166] [Batch 300/303] [D loss: 0.348974] [G loss: 0.201475] [ema: 0.999480] 
[Epoch 44/166] [Batch 0/303] [D loss: 0.359115] [G loss: 0.205211] [ema: 0.999480] 
[Epoch 44/166] [Batch 100/303] [D loss: 0.398612] [G loss: 0.184088] [ema: 0.999484] 
[Epoch 44/166] [Batch 200/303] [D loss: 0.346096] [G loss: 0.186258] [ema: 0.999488] 
[Epoch 44/166] [Batch 300/303] [D loss: 0.402665] [G loss: 0.180544] [ema: 0.999492] 
[Epoch 45/166] [Batch 0/303] [D loss: 0.379124] [G loss: 0.202467] [ema: 0.999492] 
[Epoch 45/166] [Batch 100/303] [D loss: 0.397373] [G loss: 0.178370] [ema: 0.999495] 
[Epoch 45/166] [Batch 200/303] [D loss: 0.376973] [G loss: 0.174028] [ema: 0.999499] 
[Epoch 45/166] [Batch 300/303] [D loss: 0.404495] [G loss: 0.162189] [ema: 0.999503] 
[Epoch 46/166] [Batch 0/303] [D loss: 0.409628] [G loss: 0.188318] [ema: 0.999503] 
[Epoch 46/166] [Batch 100/303] [D loss: 0.395913] [G loss: 0.178101] [ema: 0.999506] 
[Epoch 46/166] [Batch 200/303] [D loss: 0.399932] [G loss: 0.179463] [ema: 0.999510] 
[Epoch 46/166] [Batch 300/303] [D loss: 0.399395] [G loss: 0.184626] [ema: 0.999513] 
[Epoch 47/166] [Batch 0/303] [D loss: 0.402184] [G loss: 0.194922] [ema: 0.999513] 
[Epoch 47/166] [Batch 100/303] [D loss: 0.383680] [G loss: 0.166583] [ema: 0.999517] 
[Epoch 47/166] [Batch 200/303] [D loss: 0.396329] [G loss: 0.185644] [ema: 0.999520] 
[Epoch 47/166] [Batch 300/303] [D loss: 0.419657] [G loss: 0.178646] [ema: 0.999523] 
[Epoch 48/166] [Batch 0/303] [D loss: 0.396665] [G loss: 0.185905] [ema: 0.999524] 
[Epoch 48/166] [Batch 100/303] [D loss: 0.393509] [G loss: 0.193556] [ema: 0.999527] 
[Epoch 48/166] [Batch 200/303] [D loss: 0.358783] [G loss: 0.194819] [ema: 0.999530] 
[Epoch 48/166] [Batch 300/303] [D loss: 0.387600] [G loss: 0.186068] [ema: 0.999533] 
[Epoch 49/166] [Batch 0/303] [D loss: 0.361221] [G loss: 0.181825] [ema: 0.999533] 
[Epoch 49/166] [Batch 100/303] [D loss: 0.409009] [G loss: 0.192296] [ema: 0.999536] 
[Epoch 49/166] [Batch 200/303] [D loss: 0.426694] [G loss: 0.186482] [ema: 0.999539] 
[Epoch 49/166] [Batch 300/303] [D loss: 0.407400] [G loss: 0.189778] [ema: 0.999542] 
[Epoch 50/166] [Batch 0/303] [D loss: 0.378115] [G loss: 0.190362] [ema: 0.999543] 
[Epoch 50/166] [Batch 100/303] [D loss: 0.383379] [G loss: 0.192315] [ema: 0.999546] 
[Epoch 50/166] [Batch 200/303] [D loss: 0.368757] [G loss: 0.173700] [ema: 0.999549] 
[Epoch 50/166] [Batch 300/303] [D loss: 0.383579] [G loss: 0.172616] [ema: 0.999551] 
[Epoch 51/166] [Batch 0/303] [D loss: 0.369982] [G loss: 0.177277] [ema: 0.999552] 
[Epoch 51/166] [Batch 100/303] [D loss: 0.346417] [G loss: 0.188010] [ema: 0.999554] 
[Epoch 51/166] [Batch 200/303] [D loss: 0.392887] [G loss: 0.185215] [ema: 0.999557] 
[Epoch 51/166] [Batch 300/303] [D loss: 0.387459] [G loss: 0.172366] [ema: 0.999560] 
[Epoch 52/166] [Batch 0/303] [D loss: 0.384145] [G loss: 0.170652] [ema: 0.999560] 
[Epoch 52/166] [Batch 100/303] [D loss: 0.377287] [G loss: 0.177810] [ema: 0.999563] 
[Epoch 52/166] [Batch 200/303] [D loss: 0.383818] [G loss: 0.174561] [ema: 0.999566] 
[Epoch 52/166] [Batch 300/303] [D loss: 0.429536] [G loss: 0.175307] [ema: 0.999568] 
[Epoch 53/166] [Batch 0/303] [D loss: 0.415688] [G loss: 0.164554] [ema: 0.999568] 
[Epoch 53/166] [Batch 100/303] [D loss: 0.402878] [G loss: 0.185839] [ema: 0.999571] 
[Epoch 53/166] [Batch 200/303] [D loss: 0.370837] [G loss: 0.186772] [ema: 0.999574] 
[Epoch 53/166] [Batch 300/303] [D loss: 0.417235] [G loss: 0.178487] [ema: 0.999576] 
[Epoch 54/166] [Batch 0/303] [D loss: 0.409062] [G loss: 0.183982] [ema: 0.999576] 
[Epoch 54/166] [Batch 100/303] [D loss: 0.408772] [G loss: 0.177791] [ema: 0.999579] 
[Epoch 54/166] [Batch 200/303] [D loss: 0.382854] [G loss: 0.185652] [ema: 0.999582] 
[Epoch 54/166] [Batch 300/303] [D loss: 0.377021] [G loss: 0.167270] [ema: 0.999584] 
[Epoch 55/166] [Batch 0/303] [D loss: 0.387658] [G loss: 0.186776] [ema: 0.999584] 
[Epoch 55/166] [Batch 100/303] [D loss: 0.423570] [G loss: 0.176511] [ema: 0.999587] 
[Epoch 55/166] [Batch 200/303] [D loss: 0.394761] [G loss: 0.182801] [ema: 0.999589] 
[Epoch 55/166] [Batch 300/303] [D loss: 0.366710] [G loss: 0.167120] [ema: 0.999592] 
[Epoch 56/166] [Batch 0/303] [D loss: 0.397886] [G loss: 0.196218] [ema: 0.999592] 
[Epoch 56/166] [Batch 100/303] [D loss: 0.400412] [G loss: 0.204606] [ema: 0.999594] 
[Epoch 56/166] [Batch 200/303] [D loss: 0.346523] [G loss: 0.187714] [ema: 0.999596] 
[Epoch 56/166] [Batch 300/303] [D loss: 0.412355] [G loss: 0.184956] [ema: 0.999599] 
[Epoch 57/166] [Batch 0/303] [D loss: 0.347690] [G loss: 0.196194] [ema: 0.999599] 
[Epoch 57/166] [Batch 100/303] [D loss: 0.391974] [G loss: 0.184709] [ema: 0.999601] 
[Epoch 57/166] [Batch 200/303] [D loss: 0.414049] [G loss: 0.183210] [ema: 0.999603] 
[Epoch 57/166] [Batch 300/303] [D loss: 0.449801] [G loss: 0.189129] [ema: 0.999606] 
[Epoch 58/166] [Batch 0/303] [D loss: 0.414742] [G loss: 0.186105] [ema: 0.999606] 
[Epoch 58/166] [Batch 100/303] [D loss: 0.375436] [G loss: 0.200823] [ema: 0.999608] 
[Epoch 58/166] [Batch 200/303] [D loss: 0.393153] [G loss: 0.174148] [ema: 0.999610] 
[Epoch 58/166] [Batch 300/303] [D loss: 0.375316] [G loss: 0.198927] [ema: 0.999612] 
[Epoch 59/166] [Batch 0/303] [D loss: 0.385564] [G loss: 0.183827] [ema: 0.999612] 
[Epoch 59/166] [Batch 100/303] [D loss: 0.390211] [G loss: 0.175164] [ema: 0.999614] 
[Epoch 59/166] [Batch 200/303] [D loss: 0.382668] [G loss: 0.188967] [ema: 0.999617] 
[Epoch 59/166] [Batch 300/303] [D loss: 0.380671] [G loss: 0.187123] [ema: 0.999619] 
[Epoch 60/166] [Batch 0/303] [D loss: 0.414910] [G loss: 0.195397] [ema: 0.999619] 
[Epoch 60/166] [Batch 100/303] [D loss: 0.398813] [G loss: 0.192581] [ema: 0.999621] 
[Epoch 60/166] [Batch 200/303] [D loss: 0.424598] [G loss: 0.170668] [ema: 0.999623] 
[Epoch 60/166] [Batch 300/303] [D loss: 0.370414] [G loss: 0.190389] [ema: 0.999625] 
[Epoch 61/166] [Batch 0/303] [D loss: 0.364731] [G loss: 0.168532] [ema: 0.999625] 
[Epoch 61/166] [Batch 100/303] [D loss: 0.417176] [G loss: 0.207357] [ema: 0.999627] 
[Epoch 61/166] [Batch 200/303] [D loss: 0.398347] [G loss: 0.184846] [ema: 0.999629] 
[Epoch 61/166] [Batch 300/303] [D loss: 0.389842] [G loss: 0.195539] [ema: 0.999631] 
[Epoch 62/166] [Batch 0/303] [D loss: 0.401476] [G loss: 0.198935] [ema: 0.999631] 
[Epoch 62/166] [Batch 100/303] [D loss: 0.339258] [G loss: 0.188866] [ema: 0.999633] 
[Epoch 62/166] [Batch 200/303] [D loss: 0.358747] [G loss: 0.165993] [ema: 0.999635] 
[Epoch 62/166] [Batch 300/303] [D loss: 0.430733] [G loss: 0.181750] [ema: 0.999637] 
[Epoch 63/166] [Batch 0/303] [D loss: 0.355368] [G loss: 0.191114] [ema: 0.999637] 
[Epoch 63/166] [Batch 100/303] [D loss: 0.389589] [G loss: 0.193104] [ema: 0.999639] 
[Epoch 63/166] [Batch 200/303] [D loss: 0.376575] [G loss: 0.182245] [ema: 0.999641] 
[Epoch 63/166] [Batch 300/303] [D loss: 0.386440] [G loss: 0.190454] [ema: 0.999643] 
[Epoch 64/166] [Batch 0/303] [D loss: 0.363876] [G loss: 0.207450] [ema: 0.999643] 
[Epoch 64/166] [Batch 100/303] [D loss: 0.361432] [G loss: 0.209961] [ema: 0.999644] 
[Epoch 64/166] [Batch 200/303] [D loss: 0.349285] [G loss: 0.204849] [ema: 0.999646] 
[Epoch 64/166] [Batch 300/303] [D loss: 0.369309] [G loss: 0.195355] [ema: 0.999648] 
[Epoch 65/166] [Batch 0/303] [D loss: 0.375655] [G loss: 0.203378] [ema: 0.999648] 
[Epoch 65/166] [Batch 100/303] [D loss: 0.403381] [G loss: 0.177564] [ema: 0.999650] 
[Epoch 65/166] [Batch 200/303] [D loss: 0.404161] [G loss: 0.185464] [ema: 0.999652] 
[Epoch 65/166] [Batch 300/303] [D loss: 0.399541] [G loss: 0.178269] [ema: 0.999653] 
[Epoch 66/166] [Batch 0/303] [D loss: 0.417234] [G loss: 0.173980] [ema: 0.999653] 
[Epoch 66/166] [Batch 100/303] [D loss: 0.359817] [G loss: 0.192083] [ema: 0.999655] 
[Epoch 66/166] [Batch 200/303] [D loss: 0.371198] [G loss: 0.184341] [ema: 0.999657] 
[Epoch 66/166] [Batch 300/303] [D loss: 0.435152] [G loss: 0.162757] [ema: 0.999659] 
[Epoch 67/166] [Batch 0/303] [D loss: 0.450174] [G loss: 0.181458] [ema: 0.999659] 
[Epoch 67/166] [Batch 100/303] [D loss: 0.415353] [G loss: 0.183127] [ema: 0.999660] 
[Epoch 67/166] [Batch 200/303] [D loss: 0.386617] [G loss: 0.189652] [ema: 0.999662] 
[Epoch 67/166] [Batch 300/303] [D loss: 0.343844] [G loss: 0.183755] [ema: 0.999664] 
[Epoch 68/166] [Batch 0/303] [D loss: 0.372546] [G loss: 0.172125] [ema: 0.999664] 
[Epoch 68/166] [Batch 100/303] [D loss: 0.372225] [G loss: 0.182348] [ema: 0.999665] 
[Epoch 68/166] [Batch 200/303] [D loss: 0.384995] [G loss: 0.186249] [ema: 0.999667] 
[Epoch 68/166] [Batch 300/303] [D loss: 0.397579] [G loss: 0.193248] [ema: 0.999668] 
[Epoch 69/166] [Batch 0/303] [D loss: 0.423822] [G loss: 0.196558] [ema: 0.999669] 
[Epoch 69/166] [Batch 100/303] [D loss: 0.386010] [G loss: 0.195512] [ema: 0.999670] 
[Epoch 69/166] [Batch 200/303] [D loss: 0.360285] [G loss: 0.186214] [ema: 0.999672] 
[Epoch 69/166] [Batch 300/303] [D loss: 0.402169] [G loss: 0.187068] [ema: 0.999673] 
[Epoch 70/166] [Batch 0/303] [D loss: 0.369494] [G loss: 0.185629] [ema: 0.999673] 
[Epoch 70/166] [Batch 100/303] [D loss: 0.406269] [G loss: 0.190845] [ema: 0.999675] 
[Epoch 70/166] [Batch 200/303] [D loss: 0.390612] [G loss: 0.190894] [ema: 0.999676] 
[Epoch 70/166] [Batch 300/303] [D loss: 0.365604] [G loss: 0.182907] [ema: 0.999678] 
[Epoch 71/166] [Batch 0/303] [D loss: 0.361315] [G loss: 0.191802] [ema: 0.999678] 
[Epoch 71/166] [Batch 100/303] [D loss: 0.365671] [G loss: 0.194470] [ema: 0.999679] 
[Epoch 71/166] [Batch 200/303] [D loss: 0.356173] [G loss: 0.193878] [ema: 0.999681] 
[Epoch 71/166] [Batch 300/303] [D loss: 0.367146] [G loss: 0.192325] [ema: 0.999682] 
[Epoch 72/166] [Batch 0/303] [D loss: 0.379273] [G loss: 0.198958] [ema: 0.999682] 
[Epoch 72/166] [Batch 100/303] [D loss: 0.344130] [G loss: 0.191082] [ema: 0.999684] 
[Epoch 72/166] [Batch 200/303] [D loss: 0.326215] [G loss: 0.184077] [ema: 0.999685] 
[Epoch 72/166] [Batch 300/303] [D loss: 0.405564] [G loss: 0.169251] [ema: 0.999687] 
[Epoch 73/166] [Batch 0/303] [D loss: 0.422220] [G loss: 0.189450] [ema: 0.999687] 
[Epoch 73/166] [Batch 100/303] [D loss: 0.376408] [G loss: 0.184448] [ema: 0.999688] 
[Epoch 73/166] [Batch 200/303] [D loss: 0.349178] [G loss: 0.169567] [ema: 0.999689] 
[Epoch 73/166] [Batch 300/303] [D loss: 0.353998] [G loss: 0.203464] [ema: 0.999691] 
[Epoch 74/166] [Batch 0/303] [D loss: 0.371067] [G loss: 0.203939] [ema: 0.999691] 
[Epoch 74/166] [Batch 100/303] [D loss: 0.360043] [G loss: 0.202203] [ema: 0.999692] 
[Epoch 74/166] [Batch 200/303] [D loss: 0.404822] [G loss: 0.189376] [ema: 0.999694] 
[Epoch 74/166] [Batch 300/303] [D loss: 0.348589] [G loss: 0.168760] [ema: 0.999695] 
[Epoch 75/166] [Batch 0/303] [D loss: 0.442202] [G loss: 0.179087] [ema: 0.999695] 
[Epoch 75/166] [Batch 100/303] [D loss: 0.357717] [G loss: 0.199040] [ema: 0.999696] 
[Epoch 75/166] [Batch 200/303] [D loss: 0.384804] [G loss: 0.193858] [ema: 0.999698] 
[Epoch 75/166] [Batch 300/303] [D loss: 0.431959] [G loss: 0.188677] [ema: 0.999699] 
[Epoch 76/166] [Batch 0/303] [D loss: 0.403571] [G loss: 0.195817] [ema: 0.999699] 
[Epoch 76/166] [Batch 100/303] [D loss: 0.363196] [G loss: 0.207206] [ema: 0.999700] 
[Epoch 76/166] [Batch 200/303] [D loss: 0.403725] [G loss: 0.181854] [ema: 0.999702] 
[Epoch 76/166] [Batch 300/303] [D loss: 0.387737] [G loss: 0.196059] [ema: 0.999703] 
[Epoch 77/166] [Batch 0/303] [D loss: 0.379369] [G loss: 0.197874] [ema: 0.999703] 
[Epoch 77/166] [Batch 100/303] [D loss: 0.382319] [G loss: 0.195108] [ema: 0.999704] 
[Epoch 77/166] [Batch 200/303] [D loss: 0.403951] [G loss: 0.196428] [ema: 0.999705] 
[Epoch 77/166] [Batch 300/303] [D loss: 0.380094] [G loss: 0.191487] [ema: 0.999707] 
[Epoch 78/166] [Batch 0/303] [D loss: 0.369754] [G loss: 0.190428] [ema: 0.999707] 
[Epoch 78/166] [Batch 100/303] [D loss: 0.412685] [G loss: 0.178726] [ema: 0.999708] 
[Epoch 78/166] [Batch 200/303] [D loss: 0.336026] [G loss: 0.192898] [ema: 0.999709] 
[Epoch 78/166] [Batch 300/303] [D loss: 0.386465] [G loss: 0.188427] [ema: 0.999710] 
[Epoch 79/166] [Batch 0/303] [D loss: 0.380544] [G loss: 0.195320] [ema: 0.999710] 
[Epoch 79/166] [Batch 100/303] [D loss: 0.350065] [G loss: 0.183582] [ema: 0.999712] 
[Epoch 79/166] [Batch 200/303] [D loss: 0.407481] [G loss: 0.181892] [ema: 0.999713] 
[Epoch 79/166] [Batch 300/303] [D loss: 0.345186] [G loss: 0.189914] [ema: 0.999714] 
[Epoch 80/166] [Batch 0/303] [D loss: 0.342678] [G loss: 0.184911] [ema: 0.999714] 
[Epoch 80/166] [Batch 100/303] [D loss: 0.385247] [G loss: 0.189532] [ema: 0.999715] 
[Epoch 80/166] [Batch 200/303] [D loss: 0.394741] [G loss: 0.176418] [ema: 0.999716] 
[Epoch 80/166] [Batch 300/303] [D loss: 0.392630] [G loss: 0.183352] [ema: 0.999718] 
[Epoch 81/166] [Batch 0/303] [D loss: 0.364059] [G loss: 0.184836] [ema: 0.999718] 
[Epoch 81/166] [Batch 100/303] [D loss: 0.351176] [G loss: 0.184109] [ema: 0.999719] 
[Epoch 81/166] [Batch 200/303] [D loss: 0.400861] [G loss: 0.198915] [ema: 0.999720] 
[Epoch 81/166] [Batch 300/303] [D loss: 0.410155] [G loss: 0.186122] [ema: 0.999721] 
[Epoch 82/166] [Batch 0/303] [D loss: 0.407574] [G loss: 0.188570] [ema: 0.999721] 
[Epoch 82/166] [Batch 100/303] [D loss: 0.433840] [G loss: 0.189182] [ema: 0.999722] 
[Epoch 82/166] [Batch 200/303] [D loss: 0.403276] [G loss: 0.181490] [ema: 0.999723] 
[Epoch 82/166] [Batch 300/303] [D loss: 0.390732] [G loss: 0.183394] [ema: 0.999724] 
[Epoch 83/166] [Batch 0/303] [D loss: 0.372807] [G loss: 0.178866] [ema: 0.999724] 
[Epoch 83/166] [Batch 100/303] [D loss: 0.387376] [G loss: 0.203345] [ema: 0.999726] 
[Epoch 83/166] [Batch 200/303] [D loss: 0.369354] [G loss: 0.186400] [ema: 0.999727] 
[Epoch 83/166] [Batch 300/303] [D loss: 0.374781] [G loss: 0.188009] [ema: 0.999728] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_28_20_26_40/Model



[Epoch 84/166] [Batch 0/303] [D loss: 0.394776] [G loss: 0.179851] [ema: 0.999728] 
[Epoch 84/166] [Batch 100/303] [D loss: 0.366355] [G loss: 0.193938] [ema: 0.999729] 
[Epoch 84/166] [Batch 200/303] [D loss: 0.369207] [G loss: 0.187398] [ema: 0.999730] 
[Epoch 84/166] [Batch 300/303] [D loss: 0.387523] [G loss: 0.189752] [ema: 0.999731] 
[Epoch 85/166] [Batch 0/303] [D loss: 0.377630] [G loss: 0.201788] [ema: 0.999731] 
[Epoch 85/166] [Batch 100/303] [D loss: 0.390932] [G loss: 0.200901] [ema: 0.999732] 
[Epoch 85/166] [Batch 200/303] [D loss: 0.382786] [G loss: 0.181908] [ema: 0.999733] 
[Epoch 85/166] [Batch 300/303] [D loss: 0.391875] [G loss: 0.201521] [ema: 0.999734] 
[Epoch 86/166] [Batch 0/303] [D loss: 0.336632] [G loss: 0.197405] [ema: 0.999734] 
[Epoch 86/166] [Batch 100/303] [D loss: 0.386980] [G loss: 0.185402] [ema: 0.999735] 
[Epoch 86/166] [Batch 200/303] [D loss: 0.416709] [G loss: 0.164418] [ema: 0.999736] 
[Epoch 86/166] [Batch 300/303] [D loss: 0.365649] [G loss: 0.188472] [ema: 0.999737] 
[Epoch 87/166] [Batch 0/303] [D loss: 0.350723] [G loss: 0.200901] [ema: 0.999737] 
[Epoch 87/166] [Batch 100/303] [D loss: 0.281532] [G loss: 0.245781] [ema: 0.999738] 
[Epoch 87/166] [Batch 200/303] [D loss: 0.361825] [G loss: 0.176808] [ema: 0.999739] 
[Epoch 87/166] [Batch 300/303] [D loss: 0.425787] [G loss: 0.169133] [ema: 0.999740] 
[Epoch 88/166] [Batch 0/303] [D loss: 0.427673] [G loss: 0.183982] [ema: 0.999740] 
[Epoch 88/166] [Batch 100/303] [D loss: 0.444386] [G loss: 0.178196] [ema: 0.999741] 
[Epoch 88/166] [Batch 200/303] [D loss: 0.380930] [G loss: 0.188593] [ema: 0.999742] 
[Epoch 88/166] [Batch 300/303] [D loss: 0.448559] [G loss: 0.178472] [ema: 0.999743] 
[Epoch 89/166] [Batch 0/303] [D loss: 0.378424] [G loss: 0.194752] [ema: 0.999743] 
[Epoch 89/166] [Batch 100/303] [D loss: 0.432752] [G loss: 0.181267] [ema: 0.999744] 
[Epoch 89/166] [Batch 200/303] [D loss: 0.407303] [G loss: 0.174811] [ema: 0.999745] 
[Epoch 89/166] [Batch 300/303] [D loss: 0.325895] [G loss: 0.192858] [ema: 0.999746] 
[Epoch 90/166] [Batch 0/303] [D loss: 0.362325] [G loss: 0.190373] [ema: 0.999746] 
[Epoch 90/166] [Batch 100/303] [D loss: 0.394654] [G loss: 0.174514] [ema: 0.999747] 
[Epoch 90/166] [Batch 200/303] [D loss: 0.417977] [G loss: 0.191597] [ema: 0.999748] 
[Epoch 90/166] [Batch 300/303] [D loss: 0.384057] [G loss: 0.167852] [ema: 0.999749] 
[Epoch 91/166] [Batch 0/303] [D loss: 0.359219] [G loss: 0.183718] [ema: 0.999749] 
[Epoch 91/166] [Batch 100/303] [D loss: 0.416868] [G loss: 0.198668] [ema: 0.999750] 
[Epoch 91/166] [Batch 200/303] [D loss: 0.387055] [G loss: 0.194980] [ema: 0.999750] 
[Epoch 91/166] [Batch 300/303] [D loss: 0.406531] [G loss: 0.182322] [ema: 0.999751] 
[Epoch 92/166] [Batch 0/303] [D loss: 0.381026] [G loss: 0.190712] [ema: 0.999751] 
[Epoch 92/166] [Batch 100/303] [D loss: 0.384157] [G loss: 0.186620] [ema: 0.999752] 
[Epoch 92/166] [Batch 200/303] [D loss: 0.380951] [G loss: 0.190764] [ema: 0.999753] 
[Epoch 92/166] [Batch 300/303] [D loss: 0.415999] [G loss: 0.200321] [ema: 0.999754] 
[Epoch 93/166] [Batch 0/303] [D loss: 0.374014] [G loss: 0.191674] [ema: 0.999754] 
[Epoch 93/166] [Batch 100/303] [D loss: 0.358185] [G loss: 0.180068] [ema: 0.999755] 
[Epoch 93/166] [Batch 200/303] [D loss: 0.397400] [G loss: 0.173231] [ema: 0.999756] 
[Epoch 93/166] [Batch 300/303] [D loss: 0.379833] [G loss: 0.178150] [ema: 0.999757] 
[Epoch 94/166] [Batch 0/303] [D loss: 0.388191] [G loss: 0.169037] [ema: 0.999757] 
[Epoch 94/166] [Batch 100/303] [D loss: 0.366354] [G loss: 0.196143] [ema: 0.999758] 
[Epoch 94/166] [Batch 200/303] [D loss: 0.354243] [G loss: 0.195373] [ema: 0.999758] 
[Epoch 94/166] [Batch 300/303] [D loss: 0.418688] [G loss: 0.186123] [ema: 0.999759] 
[Epoch 95/166] [Batch 0/303] [D loss: 0.394631] [G loss: 0.185939] [ema: 0.999759] 
[Epoch 95/166] [Batch 100/303] [D loss: 0.395795] [G loss: 0.174454] [ema: 0.999760] 
[Epoch 95/166] [Batch 200/303] [D loss: 0.392699] [G loss: 0.196944] [ema: 0.999761] 
[Epoch 95/166] [Batch 300/303] [D loss: 0.370364] [G loss: 0.167549] [ema: 0.999762] 
[Epoch 96/166] [Batch 0/303] [D loss: 0.425248] [G loss: 0.196715] [ema: 0.999762] 
[Epoch 96/166] [Batch 100/303] [D loss: 0.390496] [G loss: 0.206186] [ema: 0.999763] 
[Epoch 96/166] [Batch 200/303] [D loss: 0.360027] [G loss: 0.184004] [ema: 0.999763] 
[Epoch 96/166] [Batch 300/303] [D loss: 0.393634] [G loss: 0.181677] [ema: 0.999764] 
[Epoch 97/166] [Batch 0/303] [D loss: 0.370936] [G loss: 0.184210] [ema: 0.999764] 
[Epoch 97/166] [Batch 100/303] [D loss: 0.366239] [G loss: 0.185561] [ema: 0.999765] 
[Epoch 97/166] [Batch 200/303] [D loss: 0.377234] [G loss: 0.186845] [ema: 0.999766] 
[Epoch 97/166] [Batch 300/303] [D loss: 0.421066] [G loss: 0.183563] [ema: 0.999767] 
[Epoch 98/166] [Batch 0/303] [D loss: 0.422416] [G loss: 0.171995] [ema: 0.999767] 
[Epoch 98/166] [Batch 100/303] [D loss: 0.395389] [G loss: 0.200989] [ema: 0.999767] 
[Epoch 98/166] [Batch 200/303] [D loss: 0.373086] [G loss: 0.186387] [ema: 0.999768] 
[Epoch 98/166] [Batch 300/303] [D loss: 0.429476] [G loss: 0.179568] [ema: 0.999769] 
[Epoch 99/166] [Batch 0/303] [D loss: 0.384651] [G loss: 0.183348] [ema: 0.999769] 
[Epoch 99/166] [Batch 100/303] [D loss: 0.395726] [G loss: 0.177568] [ema: 0.999770] 
[Epoch 99/166] [Batch 200/303] [D loss: 0.423808] [G loss: 0.188398] [ema: 0.999770] 
[Epoch 99/166] [Batch 300/303] [D loss: 0.420474] [G loss: 0.170145] [ema: 0.999771] 
[Epoch 100/166] [Batch 0/303] [D loss: 0.395991] [G loss: 0.189418] [ema: 0.999771] 
[Epoch 100/166] [Batch 100/303] [D loss: 0.405669] [G loss: 0.189800] [ema: 0.999772] 
[Epoch 100/166] [Batch 200/303] [D loss: 0.458971] [G loss: 0.176324] [ema: 0.999773] 
[Epoch 100/166] [Batch 300/303] [D loss: 0.395017] [G loss: 0.181405] [ema: 0.999774] 
[Epoch 101/166] [Batch 0/303] [D loss: 0.387159] [G loss: 0.182823] [ema: 0.999774] 
[Epoch 101/166] [Batch 100/303] [D loss: 0.350831] [G loss: 0.188469] [ema: 0.999774] 
[Epoch 101/166] [Batch 200/303] [D loss: 0.362851] [G loss: 0.202483] [ema: 0.999775] 
[Epoch 101/166] [Batch 300/303] [D loss: 0.420219] [G loss: 0.214057] [ema: 0.999776] 
[Epoch 102/166] [Batch 0/303] [D loss: 0.387572] [G loss: 0.189560] [ema: 0.999776] 
[Epoch 102/166] [Batch 100/303] [D loss: 0.442694] [G loss: 0.164288] [ema: 0.999776] 
[Epoch 102/166] [Batch 200/303] [D loss: 0.380132] [G loss: 0.200382] [ema: 0.999777] 
[Epoch 102/166] [Batch 300/303] [D loss: 0.385491] [G loss: 0.181679] [ema: 0.999778] 
[Epoch 103/166] [Batch 0/303] [D loss: 0.378003] [G loss: 0.172022] [ema: 0.999778] 
[Epoch 103/166] [Batch 100/303] [D loss: 0.373489] [G loss: 0.187264] [ema: 0.999779] 
[Epoch 103/166] [Batch 200/303] [D loss: 0.439610] [G loss: 0.184751] [ema: 0.999779] 
[Epoch 103/166] [Batch 300/303] [D loss: 0.408240] [G loss: 0.180882] [ema: 0.999780] 
[Epoch 104/166] [Batch 0/303] [D loss: 0.411697] [G loss: 0.189664] [ema: 0.999780] 
[Epoch 104/166] [Batch 100/303] [D loss: 0.413459] [G loss: 0.169178] [ema: 0.999781] 
[Epoch 104/166] [Batch 200/303] [D loss: 0.378250] [G loss: 0.179531] [ema: 0.999781] 
[Epoch 104/166] [Batch 300/303] [D loss: 0.411179] [G loss: 0.174919] [ema: 0.999782] 
[Epoch 105/166] [Batch 0/303] [D loss: 0.421633] [G loss: 0.182882] [ema: 0.999782] 
[Epoch 105/166] [Batch 100/303] [D loss: 0.372506] [G loss: 0.173535] [ema: 0.999783] 
[Epoch 105/166] [Batch 200/303] [D loss: 0.344394] [G loss: 0.185647] [ema: 0.999784] 
[Epoch 105/166] [Batch 300/303] [D loss: 0.381154] [G loss: 0.171086] [ema: 0.999784] 
[Epoch 106/166] [Batch 0/303] [D loss: 0.401241] [G loss: 0.175656] [ema: 0.999784] 
[Epoch 106/166] [Batch 100/303] [D loss: 0.368928] [G loss: 0.194106] [ema: 0.999785] 
[Epoch 106/166] [Batch 200/303] [D loss: 0.427269] [G loss: 0.180872] [ema: 0.999786] 
[Epoch 106/166] [Batch 300/303] [D loss: 0.412823] [G loss: 0.188965] [ema: 0.999786] 
[Epoch 107/166] [Batch 0/303] [D loss: 0.367774] [G loss: 0.187350] [ema: 0.999786] 
[Epoch 107/166] [Batch 100/303] [D loss: 0.387954] [G loss: 0.185257] [ema: 0.999787] 
[Epoch 107/166] [Batch 200/303] [D loss: 0.405162] [G loss: 0.186502] [ema: 0.999788] 
[Epoch 107/166] [Batch 300/303] [D loss: 0.384166] [G loss: 0.182928] [ema: 0.999788] 
[Epoch 108/166] [Batch 0/303] [D loss: 0.362295] [G loss: 0.189659] [ema: 0.999788] 
[Epoch 108/166] [Batch 100/303] [D loss: 0.373745] [G loss: 0.186086] [ema: 0.999789] 
[Epoch 108/166] [Batch 200/303] [D loss: 0.403835] [G loss: 0.180121] [ema: 0.999789] 
[Epoch 108/166] [Batch 300/303] [D loss: 0.379798] [G loss: 0.202661] [ema: 0.999790] 
[Epoch 109/166] [Batch 0/303] [D loss: 0.386358] [G loss: 0.205834] [ema: 0.999790] 
[Epoch 109/166] [Batch 100/303] [D loss: 0.392027] [G loss: 0.200232] [ema: 0.999791] 
[Epoch 109/166] [Batch 200/303] [D loss: 0.410824] [G loss: 0.166461] [ema: 0.999791] 
[Epoch 109/166] [Batch 300/303] [D loss: 0.386566] [G loss: 0.183079] [ema: 0.999792] 
[Epoch 110/166] [Batch 0/303] [D loss: 0.377847] [G loss: 0.191779] [ema: 0.999792] 
[Epoch 110/166] [Batch 100/303] [D loss: 0.356855] [G loss: 0.195915] [ema: 0.999793] 
[Epoch 110/166] [Batch 200/303] [D loss: 0.374982] [G loss: 0.187572] [ema: 0.999793] 
[Epoch 110/166] [Batch 300/303] [D loss: 0.399936] [G loss: 0.185251] [ema: 0.999794] 
[Epoch 111/166] [Batch 0/303] [D loss: 0.378772] [G loss: 0.185488] [ema: 0.999794] 
[Epoch 111/166] [Batch 100/303] [D loss: 0.436264] [G loss: 0.179098] [ema: 0.999795] 
[Epoch 111/166] [Batch 200/303] [D loss: 0.390165] [G loss: 0.180882] [ema: 0.999795] 
[Epoch 111/166] [Batch 300/303] [D loss: 0.412955] [G loss: 0.159119] [ema: 0.999796] 
[Epoch 112/166] [Batch 0/303] [D loss: 0.423167] [G loss: 0.176082] [ema: 0.999796] 
[Epoch 112/166] [Batch 100/303] [D loss: 0.407441] [G loss: 0.175716] [ema: 0.999796] 
[Epoch 112/166] [Batch 200/303] [D loss: 0.375340] [G loss: 0.157911] [ema: 0.999797] 
[Epoch 112/166] [Batch 300/303] [D loss: 0.411674] [G loss: 0.187474] [ema: 0.999798] 
[Epoch 113/166] [Batch 0/303] [D loss: 0.385374] [G loss: 0.173873] [ema: 0.999798] 
[Epoch 113/166] [Batch 100/303] [D loss: 0.405521] [G loss: 0.161053] [ema: 0.999798] 
[Epoch 113/166] [Batch 200/303] [D loss: 0.418731] [G loss: 0.178423] [ema: 0.999799] 
[Epoch 113/166] [Batch 300/303] [D loss: 0.413970] [G loss: 0.173076] [ema: 0.999799] 
[Epoch 114/166] [Batch 0/303] [D loss: 0.391252] [G loss: 0.203379] [ema: 0.999799] 
[Epoch 114/166] [Batch 100/303] [D loss: 0.376704] [G loss: 0.187048] [ema: 0.999800] 
[Epoch 114/166] [Batch 200/303] [D loss: 0.364275] [G loss: 0.186688] [ema: 0.999801] 
[Epoch 114/166] [Batch 300/303] [D loss: 0.415410] [G loss: 0.192644] [ema: 0.999801] 
[Epoch 115/166] [Batch 0/303] [D loss: 0.394630] [G loss: 0.171586] [ema: 0.999801] 
[Epoch 115/166] [Batch 100/303] [D loss: 0.398288] [G loss: 0.179550] [ema: 0.999802] 
[Epoch 115/166] [Batch 200/303] [D loss: 0.394793] [G loss: 0.189423] [ema: 0.999802] 
[Epoch 115/166] [Batch 300/303] [D loss: 0.427382] [G loss: 0.173802] [ema: 0.999803] 
[Epoch 116/166] [Batch 0/303] [D loss: 0.425307] [G loss: 0.172300] [ema: 0.999803] 
[Epoch 116/166] [Batch 100/303] [D loss: 0.376765] [G loss: 0.181702] [ema: 0.999803] 
[Epoch 116/166] [Batch 200/303] [D loss: 0.415346] [G loss: 0.156891] [ema: 0.999804] 
[Epoch 116/166] [Batch 300/303] [D loss: 0.391462] [G loss: 0.203732] [ema: 0.999804] 
[Epoch 117/166] [Batch 0/303] [D loss: 0.354370] [G loss: 0.210050] [ema: 0.999804] 
[Epoch 117/166] [Batch 100/303] [D loss: 0.396456] [G loss: 0.183786] [ema: 0.999805] 
[Epoch 117/166] [Batch 200/303] [D loss: 0.370696] [G loss: 0.186033] [ema: 0.999806] 
[Epoch 117/166] [Batch 300/303] [D loss: 0.411646] [G loss: 0.174954] [ema: 0.999806] 
[Epoch 118/166] [Batch 0/303] [D loss: 0.417893] [G loss: 0.175749] [ema: 0.999806] 
[Epoch 118/166] [Batch 100/303] [D loss: 0.414673] [G loss: 0.190314] [ema: 0.999807] 
[Epoch 118/166] [Batch 200/303] [D loss: 0.336521] [G loss: 0.175790] [ema: 0.999807] 
[Epoch 118/166] [Batch 300/303] [D loss: 0.386087] [G loss: 0.179657] [ema: 0.999808] 
[Epoch 119/166] [Batch 0/303] [D loss: 0.391600] [G loss: 0.194004] [ema: 0.999808] 
[Epoch 119/166] [Batch 100/303] [D loss: 0.448545] [G loss: 0.176866] [ema: 0.999808] 
[Epoch 119/166] [Batch 200/303] [D loss: 0.423041] [G loss: 0.160511] [ema: 0.999809] 
[Epoch 119/166] [Batch 300/303] [D loss: 0.387107] [G loss: 0.167843] [ema: 0.999809] 
[Epoch 120/166] [Batch 0/303] [D loss: 0.401265] [G loss: 0.170795] [ema: 0.999809] 
[Epoch 120/166] [Batch 100/303] [D loss: 0.398894] [G loss: 0.168689] [ema: 0.999810] 
[Epoch 120/166] [Batch 200/303] [D loss: 0.392045] [G loss: 0.175356] [ema: 0.999810] 
[Epoch 120/166] [Batch 300/303] [D loss: 0.420632] [G loss: 0.165539] [ema: 0.999811] 
[Epoch 121/166] [Batch 0/303] [D loss: 0.387444] [G loss: 0.193000] [ema: 0.999811] 
[Epoch 121/166] [Batch 100/303] [D loss: 0.385418] [G loss: 0.169884] [ema: 0.999811] 
[Epoch 121/166] [Batch 200/303] [D loss: 0.390123] [G loss: 0.162805] [ema: 0.999812] 
[Epoch 121/166] [Batch 300/303] [D loss: 0.441777] [G loss: 0.173169] [ema: 0.999812] 
[Epoch 122/166] [Batch 0/303] [D loss: 0.376605] [G loss: 0.177182] [ema: 0.999813] 
[Epoch 122/166] [Batch 100/303] [D loss: 0.437783] [G loss: 0.176574] [ema: 0.999813] 
[Epoch 122/166] [Batch 200/303] [D loss: 0.398945] [G loss: 0.179486] [ema: 0.999814] 
[Epoch 122/166] [Batch 300/303] [D loss: 0.404547] [G loss: 0.182178] [ema: 0.999814] 
[Epoch 123/166] [Batch 0/303] [D loss: 0.425681] [G loss: 0.177987] [ema: 0.999814] 
[Epoch 123/166] [Batch 100/303] [D loss: 0.426325] [G loss: 0.169718] [ema: 0.999815] 
[Epoch 123/166] [Batch 200/303] [D loss: 0.386329] [G loss: 0.167993] [ema: 0.999815] 
[Epoch 123/166] [Batch 300/303] [D loss: 0.415568] [G loss: 0.161366] [ema: 0.999816] 
[Epoch 124/166] [Batch 0/303] [D loss: 0.429977] [G loss: 0.177111] [ema: 0.999816] 
[Epoch 124/166] [Batch 100/303] [D loss: 0.476127] [G loss: 0.172103] [ema: 0.999816] 
[Epoch 124/166] [Batch 200/303] [D loss: 0.419982] [G loss: 0.176222] [ema: 0.999817] 
[Epoch 124/166] [Batch 300/303] [D loss: 0.403844] [G loss: 0.176718] [ema: 0.999817] 
[Epoch 125/166] [Batch 0/303] [D loss: 0.382775] [G loss: 0.184023] [ema: 0.999817] 
[Epoch 125/166] [Batch 100/303] [D loss: 0.423380] [G loss: 0.200932] [ema: 0.999817] 
[Epoch 125/166] [Batch 200/303] [D loss: 0.395809] [G loss: 0.177911] [ema: 0.999818] 
[Epoch 125/166] [Batch 300/303] [D loss: 0.418299] [G loss: 0.181366] [ema: 0.999818] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_28_20_26_40/Model



[Epoch 126/166] [Batch 0/303] [D loss: 0.383212] [G loss: 0.177938] [ema: 0.999818] 
[Epoch 126/166] [Batch 100/303] [D loss: 0.410946] [G loss: 0.174066] [ema: 0.999819] 
[Epoch 126/166] [Batch 200/303] [D loss: 0.413719] [G loss: 0.163385] [ema: 0.999819] 
[Epoch 126/166] [Batch 300/303] [D loss: 0.387471] [G loss: 0.187572] [ema: 0.999820] 
[Epoch 127/166] [Batch 0/303] [D loss: 0.384690] [G loss: 0.191380] [ema: 0.999820] 
[Epoch 127/166] [Batch 100/303] [D loss: 0.408630] [G loss: 0.180197] [ema: 0.999820] 
[Epoch 127/166] [Batch 200/303] [D loss: 0.386266] [G loss: 0.173487] [ema: 0.999821] 
[Epoch 127/166] [Batch 300/303] [D loss: 0.411894] [G loss: 0.165296] [ema: 0.999821] 
[Epoch 128/166] [Batch 0/303] [D loss: 0.362617] [G loss: 0.174849] [ema: 0.999821] 
[Epoch 128/166] [Batch 100/303] [D loss: 0.431477] [G loss: 0.185794] [ema: 0.999822] 
[Epoch 128/166] [Batch 200/303] [D loss: 0.392149] [G loss: 0.157779] [ema: 0.999822] 
[Epoch 128/166] [Batch 300/303] [D loss: 0.436403] [G loss: 0.174581] [ema: 0.999823] 
[Epoch 129/166] [Batch 0/303] [D loss: 0.399307] [G loss: 0.161802] [ema: 0.999823] 
[Epoch 129/166] [Batch 100/303] [D loss: 0.382560] [G loss: 0.175192] [ema: 0.999823] 
[Epoch 129/166] [Batch 200/303] [D loss: 0.433776] [G loss: 0.185770] [ema: 0.999824] 
[Epoch 129/166] [Batch 300/303] [D loss: 0.371532] [G loss: 0.167138] [ema: 0.999824] 
[Epoch 130/166] [Batch 0/303] [D loss: 0.392541] [G loss: 0.190656] [ema: 0.999824] 
[Epoch 130/166] [Batch 100/303] [D loss: 0.373228] [G loss: 0.174516] [ema: 0.999824] 
[Epoch 130/166] [Batch 200/303] [D loss: 0.401953] [G loss: 0.179922] [ema: 0.999825] 
[Epoch 130/166] [Batch 300/303] [D loss: 0.393659] [G loss: 0.173190] [ema: 0.999825] 
[Epoch 131/166] [Batch 0/303] [D loss: 0.421076] [G loss: 0.187723] [ema: 0.999825] 
[Epoch 131/166] [Batch 100/303] [D loss: 0.398086] [G loss: 0.161154] [ema: 0.999826] 
[Epoch 131/166] [Batch 200/303] [D loss: 0.401605] [G loss: 0.183158] [ema: 0.999826] 
[Epoch 131/166] [Batch 300/303] [D loss: 0.389164] [G loss: 0.175354] [ema: 0.999827] 
[Epoch 132/166] [Batch 0/303] [D loss: 0.377224] [G loss: 0.186045] [ema: 0.999827] 
[Epoch 132/166] [Batch 100/303] [D loss: 0.422650] [G loss: 0.162346] [ema: 0.999827] 
[Epoch 132/166] [Batch 200/303] [D loss: 0.386657] [G loss: 0.176321] [ema: 0.999828] 
[Epoch 132/166] [Batch 300/303] [D loss: 0.424064] [G loss: 0.172842] [ema: 0.999828] 
[Epoch 133/166] [Batch 0/303] [D loss: 0.378989] [G loss: 0.173427] [ema: 0.999828] 
[Epoch 133/166] [Batch 100/303] [D loss: 0.418419] [G loss: 0.176093] [ema: 0.999828] 
[Epoch 133/166] [Batch 200/303] [D loss: 0.386946] [G loss: 0.182683] [ema: 0.999829] 
[Epoch 133/166] [Batch 300/303] [D loss: 0.412384] [G loss: 0.163426] [ema: 0.999829] 
[Epoch 134/166] [Batch 0/303] [D loss: 0.427538] [G loss: 0.174890] [ema: 0.999829] 
[Epoch 134/166] [Batch 100/303] [D loss: 0.386627] [G loss: 0.180226] [ema: 0.999830] 
[Epoch 134/166] [Batch 200/303] [D loss: 0.427685] [G loss: 0.168370] [ema: 0.999830] 
[Epoch 134/166] [Batch 300/303] [D loss: 0.438489] [G loss: 0.172313] [ema: 0.999831] 
[Epoch 135/166] [Batch 0/303] [D loss: 0.390585] [G loss: 0.180314] [ema: 0.999831] 
[Epoch 135/166] [Batch 100/303] [D loss: 0.384020] [G loss: 0.184438] [ema: 0.999831] 
[Epoch 135/166] [Batch 200/303] [D loss: 0.391616] [G loss: 0.203626] [ema: 0.999831] 
[Epoch 135/166] [Batch 300/303] [D loss: 0.395038] [G loss: 0.188441] [ema: 0.999832] 
[Epoch 136/166] [Batch 0/303] [D loss: 0.408561] [G loss: 0.178200] [ema: 0.999832] 
[Epoch 136/166] [Batch 100/303] [D loss: 0.395873] [G loss: 0.184564] [ema: 0.999832] 
[Epoch 136/166] [Batch 200/303] [D loss: 0.394828] [G loss: 0.170278] [ema: 0.999833] 
[Epoch 136/166] [Batch 300/303] [D loss: 0.391702] [G loss: 0.175912] [ema: 0.999833] 
[Epoch 137/166] [Batch 0/303] [D loss: 0.403716] [G loss: 0.175009] [ema: 0.999833] 
[Epoch 137/166] [Batch 100/303] [D loss: 0.384834] [G loss: 0.213944] [ema: 0.999833] 
[Epoch 137/166] [Batch 200/303] [D loss: 0.384315] [G loss: 0.176222] [ema: 0.999834] 
[Epoch 137/166] [Batch 300/303] [D loss: 0.397334] [G loss: 0.151240] [ema: 0.999834] 
[Epoch 138/166] [Batch 0/303] [D loss: 0.483915] [G loss: 0.169309] [ema: 0.999834] 
[Epoch 138/166] [Batch 100/303] [D loss: 0.410069] [G loss: 0.179234] [ema: 0.999835] 
[Epoch 138/166] [Batch 200/303] [D loss: 0.389183] [G loss: 0.157412] [ema: 0.999835] 
[Epoch 138/166] [Batch 300/303] [D loss: 0.394632] [G loss: 0.177870] [ema: 0.999835] 
[Epoch 139/166] [Batch 0/303] [D loss: 0.381871] [G loss: 0.167999] [ema: 0.999835] 
[Epoch 139/166] [Batch 100/303] [D loss: 0.430549] [G loss: 0.171522] [ema: 0.999836] 
[Epoch 139/166] [Batch 200/303] [D loss: 0.407883] [G loss: 0.171516] [ema: 0.999836] 
[Epoch 139/166] [Batch 300/303] [D loss: 0.431832] [G loss: 0.186101] [ema: 0.999837] 
[Epoch 140/166] [Batch 0/303] [D loss: 0.395658] [G loss: 0.174892] [ema: 0.999837] 
[Epoch 140/166] [Batch 100/303] [D loss: 0.428111] [G loss: 0.141546] [ema: 0.999837] 
[Epoch 140/166] [Batch 200/303] [D loss: 0.445160] [G loss: 0.153496] [ema: 0.999837] 
[Epoch 140/166] [Batch 300/303] [D loss: 0.391020] [G loss: 0.173976] [ema: 0.999838] 
[Epoch 141/166] [Batch 0/303] [D loss: 0.500899] [G loss: 0.181665] [ema: 0.999838] 
[Epoch 141/166] [Batch 100/303] [D loss: 0.395530] [G loss: 0.190978] [ema: 0.999838] 
[Epoch 141/166] [Batch 200/303] [D loss: 0.431324] [G loss: 0.188105] [ema: 0.999839] 
[Epoch 141/166] [Batch 300/303] [D loss: 0.446250] [G loss: 0.179311] [ema: 0.999839] 
[Epoch 142/166] [Batch 0/303] [D loss: 0.394931] [G loss: 0.175191] [ema: 0.999839] 
[Epoch 142/166] [Batch 100/303] [D loss: 0.389719] [G loss: 0.182436] [ema: 0.999839] 
[Epoch 142/166] [Batch 200/303] [D loss: 0.391216] [G loss: 0.162449] [ema: 0.999840] 
[Epoch 142/166] [Batch 300/303] [D loss: 0.368460] [G loss: 0.183108] [ema: 0.999840] 
[Epoch 143/166] [Batch 0/303] [D loss: 0.424328] [G loss: 0.191724] [ema: 0.999840] 
[Epoch 143/166] [Batch 100/303] [D loss: 0.426997] [G loss: 0.174612] [ema: 0.999840] 
[Epoch 143/166] [Batch 200/303] [D loss: 0.400838] [G loss: 0.178402] [ema: 0.999841] 
[Epoch 143/166] [Batch 300/303] [D loss: 0.375264] [G loss: 0.193503] [ema: 0.999841] 
[Epoch 144/166] [Batch 0/303] [D loss: 0.391782] [G loss: 0.185237] [ema: 0.999841] 
[Epoch 144/166] [Batch 100/303] [D loss: 0.342895] [G loss: 0.203552] [ema: 0.999842] 
[Epoch 144/166] [Batch 200/303] [D loss: 0.445330] [G loss: 0.175197] [ema: 0.999842] 
[Epoch 144/166] [Batch 300/303] [D loss: 0.433541] [G loss: 0.186843] [ema: 0.999842] 
[Epoch 145/166] [Batch 0/303] [D loss: 0.425872] [G loss: 0.162051] [ema: 0.999842] 
[Epoch 145/166] [Batch 100/303] [D loss: 0.365530] [G loss: 0.182994] [ema: 0.999843] 
[Epoch 145/166] [Batch 200/303] [D loss: 0.410480] [G loss: 0.171311] [ema: 0.999843] 
[Epoch 145/166] [Batch 300/303] [D loss: 0.449372] [G loss: 0.177018] [ema: 0.999843] 
[Epoch 146/166] [Batch 0/303] [D loss: 0.421196] [G loss: 0.172776] [ema: 0.999843] 
[Epoch 146/166] [Batch 100/303] [D loss: 0.405699] [G loss: 0.174423] [ema: 0.999844] 
[Epoch 146/166] [Batch 200/303] [D loss: 0.420461] [G loss: 0.173207] [ema: 0.999844] 
[Epoch 146/166] [Batch 300/303] [D loss: 0.433478] [G loss: 0.168434] [ema: 0.999844] 
[Epoch 147/166] [Batch 0/303] [D loss: 0.382763] [G loss: 0.163684] [ema: 0.999844] 
[Epoch 147/166] [Batch 100/303] [D loss: 0.389111] [G loss: 0.187581] [ema: 0.999845] 
[Epoch 147/166] [Batch 200/303] [D loss: 0.435692] [G loss: 0.181194] [ema: 0.999845] 
[Epoch 147/166] [Batch 300/303] [D loss: 0.385001] [G loss: 0.201045] [ema: 0.999845] 
[Epoch 148/166] [Batch 0/303] [D loss: 0.377196] [G loss: 0.210114] [ema: 0.999845] 
[Epoch 148/166] [Batch 100/303] [D loss: 0.426578] [G loss: 0.165578] [ema: 0.999846] 
[Epoch 148/166] [Batch 200/303] [D loss: 0.399094] [G loss: 0.177142] [ema: 0.999846] 
[Epoch 148/166] [Batch 300/303] [D loss: 0.411994] [G loss: 0.188298] [ema: 0.999846] 
[Epoch 149/166] [Batch 0/303] [D loss: 0.392957] [G loss: 0.186003] [ema: 0.999846] 
[Epoch 149/166] [Batch 100/303] [D loss: 0.372992] [G loss: 0.195612] [ema: 0.999847] 
[Epoch 149/166] [Batch 200/303] [D loss: 0.386329] [G loss: 0.185056] [ema: 0.999847] 
[Epoch 149/166] [Batch 300/303] [D loss: 0.435653] [G loss: 0.180984] [ema: 0.999847] 
[Epoch 150/166] [Batch 0/303] [D loss: 0.440152] [G loss: 0.186457] [ema: 0.999848] 
[Epoch 150/166] [Batch 100/303] [D loss: 0.392164] [G loss: 0.195003] [ema: 0.999848] 
[Epoch 150/166] [Batch 200/303] [D loss: 0.427605] [G loss: 0.165790] [ema: 0.999848] 
[Epoch 150/166] [Batch 300/303] [D loss: 0.384935] [G loss: 0.182640] [ema: 0.999849] 
[Epoch 151/166] [Batch 0/303] [D loss: 0.378708] [G loss: 0.211706] [ema: 0.999849] 
[Epoch 151/166] [Batch 100/303] [D loss: 0.406852] [G loss: 0.181716] [ema: 0.999849] 
[Epoch 151/166] [Batch 200/303] [D loss: 0.430753] [G loss: 0.158031] [ema: 0.999849] 
[Epoch 151/166] [Batch 300/303] [D loss: 0.365942] [G loss: 0.169519] [ema: 0.999850] 
[Epoch 152/166] [Batch 0/303] [D loss: 0.396541] [G loss: 0.177581] [ema: 0.999850] 
[Epoch 152/166] [Batch 100/303] [D loss: 0.376464] [G loss: 0.181938] [ema: 0.999850] 
[Epoch 152/166] [Batch 200/303] [D loss: 0.409165] [G loss: 0.176229] [ema: 0.999850] 
[Epoch 152/166] [Batch 300/303] [D loss: 0.419739] [G loss: 0.169164] [ema: 0.999850] 
[Epoch 153/166] [Batch 0/303] [D loss: 0.446730] [G loss: 0.174572] [ema: 0.999850] 
[Epoch 153/166] [Batch 100/303] [D loss: 0.419086] [G loss: 0.190705] [ema: 0.999851] 
[Epoch 153/166] [Batch 200/303] [D loss: 0.372382] [G loss: 0.182561] [ema: 0.999851] 
[Epoch 153/166] [Batch 300/303] [D loss: 0.362762] [G loss: 0.190267] [ema: 0.999851] 
[Epoch 154/166] [Batch 0/303] [D loss: 0.387505] [G loss: 0.188166] [ema: 0.999851] 
[Epoch 154/166] [Batch 100/303] [D loss: 0.408098] [G loss: 0.175768] [ema: 0.999852] 
[Epoch 154/166] [Batch 200/303] [D loss: 0.365672] [G loss: 0.183439] [ema: 0.999852] 
[Epoch 154/166] [Batch 300/303] [D loss: 0.384466] [G loss: 0.173380] [ema: 0.999852] 
[Epoch 155/166] [Batch 0/303] [D loss: 0.402184] [G loss: 0.188063] [ema: 0.999852] 
[Epoch 155/166] [Batch 100/303] [D loss: 0.364495] [G loss: 0.190837] [ema: 0.999853] 
[Epoch 155/166] [Batch 200/303] [D loss: 0.423131] [G loss: 0.173517] [ema: 0.999853] 
[Epoch 155/166] [Batch 300/303] [D loss: 0.393286] [G loss: 0.172437] [ema: 0.999853] 
[Epoch 156/166] [Batch 0/303] [D loss: 0.377336] [G loss: 0.168576] [ema: 0.999853] 
[Epoch 156/166] [Batch 100/303] [D loss: 0.370034] [G loss: 0.191760] [ema: 0.999854] 
[Epoch 156/166] [Batch 200/303] [D loss: 0.401769] [G loss: 0.172000] [ema: 0.999854] 
[Epoch 156/166] [Batch 300/303] [D loss: 0.410509] [G loss: 0.164813] [ema: 0.999854] 
[Epoch 157/166] [Batch 0/303] [D loss: 0.411830] [G loss: 0.166893] [ema: 0.999854] 
[Epoch 157/166] [Batch 100/303] [D loss: 0.368977] [G loss: 0.167605] [ema: 0.999855] 
[Epoch 157/166] [Batch 200/303] [D loss: 0.410921] [G loss: 0.174061] [ema: 0.999855] 
[Epoch 157/166] [Batch 300/303] [D loss: 0.410255] [G loss: 0.163879] [ema: 0.999855] 
[Epoch 158/166] [Batch 0/303] [D loss: 0.448096] [G loss: 0.165827] [ema: 0.999855] 
[Epoch 158/166] [Batch 100/303] [D loss: 0.419680] [G loss: 0.184902] [ema: 0.999856] 
[Epoch 158/166] [Batch 200/303] [D loss: 0.403538] [G loss: 0.164486] [ema: 0.999856] 
[Epoch 158/166] [Batch 300/303] [D loss: 0.473234] [G loss: 0.172788] [ema: 0.999856] 
[Epoch 159/166] [Batch 0/303] [D loss: 0.417928] [G loss: 0.156880] [ema: 0.999856] 
[Epoch 159/166] [Batch 100/303] [D loss: 0.410583] [G loss: 0.163929] [ema: 0.999856] 
[Epoch 159/166] [Batch 200/303] [D loss: 0.403435] [G loss: 0.163225] [ema: 0.999857] 
[Epoch 159/166] [Batch 300/303] [D loss: 0.382172] [G loss: 0.182193] [ema: 0.999857] 
[Epoch 160/166] [Batch 0/303] [D loss: 0.444115] [G loss: 0.176831] [ema: 0.999857] 
[Epoch 160/166] [Batch 100/303] [D loss: 0.422432] [G loss: 0.166859] [ema: 0.999857] 
[Epoch 160/166] [Batch 200/303] [D loss: 0.416165] [G loss: 0.150881] [ema: 0.999858] 
[Epoch 160/166] [Batch 300/303] [D loss: 0.446786] [G loss: 0.160780] [ema: 0.999858] 
[Epoch 161/166] [Batch 0/303] [D loss: 0.420937] [G loss: 0.172762] [ema: 0.999858] 
[Epoch 161/166] [Batch 100/303] [D loss: 0.424167] [G loss: 0.166922] [ema: 0.999858] 
[Epoch 161/166] [Batch 200/303] [D loss: 0.407932] [G loss: 0.174249] [ema: 0.999859] 
[Epoch 161/166] [Batch 300/303] [D loss: 0.422805] [G loss: 0.170113] [ema: 0.999859] 
[Epoch 162/166] [Batch 0/303] [D loss: 0.458264] [G loss: 0.154172] [ema: 0.999859] 
[Epoch 162/166] [Batch 100/303] [D loss: 0.423171] [G loss: 0.178817] [ema: 0.999859] 
[Epoch 162/166] [Batch 200/303] [D loss: 0.414595] [G loss: 0.159801] [ema: 0.999859] 
[Epoch 162/166] [Batch 300/303] [D loss: 0.409315] [G loss: 0.154211] [ema: 0.999860] 
[Epoch 163/166] [Batch 0/303] [D loss: 0.433752] [G loss: 0.170263] [ema: 0.999860] 
[Epoch 163/166] [Batch 100/303] [D loss: 0.463401] [G loss: 0.176475] [ema: 0.999860] 
[Epoch 163/166] [Batch 200/303] [D loss: 0.416903] [G loss: 0.160030] [ema: 0.999860] 
[Epoch 163/166] [Batch 300/303] [D loss: 0.413442] [G loss: 0.168301] [ema: 0.999861] 
[Epoch 164/166] [Batch 0/303] [D loss: 0.431541] [G loss: 0.178548] [ema: 0.999861] 
[Epoch 164/166] [Batch 100/303] [D loss: 0.420555] [G loss: 0.177450] [ema: 0.999861] 
[Epoch 164/166] [Batch 200/303] [D loss: 0.462813] [G loss: 0.169789] [ema: 0.999861] 
[Epoch 164/166] [Batch 300/303] [D loss: 0.466326] [G loss: 0.177983] [ema: 0.999861] 
[Epoch 165/166] [Batch 0/303] [D loss: 0.410256] [G loss: 0.144591] [ema: 0.999861] 
[Epoch 165/166] [Batch 100/303] [D loss: 0.446350] [G loss: 0.184962] [ema: 0.999862] 
[Epoch 165/166] [Batch 200/303] [D loss: 0.444019] [G loss: 0.191392] [ema: 0.999862] 
[Epoch 165/166] [Batch 300/303] [D loss: 0.448939] [G loss: 0.158437] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
RealWorld_waist_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
RealWorld_waist_DAGHAR_Multiclass
daghar
return single class data and labels, class is RealWorld_waist_DAGHAR_Multiclass
data shape is (20664, 6, 1, 30)
label shape is (20664,)
1292
Epochs between checkpoint: 10



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_30_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_30_2024_10_28_21_01_34/Model



[Epoch 0/39] [Batch 0/1292] [D loss: 4.828017] [G loss: 2.605999] [ema: 0.000000] 
[Epoch 0/39] [Batch 100/1292] [D loss: 0.398492] [G loss: 0.272976] [ema: 0.933033] 
[Epoch 0/39] [Batch 200/1292] [D loss: 0.566929] [G loss: 0.139109] [ema: 0.965936] 
[Epoch 0/39] [Batch 300/1292] [D loss: 0.497384] [G loss: 0.162479] [ema: 0.977160] 
[Epoch 0/39] [Batch 400/1292] [D loss: 0.486091] [G loss: 0.087477] [ema: 0.982821] 
[Epoch 0/39] [Batch 500/1292] [D loss: 0.347187] [G loss: 0.237347] [ema: 0.986233] 
[Epoch 0/39] [Batch 600/1292] [D loss: 0.517320] [G loss: 0.131962] [ema: 0.988514] 
[Epoch 0/39] [Batch 700/1292] [D loss: 0.446573] [G loss: 0.180629] [ema: 0.990147] 
[Epoch 0/39] [Batch 800/1292] [D loss: 0.439129] [G loss: 0.166885] [ema: 0.991373] 
[Epoch 0/39] [Batch 900/1292] [D loss: 0.482312] [G loss: 0.154808] [ema: 0.992328] 
[Epoch 0/39] [Batch 1000/1292] [D loss: 0.432054] [G loss: 0.174396] [ema: 0.993092] 
[Epoch 0/39] [Batch 1100/1292] [D loss: 0.483995] [G loss: 0.134097] [ema: 0.993718] 
[Epoch 0/39] [Batch 1200/1292] [D loss: 0.449914] [G loss: 0.165286] [ema: 0.994240] 
[Epoch 1/39] [Batch 0/1292] [D loss: 0.417885] [G loss: 0.175320] [ema: 0.994649] 
[Epoch 1/39] [Batch 100/1292] [D loss: 0.440343] [G loss: 0.155775] [ema: 0.995033] 
[Epoch 1/39] [Batch 200/1292] [D loss: 0.393431] [G loss: 0.150769] [ema: 0.995365] 
[Epoch 1/39] [Batch 300/1292] [D loss: 0.434880] [G loss: 0.192025] [ema: 0.995656] 
[Epoch 1/39] [Batch 400/1292] [D loss: 0.472848] [G loss: 0.148413] [ema: 0.995912] 
[Epoch 1/39] [Batch 500/1292] [D loss: 0.499801] [G loss: 0.133745] [ema: 0.996139] 
[Epoch 1/39] [Batch 600/1292] [D loss: 0.429884] [G loss: 0.151340] [ema: 0.996343] 
[Epoch 1/39] [Batch 700/1292] [D loss: 0.430531] [G loss: 0.162680] [ema: 0.996526] 
[Epoch 1/39] [Batch 800/1292] [D loss: 0.471037] [G loss: 0.151801] [ema: 0.996692] 
[Epoch 1/39] [Batch 900/1292] [D loss: 0.426995] [G loss: 0.181416] [ema: 0.996843] 
[Epoch 1/39] [Batch 1000/1292] [D loss: 0.393999] [G loss: 0.218812] [ema: 0.996980] 
[Epoch 1/39] [Batch 1100/1292] [D loss: 0.432101] [G loss: 0.191743] [ema: 0.997106] 
[Epoch 1/39] [Batch 1200/1292] [D loss: 0.444680] [G loss: 0.180686] [ema: 0.997222] 
[Epoch 2/39] [Batch 0/1292] [D loss: 0.377818] [G loss: 0.162804] [ema: 0.997321] 
[Epoch 2/39] [Batch 100/1292] [D loss: 0.445019] [G loss: 0.220094] [ema: 0.997421] 
[Epoch 2/39] [Batch 200/1292] [D loss: 0.350983] [G loss: 0.144095] [ema: 0.997513] 
[Epoch 2/39] [Batch 300/1292] [D loss: 0.385584] [G loss: 0.160390] [ema: 0.997599] 
[Epoch 2/39] [Batch 400/1292] [D loss: 0.416031] [G loss: 0.172858] [ema: 0.997680] 
[Epoch 2/39] [Batch 500/1292] [D loss: 0.394160] [G loss: 0.200210] [ema: 0.997755] 
[Epoch 2/39] [Batch 600/1292] [D loss: 0.383003] [G loss: 0.188982] [ema: 0.997825] 
[Epoch 2/39] [Batch 700/1292] [D loss: 0.342026] [G loss: 0.199790] [ema: 0.997892] 
[Epoch 2/39] [Batch 800/1292] [D loss: 0.426143] [G loss: 0.186228] [ema: 0.997954] 
[Epoch 2/39] [Batch 900/1292] [D loss: 0.385562] [G loss: 0.191265] [ema: 0.998012] 
[Epoch 2/39] [Batch 1000/1292] [D loss: 0.381123] [G loss: 0.182338] [ema: 0.998068] 
[Epoch 2/39] [Batch 1100/1292] [D loss: 0.423651] [G loss: 0.174201] [ema: 0.998120] 
[Epoch 2/39] [Batch 1200/1292] [D loss: 0.337725] [G loss: 0.214188] [ema: 0.998170] 
[Epoch 3/39] [Batch 0/1292] [D loss: 0.416971] [G loss: 0.189731] [ema: 0.998213] 
[Epoch 3/39] [Batch 100/1292] [D loss: 0.379517] [G loss: 0.190871] [ema: 0.998258] 
[Epoch 3/39] [Batch 200/1292] [D loss: 0.428309] [G loss: 0.164065] [ema: 0.998301] 
[Epoch 3/39] [Batch 300/1292] [D loss: 0.380334] [G loss: 0.181104] [ema: 0.998342] 
[Epoch 3/39] [Batch 400/1292] [D loss: 0.409619] [G loss: 0.189236] [ema: 0.998380] 
[Epoch 3/39] [Batch 500/1292] [D loss: 0.356454] [G loss: 0.177405] [ema: 0.998417] 
[Epoch 3/39] [Batch 600/1292] [D loss: 0.416266] [G loss: 0.189270] [ema: 0.998453] 
[Epoch 3/39] [Batch 700/1292] [D loss: 0.435462] [G loss: 0.184900] [ema: 0.998486] 
[Epoch 3/39] [Batch 800/1292] [D loss: 0.421712] [G loss: 0.196953] [ema: 0.998519] 
[Epoch 3/39] [Batch 900/1292] [D loss: 0.385077] [G loss: 0.197737] [ema: 0.998550] 
[Epoch 3/39] [Batch 1000/1292] [D loss: 0.387364] [G loss: 0.193569] [ema: 0.998579] 
[Epoch 3/39] [Batch 1100/1292] [D loss: 0.383585] [G loss: 0.184416] [ema: 0.998608] 
[Epoch 3/39] [Batch 1200/1292] [D loss: 0.410290] [G loss: 0.183370] [ema: 0.998635] 
[Epoch 4/39] [Batch 0/1292] [D loss: 0.385618] [G loss: 0.197537] [ema: 0.998660] 
[Epoch 4/39] [Batch 100/1292] [D loss: 0.339044] [G loss: 0.200912] [ema: 0.998685] 
[Epoch 4/39] [Batch 200/1292] [D loss: 0.397909] [G loss: 0.180361] [ema: 0.998710] 
[Epoch 4/39] [Batch 300/1292] [D loss: 0.399316] [G loss: 0.175002] [ema: 0.998733] 
[Epoch 4/39] [Batch 400/1292] [D loss: 0.373567] [G loss: 0.180673] [ema: 0.998756] 
[Epoch 4/39] [Batch 500/1292] [D loss: 0.466931] [G loss: 0.168578] [ema: 0.998778] 
[Epoch 4/39] [Batch 600/1292] [D loss: 0.436805] [G loss: 0.192060] [ema: 0.998799] 
[Epoch 4/39] [Batch 700/1292] [D loss: 0.396415] [G loss: 0.171395] [ema: 0.998819] 
[Epoch 4/39] [Batch 800/1292] [D loss: 0.412051] [G loss: 0.177099] [ema: 0.998839] 
[Epoch 4/39] [Batch 900/1292] [D loss: 0.376958] [G loss: 0.172495] [ema: 0.998858] 
[Epoch 4/39] [Batch 1000/1292] [D loss: 0.398207] [G loss: 0.172165] [ema: 0.998877] 
[Epoch 4/39] [Batch 1100/1292] [D loss: 0.408436] [G loss: 0.165050] [ema: 0.998895] 
[Epoch 4/39] [Batch 1200/1292] [D loss: 0.392042] [G loss: 0.181008] [ema: 0.998912] 
[Epoch 5/39] [Batch 0/1292] [D loss: 0.413811] [G loss: 0.182127] [ema: 0.998928] 
[Epoch 5/39] [Batch 100/1292] [D loss: 0.410162] [G loss: 0.178177] [ema: 0.998944] 
[Epoch 5/39] [Batch 200/1292] [D loss: 0.405658] [G loss: 0.173356] [ema: 0.998960] 
[Epoch 5/39] [Batch 300/1292] [D loss: 0.424234] [G loss: 0.175989] [ema: 0.998975] 
[Epoch 5/39] [Batch 400/1292] [D loss: 0.375538] [G loss: 0.189962] [ema: 0.998990] 
[Epoch 5/39] [Batch 500/1292] [D loss: 0.388128] [G loss: 0.192836] [ema: 0.999005] 
[Epoch 5/39] [Batch 600/1292] [D loss: 0.399934] [G loss: 0.169567] [ema: 0.999019] 
[Epoch 5/39] [Batch 700/1292] [D loss: 0.387001] [G loss: 0.191164] [ema: 0.999032] 
[Epoch 5/39] [Batch 800/1292] [D loss: 0.383776] [G loss: 0.183346] [ema: 0.999046] 
[Epoch 5/39] [Batch 900/1292] [D loss: 0.423392] [G loss: 0.183205] [ema: 0.999059] 
[Epoch 5/39] [Batch 1000/1292] [D loss: 0.381927] [G loss: 0.181340] [ema: 0.999071] 
[Epoch 5/39] [Batch 1100/1292] [D loss: 0.398147] [G loss: 0.172226] [ema: 0.999084] 
[Epoch 5/39] [Batch 1200/1292] [D loss: 0.407176] [G loss: 0.184274] [ema: 0.999096] 
[Epoch 6/39] [Batch 0/1292] [D loss: 0.397222] [G loss: 0.186148] [ema: 0.999106] 
[Epoch 6/39] [Batch 100/1292] [D loss: 0.351948] [G loss: 0.202949] [ema: 0.999118] 
[Epoch 6/39] [Batch 200/1292] [D loss: 0.408289] [G loss: 0.167543] [ema: 0.999129] 
[Epoch 6/39] [Batch 300/1292] [D loss: 0.363747] [G loss: 0.187451] [ema: 0.999140] 
[Epoch 6/39] [Batch 400/1292] [D loss: 0.399808] [G loss: 0.183701] [ema: 0.999150] 
[Epoch 6/39] [Batch 500/1292] [D loss: 0.404419] [G loss: 0.180677] [ema: 0.999160] 
[Epoch 6/39] [Batch 600/1292] [D loss: 0.357563] [G loss: 0.172005] [ema: 0.999170] 
[Epoch 6/39] [Batch 700/1292] [D loss: 0.406204] [G loss: 0.190039] [ema: 0.999180] 
[Epoch 6/39] [Batch 800/1292] [D loss: 0.407673] [G loss: 0.200374] [ema: 0.999190] 
[Epoch 6/39] [Batch 900/1292] [D loss: 0.378717] [G loss: 0.187100] [ema: 0.999199] 
[Epoch 6/39] [Batch 1000/1292] [D loss: 0.388021] [G loss: 0.196980] [ema: 0.999208] 
[Epoch 6/39] [Batch 1100/1292] [D loss: 0.429189] [G loss: 0.161069] [ema: 0.999217] 
[Epoch 6/39] [Batch 1200/1292] [D loss: 0.376589] [G loss: 0.203378] [ema: 0.999226] 
[Epoch 7/39] [Batch 0/1292] [D loss: 0.406875] [G loss: 0.177267] [ema: 0.999234] 
[Epoch 7/39] [Batch 100/1292] [D loss: 0.389861] [G loss: 0.192986] [ema: 0.999242] 
[Epoch 7/39] [Batch 200/1292] [D loss: 0.421075] [G loss: 0.167710] [ema: 0.999250] 
[Epoch 7/39] [Batch 300/1292] [D loss: 0.396983] [G loss: 0.180392] [ema: 0.999258] 
[Epoch 7/39] [Batch 400/1292] [D loss: 0.387601] [G loss: 0.172240] [ema: 0.999266] 
[Epoch 7/39] [Batch 500/1292] [D loss: 0.390458] [G loss: 0.181100] [ema: 0.999274] 
[Epoch 7/39] [Batch 600/1292] [D loss: 0.349584] [G loss: 0.189352] [ema: 0.999282] 
[Epoch 7/39] [Batch 700/1292] [D loss: 0.383990] [G loss: 0.201882] [ema: 0.999289] 
[Epoch 7/39] [Batch 800/1292] [D loss: 0.403514] [G loss: 0.182455] [ema: 0.999296] 
[Epoch 7/39] [Batch 900/1292] [D loss: 0.421462] [G loss: 0.190768] [ema: 0.999303] 
[Epoch 7/39] [Batch 1000/1292] [D loss: 0.391366] [G loss: 0.178606] [ema: 0.999310] 
[Epoch 7/39] [Batch 1100/1292] [D loss: 0.423923] [G loss: 0.185284] [ema: 0.999317] 
[Epoch 7/39] [Batch 1200/1292] [D loss: 0.399886] [G loss: 0.197206] [ema: 0.999324] 
[Epoch 8/39] [Batch 0/1292] [D loss: 0.388044] [G loss: 0.160759] [ema: 0.999330] 
[Epoch 8/39] [Batch 100/1292] [D loss: 0.421099] [G loss: 0.174588] [ema: 0.999336] 
[Epoch 8/39] [Batch 200/1292] [D loss: 0.398367] [G loss: 0.161492] [ema: 0.999342] 
[Epoch 8/39] [Batch 300/1292] [D loss: 0.383714] [G loss: 0.178763] [ema: 0.999349] 
[Epoch 8/39] [Batch 400/1292] [D loss: 0.386772] [G loss: 0.175354] [ema: 0.999355] 
[Epoch 8/39] [Batch 500/1292] [D loss: 0.353201] [G loss: 0.176612] [ema: 0.999361] 
[Epoch 8/39] [Batch 600/1292] [D loss: 0.401699] [G loss: 0.178436] [ema: 0.999366] 
[Epoch 8/39] [Batch 700/1292] [D loss: 0.438220] [G loss: 0.180149] [ema: 0.999372] 
[Epoch 8/39] [Batch 800/1292] [D loss: 0.350432] [G loss: 0.215690] [ema: 0.999378] 
[Epoch 8/39] [Batch 900/1292] [D loss: 0.415866] [G loss: 0.172956] [ema: 0.999383] 
[Epoch 8/39] [Batch 1000/1292] [D loss: 0.420062] [G loss: 0.186578] [ema: 0.999389] 
[Epoch 8/39] [Batch 1100/1292] [D loss: 0.430057] [G loss: 0.174910] [ema: 0.999394] 
[Epoch 8/39] [Batch 1200/1292] [D loss: 0.399222] [G loss: 0.179872] [ema: 0.999399] 
[Epoch 9/39] [Batch 0/1292] [D loss: 0.380846] [G loss: 0.201075] [ema: 0.999404] 
[Epoch 9/39] [Batch 100/1292] [D loss: 0.402565] [G loss: 0.172057] [ema: 0.999409] 
[Epoch 9/39] [Batch 200/1292] [D loss: 0.392410] [G loss: 0.182568] [ema: 0.999414] 
[Epoch 9/39] [Batch 300/1292] [D loss: 0.428536] [G loss: 0.191166] [ema: 0.999419] 
[Epoch 9/39] [Batch 400/1292] [D loss: 0.377576] [G loss: 0.188072] [ema: 0.999424] 
[Epoch 9/39] [Batch 500/1292] [D loss: 0.395858] [G loss: 0.194137] [ema: 0.999429] 
[Epoch 9/39] [Batch 600/1292] [D loss: 0.394937] [G loss: 0.194807] [ema: 0.999433] 
[Epoch 9/39] [Batch 700/1292] [D loss: 0.344714] [G loss: 0.184341] [ema: 0.999438] 
[Epoch 9/39] [Batch 800/1292] [D loss: 0.403306] [G loss: 0.188972] [ema: 0.999442] 
[Epoch 9/39] [Batch 900/1292] [D loss: 0.361730] [G loss: 0.184831] [ema: 0.999447] 
[Epoch 9/39] [Batch 1000/1292] [D loss: 0.397382] [G loss: 0.183684] [ema: 0.999451] 
[Epoch 9/39] [Batch 1100/1292] [D loss: 0.413203] [G loss: 0.165146] [ema: 0.999456] 
[Epoch 9/39] [Batch 1200/1292] [D loss: 0.428781] [G loss: 0.175445] [ema: 0.999460] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_30_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_30_2024_10_28_21_01_34/Model



[Epoch 10/39] [Batch 0/1292] [D loss: 0.359126] [G loss: 0.192767] [ema: 0.999464] 
[Epoch 10/39] [Batch 100/1292] [D loss: 0.409128] [G loss: 0.171979] [ema: 0.999468] 
[Epoch 10/39] [Batch 200/1292] [D loss: 0.358069] [G loss: 0.197040] [ema: 0.999472] 
[Epoch 10/39] [Batch 300/1292] [D loss: 0.393509] [G loss: 0.173356] [ema: 0.999476] 
[Epoch 10/39] [Batch 400/1292] [D loss: 0.393073] [G loss: 0.176701] [ema: 0.999480] 
[Epoch 10/39] [Batch 500/1292] [D loss: 0.352969] [G loss: 0.206778] [ema: 0.999484] 
[Epoch 10/39] [Batch 600/1292] [D loss: 0.353307] [G loss: 0.179765] [ema: 0.999487] 
[Epoch 10/39] [Batch 700/1292] [D loss: 0.407074] [G loss: 0.181855] [ema: 0.999491] 
[Epoch 10/39] [Batch 800/1292] [D loss: 0.400974] [G loss: 0.181441] [ema: 0.999495] 
[Epoch 10/39] [Batch 900/1292] [D loss: 0.397380] [G loss: 0.177316] [ema: 0.999499] 
[Epoch 10/39] [Batch 1000/1292] [D loss: 0.387808] [G loss: 0.191281] [ema: 0.999502] 
[Epoch 10/39] [Batch 1100/1292] [D loss: 0.410692] [G loss: 0.175622] [ema: 0.999506] 
[Epoch 10/39] [Batch 1200/1292] [D loss: 0.372091] [G loss: 0.182622] [ema: 0.999509] 
[Epoch 11/39] [Batch 0/1292] [D loss: 0.417407] [G loss: 0.175423] [ema: 0.999512] 
[Epoch 11/39] [Batch 100/1292] [D loss: 0.393007] [G loss: 0.193577] [ema: 0.999516] 
[Epoch 11/39] [Batch 200/1292] [D loss: 0.363356] [G loss: 0.181583] [ema: 0.999519] 
[Epoch 11/39] [Batch 300/1292] [D loss: 0.385919] [G loss: 0.192902] [ema: 0.999522] 
[Epoch 11/39] [Batch 400/1292] [D loss: 0.383174] [G loss: 0.175827] [ema: 0.999526] 
[Epoch 11/39] [Batch 500/1292] [D loss: 0.490455] [G loss: 0.183475] [ema: 0.999529] 
[Epoch 11/39] [Batch 600/1292] [D loss: 0.372969] [G loss: 0.171972] [ema: 0.999532] 
[Epoch 11/39] [Batch 700/1292] [D loss: 0.365018] [G loss: 0.187555] [ema: 0.999535] 
[Epoch 11/39] [Batch 800/1292] [D loss: 0.398229] [G loss: 0.178177] [ema: 0.999538] 
[Epoch 11/39] [Batch 900/1292] [D loss: 0.347706] [G loss: 0.177397] [ema: 0.999541] 
[Epoch 11/39] [Batch 1000/1292] [D loss: 0.398013] [G loss: 0.185117] [ema: 0.999544] 
[Epoch 11/39] [Batch 1100/1292] [D loss: 0.423459] [G loss: 0.170856] [ema: 0.999547] 
[Epoch 11/39] [Batch 1200/1292] [D loss: 0.383770] [G loss: 0.184436] [ema: 0.999550] 
[Epoch 12/39] [Batch 0/1292] [D loss: 0.393924] [G loss: 0.184768] [ema: 0.999553] 
[Epoch 12/39] [Batch 100/1292] [D loss: 0.350625] [G loss: 0.197658] [ema: 0.999556] 
[Epoch 12/39] [Batch 200/1292] [D loss: 0.410475] [G loss: 0.185817] [ema: 0.999559] 
[Epoch 12/39] [Batch 300/1292] [D loss: 0.403625] [G loss: 0.192498] [ema: 0.999562] 
[Epoch 12/39] [Batch 400/1292] [D loss: 0.413965] [G loss: 0.189213] [ema: 0.999564] 
[Epoch 12/39] [Batch 500/1292] [D loss: 0.349812] [G loss: 0.175960] [ema: 0.999567] 
[Epoch 12/39] [Batch 600/1292] [D loss: 0.372677] [G loss: 0.171912] [ema: 0.999570] 
[Epoch 12/39] [Batch 700/1292] [D loss: 0.387473] [G loss: 0.167376] [ema: 0.999572] 
[Epoch 12/39] [Batch 800/1292] [D loss: 0.380584] [G loss: 0.190499] [ema: 0.999575] 
[Epoch 12/39] [Batch 900/1292] [D loss: 0.396924] [G loss: 0.187178] [ema: 0.999578] 
[Epoch 12/39] [Batch 1000/1292] [D loss: 0.428985] [G loss: 0.183862] [ema: 0.999580] 
[Epoch 12/39] [Batch 1100/1292] [D loss: 0.409786] [G loss: 0.182020] [ema: 0.999583] 
[Epoch 12/39] [Batch 1200/1292] [D loss: 0.401013] [G loss: 0.177543] [ema: 0.999585] 
[Epoch 13/39] [Batch 0/1292] [D loss: 0.397096] [G loss: 0.185566] [ema: 0.999587] 
[Epoch 13/39] [Batch 100/1292] [D loss: 0.361290] [G loss: 0.187101] [ema: 0.999590] 
[Epoch 13/39] [Batch 200/1292] [D loss: 0.371854] [G loss: 0.199161] [ema: 0.999592] 
[Epoch 13/39] [Batch 300/1292] [D loss: 0.396311] [G loss: 0.182644] [ema: 0.999595] 
[Epoch 13/39] [Batch 400/1292] [D loss: 0.441916] [G loss: 0.182906] [ema: 0.999597] 
[Epoch 13/39] [Batch 500/1292] [D loss: 0.407971] [G loss: 0.179207] [ema: 0.999599] 
[Epoch 13/39] [Batch 600/1292] [D loss: 0.376719] [G loss: 0.181066] [ema: 0.999602] 
[Epoch 13/39] [Batch 700/1292] [D loss: 0.379784] [G loss: 0.187620] [ema: 0.999604] 
[Epoch 13/39] [Batch 800/1292] [D loss: 0.369134] [G loss: 0.188993] [ema: 0.999606] 
[Epoch 13/39] [Batch 900/1292] [D loss: 0.350853] [G loss: 0.178634] [ema: 0.999608] 
[Epoch 13/39] [Batch 1000/1292] [D loss: 0.384871] [G loss: 0.173204] [ema: 0.999611] 
[Epoch 13/39] [Batch 1100/1292] [D loss: 0.382803] [G loss: 0.187850] [ema: 0.999613] 
[Epoch 13/39] [Batch 1200/1292] [D loss: 0.336192] [G loss: 0.202015] [ema: 0.999615] 
[Epoch 14/39] [Batch 0/1292] [D loss: 0.422114] [G loss: 0.172930] [ema: 0.999617] 
[Epoch 14/39] [Batch 100/1292] [D loss: 0.366913] [G loss: 0.188688] [ema: 0.999619] 
[Epoch 14/39] [Batch 200/1292] [D loss: 0.405433] [G loss: 0.162496] [ema: 0.999621] 
[Epoch 14/39] [Batch 300/1292] [D loss: 0.393724] [G loss: 0.171245] [ema: 0.999623] 
[Epoch 14/39] [Batch 400/1292] [D loss: 0.391345] [G loss: 0.181311] [ema: 0.999625] 
[Epoch 14/39] [Batch 500/1292] [D loss: 0.339570] [G loss: 0.194916] [ema: 0.999627] 
[Epoch 14/39] [Batch 600/1292] [D loss: 0.401755] [G loss: 0.165656] [ema: 0.999629] 
[Epoch 14/39] [Batch 700/1292] [D loss: 0.402461] [G loss: 0.198649] [ema: 0.999631] 
[Epoch 14/39] [Batch 800/1292] [D loss: 0.442727] [G loss: 0.189532] [ema: 0.999633] 
[Epoch 14/39] [Batch 900/1292] [D loss: 0.413032] [G loss: 0.170897] [ema: 0.999635] 
[Epoch 14/39] [Batch 1000/1292] [D loss: 0.384182] [G loss: 0.189749] [ema: 0.999637] 
[Epoch 14/39] [Batch 1100/1292] [D loss: 0.381446] [G loss: 0.205494] [ema: 0.999639] 
[Epoch 14/39] [Batch 1200/1292] [D loss: 0.399615] [G loss: 0.192126] [ema: 0.999641] 
[Epoch 15/39] [Batch 0/1292] [D loss: 0.388088] [G loss: 0.192754] [ema: 0.999642] 
[Epoch 15/39] [Batch 100/1292] [D loss: 0.343138] [G loss: 0.202643] [ema: 0.999644] 
[Epoch 15/39] [Batch 200/1292] [D loss: 0.348652] [G loss: 0.180316] [ema: 0.999646] 
[Epoch 15/39] [Batch 300/1292] [D loss: 0.408173] [G loss: 0.180655] [ema: 0.999648] 
[Epoch 15/39] [Batch 400/1292] [D loss: 0.302958] [G loss: 0.204635] [ema: 0.999650] 
[Epoch 15/39] [Batch 500/1292] [D loss: 0.381353] [G loss: 0.176314] [ema: 0.999651] 
[Epoch 15/39] [Batch 600/1292] [D loss: 0.400667] [G loss: 0.178831] [ema: 0.999653] 
[Epoch 15/39] [Batch 700/1292] [D loss: 0.436454] [G loss: 0.193806] [ema: 0.999655] 
[Epoch 15/39] [Batch 800/1292] [D loss: 0.388768] [G loss: 0.188977] [ema: 0.999657] 
[Epoch 15/39] [Batch 900/1292] [D loss: 0.416529] [G loss: 0.178966] [ema: 0.999658] 
[Epoch 15/39] [Batch 1000/1292] [D loss: 0.387211] [G loss: 0.175798] [ema: 0.999660] 
[Epoch 15/39] [Batch 1100/1292] [D loss: 0.422529] [G loss: 0.173629] [ema: 0.999662] 
[Epoch 15/39] [Batch 1200/1292] [D loss: 0.355524] [G loss: 0.178114] [ema: 0.999663] 
[Epoch 16/39] [Batch 0/1292] [D loss: 0.370828] [G loss: 0.188619] [ema: 0.999665] 
[Epoch 16/39] [Batch 100/1292] [D loss: 0.377641] [G loss: 0.165094] [ema: 0.999666] 
[Epoch 16/39] [Batch 200/1292] [D loss: 0.419030] [G loss: 0.179769] [ema: 0.999668] 
[Epoch 16/39] [Batch 300/1292] [D loss: 0.386694] [G loss: 0.194892] [ema: 0.999670] 
[Epoch 16/39] [Batch 400/1292] [D loss: 0.388458] [G loss: 0.196751] [ema: 0.999671] 
[Epoch 16/39] [Batch 500/1292] [D loss: 0.387389] [G loss: 0.183815] [ema: 0.999673] 
[Epoch 16/39] [Batch 600/1292] [D loss: 0.384418] [G loss: 0.180090] [ema: 0.999674] 
[Epoch 16/39] [Batch 700/1292] [D loss: 0.357796] [G loss: 0.189956] [ema: 0.999676] 
[Epoch 16/39] [Batch 800/1292] [D loss: 0.402344] [G loss: 0.185685] [ema: 0.999677] 
[Epoch 16/39] [Batch 900/1292] [D loss: 0.416695] [G loss: 0.184879] [ema: 0.999679] 
[Epoch 16/39] [Batch 1000/1292] [D loss: 0.419270] [G loss: 0.201651] [ema: 0.999680] 
[Epoch 16/39] [Batch 1100/1292] [D loss: 0.407190] [G loss: 0.180109] [ema: 0.999682] 
[Epoch 16/39] [Batch 1200/1292] [D loss: 0.413729] [G loss: 0.186929] [ema: 0.999683] 
[Epoch 17/39] [Batch 0/1292] [D loss: 0.397618] [G loss: 0.194395] [ema: 0.999684] 
[Epoch 17/39] [Batch 100/1292] [D loss: 0.387893] [G loss: 0.169675] [ema: 0.999686] 
[Epoch 17/39] [Batch 200/1292] [D loss: 0.419952] [G loss: 0.186549] [ema: 0.999687] 
[Epoch 17/39] [Batch 300/1292] [D loss: 0.395484] [G loss: 0.197600] [ema: 0.999689] 
[Epoch 17/39] [Batch 400/1292] [D loss: 0.431537] [G loss: 0.178798] [ema: 0.999690] 
[Epoch 17/39] [Batch 500/1292] [D loss: 0.356496] [G loss: 0.185294] [ema: 0.999691] 
[Epoch 17/39] [Batch 600/1292] [D loss: 0.387649] [G loss: 0.175347] [ema: 0.999693] 
[Epoch 17/39] [Batch 700/1292] [D loss: 0.366646] [G loss: 0.192243] [ema: 0.999694] 
[Epoch 17/39] [Batch 800/1292] [D loss: 0.409257] [G loss: 0.185350] [ema: 0.999696] 
[Epoch 17/39] [Batch 900/1292] [D loss: 0.381589] [G loss: 0.165933] [ema: 0.999697] 
[Epoch 17/39] [Batch 1000/1292] [D loss: 0.385484] [G loss: 0.167059] [ema: 0.999698] 
[Epoch 17/39] [Batch 1100/1292] [D loss: 0.384001] [G loss: 0.183217] [ema: 0.999700] 
[Epoch 17/39] [Batch 1200/1292] [D loss: 0.396630] [G loss: 0.191026] [ema: 0.999701] 
[Epoch 18/39] [Batch 0/1292] [D loss: 0.386521] [G loss: 0.197607] [ema: 0.999702] 
[Epoch 18/39] [Batch 100/1292] [D loss: 0.356867] [G loss: 0.187646] [ema: 0.999703] 
[Epoch 18/39] [Batch 200/1292] [D loss: 0.378636] [G loss: 0.174620] [ema: 0.999705] 
[Epoch 18/39] [Batch 300/1292] [D loss: 0.403962] [G loss: 0.171612] [ema: 0.999706] 
[Epoch 18/39] [Batch 400/1292] [D loss: 0.447890] [G loss: 0.176296] [ema: 0.999707] 
[Epoch 18/39] [Batch 500/1292] [D loss: 0.344799] [G loss: 0.181866] [ema: 0.999708] 
[Epoch 18/39] [Batch 600/1292] [D loss: 0.380353] [G loss: 0.179244] [ema: 0.999709] 
[Epoch 18/39] [Batch 700/1292] [D loss: 0.421064] [G loss: 0.178948] [ema: 0.999711] 
[Epoch 18/39] [Batch 800/1292] [D loss: 0.327690] [G loss: 0.201491] [ema: 0.999712] 
[Epoch 18/39] [Batch 900/1292] [D loss: 0.425628] [G loss: 0.193754] [ema: 0.999713] 
[Epoch 18/39] [Batch 1000/1292] [D loss: 0.424352] [G loss: 0.177520] [ema: 0.999714] 
[Epoch 18/39] [Batch 1100/1292] [D loss: 0.401356] [G loss: 0.194904] [ema: 0.999715] 
[Epoch 18/39] [Batch 1200/1292] [D loss: 0.400592] [G loss: 0.172792] [ema: 0.999717] 
[Epoch 19/39] [Batch 0/1292] [D loss: 0.419285] [G loss: 0.175538] [ema: 0.999718] 
[Epoch 19/39] [Batch 100/1292] [D loss: 0.391139] [G loss: 0.194168] [ema: 0.999719] 
[Epoch 19/39] [Batch 200/1292] [D loss: 0.409612] [G loss: 0.158101] [ema: 0.999720] 
[Epoch 19/39] [Batch 300/1292] [D loss: 0.432170] [G loss: 0.180814] [ema: 0.999721] 
[Epoch 19/39] [Batch 400/1292] [D loss: 0.416746] [G loss: 0.186564] [ema: 0.999722] 
[Epoch 19/39] [Batch 500/1292] [D loss: 0.410584] [G loss: 0.186071] [ema: 0.999723] 
[Epoch 19/39] [Batch 600/1292] [D loss: 0.417500] [G loss: 0.177370] [ema: 0.999724] 
[Epoch 19/39] [Batch 700/1292] [D loss: 0.423282] [G loss: 0.179759] [ema: 0.999726] 
[Epoch 19/39] [Batch 800/1292] [D loss: 0.397425] [G loss: 0.178464] [ema: 0.999727] 
[Epoch 19/39] [Batch 900/1292] [D loss: 0.353600] [G loss: 0.187132] [ema: 0.999728] 
[Epoch 19/39] [Batch 1000/1292] [D loss: 0.402522] [G loss: 0.186984] [ema: 0.999729] 
[Epoch 19/39] [Batch 1100/1292] [D loss: 0.335310] [G loss: 0.181741] [ema: 0.999730] 
[Epoch 19/39] [Batch 1200/1292] [D loss: 0.376688] [G loss: 0.177959] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_30_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_30_2024_10_28_21_01_34/Model



[Epoch 20/39] [Batch 0/1292] [D loss: 0.366566] [G loss: 0.175047] [ema: 0.999732] 
[Epoch 20/39] [Batch 100/1292] [D loss: 0.368768] [G loss: 0.185098] [ema: 0.999733] 
[Epoch 20/39] [Batch 200/1292] [D loss: 0.425610] [G loss: 0.171921] [ema: 0.999734] 
[Epoch 20/39] [Batch 300/1292] [D loss: 0.436632] [G loss: 0.183964] [ema: 0.999735] 
[Epoch 20/39] [Batch 400/1292] [D loss: 0.405391] [G loss: 0.166517] [ema: 0.999736] 
[Epoch 20/39] [Batch 500/1292] [D loss: 0.429810] [G loss: 0.168537] [ema: 0.999737] 
[Epoch 20/39] [Batch 600/1292] [D loss: 0.404494] [G loss: 0.178172] [ema: 0.999738] 
[Epoch 20/39] [Batch 700/1292] [D loss: 0.402783] [G loss: 0.170222] [ema: 0.999739] 
[Epoch 20/39] [Batch 800/1292] [D loss: 0.426460] [G loss: 0.172750] [ema: 0.999740] 
[Epoch 20/39] [Batch 900/1292] [D loss: 0.423101] [G loss: 0.177207] [ema: 0.999741] 
[Epoch 20/39] [Batch 1000/1292] [D loss: 0.410920] [G loss: 0.175844] [ema: 0.999742] 
[Epoch 20/39] [Batch 1100/1292] [D loss: 0.376065] [G loss: 0.173395] [ema: 0.999743] 
[Epoch 20/39] [Batch 1200/1292] [D loss: 0.427008] [G loss: 0.171274] [ema: 0.999744] 
[Epoch 21/39] [Batch 0/1292] [D loss: 0.418612] [G loss: 0.179445] [ema: 0.999745] 
[Epoch 21/39] [Batch 100/1292] [D loss: 0.391522] [G loss: 0.176254] [ema: 0.999745] 
[Epoch 21/39] [Batch 200/1292] [D loss: 0.390392] [G loss: 0.179006] [ema: 0.999746] 
[Epoch 21/39] [Batch 300/1292] [D loss: 0.391489] [G loss: 0.180028] [ema: 0.999747] 
[Epoch 21/39] [Batch 400/1292] [D loss: 0.425806] [G loss: 0.171580] [ema: 0.999748] 
[Epoch 21/39] [Batch 500/1292] [D loss: 0.399156] [G loss: 0.156346] [ema: 0.999749] 
[Epoch 21/39] [Batch 600/1292] [D loss: 0.412587] [G loss: 0.190465] [ema: 0.999750] 
[Epoch 21/39] [Batch 700/1292] [D loss: 0.399266] [G loss: 0.173593] [ema: 0.999751] 
[Epoch 21/39] [Batch 800/1292] [D loss: 0.400222] [G loss: 0.185447] [ema: 0.999752] 
[Epoch 21/39] [Batch 900/1292] [D loss: 0.411188] [G loss: 0.186582] [ema: 0.999753] 
[Epoch 21/39] [Batch 1000/1292] [D loss: 0.418571] [G loss: 0.202175] [ema: 0.999754] 
[Epoch 21/39] [Batch 1100/1292] [D loss: 0.363010] [G loss: 0.187573] [ema: 0.999755] 
[Epoch 21/39] [Batch 1200/1292] [D loss: 0.380344] [G loss: 0.209474] [ema: 0.999755] 
[Epoch 22/39] [Batch 0/1292] [D loss: 0.430867] [G loss: 0.170238] [ema: 0.999756] 
[Epoch 22/39] [Batch 100/1292] [D loss: 0.381258] [G loss: 0.192000] [ema: 0.999757] 
[Epoch 22/39] [Batch 200/1292] [D loss: 0.392215] [G loss: 0.180162] [ema: 0.999758] 
[Epoch 22/39] [Batch 300/1292] [D loss: 0.389760] [G loss: 0.174555] [ema: 0.999759] 
[Epoch 22/39] [Batch 400/1292] [D loss: 0.408390] [G loss: 0.181526] [ema: 0.999760] 
[Epoch 22/39] [Batch 500/1292] [D loss: 0.371402] [G loss: 0.178825] [ema: 0.999760] 
[Epoch 22/39] [Batch 600/1292] [D loss: 0.367429] [G loss: 0.184982] [ema: 0.999761] 
[Epoch 22/39] [Batch 700/1292] [D loss: 0.414153] [G loss: 0.181223] [ema: 0.999762] 
[Epoch 22/39] [Batch 800/1292] [D loss: 0.399597] [G loss: 0.194480] [ema: 0.999763] 
[Epoch 22/39] [Batch 900/1292] [D loss: 0.380128] [G loss: 0.181367] [ema: 0.999764] 
[Epoch 22/39] [Batch 1000/1292] [D loss: 0.409350] [G loss: 0.208732] [ema: 0.999764] 
[Epoch 22/39] [Batch 1100/1292] [D loss: 0.396307] [G loss: 0.178705] [ema: 0.999765] 
[Epoch 22/39] [Batch 1200/1292] [D loss: 0.379797] [G loss: 0.190191] [ema: 0.999766] 
[Epoch 23/39] [Batch 0/1292] [D loss: 0.420897] [G loss: 0.177532] [ema: 0.999767] 
[Epoch 23/39] [Batch 100/1292] [D loss: 0.345043] [G loss: 0.201939] [ema: 0.999768] 
[Epoch 23/39] [Batch 200/1292] [D loss: 0.412063] [G loss: 0.189382] [ema: 0.999768] 
[Epoch 23/39] [Batch 300/1292] [D loss: 0.389389] [G loss: 0.189721] [ema: 0.999769] 
[Epoch 23/39] [Batch 400/1292] [D loss: 0.384740] [G loss: 0.163555] [ema: 0.999770] 
[Epoch 23/39] [Batch 500/1292] [D loss: 0.403389] [G loss: 0.191969] [ema: 0.999771] 
[Epoch 23/39] [Batch 600/1292] [D loss: 0.365030] [G loss: 0.189517] [ema: 0.999771] 
[Epoch 23/39] [Batch 700/1292] [D loss: 0.397525] [G loss: 0.176084] [ema: 0.999772] 
[Epoch 23/39] [Batch 800/1292] [D loss: 0.393682] [G loss: 0.195489] [ema: 0.999773] 
[Epoch 23/39] [Batch 900/1292] [D loss: 0.413729] [G loss: 0.185993] [ema: 0.999774] 
[Epoch 23/39] [Batch 1000/1292] [D loss: 0.354968] [G loss: 0.185486] [ema: 0.999774] 
[Epoch 23/39] [Batch 1100/1292] [D loss: 0.360696] [G loss: 0.204731] [ema: 0.999775] 
[Epoch 23/39] [Batch 1200/1292] [D loss: 0.361175] [G loss: 0.202743] [ema: 0.999776] 
[Epoch 24/39] [Batch 0/1292] [D loss: 0.375395] [G loss: 0.189848] [ema: 0.999776] 
[Epoch 24/39] [Batch 100/1292] [D loss: 0.388963] [G loss: 0.170470] [ema: 0.999777] 
[Epoch 24/39] [Batch 200/1292] [D loss: 0.410656] [G loss: 0.189580] [ema: 0.999778] 
[Epoch 24/39] [Batch 300/1292] [D loss: 0.378532] [G loss: 0.188120] [ema: 0.999779] 
[Epoch 24/39] [Batch 400/1292] [D loss: 0.379458] [G loss: 0.168004] [ema: 0.999779] 
[Epoch 24/39] [Batch 500/1292] [D loss: 0.425706] [G loss: 0.180788] [ema: 0.999780] 
[Epoch 24/39] [Batch 600/1292] [D loss: 0.373811] [G loss: 0.174891] [ema: 0.999781] 
[Epoch 24/39] [Batch 700/1292] [D loss: 0.375539] [G loss: 0.190709] [ema: 0.999781] 
[Epoch 24/39] [Batch 800/1292] [D loss: 0.422982] [G loss: 0.192214] [ema: 0.999782] 
[Epoch 24/39] [Batch 900/1292] [D loss: 0.369270] [G loss: 0.193025] [ema: 0.999783] 
[Epoch 24/39] [Batch 1000/1292] [D loss: 0.396179] [G loss: 0.179967] [ema: 0.999783] 
[Epoch 24/39] [Batch 1100/1292] [D loss: 0.354877] [G loss: 0.175721] [ema: 0.999784] 
[Epoch 24/39] [Batch 1200/1292] [D loss: 0.410225] [G loss: 0.194804] [ema: 0.999785] 
[Epoch 25/39] [Batch 0/1292] [D loss: 0.434459] [G loss: 0.187833] [ema: 0.999785] 
[Epoch 25/39] [Batch 100/1292] [D loss: 0.397781] [G loss: 0.194215] [ema: 0.999786] 
[Epoch 25/39] [Batch 200/1292] [D loss: 0.361182] [G loss: 0.192016] [ema: 0.999787] 
[Epoch 25/39] [Batch 300/1292] [D loss: 0.425479] [G loss: 0.177666] [ema: 0.999787] 
[Epoch 25/39] [Batch 400/1292] [D loss: 0.441657] [G loss: 0.185561] [ema: 0.999788] 
[Epoch 25/39] [Batch 500/1292] [D loss: 0.420945] [G loss: 0.177922] [ema: 0.999789] 
[Epoch 25/39] [Batch 600/1292] [D loss: 0.396734] [G loss: 0.180183] [ema: 0.999789] 
[Epoch 25/39] [Batch 700/1292] [D loss: 0.390952] [G loss: 0.186764] [ema: 0.999790] 
[Epoch 25/39] [Batch 800/1292] [D loss: 0.424549] [G loss: 0.173488] [ema: 0.999791] 
[Epoch 25/39] [Batch 900/1292] [D loss: 0.418828] [G loss: 0.180268] [ema: 0.999791] 
[Epoch 25/39] [Batch 1000/1292] [D loss: 0.350596] [G loss: 0.174297] [ema: 0.999792] 
[Epoch 25/39] [Batch 1100/1292] [D loss: 0.414724] [G loss: 0.190982] [ema: 0.999792] 
[Epoch 25/39] [Batch 1200/1292] [D loss: 0.414488] [G loss: 0.180987] [ema: 0.999793] 
[Epoch 26/39] [Batch 0/1292] [D loss: 0.384509] [G loss: 0.183072] [ema: 0.999794] 
[Epoch 26/39] [Batch 100/1292] [D loss: 0.388347] [G loss: 0.172850] [ema: 0.999794] 
[Epoch 26/39] [Batch 200/1292] [D loss: 0.392939] [G loss: 0.168206] [ema: 0.999795] 
[Epoch 26/39] [Batch 300/1292] [D loss: 0.389739] [G loss: 0.190186] [ema: 0.999796] 
[Epoch 26/39] [Batch 400/1292] [D loss: 0.419242] [G loss: 0.175950] [ema: 0.999796] 
[Epoch 26/39] [Batch 500/1292] [D loss: 0.403872] [G loss: 0.173381] [ema: 0.999797] 
[Epoch 26/39] [Batch 600/1292] [D loss: 0.388743] [G loss: 0.163996] [ema: 0.999797] 
[Epoch 26/39] [Batch 700/1292] [D loss: 0.368385] [G loss: 0.186735] [ema: 0.999798] 
[Epoch 26/39] [Batch 800/1292] [D loss: 0.412985] [G loss: 0.173928] [ema: 0.999798] 
[Epoch 26/39] [Batch 900/1292] [D loss: 0.425327] [G loss: 0.182711] [ema: 0.999799] 
[Epoch 26/39] [Batch 1000/1292] [D loss: 0.406846] [G loss: 0.177984] [ema: 0.999800] 
[Epoch 26/39] [Batch 1100/1292] [D loss: 0.398545] [G loss: 0.170406] [ema: 0.999800] 
[Epoch 26/39] [Batch 1200/1292] [D loss: 0.421339] [G loss: 0.181402] [ema: 0.999801] 
[Epoch 27/39] [Batch 0/1292] [D loss: 0.412512] [G loss: 0.181331] [ema: 0.999801] 
[Epoch 27/39] [Batch 100/1292] [D loss: 0.342509] [G loss: 0.176694] [ema: 0.999802] 
[Epoch 27/39] [Batch 200/1292] [D loss: 0.411751] [G loss: 0.182246] [ema: 0.999802] 
[Epoch 27/39] [Batch 300/1292] [D loss: 0.366340] [G loss: 0.180757] [ema: 0.999803] 
[Epoch 27/39] [Batch 400/1292] [D loss: 0.381382] [G loss: 0.186030] [ema: 0.999804] 
[Epoch 27/39] [Batch 500/1292] [D loss: 0.363725] [G loss: 0.175938] [ema: 0.999804] 
[Epoch 27/39] [Batch 600/1292] [D loss: 0.438942] [G loss: 0.175200] [ema: 0.999805] 
[Epoch 27/39] [Batch 700/1292] [D loss: 0.422979] [G loss: 0.169890] [ema: 0.999805] 
[Epoch 27/39] [Batch 800/1292] [D loss: 0.397847] [G loss: 0.178883] [ema: 0.999806] 
[Epoch 27/39] [Batch 900/1292] [D loss: 0.396573] [G loss: 0.179765] [ema: 0.999806] 
[Epoch 27/39] [Batch 1000/1292] [D loss: 0.409933] [G loss: 0.187084] [ema: 0.999807] 
[Epoch 27/39] [Batch 1100/1292] [D loss: 0.381516] [G loss: 0.187981] [ema: 0.999807] 
[Epoch 27/39] [Batch 1200/1292] [D loss: 0.420727] [G loss: 0.180301] [ema: 0.999808] 
[Epoch 28/39] [Batch 0/1292] [D loss: 0.378359] [G loss: 0.188954] [ema: 0.999808] 
[Epoch 28/39] [Batch 100/1292] [D loss: 0.347853] [G loss: 0.194872] [ema: 0.999809] 
[Epoch 28/39] [Batch 200/1292] [D loss: 0.413244] [G loss: 0.159825] [ema: 0.999809] 
[Epoch 28/39] [Batch 300/1292] [D loss: 0.425541] [G loss: 0.167611] [ema: 0.999810] 
[Epoch 28/39] [Batch 400/1292] [D loss: 0.373162] [G loss: 0.163548] [ema: 0.999811] 
[Epoch 28/39] [Batch 500/1292] [D loss: 0.410621] [G loss: 0.183356] [ema: 0.999811] 
[Epoch 28/39] [Batch 600/1292] [D loss: 0.362751] [G loss: 0.173887] [ema: 0.999812] 
[Epoch 28/39] [Batch 700/1292] [D loss: 0.409860] [G loss: 0.172357] [ema: 0.999812] 
[Epoch 28/39] [Batch 800/1292] [D loss: 0.364513] [G loss: 0.177925] [ema: 0.999813] 
[Epoch 28/39] [Batch 900/1292] [D loss: 0.372593] [G loss: 0.186636] [ema: 0.999813] 
[Epoch 28/39] [Batch 1000/1292] [D loss: 0.414140] [G loss: 0.160296] [ema: 0.999814] 
[Epoch 28/39] [Batch 1100/1292] [D loss: 0.409096] [G loss: 0.176686] [ema: 0.999814] 
[Epoch 28/39] [Batch 1200/1292] [D loss: 0.377503] [G loss: 0.179893] [ema: 0.999815] 
[Epoch 29/39] [Batch 0/1292] [D loss: 0.383405] [G loss: 0.179237] [ema: 0.999815] 
[Epoch 29/39] [Batch 100/1292] [D loss: 0.385092] [G loss: 0.174215] [ema: 0.999816] 
[Epoch 29/39] [Batch 200/1292] [D loss: 0.410993] [G loss: 0.169162] [ema: 0.999816] 
[Epoch 29/39] [Batch 300/1292] [D loss: 0.418275] [G loss: 0.167651] [ema: 0.999816] 
[Epoch 29/39] [Batch 400/1292] [D loss: 0.421765] [G loss: 0.178340] [ema: 0.999817] 
[Epoch 29/39] [Batch 500/1292] [D loss: 0.430141] [G loss: 0.180110] [ema: 0.999817] 
[Epoch 29/39] [Batch 600/1292] [D loss: 0.388760] [G loss: 0.172757] [ema: 0.999818] 
[Epoch 29/39] [Batch 700/1292] [D loss: 0.407237] [G loss: 0.185170] [ema: 0.999818] 
[Epoch 29/39] [Batch 800/1292] [D loss: 0.472253] [G loss: 0.184274] [ema: 0.999819] 
[Epoch 29/39] [Batch 900/1292] [D loss: 0.375424] [G loss: 0.186768] [ema: 0.999819] 
[Epoch 29/39] [Batch 1000/1292] [D loss: 0.416778] [G loss: 0.182282] [ema: 0.999820] 
[Epoch 29/39] [Batch 1100/1292] [D loss: 0.425819] [G loss: 0.175495] [ema: 0.999820] 
[Epoch 29/39] [Batch 1200/1292] [D loss: 0.396051] [G loss: 0.182604] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_30_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_30_2024_10_28_21_01_34/Model



[Epoch 30/39] [Batch 0/1292] [D loss: 0.406727] [G loss: 0.183349] [ema: 0.999821] 
[Epoch 30/39] [Batch 100/1292] [D loss: 0.398590] [G loss: 0.182650] [ema: 0.999822] 
[Epoch 30/39] [Batch 200/1292] [D loss: 0.375610] [G loss: 0.182518] [ema: 0.999822] 
[Epoch 30/39] [Batch 300/1292] [D loss: 0.412586] [G loss: 0.156725] [ema: 0.999823] 
[Epoch 30/39] [Batch 400/1292] [D loss: 0.368585] [G loss: 0.188687] [ema: 0.999823] 
[Epoch 30/39] [Batch 500/1292] [D loss: 0.423292] [G loss: 0.187288] [ema: 0.999823] 
[Epoch 30/39] [Batch 600/1292] [D loss: 0.394868] [G loss: 0.166756] [ema: 0.999824] 
[Epoch 30/39] [Batch 700/1292] [D loss: 0.408210] [G loss: 0.182762] [ema: 0.999824] 
[Epoch 30/39] [Batch 800/1292] [D loss: 0.434942] [G loss: 0.182051] [ema: 0.999825] 
[Epoch 30/39] [Batch 900/1292] [D loss: 0.384143] [G loss: 0.173132] [ema: 0.999825] 
[Epoch 30/39] [Batch 1000/1292] [D loss: 0.373290] [G loss: 0.173369] [ema: 0.999826] 
[Epoch 30/39] [Batch 1100/1292] [D loss: 0.391542] [G loss: 0.184883] [ema: 0.999826] 
[Epoch 30/39] [Batch 1200/1292] [D loss: 0.422494] [G loss: 0.186467] [ema: 0.999827] 
[Epoch 31/39] [Batch 0/1292] [D loss: 0.380866] [G loss: 0.173703] [ema: 0.999827] 
[Epoch 31/39] [Batch 100/1292] [D loss: 0.426739] [G loss: 0.181101] [ema: 0.999827] 
[Epoch 31/39] [Batch 200/1292] [D loss: 0.370745] [G loss: 0.180740] [ema: 0.999828] 
[Epoch 31/39] [Batch 300/1292] [D loss: 0.413565] [G loss: 0.192580] [ema: 0.999828] 
[Epoch 31/39] [Batch 400/1292] [D loss: 0.386949] [G loss: 0.177737] [ema: 0.999829] 
[Epoch 31/39] [Batch 500/1292] [D loss: 0.399487] [G loss: 0.171058] [ema: 0.999829] 
[Epoch 31/39] [Batch 600/1292] [D loss: 0.429749] [G loss: 0.185534] [ema: 0.999830] 
[Epoch 31/39] [Batch 700/1292] [D loss: 0.380832] [G loss: 0.180406] [ema: 0.999830] 
[Epoch 31/39] [Batch 800/1292] [D loss: 0.395848] [G loss: 0.182877] [ema: 0.999830] 
[Epoch 31/39] [Batch 900/1292] [D loss: 0.406490] [G loss: 0.178805] [ema: 0.999831] 
[Epoch 31/39] [Batch 1000/1292] [D loss: 0.374064] [G loss: 0.178802] [ema: 0.999831] 
[Epoch 31/39] [Batch 1100/1292] [D loss: 0.384068] [G loss: 0.179204] [ema: 0.999832] 
[Epoch 31/39] [Batch 1200/1292] [D loss: 0.381008] [G loss: 0.175194] [ema: 0.999832] 
[Epoch 32/39] [Batch 0/1292] [D loss: 0.403957] [G loss: 0.175591] [ema: 0.999832] 
[Epoch 32/39] [Batch 100/1292] [D loss: 0.455505] [G loss: 0.184967] [ema: 0.999833] 
[Epoch 32/39] [Batch 200/1292] [D loss: 0.417250] [G loss: 0.182960] [ema: 0.999833] 
[Epoch 32/39] [Batch 300/1292] [D loss: 0.436717] [G loss: 0.170237] [ema: 0.999834] 
[Epoch 32/39] [Batch 400/1292] [D loss: 0.437702] [G loss: 0.180026] [ema: 0.999834] 
[Epoch 32/39] [Batch 500/1292] [D loss: 0.376010] [G loss: 0.168782] [ema: 0.999834] 
[Epoch 32/39] [Batch 600/1292] [D loss: 0.398010] [G loss: 0.171595] [ema: 0.999835] 
[Epoch 32/39] [Batch 700/1292] [D loss: 0.409093] [G loss: 0.164373] [ema: 0.999835] 
[Epoch 32/39] [Batch 800/1292] [D loss: 0.435737] [G loss: 0.169786] [ema: 0.999836] 
[Epoch 32/39] [Batch 900/1292] [D loss: 0.396896] [G loss: 0.183185] [ema: 0.999836] 
[Epoch 32/39] [Batch 1000/1292] [D loss: 0.448666] [G loss: 0.187672] [ema: 0.999836] 
[Epoch 32/39] [Batch 1100/1292] [D loss: 0.393564] [G loss: 0.181401] [ema: 0.999837] 
[Epoch 32/39] [Batch 1200/1292] [D loss: 0.433845] [G loss: 0.167362] [ema: 0.999837] 
[Epoch 33/39] [Batch 0/1292] [D loss: 0.374376] [G loss: 0.186334] [ema: 0.999837] 
[Epoch 33/39] [Batch 100/1292] [D loss: 0.399424] [G loss: 0.175252] [ema: 0.999838] 
[Epoch 33/39] [Batch 200/1292] [D loss: 0.435169] [G loss: 0.170442] [ema: 0.999838] 
[Epoch 33/39] [Batch 300/1292] [D loss: 0.393194] [G loss: 0.190877] [ema: 0.999839] 
[Epoch 33/39] [Batch 400/1292] [D loss: 0.387134] [G loss: 0.167701] [ema: 0.999839] 
[Epoch 33/39] [Batch 500/1292] [D loss: 0.402776] [G loss: 0.176522] [ema: 0.999839] 
[Epoch 33/39] [Batch 600/1292] [D loss: 0.405740] [G loss: 0.170334] [ema: 0.999840] 
[Epoch 33/39] [Batch 700/1292] [D loss: 0.393462] [G loss: 0.166900] [ema: 0.999840] 
[Epoch 33/39] [Batch 800/1292] [D loss: 0.388412] [G loss: 0.167282] [ema: 0.999840] 
[Epoch 33/39] [Batch 900/1292] [D loss: 0.416151] [G loss: 0.179106] [ema: 0.999841] 
[Epoch 33/39] [Batch 1000/1292] [D loss: 0.414080] [G loss: 0.173002] [ema: 0.999841] 
[Epoch 33/39] [Batch 1100/1292] [D loss: 0.413241] [G loss: 0.170526] [ema: 0.999842] 
[Epoch 33/39] [Batch 1200/1292] [D loss: 0.401155] [G loss: 0.165405] [ema: 0.999842] 
[Epoch 34/39] [Batch 0/1292] [D loss: 0.407874] [G loss: 0.188810] [ema: 0.999842] 
[Epoch 34/39] [Batch 100/1292] [D loss: 0.403218] [G loss: 0.168618] [ema: 0.999843] 
[Epoch 34/39] [Batch 200/1292] [D loss: 0.369738] [G loss: 0.174155] [ema: 0.999843] 
[Epoch 34/39] [Batch 300/1292] [D loss: 0.382550] [G loss: 0.186104] [ema: 0.999843] 
[Epoch 34/39] [Batch 400/1292] [D loss: 0.421279] [G loss: 0.187166] [ema: 0.999844] 
[Epoch 34/39] [Batch 500/1292] [D loss: 0.422140] [G loss: 0.166337] [ema: 0.999844] 
[Epoch 34/39] [Batch 600/1292] [D loss: 0.383002] [G loss: 0.187876] [ema: 0.999844] 
[Epoch 34/39] [Batch 700/1292] [D loss: 0.372868] [G loss: 0.177975] [ema: 0.999845] 
[Epoch 34/39] [Batch 800/1292] [D loss: 0.390561] [G loss: 0.179962] [ema: 0.999845] 
[Epoch 34/39] [Batch 900/1292] [D loss: 0.398021] [G loss: 0.172869] [ema: 0.999845] 
[Epoch 34/39] [Batch 1000/1292] [D loss: 0.397934] [G loss: 0.177753] [ema: 0.999846] 
[Epoch 34/39] [Batch 1100/1292] [D loss: 0.372068] [G loss: 0.185642] [ema: 0.999846] 
[Epoch 34/39] [Batch 1200/1292] [D loss: 0.399806] [G loss: 0.173310] [ema: 0.999846] 
[Epoch 35/39] [Batch 0/1292] [D loss: 0.403245] [G loss: 0.184087] [ema: 0.999847] 
[Epoch 35/39] [Batch 100/1292] [D loss: 0.408755] [G loss: 0.176448] [ema: 0.999847] 
[Epoch 35/39] [Batch 200/1292] [D loss: 0.375247] [G loss: 0.173997] [ema: 0.999847] 
[Epoch 35/39] [Batch 300/1292] [D loss: 0.398545] [G loss: 0.189885] [ema: 0.999848] 
[Epoch 35/39] [Batch 400/1292] [D loss: 0.371077] [G loss: 0.173533] [ema: 0.999848] 
[Epoch 35/39] [Batch 500/1292] [D loss: 0.402962] [G loss: 0.185799] [ema: 0.999848] 
[Epoch 35/39] [Batch 600/1292] [D loss: 0.437440] [G loss: 0.178990] [ema: 0.999849] 
[Epoch 35/39] [Batch 700/1292] [D loss: 0.404596] [G loss: 0.179094] [ema: 0.999849] 
[Epoch 35/39] [Batch 800/1292] [D loss: 0.426476] [G loss: 0.167036] [ema: 0.999849] 
[Epoch 35/39] [Batch 900/1292] [D loss: 0.428425] [G loss: 0.180917] [ema: 0.999850] 
[Epoch 35/39] [Batch 1000/1292] [D loss: 0.340537] [G loss: 0.185569] [ema: 0.999850] 
[Epoch 35/39] [Batch 1100/1292] [D loss: 0.385278] [G loss: 0.164957] [ema: 0.999850] 
[Epoch 35/39] [Batch 1200/1292] [D loss: 0.352772] [G loss: 0.179274] [ema: 0.999851] 
[Epoch 36/39] [Batch 0/1292] [D loss: 0.378023] [G loss: 0.171558] [ema: 0.999851] 
[Epoch 36/39] [Batch 100/1292] [D loss: 0.439719] [G loss: 0.178997] [ema: 0.999851] 
[Epoch 36/39] [Batch 200/1292] [D loss: 0.399571] [G loss: 0.188659] [ema: 0.999852] 
[Epoch 36/39] [Batch 300/1292] [D loss: 0.386735] [G loss: 0.171796] [ema: 0.999852] 
[Epoch 36/39] [Batch 400/1292] [D loss: 0.461691] [G loss: 0.197199] [ema: 0.999852] 
[Epoch 36/39] [Batch 500/1292] [D loss: 0.421548] [G loss: 0.169921] [ema: 0.999853] 
[Epoch 36/39] [Batch 600/1292] [D loss: 0.379812] [G loss: 0.180996] [ema: 0.999853] 
[Epoch 36/39] [Batch 700/1292] [D loss: 0.392584] [G loss: 0.171706] [ema: 0.999853] 
[Epoch 36/39] [Batch 800/1292] [D loss: 0.399393] [G loss: 0.174286] [ema: 0.999854] 
[Epoch 36/39] [Batch 900/1292] [D loss: 0.405927] [G loss: 0.169741] [ema: 0.999854] 
[Epoch 36/39] [Batch 1000/1292] [D loss: 0.386396] [G loss: 0.189324] [ema: 0.999854] 
[Epoch 36/39] [Batch 1100/1292] [D loss: 0.363795] [G loss: 0.176842] [ema: 0.999854] 
[Epoch 36/39] [Batch 1200/1292] [D loss: 0.419108] [G loss: 0.172095] [ema: 0.999855] 
[Epoch 37/39] [Batch 0/1292] [D loss: 0.405046] [G loss: 0.180849] [ema: 0.999855] 
[Epoch 37/39] [Batch 100/1292] [D loss: 0.372589] [G loss: 0.171411] [ema: 0.999855] 
[Epoch 37/39] [Batch 200/1292] [D loss: 0.412542] [G loss: 0.181125] [ema: 0.999856] 
[Epoch 37/39] [Batch 300/1292] [D loss: 0.401478] [G loss: 0.176979] [ema: 0.999856] 
[Epoch 37/39] [Batch 400/1292] [D loss: 0.407869] [G loss: 0.155195] [ema: 0.999856] 
[Epoch 37/39] [Batch 500/1292] [D loss: 0.363820] [G loss: 0.178270] [ema: 0.999857] 
[Epoch 37/39] [Batch 600/1292] [D loss: 0.426443] [G loss: 0.181788] [ema: 0.999857] 
[Epoch 37/39] [Batch 700/1292] [D loss: 0.387968] [G loss: 0.173685] [ema: 0.999857] 
[Epoch 37/39] [Batch 800/1292] [D loss: 0.447576] [G loss: 0.178985] [ema: 0.999857] 
[Epoch 37/39] [Batch 900/1292] [D loss: 0.371088] [G loss: 0.183883] [ema: 0.999858] 
[Epoch 37/39] [Batch 1000/1292] [D loss: 0.394936] [G loss: 0.174308] [ema: 0.999858] 
[Epoch 37/39] [Batch 1100/1292] [D loss: 0.423598] [G loss: 0.186361] [ema: 0.999858] 
[Epoch 37/39] [Batch 1200/1292] [D loss: 0.388415] [G loss: 0.188419] [ema: 0.999859] 
[Epoch 38/39] [Batch 0/1292] [D loss: 0.392127] [G loss: 0.161800] [ema: 0.999859] 
[Epoch 38/39] [Batch 100/1292] [D loss: 0.367561] [G loss: 0.185870] [ema: 0.999859] 
[Epoch 38/39] [Batch 200/1292] [D loss: 0.400830] [G loss: 0.183418] [ema: 0.999859] 
[Epoch 38/39] [Batch 300/1292] [D loss: 0.397031] [G loss: 0.182932] [ema: 0.999860] 
[Epoch 38/39] [Batch 400/1292] [D loss: 0.394540] [G loss: 0.184386] [ema: 0.999860] 
[Epoch 38/39] [Batch 500/1292] [D loss: 0.388620] [G loss: 0.189852] [ema: 0.999860] 
[Epoch 38/39] [Batch 600/1292] [D loss: 0.409350] [G loss: 0.184728] [ema: 0.999861] 
[Epoch 38/39] [Batch 700/1292] [D loss: 0.402115] [G loss: 0.183095] [ema: 0.999861] 
[Epoch 38/39] [Batch 800/1292] [D loss: 0.403877] [G loss: 0.187516] [ema: 0.999861] 
[Epoch 38/39] [Batch 900/1292] [D loss: 0.403866] [G loss: 0.187181] [ema: 0.999861] 
[Epoch 38/39] [Batch 1000/1292] [D loss: 0.386272] [G loss: 0.187207] [ema: 0.999862] 
[Epoch 38/39] [Batch 1100/1292] [D loss: 0.402918] [G loss: 0.194617] [ema: 0.999862] 
[Epoch 38/39] [Batch 1200/1292] [D loss: 0.411170] [G loss: 0.182948] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
KuHar_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
KuHar_DAGHAR_Multiclass
daghar
return single class data and labels, class is KuHar_DAGHAR_Multiclass
data shape is (2784, 6, 1, 30)
label shape is (2784,)
174
Epochs between checkpoint: 72



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_30_100/KuHar_DAGHAR_Multiclass_50000_D_30_2024_10_28_21_34_11/Model



[Epoch 0/288] [Batch 0/174] [D loss: 4.414276] [G loss: 2.366574] [ema: 0.000000] 
[Epoch 0/288] [Batch 100/174] [D loss: 0.343548] [G loss: 0.294645] [ema: 0.933033] 
[Epoch 1/288] [Batch 0/174] [D loss: 0.557173] [G loss: 0.219802] [ema: 0.960947] 
[Epoch 1/288] [Batch 100/174] [D loss: 0.484703] [G loss: 0.174065] [ema: 0.975020] 
[Epoch 2/288] [Batch 0/174] [D loss: 0.586087] [G loss: 0.177910] [ema: 0.980279] 
[Epoch 2/288] [Batch 100/174] [D loss: 0.434618] [G loss: 0.161206] [ema: 0.984647] 
[Epoch 3/288] [Batch 0/174] [D loss: 0.371531] [G loss: 0.192312] [ema: 0.986809] 
[Epoch 3/288] [Batch 100/174] [D loss: 0.463544] [G loss: 0.152521] [ema: 0.988918] 
[Epoch 4/288] [Batch 0/174] [D loss: 0.433234] [G loss: 0.164984] [ema: 0.990090] 
[Epoch 4/288] [Batch 100/174] [D loss: 0.420874] [G loss: 0.186080] [ema: 0.991330] 
[Epoch 5/288] [Batch 0/174] [D loss: 0.404542] [G loss: 0.144401] [ema: 0.992064] 
[Epoch 5/288] [Batch 100/174] [D loss: 0.434129] [G loss: 0.163703] [ema: 0.992880] 
[Epoch 6/288] [Batch 0/174] [D loss: 0.446924] [G loss: 0.173517] [ema: 0.993383] 
[Epoch 6/288] [Batch 100/174] [D loss: 0.420582] [G loss: 0.186056] [ema: 0.993959] 
[Epoch 7/288] [Batch 0/174] [D loss: 0.386690] [G loss: 0.193068] [ema: 0.994325] 
[Epoch 7/288] [Batch 100/174] [D loss: 0.425084] [G loss: 0.173457] [ema: 0.994755] 
[Epoch 8/288] [Batch 0/174] [D loss: 0.406431] [G loss: 0.162503] [ema: 0.995033] 
[Epoch 8/288] [Batch 100/174] [D loss: 0.381330] [G loss: 0.184272] [ema: 0.995365] 
[Epoch 9/288] [Batch 0/174] [D loss: 0.401063] [G loss: 0.168642] [ema: 0.995584] 
[Epoch 9/288] [Batch 100/174] [D loss: 0.386002] [G loss: 0.185522] [ema: 0.995848] 
[Epoch 10/288] [Batch 0/174] [D loss: 0.446499] [G loss: 0.171825] [ema: 0.996024] 
[Epoch 10/288] [Batch 100/174] [D loss: 0.427542] [G loss: 0.173159] [ema: 0.996240] 
[Epoch 11/288] [Batch 0/174] [D loss: 0.427110] [G loss: 0.155023] [ema: 0.996385] 
[Epoch 11/288] [Batch 100/174] [D loss: 0.448959] [G loss: 0.189120] [ema: 0.996564] 
[Epoch 12/288] [Batch 0/174] [D loss: 0.425983] [G loss: 0.181178] [ema: 0.996686] 
[Epoch 12/288] [Batch 100/174] [D loss: 0.409780] [G loss: 0.167903] [ema: 0.996837] 
[Epoch 13/288] [Batch 0/174] [D loss: 0.483708] [G loss: 0.188818] [ema: 0.996940] 
[Epoch 13/288] [Batch 100/174] [D loss: 0.367852] [G loss: 0.167243] [ema: 0.997070] 
[Epoch 14/288] [Batch 0/174] [D loss: 0.383247] [G loss: 0.186277] [ema: 0.997159] 
[Epoch 14/288] [Batch 100/174] [D loss: 0.349173] [G loss: 0.205431] [ema: 0.997271] 
[Epoch 15/288] [Batch 0/174] [D loss: 0.392880] [G loss: 0.203504] [ema: 0.997348] 
[Epoch 15/288] [Batch 100/174] [D loss: 0.386956] [G loss: 0.227070] [ema: 0.997446] 
[Epoch 16/288] [Batch 0/174] [D loss: 0.360293] [G loss: 0.210923] [ema: 0.997513] 
[Epoch 16/288] [Batch 100/174] [D loss: 0.341902] [G loss: 0.205717] [ema: 0.997599] 
[Epoch 17/288] [Batch 0/174] [D loss: 0.374949] [G loss: 0.241557] [ema: 0.997659] 
[Epoch 17/288] [Batch 100/174] [D loss: 0.394894] [G loss: 0.204708] [ema: 0.997736] 
[Epoch 18/288] [Batch 0/174] [D loss: 0.398377] [G loss: 0.164264] [ema: 0.997789] 
[Epoch 18/288] [Batch 100/174] [D loss: 0.370096] [G loss: 0.182751] [ema: 0.997858] 
[Epoch 19/288] [Batch 0/174] [D loss: 0.369711] [G loss: 0.176326] [ema: 0.997906] 
[Epoch 19/288] [Batch 100/174] [D loss: 0.391311] [G loss: 0.152485] [ema: 0.997967] 
[Epoch 20/288] [Batch 0/174] [D loss: 0.363739] [G loss: 0.166510] [ema: 0.998010] 
[Epoch 20/288] [Batch 100/174] [D loss: 0.368105] [G loss: 0.170439] [ema: 0.998066] 
[Epoch 21/288] [Batch 0/174] [D loss: 0.380927] [G loss: 0.182818] [ema: 0.998105] 
[Epoch 21/288] [Batch 100/174] [D loss: 0.363999] [G loss: 0.165303] [ema: 0.998155] 
[Epoch 22/288] [Batch 0/174] [D loss: 0.403374] [G loss: 0.206817] [ema: 0.998191] 
[Epoch 22/288] [Batch 100/174] [D loss: 0.363494] [G loss: 0.172163] [ema: 0.998237] 
[Epoch 23/288] [Batch 0/174] [D loss: 0.368541] [G loss: 0.202713] [ema: 0.998269] 
[Epoch 23/288] [Batch 100/174] [D loss: 0.446661] [G loss: 0.180149] [ema: 0.998312] 
[Epoch 24/288] [Batch 0/174] [D loss: 0.401841] [G loss: 0.199418] [ema: 0.998342] 
[Epoch 24/288] [Batch 100/174] [D loss: 0.364170] [G loss: 0.192768] [ema: 0.998380] 
[Epoch 25/288] [Batch 0/174] [D loss: 0.373819] [G loss: 0.186365] [ema: 0.998408] 
[Epoch 25/288] [Batch 100/174] [D loss: 0.364699] [G loss: 0.170944] [ema: 0.998444] 
[Epoch 26/288] [Batch 0/174] [D loss: 0.391485] [G loss: 0.212191] [ema: 0.998469] 
[Epoch 26/288] [Batch 100/174] [D loss: 0.404143] [G loss: 0.188995] [ema: 0.998502] 
[Epoch 27/288] [Batch 0/174] [D loss: 0.386013] [G loss: 0.182706] [ema: 0.998526] 
[Epoch 27/288] [Batch 100/174] [D loss: 0.365692] [G loss: 0.188312] [ema: 0.998556] 
[Epoch 28/288] [Batch 0/174] [D loss: 0.390197] [G loss: 0.183265] [ema: 0.998578] 
[Epoch 28/288] [Batch 100/174] [D loss: 0.376685] [G loss: 0.202589] [ema: 0.998607] 
[Epoch 29/288] [Batch 0/174] [D loss: 0.379879] [G loss: 0.186179] [ema: 0.998627] 
[Epoch 29/288] [Batch 100/174] [D loss: 0.377425] [G loss: 0.187639] [ema: 0.998654] 
[Epoch 30/288] [Batch 0/174] [D loss: 0.405686] [G loss: 0.184900] [ema: 0.998673] 
[Epoch 30/288] [Batch 100/174] [D loss: 0.363364] [G loss: 0.188813] [ema: 0.998698] 
[Epoch 31/288] [Batch 0/174] [D loss: 0.386529] [G loss: 0.193135] [ema: 0.998716] 
[Epoch 31/288] [Batch 100/174] [D loss: 0.401329] [G loss: 0.177178] [ema: 0.998739] 
[Epoch 32/288] [Batch 0/174] [D loss: 0.375497] [G loss: 0.213599] [ema: 0.998756] 
[Epoch 32/288] [Batch 100/174] [D loss: 0.373635] [G loss: 0.192671] [ema: 0.998778] 
[Epoch 33/288] [Batch 0/174] [D loss: 0.395963] [G loss: 0.191183] [ema: 0.998794] 
[Epoch 33/288] [Batch 100/174] [D loss: 0.386774] [G loss: 0.196224] [ema: 0.998814] 
[Epoch 34/288] [Batch 0/174] [D loss: 0.372210] [G loss: 0.188169] [ema: 0.998829] 
[Epoch 34/288] [Batch 100/174] [D loss: 0.363661] [G loss: 0.192887] [ema: 0.998848] 
[Epoch 35/288] [Batch 0/174] [D loss: 0.394928] [G loss: 0.173009] [ema: 0.998862] 
[Epoch 35/288] [Batch 100/174] [D loss: 0.392772] [G loss: 0.172755] [ema: 0.998881] 
[Epoch 36/288] [Batch 0/174] [D loss: 0.366751] [G loss: 0.217873] [ema: 0.998894] 
[Epoch 36/288] [Batch 100/174] [D loss: 0.414944] [G loss: 0.210212] [ema: 0.998911] 
[Epoch 37/288] [Batch 0/174] [D loss: 0.410178] [G loss: 0.184431] [ema: 0.998924] 
[Epoch 37/288] [Batch 100/174] [D loss: 0.389244] [G loss: 0.184093] [ema: 0.998940] 
[Epoch 38/288] [Batch 0/174] [D loss: 0.387196] [G loss: 0.182691] [ema: 0.998952] 
[Epoch 38/288] [Batch 100/174] [D loss: 0.423519] [G loss: 0.170988] [ema: 0.998968] 
[Epoch 39/288] [Batch 0/174] [D loss: 0.374147] [G loss: 0.191327] [ema: 0.998979] 
[Epoch 39/288] [Batch 100/174] [D loss: 0.395684] [G loss: 0.176821] [ema: 0.998994] 
[Epoch 40/288] [Batch 0/174] [D loss: 0.346352] [G loss: 0.206353] [ema: 0.999005] 
[Epoch 40/288] [Batch 100/174] [D loss: 0.346510] [G loss: 0.175330] [ema: 0.999019] 
[Epoch 41/288] [Batch 0/174] [D loss: 0.426278] [G loss: 0.188971] [ema: 0.999029] 
[Epoch 41/288] [Batch 100/174] [D loss: 0.425424] [G loss: 0.175372] [ema: 0.999042] 
[Epoch 42/288] [Batch 0/174] [D loss: 0.417010] [G loss: 0.184040] [ema: 0.999052] 
[Epoch 42/288] [Batch 100/174] [D loss: 0.403237] [G loss: 0.189130] [ema: 0.999065] 
[Epoch 43/288] [Batch 0/174] [D loss: 0.383801] [G loss: 0.190176] [ema: 0.999074] 
[Epoch 43/288] [Batch 100/174] [D loss: 0.393689] [G loss: 0.179533] [ema: 0.999086] 
[Epoch 44/288] [Batch 0/174] [D loss: 0.367083] [G loss: 0.188989] [ema: 0.999095] 
[Epoch 44/288] [Batch 100/174] [D loss: 0.404783] [G loss: 0.191494] [ema: 0.999107] 
[Epoch 45/288] [Batch 0/174] [D loss: 0.343215] [G loss: 0.190960] [ema: 0.999115] 
[Epoch 45/288] [Batch 100/174] [D loss: 0.378971] [G loss: 0.195024] [ema: 0.999126] 
[Epoch 46/288] [Batch 0/174] [D loss: 0.358524] [G loss: 0.196467] [ema: 0.999134] 
[Epoch 46/288] [Batch 100/174] [D loss: 0.403887] [G loss: 0.176472] [ema: 0.999145] 
[Epoch 47/288] [Batch 0/174] [D loss: 0.345730] [G loss: 0.190915] [ema: 0.999153] 
[Epoch 47/288] [Batch 100/174] [D loss: 0.371515] [G loss: 0.191004] [ema: 0.999163] 
[Epoch 48/288] [Batch 0/174] [D loss: 0.425009] [G loss: 0.192369] [ema: 0.999170] 
[Epoch 48/288] [Batch 100/174] [D loss: 0.404612] [G loss: 0.192914] [ema: 0.999180] 
[Epoch 49/288] [Batch 0/174] [D loss: 0.382325] [G loss: 0.195255] [ema: 0.999187] 
[Epoch 49/288] [Batch 100/174] [D loss: 0.371491] [G loss: 0.183261] [ema: 0.999197] 
[Epoch 50/288] [Batch 0/174] [D loss: 0.425622] [G loss: 0.184886] [ema: 0.999204] 
[Epoch 50/288] [Batch 100/174] [D loss: 0.394052] [G loss: 0.194499] [ema: 0.999213] 
[Epoch 51/288] [Batch 0/174] [D loss: 0.424930] [G loss: 0.190347] [ema: 0.999219] 
[Epoch 51/288] [Batch 100/174] [D loss: 0.401418] [G loss: 0.175564] [ema: 0.999228] 
[Epoch 52/288] [Batch 0/174] [D loss: 0.389842] [G loss: 0.193840] [ema: 0.999234] 
[Epoch 52/288] [Batch 100/174] [D loss: 0.345976] [G loss: 0.184657] [ema: 0.999243] 
[Epoch 53/288] [Batch 0/174] [D loss: 0.402661] [G loss: 0.185417] [ema: 0.999249] 
[Epoch 53/288] [Batch 100/174] [D loss: 0.399811] [G loss: 0.160872] [ema: 0.999257] 
[Epoch 54/288] [Batch 0/174] [D loss: 0.351221] [G loss: 0.179391] [ema: 0.999263] 
[Epoch 54/288] [Batch 100/174] [D loss: 0.435746] [G loss: 0.199077] [ema: 0.999270] 
[Epoch 55/288] [Batch 0/174] [D loss: 0.396887] [G loss: 0.175149] [ema: 0.999276] 
[Epoch 55/288] [Batch 100/174] [D loss: 0.391742] [G loss: 0.183764] [ema: 0.999283] 
[Epoch 56/288] [Batch 0/174] [D loss: 0.433618] [G loss: 0.178271] [ema: 0.999289] 
[Epoch 56/288] [Batch 100/174] [D loss: 0.406013] [G loss: 0.180739] [ema: 0.999296] 
[Epoch 57/288] [Batch 0/174] [D loss: 0.399309] [G loss: 0.184757] [ema: 0.999301] 
[Epoch 57/288] [Batch 100/174] [D loss: 0.425984] [G loss: 0.182700] [ema: 0.999308] 
[Epoch 58/288] [Batch 0/174] [D loss: 0.397329] [G loss: 0.208972] [ema: 0.999313] 
[Epoch 58/288] [Batch 100/174] [D loss: 0.366247] [G loss: 0.186772] [ema: 0.999320] 
[Epoch 59/288] [Batch 0/174] [D loss: 0.396829] [G loss: 0.182362] [ema: 0.999325] 
[Epoch 59/288] [Batch 100/174] [D loss: 0.412892] [G loss: 0.183986] [ema: 0.999332] 
[Epoch 60/288] [Batch 0/174] [D loss: 0.395833] [G loss: 0.178475] [ema: 0.999336] 
[Epoch 60/288] [Batch 100/174] [D loss: 0.356364] [G loss: 0.192215] [ema: 0.999343] 
[Epoch 61/288] [Batch 0/174] [D loss: 0.363552] [G loss: 0.182011] [ema: 0.999347] 
[Epoch 61/288] [Batch 100/174] [D loss: 0.434872] [G loss: 0.174708] [ema: 0.999353] 
[Epoch 62/288] [Batch 0/174] [D loss: 0.386145] [G loss: 0.188441] [ema: 0.999358] 
[Epoch 62/288] [Batch 100/174] [D loss: 0.382721] [G loss: 0.194217] [ema: 0.999364] 
[Epoch 63/288] [Batch 0/174] [D loss: 0.404104] [G loss: 0.192310] [ema: 0.999368] 
[Epoch 63/288] [Batch 100/174] [D loss: 0.373255] [G loss: 0.178784] [ema: 0.999374] 
[Epoch 64/288] [Batch 0/174] [D loss: 0.369264] [G loss: 0.190772] [ema: 0.999378] 
[Epoch 64/288] [Batch 100/174] [D loss: 0.401068] [G loss: 0.176021] [ema: 0.999383] 
[Epoch 65/288] [Batch 0/174] [D loss: 0.411088] [G loss: 0.189586] [ema: 0.999387] 
[Epoch 65/288] [Batch 100/174] [D loss: 0.391779] [G loss: 0.182910] [ema: 0.999393] 
[Epoch 66/288] [Batch 0/174] [D loss: 0.385123] [G loss: 0.191870] [ema: 0.999397] 
[Epoch 66/288] [Batch 100/174] [D loss: 0.384569] [G loss: 0.178358] [ema: 0.999402] 
[Epoch 67/288] [Batch 0/174] [D loss: 0.420141] [G loss: 0.180536] [ema: 0.999406] 
[Epoch 67/288] [Batch 100/174] [D loss: 0.405760] [G loss: 0.194814] [ema: 0.999411] 
[Epoch 68/288] [Batch 0/174] [D loss: 0.393770] [G loss: 0.186989] [ema: 0.999414] 
[Epoch 68/288] [Batch 100/174] [D loss: 0.395501] [G loss: 0.174501] [ema: 0.999419] 
[Epoch 69/288] [Batch 0/174] [D loss: 0.409517] [G loss: 0.191209] [ema: 0.999423] 
[Epoch 69/288] [Batch 100/174] [D loss: 0.345896] [G loss: 0.220909] [ema: 0.999428] 
[Epoch 70/288] [Batch 0/174] [D loss: 0.392813] [G loss: 0.183532] [ema: 0.999431] 
[Epoch 70/288] [Batch 100/174] [D loss: 0.362448] [G loss: 0.190684] [ema: 0.999436] 
[Epoch 71/288] [Batch 0/174] [D loss: 0.363159] [G loss: 0.181874] [ema: 0.999439] 
[Epoch 71/288] [Batch 100/174] [D loss: 0.373541] [G loss: 0.177240] [ema: 0.999444] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_30_100/KuHar_DAGHAR_Multiclass_50000_D_30_2024_10_28_21_34_11/Model



[Epoch 72/288] [Batch 0/174] [D loss: 0.384930] [G loss: 0.190051] [ema: 0.999447] 
[Epoch 72/288] [Batch 100/174] [D loss: 0.413627] [G loss: 0.180713] [ema: 0.999451] 
[Epoch 73/288] [Batch 0/174] [D loss: 0.410474] [G loss: 0.189624] [ema: 0.999454] 
[Epoch 73/288] [Batch 100/174] [D loss: 0.409719] [G loss: 0.179606] [ema: 0.999459] 
[Epoch 74/288] [Batch 0/174] [D loss: 0.355603] [G loss: 0.198004] [ema: 0.999462] 
[Epoch 74/288] [Batch 100/174] [D loss: 0.387315] [G loss: 0.181926] [ema: 0.999466] 
[Epoch 75/288] [Batch 0/174] [D loss: 0.383784] [G loss: 0.187843] [ema: 0.999469] 
[Epoch 75/288] [Batch 100/174] [D loss: 0.388210] [G loss: 0.189982] [ema: 0.999473] 
[Epoch 76/288] [Batch 0/174] [D loss: 0.378777] [G loss: 0.204449] [ema: 0.999476] 
[Epoch 76/288] [Batch 100/174] [D loss: 0.412759] [G loss: 0.159651] [ema: 0.999480] 
[Epoch 77/288] [Batch 0/174] [D loss: 0.432044] [G loss: 0.180249] [ema: 0.999483] 
[Epoch 77/288] [Batch 100/174] [D loss: 0.380086] [G loss: 0.179197] [ema: 0.999487] 
[Epoch 78/288] [Batch 0/174] [D loss: 0.383932] [G loss: 0.174931] [ema: 0.999489] 
[Epoch 78/288] [Batch 100/174] [D loss: 0.407965] [G loss: 0.177245] [ema: 0.999493] 
[Epoch 79/288] [Batch 0/174] [D loss: 0.378153] [G loss: 0.188024] [ema: 0.999496] 
[Epoch 79/288] [Batch 100/174] [D loss: 0.349469] [G loss: 0.194189] [ema: 0.999500] 
[Epoch 80/288] [Batch 0/174] [D loss: 0.385563] [G loss: 0.180399] [ema: 0.999502] 
[Epoch 80/288] [Batch 100/174] [D loss: 0.376547] [G loss: 0.186015] [ema: 0.999506] 
[Epoch 81/288] [Batch 0/174] [D loss: 0.389964] [G loss: 0.186065] [ema: 0.999508] 
[Epoch 81/288] [Batch 100/174] [D loss: 0.363203] [G loss: 0.182288] [ema: 0.999512] 
[Epoch 82/288] [Batch 0/174] [D loss: 0.376230] [G loss: 0.201080] [ema: 0.999514] 
[Epoch 82/288] [Batch 100/174] [D loss: 0.367268] [G loss: 0.184253] [ema: 0.999518] 
[Epoch 83/288] [Batch 0/174] [D loss: 0.378378] [G loss: 0.192963] [ema: 0.999520] 
[Epoch 83/288] [Batch 100/174] [D loss: 0.380574] [G loss: 0.179484] [ema: 0.999523] 
[Epoch 84/288] [Batch 0/174] [D loss: 0.393027] [G loss: 0.184428] [ema: 0.999526] 
[Epoch 84/288] [Batch 100/174] [D loss: 0.420341] [G loss: 0.180588] [ema: 0.999529] 
[Epoch 85/288] [Batch 0/174] [D loss: 0.353160] [G loss: 0.192237] [ema: 0.999531] 
[Epoch 85/288] [Batch 100/174] [D loss: 0.426104] [G loss: 0.191296] [ema: 0.999535] 
[Epoch 86/288] [Batch 0/174] [D loss: 0.393104] [G loss: 0.184120] [ema: 0.999537] 
[Epoch 86/288] [Batch 100/174] [D loss: 0.427818] [G loss: 0.173402] [ema: 0.999540] 
[Epoch 87/288] [Batch 0/174] [D loss: 0.368651] [G loss: 0.187286] [ema: 0.999542] 
[Epoch 87/288] [Batch 100/174] [D loss: 0.358136] [G loss: 0.176932] [ema: 0.999545] 
[Epoch 88/288] [Batch 0/174] [D loss: 0.394754] [G loss: 0.192321] [ema: 0.999547] 
[Epoch 88/288] [Batch 100/174] [D loss: 0.381348] [G loss: 0.175604] [ema: 0.999550] 
[Epoch 89/288] [Batch 0/174] [D loss: 0.393920] [G loss: 0.195304] [ema: 0.999553] 
[Epoch 89/288] [Batch 100/174] [D loss: 0.372953] [G loss: 0.198772] [ema: 0.999555] 
[Epoch 90/288] [Batch 0/174] [D loss: 0.397731] [G loss: 0.165061] [ema: 0.999557] 
[Epoch 90/288] [Batch 100/174] [D loss: 0.375910] [G loss: 0.185691] [ema: 0.999560] 
[Epoch 91/288] [Batch 0/174] [D loss: 0.374717] [G loss: 0.201907] [ema: 0.999562] 
[Epoch 91/288] [Batch 100/174] [D loss: 0.439141] [G loss: 0.178400] [ema: 0.999565] 
[Epoch 92/288] [Batch 0/174] [D loss: 0.388217] [G loss: 0.181877] [ema: 0.999567] 
[Epoch 92/288] [Batch 100/174] [D loss: 0.362145] [G loss: 0.173698] [ema: 0.999570] 
[Epoch 93/288] [Batch 0/174] [D loss: 0.400638] [G loss: 0.210514] [ema: 0.999572] 
[Epoch 93/288] [Batch 100/174] [D loss: 0.400954] [G loss: 0.186445] [ema: 0.999574] 
[Epoch 94/288] [Batch 0/174] [D loss: 0.411980] [G loss: 0.182486] [ema: 0.999576] 
[Epoch 94/288] [Batch 100/174] [D loss: 0.413670] [G loss: 0.177898] [ema: 0.999579] 
[Epoch 95/288] [Batch 0/174] [D loss: 0.409655] [G loss: 0.174746] [ema: 0.999581] 
[Epoch 95/288] [Batch 100/174] [D loss: 0.402897] [G loss: 0.169651] [ema: 0.999583] 
[Epoch 96/288] [Batch 0/174] [D loss: 0.388267] [G loss: 0.182132] [ema: 0.999585] 
[Epoch 96/288] [Batch 100/174] [D loss: 0.368707] [G loss: 0.187897] [ema: 0.999588] 
[Epoch 97/288] [Batch 0/174] [D loss: 0.414047] [G loss: 0.185281] [ema: 0.999589] 
[Epoch 97/288] [Batch 100/174] [D loss: 0.368041] [G loss: 0.194142] [ema: 0.999592] 
[Epoch 98/288] [Batch 0/174] [D loss: 0.394994] [G loss: 0.192749] [ema: 0.999594] 
[Epoch 98/288] [Batch 100/174] [D loss: 0.436894] [G loss: 0.192880] [ema: 0.999596] 
[Epoch 99/288] [Batch 0/174] [D loss: 0.375102] [G loss: 0.181559] [ema: 0.999598] 
[Epoch 99/288] [Batch 100/174] [D loss: 0.347960] [G loss: 0.182340] [ema: 0.999600] 
[Epoch 100/288] [Batch 0/174] [D loss: 0.384576] [G loss: 0.181841] [ema: 0.999602] 
[Epoch 100/288] [Batch 100/174] [D loss: 0.374193] [G loss: 0.178956] [ema: 0.999604] 
[Epoch 101/288] [Batch 0/174] [D loss: 0.413300] [G loss: 0.186401] [ema: 0.999606] 
[Epoch 101/288] [Batch 100/174] [D loss: 0.387341] [G loss: 0.177855] [ema: 0.999608] 
[Epoch 102/288] [Batch 0/174] [D loss: 0.393296] [G loss: 0.191320] [ema: 0.999610] 
[Epoch 102/288] [Batch 100/174] [D loss: 0.423756] [G loss: 0.175021] [ema: 0.999612] 
[Epoch 103/288] [Batch 0/174] [D loss: 0.343321] [G loss: 0.217847] [ema: 0.999613] 
[Epoch 103/288] [Batch 100/174] [D loss: 0.412289] [G loss: 0.170865] [ema: 0.999615] 
[Epoch 104/288] [Batch 0/174] [D loss: 0.408373] [G loss: 0.190615] [ema: 0.999617] 
[Epoch 104/288] [Batch 100/174] [D loss: 0.399663] [G loss: 0.181592] [ema: 0.999619] 
[Epoch 105/288] [Batch 0/174] [D loss: 0.337467] [G loss: 0.197618] [ema: 0.999621] 
[Epoch 105/288] [Batch 100/174] [D loss: 0.364593] [G loss: 0.193611] [ema: 0.999623] 
[Epoch 106/288] [Batch 0/174] [D loss: 0.384213] [G loss: 0.187819] [ema: 0.999624] 
[Epoch 106/288] [Batch 100/174] [D loss: 0.376797] [G loss: 0.185754] [ema: 0.999626] 
[Epoch 107/288] [Batch 0/174] [D loss: 0.401916] [G loss: 0.193440] [ema: 0.999628] 
[Epoch 107/288] [Batch 100/174] [D loss: 0.376280] [G loss: 0.185968] [ema: 0.999630] 
[Epoch 108/288] [Batch 0/174] [D loss: 0.375282] [G loss: 0.194359] [ema: 0.999631] 
[Epoch 108/288] [Batch 100/174] [D loss: 0.430319] [G loss: 0.168203] [ema: 0.999633] 
[Epoch 109/288] [Batch 0/174] [D loss: 0.392710] [G loss: 0.179141] [ema: 0.999635] 
[Epoch 109/288] [Batch 100/174] [D loss: 0.363640] [G loss: 0.183363] [ema: 0.999637] 
[Epoch 110/288] [Batch 0/174] [D loss: 0.366238] [G loss: 0.190073] [ema: 0.999638] 
[Epoch 110/288] [Batch 100/174] [D loss: 0.379860] [G loss: 0.190075] [ema: 0.999640] 
[Epoch 111/288] [Batch 0/174] [D loss: 0.392878] [G loss: 0.189067] [ema: 0.999641] 
[Epoch 111/288] [Batch 100/174] [D loss: 0.372930] [G loss: 0.186346] [ema: 0.999643] 
[Epoch 112/288] [Batch 0/174] [D loss: 0.392565] [G loss: 0.193568] [ema: 0.999644] 
[Epoch 112/288] [Batch 100/174] [D loss: 0.416340] [G loss: 0.176177] [ema: 0.999646] 
[Epoch 113/288] [Batch 0/174] [D loss: 0.394791] [G loss: 0.182741] [ema: 0.999648] 
[Epoch 113/288] [Batch 100/174] [D loss: 0.369195] [G loss: 0.188192] [ema: 0.999649] 
[Epoch 114/288] [Batch 0/174] [D loss: 0.395200] [G loss: 0.193518] [ema: 0.999651] 
[Epoch 114/288] [Batch 100/174] [D loss: 0.403437] [G loss: 0.189825] [ema: 0.999652] 
[Epoch 115/288] [Batch 0/174] [D loss: 0.401114] [G loss: 0.176707] [ema: 0.999654] 
[Epoch 115/288] [Batch 100/174] [D loss: 0.368129] [G loss: 0.178177] [ema: 0.999655] 
[Epoch 116/288] [Batch 0/174] [D loss: 0.391641] [G loss: 0.180910] [ema: 0.999657] 
[Epoch 116/288] [Batch 100/174] [D loss: 0.387619] [G loss: 0.181771] [ema: 0.999658] 
[Epoch 117/288] [Batch 0/174] [D loss: 0.373672] [G loss: 0.200235] [ema: 0.999660] 
[Epoch 117/288] [Batch 100/174] [D loss: 0.357922] [G loss: 0.179642] [ema: 0.999661] 
[Epoch 118/288] [Batch 0/174] [D loss: 0.362360] [G loss: 0.209589] [ema: 0.999662] 
[Epoch 118/288] [Batch 100/174] [D loss: 0.355261] [G loss: 0.179883] [ema: 0.999664] 
[Epoch 119/288] [Batch 0/174] [D loss: 0.392960] [G loss: 0.178483] [ema: 0.999665] 
[Epoch 119/288] [Batch 100/174] [D loss: 0.398716] [G loss: 0.174273] [ema: 0.999667] 
[Epoch 120/288] [Batch 0/174] [D loss: 0.426035] [G loss: 0.185845] [ema: 0.999668] 
[Epoch 120/288] [Batch 100/174] [D loss: 0.385244] [G loss: 0.174307] [ema: 0.999670] 
[Epoch 121/288] [Batch 0/174] [D loss: 0.378700] [G loss: 0.175192] [ema: 0.999671] 
[Epoch 121/288] [Batch 100/174] [D loss: 0.371855] [G loss: 0.178259] [ema: 0.999672] 
[Epoch 122/288] [Batch 0/174] [D loss: 0.357404] [G loss: 0.195533] [ema: 0.999674] 
[Epoch 122/288] [Batch 100/174] [D loss: 0.384250] [G loss: 0.180180] [ema: 0.999675] 
[Epoch 123/288] [Batch 0/174] [D loss: 0.374832] [G loss: 0.195994] [ema: 0.999676] 
[Epoch 123/288] [Batch 100/174] [D loss: 0.367635] [G loss: 0.185640] [ema: 0.999678] 
[Epoch 124/288] [Batch 0/174] [D loss: 0.390231] [G loss: 0.194185] [ema: 0.999679] 
[Epoch 124/288] [Batch 100/174] [D loss: 0.414598] [G loss: 0.188203] [ema: 0.999680] 
[Epoch 125/288] [Batch 0/174] [D loss: 0.397369] [G loss: 0.185334] [ema: 0.999681] 
[Epoch 125/288] [Batch 100/174] [D loss: 0.398519] [G loss: 0.193536] [ema: 0.999683] 
[Epoch 126/288] [Batch 0/174] [D loss: 0.352458] [G loss: 0.191923] [ema: 0.999684] 
[Epoch 126/288] [Batch 100/174] [D loss: 0.373379] [G loss: 0.174693] [ema: 0.999685] 
[Epoch 127/288] [Batch 0/174] [D loss: 0.401578] [G loss: 0.174523] [ema: 0.999686] 
[Epoch 127/288] [Batch 100/174] [D loss: 0.352052] [G loss: 0.188326] [ema: 0.999688] 
[Epoch 128/288] [Batch 0/174] [D loss: 0.348363] [G loss: 0.180457] [ema: 0.999689] 
[Epoch 128/288] [Batch 100/174] [D loss: 0.406291] [G loss: 0.185235] [ema: 0.999690] 
[Epoch 129/288] [Batch 0/174] [D loss: 0.443767] [G loss: 0.188827] [ema: 0.999691] 
[Epoch 129/288] [Batch 100/174] [D loss: 0.360195] [G loss: 0.196659] [ema: 0.999693] 
[Epoch 130/288] [Batch 0/174] [D loss: 0.375250] [G loss: 0.189253] [ema: 0.999694] 
[Epoch 130/288] [Batch 100/174] [D loss: 0.342934] [G loss: 0.186020] [ema: 0.999695] 
[Epoch 131/288] [Batch 0/174] [D loss: 0.380319] [G loss: 0.182363] [ema: 0.999696] 
[Epoch 131/288] [Batch 100/174] [D loss: 0.415678] [G loss: 0.180499] [ema: 0.999697] 
[Epoch 132/288] [Batch 0/174] [D loss: 0.371556] [G loss: 0.196740] [ema: 0.999698] 
[Epoch 132/288] [Batch 100/174] [D loss: 0.427742] [G loss: 0.182361] [ema: 0.999700] 
[Epoch 133/288] [Batch 0/174] [D loss: 0.414185] [G loss: 0.186109] [ema: 0.999701] 
[Epoch 133/288] [Batch 100/174] [D loss: 0.340451] [G loss: 0.196878] [ema: 0.999702] 
[Epoch 134/288] [Batch 0/174] [D loss: 0.358834] [G loss: 0.182245] [ema: 0.999703] 
[Epoch 134/288] [Batch 100/174] [D loss: 0.416408] [G loss: 0.187324] [ema: 0.999704] 
[Epoch 135/288] [Batch 0/174] [D loss: 0.375781] [G loss: 0.183024] [ema: 0.999705] 
[Epoch 135/288] [Batch 100/174] [D loss: 0.360898] [G loss: 0.181737] [ema: 0.999706] 
[Epoch 136/288] [Batch 0/174] [D loss: 0.397698] [G loss: 0.191704] [ema: 0.999707] 
[Epoch 136/288] [Batch 100/174] [D loss: 0.397888] [G loss: 0.186454] [ema: 0.999708] 
[Epoch 137/288] [Batch 0/174] [D loss: 0.357435] [G loss: 0.199322] [ema: 0.999709] 
[Epoch 137/288] [Batch 100/174] [D loss: 0.388370] [G loss: 0.180191] [ema: 0.999710] 
[Epoch 138/288] [Batch 0/174] [D loss: 0.375308] [G loss: 0.192425] [ema: 0.999711] 
[Epoch 138/288] [Batch 100/174] [D loss: 0.382477] [G loss: 0.181392] [ema: 0.999713] 
[Epoch 139/288] [Batch 0/174] [D loss: 0.397973] [G loss: 0.187941] [ema: 0.999713] 
[Epoch 139/288] [Batch 100/174] [D loss: 0.374545] [G loss: 0.199402] [ema: 0.999715] 
[Epoch 140/288] [Batch 0/174] [D loss: 0.350734] [G loss: 0.188334] [ema: 0.999715] 
[Epoch 140/288] [Batch 100/174] [D loss: 0.461830] [G loss: 0.169926] [ema: 0.999717] 
[Epoch 141/288] [Batch 0/174] [D loss: 0.364135] [G loss: 0.187037] [ema: 0.999718] 
[Epoch 141/288] [Batch 100/174] [D loss: 0.363458] [G loss: 0.191945] [ema: 0.999719] 
[Epoch 142/288] [Batch 0/174] [D loss: 0.407526] [G loss: 0.185435] [ema: 0.999720] 
[Epoch 142/288] [Batch 100/174] [D loss: 0.454352] [G loss: 0.185975] [ema: 0.999721] 
[Epoch 143/288] [Batch 0/174] [D loss: 0.386976] [G loss: 0.193768] [ema: 0.999721] 
[Epoch 143/288] [Batch 100/174] [D loss: 0.377218] [G loss: 0.178771] [ema: 0.999723] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_30_100/KuHar_DAGHAR_Multiclass_50000_D_30_2024_10_28_21_34_11/Model



[Epoch 144/288] [Batch 0/174] [D loss: 0.424208] [G loss: 0.194784] [ema: 0.999723] 
[Epoch 144/288] [Batch 100/174] [D loss: 0.427893] [G loss: 0.187530] [ema: 0.999724] 
[Epoch 145/288] [Batch 0/174] [D loss: 0.353121] [G loss: 0.178585] [ema: 0.999725] 
[Epoch 145/288] [Batch 100/174] [D loss: 0.372681] [G loss: 0.189616] [ema: 0.999726] 
[Epoch 146/288] [Batch 0/174] [D loss: 0.388395] [G loss: 0.192451] [ema: 0.999727] 
[Epoch 146/288] [Batch 100/174] [D loss: 0.378423] [G loss: 0.187605] [ema: 0.999728] 
[Epoch 147/288] [Batch 0/174] [D loss: 0.360661] [G loss: 0.183965] [ema: 0.999729] 
[Epoch 147/288] [Batch 100/174] [D loss: 0.397177] [G loss: 0.188294] [ema: 0.999730] 
[Epoch 148/288] [Batch 0/174] [D loss: 0.419115] [G loss: 0.188161] [ema: 0.999731] 
[Epoch 148/288] [Batch 100/174] [D loss: 0.384174] [G loss: 0.176308] [ema: 0.999732] 
[Epoch 149/288] [Batch 0/174] [D loss: 0.392176] [G loss: 0.202402] [ema: 0.999733] 
[Epoch 149/288] [Batch 100/174] [D loss: 0.399587] [G loss: 0.184543] [ema: 0.999734] 
[Epoch 150/288] [Batch 0/174] [D loss: 0.405627] [G loss: 0.180124] [ema: 0.999734] 
[Epoch 150/288] [Batch 100/174] [D loss: 0.362341] [G loss: 0.194011] [ema: 0.999735] 
[Epoch 151/288] [Batch 0/174] [D loss: 0.417829] [G loss: 0.174854] [ema: 0.999736] 
[Epoch 151/288] [Batch 100/174] [D loss: 0.378123] [G loss: 0.192241] [ema: 0.999737] 
[Epoch 152/288] [Batch 0/174] [D loss: 0.347839] [G loss: 0.184999] [ema: 0.999738] 
[Epoch 152/288] [Batch 100/174] [D loss: 0.354730] [G loss: 0.189124] [ema: 0.999739] 
[Epoch 153/288] [Batch 0/174] [D loss: 0.353426] [G loss: 0.195245] [ema: 0.999740] 
[Epoch 153/288] [Batch 100/174] [D loss: 0.380562] [G loss: 0.193513] [ema: 0.999741] 
[Epoch 154/288] [Batch 0/174] [D loss: 0.408850] [G loss: 0.184865] [ema: 0.999741] 
[Epoch 154/288] [Batch 100/174] [D loss: 0.421123] [G loss: 0.175795] [ema: 0.999742] 
[Epoch 155/288] [Batch 0/174] [D loss: 0.404039] [G loss: 0.195938] [ema: 0.999743] 
[Epoch 155/288] [Batch 100/174] [D loss: 0.386147] [G loss: 0.182096] [ema: 0.999744] 
[Epoch 156/288] [Batch 0/174] [D loss: 0.415027] [G loss: 0.182120] [ema: 0.999745] 
[Epoch 156/288] [Batch 100/174] [D loss: 0.402224] [G loss: 0.178875] [ema: 0.999746] 
[Epoch 157/288] [Batch 0/174] [D loss: 0.373720] [G loss: 0.178804] [ema: 0.999746] 
[Epoch 157/288] [Batch 100/174] [D loss: 0.385989] [G loss: 0.192285] [ema: 0.999747] 
[Epoch 158/288] [Batch 0/174] [D loss: 0.352470] [G loss: 0.180945] [ema: 0.999748] 
[Epoch 158/288] [Batch 100/174] [D loss: 0.371598] [G loss: 0.183876] [ema: 0.999749] 
[Epoch 159/288] [Batch 0/174] [D loss: 0.395592] [G loss: 0.189290] [ema: 0.999749] 
[Epoch 159/288] [Batch 100/174] [D loss: 0.363617] [G loss: 0.190180] [ema: 0.999750] 
[Epoch 160/288] [Batch 0/174] [D loss: 0.381794] [G loss: 0.188284] [ema: 0.999751] 
[Epoch 160/288] [Batch 100/174] [D loss: 0.369486] [G loss: 0.186103] [ema: 0.999752] 
[Epoch 161/288] [Batch 0/174] [D loss: 0.414068] [G loss: 0.188817] [ema: 0.999753] 
[Epoch 161/288] [Batch 100/174] [D loss: 0.402803] [G loss: 0.177384] [ema: 0.999753] 
[Epoch 162/288] [Batch 0/174] [D loss: 0.373904] [G loss: 0.184977] [ema: 0.999754] 
[Epoch 162/288] [Batch 100/174] [D loss: 0.403053] [G loss: 0.187907] [ema: 0.999755] 
[Epoch 163/288] [Batch 0/174] [D loss: 0.420357] [G loss: 0.188499] [ema: 0.999756] 
[Epoch 163/288] [Batch 100/174] [D loss: 0.386013] [G loss: 0.188176] [ema: 0.999756] 
[Epoch 164/288] [Batch 0/174] [D loss: 0.405184] [G loss: 0.175806] [ema: 0.999757] 
[Epoch 164/288] [Batch 100/174] [D loss: 0.363443] [G loss: 0.189903] [ema: 0.999758] 
[Epoch 165/288] [Batch 0/174] [D loss: 0.386246] [G loss: 0.199119] [ema: 0.999759] 
[Epoch 165/288] [Batch 100/174] [D loss: 0.390036] [G loss: 0.189140] [ema: 0.999759] 
[Epoch 166/288] [Batch 0/174] [D loss: 0.365674] [G loss: 0.178488] [ema: 0.999760] 
[Epoch 166/288] [Batch 100/174] [D loss: 0.388575] [G loss: 0.178046] [ema: 0.999761] 
[Epoch 167/288] [Batch 0/174] [D loss: 0.400157] [G loss: 0.175500] [ema: 0.999761] 
[Epoch 167/288] [Batch 100/174] [D loss: 0.365555] [G loss: 0.177340] [ema: 0.999762] 
[Epoch 168/288] [Batch 0/174] [D loss: 0.400925] [G loss: 0.203507] [ema: 0.999763] 
[Epoch 168/288] [Batch 100/174] [D loss: 0.412240] [G loss: 0.187217] [ema: 0.999764] 
[Epoch 169/288] [Batch 0/174] [D loss: 0.401179] [G loss: 0.194971] [ema: 0.999764] 
[Epoch 169/288] [Batch 100/174] [D loss: 0.389307] [G loss: 0.186026] [ema: 0.999765] 
[Epoch 170/288] [Batch 0/174] [D loss: 0.364944] [G loss: 0.187282] [ema: 0.999766] 
[Epoch 170/288] [Batch 100/174] [D loss: 0.413004] [G loss: 0.173434] [ema: 0.999766] 
[Epoch 171/288] [Batch 0/174] [D loss: 0.409763] [G loss: 0.197292] [ema: 0.999767] 
[Epoch 171/288] [Batch 100/174] [D loss: 0.375582] [G loss: 0.180249] [ema: 0.999768] 
[Epoch 172/288] [Batch 0/174] [D loss: 0.407798] [G loss: 0.186971] [ema: 0.999768] 
[Epoch 172/288] [Batch 100/174] [D loss: 0.381527] [G loss: 0.183525] [ema: 0.999769] 
[Epoch 173/288] [Batch 0/174] [D loss: 0.376476] [G loss: 0.195002] [ema: 0.999770] 
[Epoch 173/288] [Batch 100/174] [D loss: 0.399885] [G loss: 0.189766] [ema: 0.999771] 
[Epoch 174/288] [Batch 0/174] [D loss: 0.425231] [G loss: 0.181479] [ema: 0.999771] 
[Epoch 174/288] [Batch 100/174] [D loss: 0.429150] [G loss: 0.188155] [ema: 0.999772] 
[Epoch 175/288] [Batch 0/174] [D loss: 0.415579] [G loss: 0.192039] [ema: 0.999772] 
[Epoch 175/288] [Batch 100/174] [D loss: 0.369261] [G loss: 0.179483] [ema: 0.999773] 
[Epoch 176/288] [Batch 0/174] [D loss: 0.385809] [G loss: 0.192687] [ema: 0.999774] 
[Epoch 176/288] [Batch 100/174] [D loss: 0.363775] [G loss: 0.185255] [ema: 0.999774] 
[Epoch 177/288] [Batch 0/174] [D loss: 0.433945] [G loss: 0.195358] [ema: 0.999775] 
[Epoch 177/288] [Batch 100/174] [D loss: 0.389995] [G loss: 0.189942] [ema: 0.999776] 
[Epoch 178/288] [Batch 0/174] [D loss: 0.388876] [G loss: 0.176111] [ema: 0.999776] 
[Epoch 178/288] [Batch 100/174] [D loss: 0.426099] [G loss: 0.175876] [ema: 0.999777] 
[Epoch 179/288] [Batch 0/174] [D loss: 0.378365] [G loss: 0.189336] [ema: 0.999777] 
[Epoch 179/288] [Batch 100/174] [D loss: 0.374569] [G loss: 0.196879] [ema: 0.999778] 
[Epoch 180/288] [Batch 0/174] [D loss: 0.383893] [G loss: 0.187862] [ema: 0.999779] 
[Epoch 180/288] [Batch 100/174] [D loss: 0.373077] [G loss: 0.175068] [ema: 0.999779] 
[Epoch 181/288] [Batch 0/174] [D loss: 0.393864] [G loss: 0.191756] [ema: 0.999780] 
[Epoch 181/288] [Batch 100/174] [D loss: 0.400001] [G loss: 0.181364] [ema: 0.999781] 
[Epoch 182/288] [Batch 0/174] [D loss: 0.408341] [G loss: 0.176602] [ema: 0.999781] 
[Epoch 182/288] [Batch 100/174] [D loss: 0.363432] [G loss: 0.192702] [ema: 0.999782] 
[Epoch 183/288] [Batch 0/174] [D loss: 0.431385] [G loss: 0.183988] [ema: 0.999782] 
[Epoch 183/288] [Batch 100/174] [D loss: 0.413315] [G loss: 0.173012] [ema: 0.999783] 
[Epoch 184/288] [Batch 0/174] [D loss: 0.390547] [G loss: 0.194570] [ema: 0.999784] 
[Epoch 184/288] [Batch 100/174] [D loss: 0.377007] [G loss: 0.181119] [ema: 0.999784] 
[Epoch 185/288] [Batch 0/174] [D loss: 0.371977] [G loss: 0.187010] [ema: 0.999785] 
[Epoch 185/288] [Batch 100/174] [D loss: 0.395421] [G loss: 0.190565] [ema: 0.999785] 
[Epoch 186/288] [Batch 0/174] [D loss: 0.423088] [G loss: 0.192363] [ema: 0.999786] 
[Epoch 186/288] [Batch 100/174] [D loss: 0.388125] [G loss: 0.186050] [ema: 0.999787] 
[Epoch 187/288] [Batch 0/174] [D loss: 0.396824] [G loss: 0.197900] [ema: 0.999787] 
[Epoch 187/288] [Batch 100/174] [D loss: 0.369859] [G loss: 0.174752] [ema: 0.999788] 
[Epoch 188/288] [Batch 0/174] [D loss: 0.378927] [G loss: 0.194201] [ema: 0.999788] 
[Epoch 188/288] [Batch 100/174] [D loss: 0.371276] [G loss: 0.189139] [ema: 0.999789] 
[Epoch 189/288] [Batch 0/174] [D loss: 0.404354] [G loss: 0.166370] [ema: 0.999789] 
[Epoch 189/288] [Batch 100/174] [D loss: 0.393498] [G loss: 0.191612] [ema: 0.999790] 
[Epoch 190/288] [Batch 0/174] [D loss: 0.387775] [G loss: 0.181021] [ema: 0.999790] 
[Epoch 190/288] [Batch 100/174] [D loss: 0.388655] [G loss: 0.179931] [ema: 0.999791] 
[Epoch 191/288] [Batch 0/174] [D loss: 0.368277] [G loss: 0.192408] [ema: 0.999791] 
[Epoch 191/288] [Batch 100/174] [D loss: 0.378781] [G loss: 0.186410] [ema: 0.999792] 
[Epoch 192/288] [Batch 0/174] [D loss: 0.382155] [G loss: 0.192775] [ema: 0.999793] 
[Epoch 192/288] [Batch 100/174] [D loss: 0.377909] [G loss: 0.194593] [ema: 0.999793] 
[Epoch 193/288] [Batch 0/174] [D loss: 0.380881] [G loss: 0.184537] [ema: 0.999794] 
[Epoch 193/288] [Batch 100/174] [D loss: 0.392149] [G loss: 0.183190] [ema: 0.999794] 
[Epoch 194/288] [Batch 0/174] [D loss: 0.441733] [G loss: 0.179820] [ema: 0.999795] 
[Epoch 194/288] [Batch 100/174] [D loss: 0.350849] [G loss: 0.194286] [ema: 0.999795] 
[Epoch 195/288] [Batch 0/174] [D loss: 0.402134] [G loss: 0.191524] [ema: 0.999796] 
[Epoch 195/288] [Batch 100/174] [D loss: 0.389022] [G loss: 0.189575] [ema: 0.999796] 
[Epoch 196/288] [Batch 0/174] [D loss: 0.404341] [G loss: 0.185821] [ema: 0.999797] 
[Epoch 196/288] [Batch 100/174] [D loss: 0.372300] [G loss: 0.191841] [ema: 0.999797] 
[Epoch 197/288] [Batch 0/174] [D loss: 0.384804] [G loss: 0.188877] [ema: 0.999798] 
[Epoch 197/288] [Batch 100/174] [D loss: 0.426051] [G loss: 0.184117] [ema: 0.999798] 
[Epoch 198/288] [Batch 0/174] [D loss: 0.371452] [G loss: 0.188725] [ema: 0.999799] 
[Epoch 198/288] [Batch 100/174] [D loss: 0.408444] [G loss: 0.175210] [ema: 0.999799] 
[Epoch 199/288] [Batch 0/174] [D loss: 0.391297] [G loss: 0.191744] [ema: 0.999800] 
[Epoch 199/288] [Batch 100/174] [D loss: 0.373310] [G loss: 0.176073] [ema: 0.999800] 
[Epoch 200/288] [Batch 0/174] [D loss: 0.405759] [G loss: 0.180711] [ema: 0.999801] 
[Epoch 200/288] [Batch 100/174] [D loss: 0.353627] [G loss: 0.192577] [ema: 0.999801] 
[Epoch 201/288] [Batch 0/174] [D loss: 0.416042] [G loss: 0.182891] [ema: 0.999802] 
[Epoch 201/288] [Batch 100/174] [D loss: 0.367106] [G loss: 0.186583] [ema: 0.999802] 
[Epoch 202/288] [Batch 0/174] [D loss: 0.408022] [G loss: 0.180440] [ema: 0.999803] 
[Epoch 202/288] [Batch 100/174] [D loss: 0.410994] [G loss: 0.179051] [ema: 0.999803] 
[Epoch 203/288] [Batch 0/174] [D loss: 0.406460] [G loss: 0.182166] [ema: 0.999804] 
[Epoch 203/288] [Batch 100/174] [D loss: 0.374477] [G loss: 0.193992] [ema: 0.999804] 
[Epoch 204/288] [Batch 0/174] [D loss: 0.395194] [G loss: 0.181659] [ema: 0.999805] 
[Epoch 204/288] [Batch 100/174] [D loss: 0.396015] [G loss: 0.183444] [ema: 0.999805] 
[Epoch 205/288] [Batch 0/174] [D loss: 0.387749] [G loss: 0.183880] [ema: 0.999806] 
[Epoch 205/288] [Batch 100/174] [D loss: 0.395965] [G loss: 0.187640] [ema: 0.999806] 
[Epoch 206/288] [Batch 0/174] [D loss: 0.364165] [G loss: 0.181011] [ema: 0.999807] 
[Epoch 206/288] [Batch 100/174] [D loss: 0.416572] [G loss: 0.184084] [ema: 0.999807] 
[Epoch 207/288] [Batch 0/174] [D loss: 0.374302] [G loss: 0.195476] [ema: 0.999808] 
[Epoch 207/288] [Batch 100/174] [D loss: 0.380015] [G loss: 0.182366] [ema: 0.999808] 
[Epoch 208/288] [Batch 0/174] [D loss: 0.400729] [G loss: 0.185876] [ema: 0.999808] 
[Epoch 208/288] [Batch 100/174] [D loss: 0.378738] [G loss: 0.180071] [ema: 0.999809] 
[Epoch 209/288] [Batch 0/174] [D loss: 0.366480] [G loss: 0.193241] [ema: 0.999809] 
[Epoch 209/288] [Batch 100/174] [D loss: 0.436026] [G loss: 0.169507] [ema: 0.999810] 
[Epoch 210/288] [Batch 0/174] [D loss: 0.379187] [G loss: 0.202091] [ema: 0.999810] 
[Epoch 210/288] [Batch 100/174] [D loss: 0.370257] [G loss: 0.187424] [ema: 0.999811] 
[Epoch 211/288] [Batch 0/174] [D loss: 0.392975] [G loss: 0.182459] [ema: 0.999811] 
[Epoch 211/288] [Batch 100/174] [D loss: 0.407758] [G loss: 0.174895] [ema: 0.999812] 
[Epoch 212/288] [Batch 0/174] [D loss: 0.370826] [G loss: 0.195898] [ema: 0.999812] 
[Epoch 212/288] [Batch 100/174] [D loss: 0.380941] [G loss: 0.176401] [ema: 0.999813] 
[Epoch 213/288] [Batch 0/174] [D loss: 0.379593] [G loss: 0.191363] [ema: 0.999813] 
[Epoch 213/288] [Batch 100/174] [D loss: 0.420565] [G loss: 0.182936] [ema: 0.999813] 
[Epoch 214/288] [Batch 0/174] [D loss: 0.371031] [G loss: 0.186013] [ema: 0.999814] 
[Epoch 214/288] [Batch 100/174] [D loss: 0.391629] [G loss: 0.176836] [ema: 0.999814] 
[Epoch 215/288] [Batch 0/174] [D loss: 0.367406] [G loss: 0.183578] [ema: 0.999815] 
[Epoch 215/288] [Batch 100/174] [D loss: 0.374719] [G loss: 0.181196] [ema: 0.999815] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_30_100/KuHar_DAGHAR_Multiclass_50000_D_30_2024_10_28_21_34_11/Model



[Epoch 216/288] [Batch 0/174] [D loss: 0.372167] [G loss: 0.177260] [ema: 0.999816] 
[Epoch 216/288] [Batch 100/174] [D loss: 0.398134] [G loss: 0.184600] [ema: 0.999816] 
[Epoch 217/288] [Batch 0/174] [D loss: 0.386731] [G loss: 0.173582] [ema: 0.999816] 
[Epoch 217/288] [Batch 100/174] [D loss: 0.402871] [G loss: 0.179124] [ema: 0.999817] 
[Epoch 218/288] [Batch 0/174] [D loss: 0.344150] [G loss: 0.191821] [ema: 0.999817] 
[Epoch 218/288] [Batch 100/174] [D loss: 0.380740] [G loss: 0.189775] [ema: 0.999818] 
[Epoch 219/288] [Batch 0/174] [D loss: 0.363548] [G loss: 0.191041] [ema: 0.999818] 
[Epoch 219/288] [Batch 100/174] [D loss: 0.368332] [G loss: 0.184380] [ema: 0.999819] 
[Epoch 220/288] [Batch 0/174] [D loss: 0.411834] [G loss: 0.178749] [ema: 0.999819] 
[Epoch 220/288] [Batch 100/174] [D loss: 0.409053] [G loss: 0.185403] [ema: 0.999819] 
[Epoch 221/288] [Batch 0/174] [D loss: 0.387870] [G loss: 0.177683] [ema: 0.999820] 
[Epoch 221/288] [Batch 100/174] [D loss: 0.391001] [G loss: 0.197202] [ema: 0.999820] 
[Epoch 222/288] [Batch 0/174] [D loss: 0.396147] [G loss: 0.192158] [ema: 0.999821] 
[Epoch 222/288] [Batch 100/174] [D loss: 0.392071] [G loss: 0.189999] [ema: 0.999821] 
[Epoch 223/288] [Batch 0/174] [D loss: 0.437984] [G loss: 0.196149] [ema: 0.999821] 
[Epoch 223/288] [Batch 100/174] [D loss: 0.406249] [G loss: 0.176051] [ema: 0.999822] 
[Epoch 224/288] [Batch 0/174] [D loss: 0.405038] [G loss: 0.186905] [ema: 0.999822] 
[Epoch 224/288] [Batch 100/174] [D loss: 0.375402] [G loss: 0.183535] [ema: 0.999823] 
[Epoch 225/288] [Batch 0/174] [D loss: 0.362564] [G loss: 0.189222] [ema: 0.999823] 
[Epoch 225/288] [Batch 100/174] [D loss: 0.342689] [G loss: 0.196470] [ema: 0.999823] 
[Epoch 226/288] [Batch 0/174] [D loss: 0.398003] [G loss: 0.181661] [ema: 0.999824] 
[Epoch 226/288] [Batch 100/174] [D loss: 0.378295] [G loss: 0.181953] [ema: 0.999824] 
[Epoch 227/288] [Batch 0/174] [D loss: 0.370449] [G loss: 0.186010] [ema: 0.999825] 
[Epoch 227/288] [Batch 100/174] [D loss: 0.403782] [G loss: 0.178181] [ema: 0.999825] 
[Epoch 228/288] [Batch 0/174] [D loss: 0.385596] [G loss: 0.186306] [ema: 0.999825] 
[Epoch 228/288] [Batch 100/174] [D loss: 0.395322] [G loss: 0.181492] [ema: 0.999826] 
[Epoch 229/288] [Batch 0/174] [D loss: 0.366112] [G loss: 0.195722] [ema: 0.999826] 
[Epoch 229/288] [Batch 100/174] [D loss: 0.415036] [G loss: 0.188310] [ema: 0.999826] 
[Epoch 230/288] [Batch 0/174] [D loss: 0.386930] [G loss: 0.191378] [ema: 0.999827] 
[Epoch 230/288] [Batch 100/174] [D loss: 0.391182] [G loss: 0.185448] [ema: 0.999827] 
[Epoch 231/288] [Batch 0/174] [D loss: 0.364653] [G loss: 0.191502] [ema: 0.999828] 
[Epoch 231/288] [Batch 100/174] [D loss: 0.401498] [G loss: 0.178816] [ema: 0.999828] 
[Epoch 232/288] [Batch 0/174] [D loss: 0.389311] [G loss: 0.188381] [ema: 0.999828] 
[Epoch 232/288] [Batch 100/174] [D loss: 0.418898] [G loss: 0.183382] [ema: 0.999829] 
[Epoch 233/288] [Batch 0/174] [D loss: 0.385016] [G loss: 0.187382] [ema: 0.999829] 
[Epoch 233/288] [Batch 100/174] [D loss: 0.364374] [G loss: 0.183395] [ema: 0.999829] 
[Epoch 234/288] [Batch 0/174] [D loss: 0.410921] [G loss: 0.189580] [ema: 0.999830] 
[Epoch 234/288] [Batch 100/174] [D loss: 0.396678] [G loss: 0.190943] [ema: 0.999830] 
[Epoch 235/288] [Batch 0/174] [D loss: 0.399209] [G loss: 0.184262] [ema: 0.999830] 
[Epoch 235/288] [Batch 100/174] [D loss: 0.369293] [G loss: 0.183161] [ema: 0.999831] 
[Epoch 236/288] [Batch 0/174] [D loss: 0.405277] [G loss: 0.188513] [ema: 0.999831] 
[Epoch 236/288] [Batch 100/174] [D loss: 0.395382] [G loss: 0.184292] [ema: 0.999832] 
[Epoch 237/288] [Batch 0/174] [D loss: 0.389033] [G loss: 0.190447] [ema: 0.999832] 
[Epoch 237/288] [Batch 100/174] [D loss: 0.417596] [G loss: 0.186365] [ema: 0.999832] 
[Epoch 238/288] [Batch 0/174] [D loss: 0.411857] [G loss: 0.184184] [ema: 0.999833] 
[Epoch 238/288] [Batch 100/174] [D loss: 0.394504] [G loss: 0.188814] [ema: 0.999833] 
[Epoch 239/288] [Batch 0/174] [D loss: 0.378983] [G loss: 0.190863] [ema: 0.999833] 
[Epoch 239/288] [Batch 100/174] [D loss: 0.437150] [G loss: 0.186425] [ema: 0.999834] 
[Epoch 240/288] [Batch 0/174] [D loss: 0.369536] [G loss: 0.200399] [ema: 0.999834] 
[Epoch 240/288] [Batch 100/174] [D loss: 0.380960] [G loss: 0.192466] [ema: 0.999834] 
[Epoch 241/288] [Batch 0/174] [D loss: 0.378244] [G loss: 0.191251] [ema: 0.999835] 
[Epoch 241/288] [Batch 100/174] [D loss: 0.421920] [G loss: 0.180334] [ema: 0.999835] 
[Epoch 242/288] [Batch 0/174] [D loss: 0.379242] [G loss: 0.190779] [ema: 0.999835] 
[Epoch 242/288] [Batch 100/174] [D loss: 0.399209] [G loss: 0.177423] [ema: 0.999836] 
[Epoch 243/288] [Batch 0/174] [D loss: 0.353124] [G loss: 0.184414] [ema: 0.999836] 
[Epoch 243/288] [Batch 100/174] [D loss: 0.400583] [G loss: 0.178023] [ema: 0.999836] 
[Epoch 244/288] [Batch 0/174] [D loss: 0.390483] [G loss: 0.185451] [ema: 0.999837] 
[Epoch 244/288] [Batch 100/174] [D loss: 0.349376] [G loss: 0.188080] [ema: 0.999837] 
[Epoch 245/288] [Batch 0/174] [D loss: 0.374177] [G loss: 0.195392] [ema: 0.999837] 
[Epoch 245/288] [Batch 100/174] [D loss: 0.346041] [G loss: 0.189201] [ema: 0.999838] 
[Epoch 246/288] [Batch 0/174] [D loss: 0.368728] [G loss: 0.198289] [ema: 0.999838] 
[Epoch 246/288] [Batch 100/174] [D loss: 0.418268] [G loss: 0.174053] [ema: 0.999838] 
[Epoch 247/288] [Batch 0/174] [D loss: 0.406077] [G loss: 0.181121] [ema: 0.999839] 
[Epoch 247/288] [Batch 100/174] [D loss: 0.372913] [G loss: 0.182481] [ema: 0.999839] 
[Epoch 248/288] [Batch 0/174] [D loss: 0.386043] [G loss: 0.172489] [ema: 0.999839] 
[Epoch 248/288] [Batch 100/174] [D loss: 0.397964] [G loss: 0.181605] [ema: 0.999840] 
[Epoch 249/288] [Batch 0/174] [D loss: 0.386088] [G loss: 0.197277] [ema: 0.999840] 
[Epoch 249/288] [Batch 100/174] [D loss: 0.352598] [G loss: 0.191120] [ema: 0.999840] 
[Epoch 250/288] [Batch 0/174] [D loss: 0.425305] [G loss: 0.184905] [ema: 0.999841] 
[Epoch 250/288] [Batch 100/174] [D loss: 0.375786] [G loss: 0.184758] [ema: 0.999841] 
[Epoch 251/288] [Batch 0/174] [D loss: 0.401615] [G loss: 0.187306] [ema: 0.999841] 
[Epoch 251/288] [Batch 100/174] [D loss: 0.383840] [G loss: 0.178186] [ema: 0.999842] 
[Epoch 252/288] [Batch 0/174] [D loss: 0.371673] [G loss: 0.183077] [ema: 0.999842] 
[Epoch 252/288] [Batch 100/174] [D loss: 0.343095] [G loss: 0.192825] [ema: 0.999842] 
[Epoch 253/288] [Batch 0/174] [D loss: 0.380904] [G loss: 0.192207] [ema: 0.999843] 
[Epoch 253/288] [Batch 100/174] [D loss: 0.391364] [G loss: 0.183086] [ema: 0.999843] 
[Epoch 254/288] [Batch 0/174] [D loss: 0.383990] [G loss: 0.186824] [ema: 0.999843] 
[Epoch 254/288] [Batch 100/174] [D loss: 0.372421] [G loss: 0.177666] [ema: 0.999844] 
[Epoch 255/288] [Batch 0/174] [D loss: 0.391319] [G loss: 0.186598] [ema: 0.999844] 
[Epoch 255/288] [Batch 100/174] [D loss: 0.389059] [G loss: 0.181806] [ema: 0.999844] 
[Epoch 256/288] [Batch 0/174] [D loss: 0.420399] [G loss: 0.186656] [ema: 0.999844] 
[Epoch 256/288] [Batch 100/174] [D loss: 0.442894] [G loss: 0.182067] [ema: 0.999845] 
[Epoch 257/288] [Batch 0/174] [D loss: 0.409162] [G loss: 0.185851] [ema: 0.999845] 
[Epoch 257/288] [Batch 100/174] [D loss: 0.396301] [G loss: 0.182173] [ema: 0.999845] 
[Epoch 258/288] [Batch 0/174] [D loss: 0.380396] [G loss: 0.189221] [ema: 0.999846] 
[Epoch 258/288] [Batch 100/174] [D loss: 0.428852] [G loss: 0.180450] [ema: 0.999846] 
[Epoch 259/288] [Batch 0/174] [D loss: 0.409814] [G loss: 0.195582] [ema: 0.999846] 
[Epoch 259/288] [Batch 100/174] [D loss: 0.400022] [G loss: 0.186270] [ema: 0.999847] 
[Epoch 260/288] [Batch 0/174] [D loss: 0.440065] [G loss: 0.182172] [ema: 0.999847] 
[Epoch 260/288] [Batch 100/174] [D loss: 0.369908] [G loss: 0.186046] [ema: 0.999847] 
[Epoch 261/288] [Batch 0/174] [D loss: 0.360928] [G loss: 0.198161] [ema: 0.999847] 
[Epoch 261/288] [Batch 100/174] [D loss: 0.370531] [G loss: 0.192091] [ema: 0.999848] 
[Epoch 262/288] [Batch 0/174] [D loss: 0.414865] [G loss: 0.174047] [ema: 0.999848] 
[Epoch 262/288] [Batch 100/174] [D loss: 0.390641] [G loss: 0.177989] [ema: 0.999848] 
[Epoch 263/288] [Batch 0/174] [D loss: 0.401931] [G loss: 0.193250] [ema: 0.999849] 
[Epoch 263/288] [Batch 100/174] [D loss: 0.355536] [G loss: 0.186602] [ema: 0.999849] 
[Epoch 264/288] [Batch 0/174] [D loss: 0.400083] [G loss: 0.182906] [ema: 0.999849] 
[Epoch 264/288] [Batch 100/174] [D loss: 0.406840] [G loss: 0.173160] [ema: 0.999849] 
[Epoch 265/288] [Batch 0/174] [D loss: 0.391962] [G loss: 0.192785] [ema: 0.999850] 
[Epoch 265/288] [Batch 100/174] [D loss: 0.378968] [G loss: 0.184193] [ema: 0.999850] 
[Epoch 266/288] [Batch 0/174] [D loss: 0.409141] [G loss: 0.186078] [ema: 0.999850] 
[Epoch 266/288] [Batch 100/174] [D loss: 0.393351] [G loss: 0.189281] [ema: 0.999851] 
[Epoch 267/288] [Batch 0/174] [D loss: 0.421620] [G loss: 0.167457] [ema: 0.999851] 
[Epoch 267/288] [Batch 100/174] [D loss: 0.399715] [G loss: 0.181452] [ema: 0.999851] 
[Epoch 268/288] [Batch 0/174] [D loss: 0.394546] [G loss: 0.184207] [ema: 0.999851] 
[Epoch 268/288] [Batch 100/174] [D loss: 0.356841] [G loss: 0.184858] [ema: 0.999852] 
[Epoch 269/288] [Batch 0/174] [D loss: 0.356375] [G loss: 0.188319] [ema: 0.999852] 
[Epoch 269/288] [Batch 100/174] [D loss: 0.415181] [G loss: 0.182251] [ema: 0.999852] 
[Epoch 270/288] [Batch 0/174] [D loss: 0.417486] [G loss: 0.185842] [ema: 0.999852] 
[Epoch 270/288] [Batch 100/174] [D loss: 0.412056] [G loss: 0.180148] [ema: 0.999853] 
[Epoch 271/288] [Batch 0/174] [D loss: 0.374868] [G loss: 0.195492] [ema: 0.999853] 
[Epoch 271/288] [Batch 100/174] [D loss: 0.384127] [G loss: 0.187686] [ema: 0.999853] 
[Epoch 272/288] [Batch 0/174] [D loss: 0.411554] [G loss: 0.180307] [ema: 0.999854] 
[Epoch 272/288] [Batch 100/174] [D loss: 0.335318] [G loss: 0.191686] [ema: 0.999854] 
[Epoch 273/288] [Batch 0/174] [D loss: 0.366710] [G loss: 0.194804] [ema: 0.999854] 
[Epoch 273/288] [Batch 100/174] [D loss: 0.385221] [G loss: 0.183089] [ema: 0.999854] 
[Epoch 274/288] [Batch 0/174] [D loss: 0.369831] [G loss: 0.195066] [ema: 0.999855] 
[Epoch 274/288] [Batch 100/174] [D loss: 0.402820] [G loss: 0.188767] [ema: 0.999855] 
[Epoch 275/288] [Batch 0/174] [D loss: 0.389266] [G loss: 0.190585] [ema: 0.999855] 
[Epoch 275/288] [Batch 100/174] [D loss: 0.430729] [G loss: 0.179384] [ema: 0.999855] 
[Epoch 276/288] [Batch 0/174] [D loss: 0.402732] [G loss: 0.185046] [ema: 0.999856] 
[Epoch 276/288] [Batch 100/174] [D loss: 0.352139] [G loss: 0.194355] [ema: 0.999856] 
[Epoch 277/288] [Batch 0/174] [D loss: 0.349692] [G loss: 0.192208] [ema: 0.999856] 
[Epoch 277/288] [Batch 100/174] [D loss: 0.375091] [G loss: 0.186607] [ema: 0.999856] 
[Epoch 278/288] [Batch 0/174] [D loss: 0.392078] [G loss: 0.190163] [ema: 0.999857] 
[Epoch 278/288] [Batch 100/174] [D loss: 0.413880] [G loss: 0.175689] [ema: 0.999857] 
[Epoch 279/288] [Batch 0/174] [D loss: 0.388284] [G loss: 0.191091] [ema: 0.999857] 
[Epoch 279/288] [Batch 100/174] [D loss: 0.364067] [G loss: 0.182014] [ema: 0.999858] 
[Epoch 280/288] [Batch 0/174] [D loss: 0.420818] [G loss: 0.197248] [ema: 0.999858] 
[Epoch 280/288] [Batch 100/174] [D loss: 0.346514] [G loss: 0.182288] [ema: 0.999858] 
[Epoch 281/288] [Batch 0/174] [D loss: 0.402865] [G loss: 0.175865] [ema: 0.999858] 
[Epoch 281/288] [Batch 100/174] [D loss: 0.389820] [G loss: 0.184016] [ema: 0.999859] 
[Epoch 282/288] [Batch 0/174] [D loss: 0.345435] [G loss: 0.190664] [ema: 0.999859] 
[Epoch 282/288] [Batch 100/174] [D loss: 0.362623] [G loss: 0.185692] [ema: 0.999859] 
[Epoch 283/288] [Batch 0/174] [D loss: 0.379208] [G loss: 0.188199] [ema: 0.999859] 
[Epoch 283/288] [Batch 100/174] [D loss: 0.375223] [G loss: 0.192405] [ema: 0.999860] 
[Epoch 284/288] [Batch 0/174] [D loss: 0.397632] [G loss: 0.184073] [ema: 0.999860] 
[Epoch 284/288] [Batch 100/174] [D loss: 0.402398] [G loss: 0.177833] [ema: 0.999860] 
[Epoch 285/288] [Batch 0/174] [D loss: 0.337574] [G loss: 0.194749] [ema: 0.999860] 
[Epoch 285/288] [Batch 100/174] [D loss: 0.382598] [G loss: 0.183964] [ema: 0.999861] 
[Epoch 286/288] [Batch 0/174] [D loss: 0.366423] [G loss: 0.183358] [ema: 0.999861] 
[Epoch 286/288] [Batch 100/174] [D loss: 0.416601] [G loss: 0.172748] [ema: 0.999861] 
[Epoch 287/288] [Batch 0/174] [D loss: 0.374976] [G loss: 0.194898] [ema: 0.999861] 
[Epoch 287/288] [Batch 100/174] [D loss: 0.381768] [G loss: 0.186273] [ema: 0.999861] 
