
 Starting training
Total of classes being trained: 1

['UCI_DAGHAR_Multiclass.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
UCI_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
UCI_DAGHAR_Multiclass
daghar
return single class data and labels, class is UCI_DAGHAR_Multiclass
data shape is (36788, 3, 1, 60)
label shape is (36788,)
2300
Epochs between checkpoint: 6



Saving checkpoint 1 in logs/daghar_all_50000_3axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_3axis_2024_10_30_18_51_41/Model



[Epoch 0/22] [Batch 0/2300] [D loss: 1.380267] [G loss: 0.338296] [ema: 0.000000] 
[Epoch 0/22] [Batch 100/2300] [D loss: 0.548169] [G loss: 0.186979] [ema: 0.933033] 
[Epoch 0/22] [Batch 200/2300] [D loss: 0.549271] [G loss: 0.137679] [ema: 0.965936] 
[Epoch 0/22] [Batch 300/2300] [D loss: 0.463816] [G loss: 0.144257] [ema: 0.977160] 
[Epoch 0/22] [Batch 400/2300] [D loss: 0.533372] [G loss: 0.132214] [ema: 0.982821] 
[Epoch 0/22] [Batch 500/2300] [D loss: 0.448621] [G loss: 0.151723] [ema: 0.986233] 
[Epoch 0/22] [Batch 600/2300] [D loss: 0.445588] [G loss: 0.168047] [ema: 0.988514] 
[Epoch 0/22] [Batch 700/2300] [D loss: 0.466188] [G loss: 0.165436] [ema: 0.990147] 
[Epoch 0/22] [Batch 800/2300] [D loss: 0.468439] [G loss: 0.128796] [ema: 0.991373] 
[Epoch 0/22] [Batch 900/2300] [D loss: 0.385135] [G loss: 0.209405] [ema: 0.992328] 
[Epoch 0/22] [Batch 1000/2300] [D loss: 0.441112] [G loss: 0.161757] [ema: 0.993092] 
[Epoch 0/22] [Batch 1100/2300] [D loss: 0.476988] [G loss: 0.174634] [ema: 0.993718] 
[Epoch 0/22] [Batch 1200/2300] [D loss: 0.414153] [G loss: 0.155289] [ema: 0.994240] 
[Epoch 0/22] [Batch 1300/2300] [D loss: 0.474353] [G loss: 0.184620] [ema: 0.994682] 
[Epoch 0/22] [Batch 1400/2300] [D loss: 0.451199] [G loss: 0.136160] [ema: 0.995061] 
[Epoch 0/22] [Batch 1500/2300] [D loss: 0.428208] [G loss: 0.178381] [ema: 0.995390] 
[Epoch 0/22] [Batch 1600/2300] [D loss: 0.416005] [G loss: 0.179106] [ema: 0.995677] 
[Epoch 0/22] [Batch 1700/2300] [D loss: 0.464338] [G loss: 0.163810] [ema: 0.995931] 
[Epoch 0/22] [Batch 1800/2300] [D loss: 0.457445] [G loss: 0.167649] [ema: 0.996157] 
[Epoch 0/22] [Batch 1900/2300] [D loss: 0.412317] [G loss: 0.201580] [ema: 0.996359] 
[Epoch 0/22] [Batch 2000/2300] [D loss: 0.426870] [G loss: 0.175884] [ema: 0.996540] 
[Epoch 0/22] [Batch 2100/2300] [D loss: 0.385870] [G loss: 0.168640] [ema: 0.996705] 
[Epoch 0/22] [Batch 2200/2300] [D loss: 0.418623] [G loss: 0.162673] [ema: 0.996854] 
[Epoch 1/22] [Batch 0/2300] [D loss: 0.322099] [G loss: 0.230028] [ema: 0.996991] 
[Epoch 1/22] [Batch 100/2300] [D loss: 0.437374] [G loss: 0.157007] [ema: 0.997116] 
[Epoch 1/22] [Batch 200/2300] [D loss: 0.397858] [G loss: 0.192221] [ema: 0.997231] 
[Epoch 1/22] [Batch 300/2300] [D loss: 0.426479] [G loss: 0.208514] [ema: 0.997338] 
[Epoch 1/22] [Batch 400/2300] [D loss: 0.424765] [G loss: 0.174923] [ema: 0.997436] 
[Epoch 1/22] [Batch 500/2300] [D loss: 0.366463] [G loss: 0.165543] [ema: 0.997528] 
[Epoch 1/22] [Batch 600/2300] [D loss: 0.367273] [G loss: 0.180552] [ema: 0.997613] 
[Epoch 1/22] [Batch 700/2300] [D loss: 0.421847] [G loss: 0.176914] [ema: 0.997692] 
[Epoch 1/22] [Batch 800/2300] [D loss: 0.424833] [G loss: 0.169154] [ema: 0.997767] 
[Epoch 1/22] [Batch 900/2300] [D loss: 0.462940] [G loss: 0.190580] [ema: 0.997836] 
[Epoch 1/22] [Batch 1000/2300] [D loss: 0.401397] [G loss: 0.179693] [ema: 0.997902] 
[Epoch 1/22] [Batch 1100/2300] [D loss: 0.367261] [G loss: 0.186735] [ema: 0.997963] 
[Epoch 1/22] [Batch 1200/2300] [D loss: 0.464062] [G loss: 0.158766] [ema: 0.998022] 
[Epoch 1/22] [Batch 1300/2300] [D loss: 0.393034] [G loss: 0.164911] [ema: 0.998076] 
[Epoch 1/22] [Batch 1400/2300] [D loss: 0.443070] [G loss: 0.185602] [ema: 0.998128] 
[Epoch 1/22] [Batch 1500/2300] [D loss: 0.422087] [G loss: 0.165668] [ema: 0.998178] 
[Epoch 1/22] [Batch 1600/2300] [D loss: 0.433063] [G loss: 0.169307] [ema: 0.998224] 
[Epoch 1/22] [Batch 1700/2300] [D loss: 0.434107] [G loss: 0.190500] [ema: 0.998269] 
[Epoch 1/22] [Batch 1800/2300] [D loss: 0.421426] [G loss: 0.180649] [ema: 0.998311] 
[Epoch 1/22] [Batch 1900/2300] [D loss: 0.404709] [G loss: 0.173106] [ema: 0.998351] 
[Epoch 1/22] [Batch 2000/2300] [D loss: 0.469934] [G loss: 0.148565] [ema: 0.998389] 
[Epoch 1/22] [Batch 2100/2300] [D loss: 0.416934] [G loss: 0.180360] [ema: 0.998426] 
[Epoch 1/22] [Batch 2200/2300] [D loss: 0.432256] [G loss: 0.176727] [ema: 0.998461] 
[Epoch 2/22] [Batch 0/2300] [D loss: 0.437571] [G loss: 0.165774] [ema: 0.998494] 
[Epoch 2/22] [Batch 100/2300] [D loss: 0.391452] [G loss: 0.190782] [ema: 0.998526] 
[Epoch 2/22] [Batch 200/2300] [D loss: 0.402415] [G loss: 0.174052] [ema: 0.998557] 
[Epoch 2/22] [Batch 300/2300] [D loss: 0.419743] [G loss: 0.167871] [ema: 0.998586] 
[Epoch 2/22] [Batch 400/2300] [D loss: 0.403479] [G loss: 0.167218] [ema: 0.998615] 
[Epoch 2/22] [Batch 500/2300] [D loss: 0.375495] [G loss: 0.157212] [ema: 0.998642] 
[Epoch 2/22] [Batch 600/2300] [D loss: 0.410242] [G loss: 0.162465] [ema: 0.998668] 
[Epoch 2/22] [Batch 700/2300] [D loss: 0.379356] [G loss: 0.183360] [ema: 0.998693] 
[Epoch 2/22] [Batch 800/2300] [D loss: 0.404277] [G loss: 0.173635] [ema: 0.998717] 
[Epoch 2/22] [Batch 900/2300] [D loss: 0.434481] [G loss: 0.166183] [ema: 0.998741] 
[Epoch 2/22] [Batch 1000/2300] [D loss: 0.435110] [G loss: 0.160945] [ema: 0.998763] 
[Epoch 2/22] [Batch 1100/2300] [D loss: 0.386160] [G loss: 0.165578] [ema: 0.998785] 
[Epoch 2/22] [Batch 1200/2300] [D loss: 0.417971] [G loss: 0.170790] [ema: 0.998806] 
[Epoch 2/22] [Batch 1300/2300] [D loss: 0.432325] [G loss: 0.186336] [ema: 0.998826] 
[Epoch 2/22] [Batch 1400/2300] [D loss: 0.388163] [G loss: 0.159546] [ema: 0.998845] 
[Epoch 2/22] [Batch 1500/2300] [D loss: 0.392316] [G loss: 0.174265] [ema: 0.998864] 
[Epoch 2/22] [Batch 1600/2300] [D loss: 0.390229] [G loss: 0.170764] [ema: 0.998883] 
[Epoch 2/22] [Batch 1700/2300] [D loss: 0.369235] [G loss: 0.191550] [ema: 0.998900] 
[Epoch 2/22] [Batch 1800/2300] [D loss: 0.361192] [G loss: 0.197318] [ema: 0.998918] 
[Epoch 2/22] [Batch 1900/2300] [D loss: 0.397197] [G loss: 0.195947] [ema: 0.998934] 
[Epoch 2/22] [Batch 2000/2300] [D loss: 0.407557] [G loss: 0.193853] [ema: 0.998950] 
[Epoch 2/22] [Batch 2100/2300] [D loss: 0.417977] [G loss: 0.169499] [ema: 0.998966] 
[Epoch 2/22] [Batch 2200/2300] [D loss: 0.421576] [G loss: 0.174284] [ema: 0.998981] 
[Epoch 3/22] [Batch 0/2300] [D loss: 0.381303] [G loss: 0.191200] [ema: 0.998996] 
[Epoch 3/22] [Batch 100/2300] [D loss: 0.426973] [G loss: 0.185518] [ema: 0.999010] 
[Epoch 3/22] [Batch 200/2300] [D loss: 0.397423] [G loss: 0.148440] [ema: 0.999024] 
[Epoch 3/22] [Batch 300/2300] [D loss: 0.439824] [G loss: 0.179483] [ema: 0.999038] 
[Epoch 3/22] [Batch 400/2300] [D loss: 0.410483] [G loss: 0.178136] [ema: 0.999051] 
[Epoch 3/22] [Batch 500/2300] [D loss: 0.418960] [G loss: 0.153732] [ema: 0.999064] 
[Epoch 3/22] [Batch 600/2300] [D loss: 0.375996] [G loss: 0.186341] [ema: 0.999076] 
[Epoch 3/22] [Batch 700/2300] [D loss: 0.444014] [G loss: 0.195434] [ema: 0.999088] 
[Epoch 3/22] [Batch 800/2300] [D loss: 0.420086] [G loss: 0.165335] [ema: 0.999100] 
[Epoch 3/22] [Batch 900/2300] [D loss: 0.377058] [G loss: 0.172209] [ema: 0.999112] 
[Epoch 3/22] [Batch 1000/2300] [D loss: 0.337634] [G loss: 0.185805] [ema: 0.999123] 
[Epoch 3/22] [Batch 1100/2300] [D loss: 0.399571] [G loss: 0.182609] [ema: 0.999134] 
[Epoch 3/22] [Batch 1200/2300] [D loss: 0.394613] [G loss: 0.181421] [ema: 0.999145] 
[Epoch 3/22] [Batch 1300/2300] [D loss: 0.402128] [G loss: 0.157310] [ema: 0.999155] 
[Epoch 3/22] [Batch 1400/2300] [D loss: 0.368507] [G loss: 0.192828] [ema: 0.999165] 
[Epoch 3/22] [Batch 1500/2300] [D loss: 0.366561] [G loss: 0.189089] [ema: 0.999175] 
[Epoch 3/22] [Batch 1600/2300] [D loss: 0.391980] [G loss: 0.184591] [ema: 0.999185] 
[Epoch 3/22] [Batch 1700/2300] [D loss: 0.393582] [G loss: 0.176836] [ema: 0.999194] 
[Epoch 3/22] [Batch 1800/2300] [D loss: 0.398099] [G loss: 0.185484] [ema: 0.999204] 
[Epoch 3/22] [Batch 1900/2300] [D loss: 0.385901] [G loss: 0.185286] [ema: 0.999213] 
[Epoch 3/22] [Batch 2000/2300] [D loss: 0.395888] [G loss: 0.172162] [ema: 0.999221] 
[Epoch 3/22] [Batch 2100/2300] [D loss: 0.396598] [G loss: 0.181409] [ema: 0.999230] 
[Epoch 3/22] [Batch 2200/2300] [D loss: 0.359057] [G loss: 0.178427] [ema: 0.999239] 
[Epoch 4/22] [Batch 0/2300] [D loss: 0.416908] [G loss: 0.194382] [ema: 0.999247] 
[Epoch 4/22] [Batch 100/2300] [D loss: 0.368970] [G loss: 0.181566] [ema: 0.999255] 
[Epoch 4/22] [Batch 200/2300] [D loss: 0.385876] [G loss: 0.191527] [ema: 0.999263] 
[Epoch 4/22] [Batch 300/2300] [D loss: 0.372964] [G loss: 0.197339] [ema: 0.999271] 
[Epoch 4/22] [Batch 400/2300] [D loss: 0.364805] [G loss: 0.180881] [ema: 0.999278] 
[Epoch 4/22] [Batch 500/2300] [D loss: 0.403623] [G loss: 0.179635] [ema: 0.999286] 
[Epoch 4/22] [Batch 600/2300] [D loss: 0.403190] [G loss: 0.186559] [ema: 0.999293] 
[Epoch 4/22] [Batch 700/2300] [D loss: 0.390718] [G loss: 0.185960] [ema: 0.999300] 
[Epoch 4/22] [Batch 800/2300] [D loss: 0.386125] [G loss: 0.185105] [ema: 0.999307] 
[Epoch 4/22] [Batch 900/2300] [D loss: 0.386066] [G loss: 0.173809] [ema: 0.999314] 
[Epoch 4/22] [Batch 1000/2300] [D loss: 0.402038] [G loss: 0.170252] [ema: 0.999321] 
[Epoch 4/22] [Batch 1100/2300] [D loss: 0.364118] [G loss: 0.181393] [ema: 0.999327] 
[Epoch 4/22] [Batch 1200/2300] [D loss: 0.396027] [G loss: 0.174416] [ema: 0.999334] 
[Epoch 4/22] [Batch 1300/2300] [D loss: 0.419151] [G loss: 0.177641] [ema: 0.999340] 
[Epoch 4/22] [Batch 1400/2300] [D loss: 0.385244] [G loss: 0.183212] [ema: 0.999346] 
[Epoch 4/22] [Batch 1500/2300] [D loss: 0.391479] [G loss: 0.198972] [ema: 0.999352] 
[Epoch 4/22] [Batch 1600/2300] [D loss: 0.389611] [G loss: 0.169213] [ema: 0.999358] 
[Epoch 4/22] [Batch 1700/2300] [D loss: 0.376998] [G loss: 0.170535] [ema: 0.999364] 
[Epoch 4/22] [Batch 1800/2300] [D loss: 0.386687] [G loss: 0.198050] [ema: 0.999370] 
[Epoch 4/22] [Batch 1900/2300] [D loss: 0.379622] [G loss: 0.180452] [ema: 0.999376] 
[Epoch 4/22] [Batch 2000/2300] [D loss: 0.414302] [G loss: 0.195604] [ema: 0.999381] 
[Epoch 4/22] [Batch 2100/2300] [D loss: 0.412078] [G loss: 0.192035] [ema: 0.999387] 
[Epoch 4/22] [Batch 2200/2300] [D loss: 0.395093] [G loss: 0.201876] [ema: 0.999392] 
[Epoch 5/22] [Batch 0/2300] [D loss: 0.343772] [G loss: 0.192215] [ema: 0.999397] 
[Epoch 5/22] [Batch 100/2300] [D loss: 0.369982] [G loss: 0.194687] [ema: 0.999403] 
[Epoch 5/22] [Batch 200/2300] [D loss: 0.404262] [G loss: 0.190300] [ema: 0.999408] 
[Epoch 5/22] [Batch 300/2300] [D loss: 0.393554] [G loss: 0.185144] [ema: 0.999413] 
[Epoch 5/22] [Batch 400/2300] [D loss: 0.373495] [G loss: 0.182495] [ema: 0.999418] 
[Epoch 5/22] [Batch 500/2300] [D loss: 0.444890] [G loss: 0.175529] [ema: 0.999423] 
[Epoch 5/22] [Batch 600/2300] [D loss: 0.354986] [G loss: 0.188034] [ema: 0.999427] 
[Epoch 5/22] [Batch 700/2300] [D loss: 0.405103] [G loss: 0.190260] [ema: 0.999432] 
[Epoch 5/22] [Batch 800/2300] [D loss: 0.360729] [G loss: 0.186809] [ema: 0.999437] 
[Epoch 5/22] [Batch 900/2300] [D loss: 0.410935] [G loss: 0.183118] [ema: 0.999441] 
[Epoch 5/22] [Batch 1000/2300] [D loss: 0.386816] [G loss: 0.190286] [ema: 0.999446] 
[Epoch 5/22] [Batch 1100/2300] [D loss: 0.380795] [G loss: 0.202788] [ema: 0.999450] 
[Epoch 5/22] [Batch 1200/2300] [D loss: 0.350727] [G loss: 0.189921] [ema: 0.999454] 
[Epoch 5/22] [Batch 1300/2300] [D loss: 0.380531] [G loss: 0.198167] [ema: 0.999459] 
[Epoch 5/22] [Batch 1400/2300] [D loss: 0.387731] [G loss: 0.198963] [ema: 0.999463] 
[Epoch 5/22] [Batch 1500/2300] [D loss: 0.358036] [G loss: 0.185048] [ema: 0.999467] 
[Epoch 5/22] [Batch 1600/2300] [D loss: 0.385163] [G loss: 0.185838] [ema: 0.999471] 
[Epoch 5/22] [Batch 1700/2300] [D loss: 0.364272] [G loss: 0.174151] [ema: 0.999475] 
[Epoch 5/22] [Batch 1800/2300] [D loss: 0.397229] [G loss: 0.205884] [ema: 0.999479] 
[Epoch 5/22] [Batch 1900/2300] [D loss: 0.398543] [G loss: 0.182062] [ema: 0.999483] 
[Epoch 5/22] [Batch 2000/2300] [D loss: 0.392776] [G loss: 0.196643] [ema: 0.999487] 
[Epoch 5/22] [Batch 2100/2300] [D loss: 0.364131] [G loss: 0.200771] [ema: 0.999490] 
[Epoch 5/22] [Batch 2200/2300] [D loss: 0.368897] [G loss: 0.174630] [ema: 0.999494] 



Saving checkpoint 2 in logs/daghar_all_50000_3axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_3axis_2024_10_30_18_51_41/Model



[Epoch 6/22] [Batch 0/2300] [D loss: 0.365020] [G loss: 0.214067] [ema: 0.999498] 
[Epoch 6/22] [Batch 100/2300] [D loss: 0.339986] [G loss: 0.183536] [ema: 0.999501] 
[Epoch 6/22] [Batch 200/2300] [D loss: 0.409615] [G loss: 0.197539] [ema: 0.999505] 
[Epoch 6/22] [Batch 300/2300] [D loss: 0.380843] [G loss: 0.180323] [ema: 0.999509] 
[Epoch 6/22] [Batch 400/2300] [D loss: 0.350301] [G loss: 0.185018] [ema: 0.999512] 
[Epoch 6/22] [Batch 500/2300] [D loss: 0.378618] [G loss: 0.191836] [ema: 0.999515] 
[Epoch 6/22] [Batch 600/2300] [D loss: 0.375451] [G loss: 0.181754] [ema: 0.999519] 
[Epoch 6/22] [Batch 700/2300] [D loss: 0.337225] [G loss: 0.198238] [ema: 0.999522] 
[Epoch 6/22] [Batch 800/2300] [D loss: 0.300043] [G loss: 0.206680] [ema: 0.999525] 
[Epoch 6/22] [Batch 900/2300] [D loss: 0.357805] [G loss: 0.193277] [ema: 0.999529] 
[Epoch 6/22] [Batch 1000/2300] [D loss: 0.366491] [G loss: 0.206504] [ema: 0.999532] 
[Epoch 6/22] [Batch 1100/2300] [D loss: 0.378927] [G loss: 0.162157] [ema: 0.999535] 
[Epoch 6/22] [Batch 1200/2300] [D loss: 0.325469] [G loss: 0.173916] [ema: 0.999538] 
[Epoch 6/22] [Batch 1300/2300] [D loss: 0.332280] [G loss: 0.201666] [ema: 0.999541] 
[Epoch 6/22] [Batch 1400/2300] [D loss: 0.350040] [G loss: 0.197787] [ema: 0.999544] 
[Epoch 6/22] [Batch 1500/2300] [D loss: 0.342337] [G loss: 0.178854] [ema: 0.999547] 
[Epoch 6/22] [Batch 1600/2300] [D loss: 0.387217] [G loss: 0.220738] [ema: 0.999550] 
[Epoch 6/22] [Batch 1700/2300] [D loss: 0.394061] [G loss: 0.186013] [ema: 0.999553] 
[Epoch 6/22] [Batch 1800/2300] [D loss: 0.404426] [G loss: 0.193544] [ema: 0.999556] 
[Epoch 6/22] [Batch 1900/2300] [D loss: 0.450983] [G loss: 0.163959] [ema: 0.999559] 
[Epoch 6/22] [Batch 2000/2300] [D loss: 0.398733] [G loss: 0.193783] [ema: 0.999561] 
[Epoch 6/22] [Batch 2100/2300] [D loss: 0.359689] [G loss: 0.199081] [ema: 0.999564] 
[Epoch 6/22] [Batch 2200/2300] [D loss: 0.369214] [G loss: 0.179558] [ema: 0.999567] 
[Epoch 7/22] [Batch 0/2300] [D loss: 0.392581] [G loss: 0.207723] [ema: 0.999570] 
[Epoch 7/22] [Batch 100/2300] [D loss: 0.400872] [G loss: 0.197330] [ema: 0.999572] 
[Epoch 7/22] [Batch 200/2300] [D loss: 0.397911] [G loss: 0.206307] [ema: 0.999575] 
[Epoch 7/22] [Batch 300/2300] [D loss: 0.361901] [G loss: 0.181535] [ema: 0.999577] 
[Epoch 7/22] [Batch 400/2300] [D loss: 0.371969] [G loss: 0.189928] [ema: 0.999580] 
[Epoch 7/22] [Batch 500/2300] [D loss: 0.380622] [G loss: 0.184924] [ema: 0.999583] 
[Epoch 7/22] [Batch 600/2300] [D loss: 0.379382] [G loss: 0.217229] [ema: 0.999585] 
[Epoch 7/22] [Batch 700/2300] [D loss: 0.373999] [G loss: 0.191550] [ema: 0.999587] 
[Epoch 7/22] [Batch 800/2300] [D loss: 0.349744] [G loss: 0.187161] [ema: 0.999590] 
[Epoch 7/22] [Batch 900/2300] [D loss: 0.442470] [G loss: 0.191467] [ema: 0.999592] 
[Epoch 7/22] [Batch 1000/2300] [D loss: 0.331958] [G loss: 0.192735] [ema: 0.999595] 
[Epoch 7/22] [Batch 1100/2300] [D loss: 0.346896] [G loss: 0.198343] [ema: 0.999597] 
[Epoch 7/22] [Batch 1200/2300] [D loss: 0.416446] [G loss: 0.184776] [ema: 0.999599] 
[Epoch 7/22] [Batch 1300/2300] [D loss: 0.348515] [G loss: 0.180136] [ema: 0.999602] 
[Epoch 7/22] [Batch 1400/2300] [D loss: 0.374373] [G loss: 0.196837] [ema: 0.999604] 
[Epoch 7/22] [Batch 1500/2300] [D loss: 0.393276] [G loss: 0.181341] [ema: 0.999606] 
[Epoch 7/22] [Batch 1600/2300] [D loss: 0.397895] [G loss: 0.189183] [ema: 0.999608] 
[Epoch 7/22] [Batch 1700/2300] [D loss: 0.377261] [G loss: 0.183723] [ema: 0.999611] 
[Epoch 7/22] [Batch 1800/2300] [D loss: 0.396455] [G loss: 0.198599] [ema: 0.999613] 
[Epoch 7/22] [Batch 1900/2300] [D loss: 0.351403] [G loss: 0.180383] [ema: 0.999615] 
[Epoch 7/22] [Batch 2000/2300] [D loss: 0.350898] [G loss: 0.207289] [ema: 0.999617] 
[Epoch 7/22] [Batch 2100/2300] [D loss: 0.381794] [G loss: 0.176122] [ema: 0.999619] 
[Epoch 7/22] [Batch 2200/2300] [D loss: 0.407852] [G loss: 0.185727] [ema: 0.999621] 
[Epoch 8/22] [Batch 0/2300] [D loss: 0.351024] [G loss: 0.207183] [ema: 0.999623] 
[Epoch 8/22] [Batch 100/2300] [D loss: 0.381912] [G loss: 0.196455] [ema: 0.999625] 
[Epoch 8/22] [Batch 200/2300] [D loss: 0.349899] [G loss: 0.198606] [ema: 0.999627] 
[Epoch 8/22] [Batch 300/2300] [D loss: 0.396419] [G loss: 0.189161] [ema: 0.999629] 
[Epoch 8/22] [Batch 400/2300] [D loss: 0.388028] [G loss: 0.185178] [ema: 0.999631] 
[Epoch 8/22] [Batch 500/2300] [D loss: 0.312017] [G loss: 0.203679] [ema: 0.999633] 
[Epoch 8/22] [Batch 600/2300] [D loss: 0.359760] [G loss: 0.201081] [ema: 0.999635] 
[Epoch 8/22] [Batch 700/2300] [D loss: 0.442338] [G loss: 0.185533] [ema: 0.999637] 
[Epoch 8/22] [Batch 800/2300] [D loss: 0.367810] [G loss: 0.172217] [ema: 0.999639] 
[Epoch 8/22] [Batch 900/2300] [D loss: 0.398446] [G loss: 0.193796] [ema: 0.999641] 
[Epoch 8/22] [Batch 1000/2300] [D loss: 0.373846] [G loss: 0.189275] [ema: 0.999643] 
[Epoch 8/22] [Batch 1100/2300] [D loss: 0.377074] [G loss: 0.190686] [ema: 0.999645] 
[Epoch 8/22] [Batch 1200/2300] [D loss: 0.386086] [G loss: 0.197256] [ema: 0.999646] 
[Epoch 8/22] [Batch 1300/2300] [D loss: 0.361744] [G loss: 0.193678] [ema: 0.999648] 
[Epoch 8/22] [Batch 1400/2300] [D loss: 0.354479] [G loss: 0.204109] [ema: 0.999650] 
[Epoch 8/22] [Batch 1500/2300] [D loss: 0.363481] [G loss: 0.193698] [ema: 0.999652] 
[Epoch 8/22] [Batch 1600/2300] [D loss: 0.331321] [G loss: 0.202763] [ema: 0.999653] 
[Epoch 8/22] [Batch 1700/2300] [D loss: 0.364625] [G loss: 0.204346] [ema: 0.999655] 
[Epoch 8/22] [Batch 1800/2300] [D loss: 0.387154] [G loss: 0.207154] [ema: 0.999657] 
[Epoch 8/22] [Batch 1900/2300] [D loss: 0.380493] [G loss: 0.197429] [ema: 0.999659] 
[Epoch 8/22] [Batch 2000/2300] [D loss: 0.363985] [G loss: 0.184003] [ema: 0.999660] 
[Epoch 8/22] [Batch 2100/2300] [D loss: 0.403968] [G loss: 0.196921] [ema: 0.999662] 
[Epoch 8/22] [Batch 2200/2300] [D loss: 0.377042] [G loss: 0.195240] [ema: 0.999664] 
[Epoch 9/22] [Batch 0/2300] [D loss: 0.355269] [G loss: 0.215236] [ema: 0.999665] 
[Epoch 9/22] [Batch 100/2300] [D loss: 0.361708] [G loss: 0.213511] [ema: 0.999667] 
[Epoch 9/22] [Batch 200/2300] [D loss: 0.365544] [G loss: 0.196148] [ema: 0.999668] 
[Epoch 9/22] [Batch 300/2300] [D loss: 0.381393] [G loss: 0.202985] [ema: 0.999670] 
[Epoch 9/22] [Batch 400/2300] [D loss: 0.360842] [G loss: 0.208842] [ema: 0.999672] 
[Epoch 9/22] [Batch 500/2300] [D loss: 0.341729] [G loss: 0.214239] [ema: 0.999673] 
[Epoch 9/22] [Batch 600/2300] [D loss: 0.370490] [G loss: 0.212621] [ema: 0.999675] 
[Epoch 9/22] [Batch 700/2300] [D loss: 0.402799] [G loss: 0.200787] [ema: 0.999676] 
[Epoch 9/22] [Batch 800/2300] [D loss: 0.350755] [G loss: 0.185517] [ema: 0.999678] 
[Epoch 9/22] [Batch 900/2300] [D loss: 0.330373] [G loss: 0.186873] [ema: 0.999679] 
[Epoch 9/22] [Batch 1000/2300] [D loss: 0.351915] [G loss: 0.217424] [ema: 0.999681] 
[Epoch 9/22] [Batch 1100/2300] [D loss: 0.339504] [G loss: 0.215401] [ema: 0.999682] 
[Epoch 9/22] [Batch 1200/2300] [D loss: 0.384727] [G loss: 0.184825] [ema: 0.999684] 
[Epoch 9/22] [Batch 1300/2300] [D loss: 0.342484] [G loss: 0.196690] [ema: 0.999685] 
[Epoch 9/22] [Batch 1400/2300] [D loss: 0.455999] [G loss: 0.205284] [ema: 0.999686] 
[Epoch 9/22] [Batch 1500/2300] [D loss: 0.365238] [G loss: 0.206553] [ema: 0.999688] 
[Epoch 9/22] [Batch 1600/2300] [D loss: 0.309445] [G loss: 0.203074] [ema: 0.999689] 
[Epoch 9/22] [Batch 1700/2300] [D loss: 0.345039] [G loss: 0.204833] [ema: 0.999691] 
[Epoch 9/22] [Batch 1800/2300] [D loss: 0.327638] [G loss: 0.194283] [ema: 0.999692] 
[Epoch 9/22] [Batch 1900/2300] [D loss: 0.344716] [G loss: 0.202439] [ema: 0.999693] 
[Epoch 9/22] [Batch 2000/2300] [D loss: 0.408759] [G loss: 0.204759] [ema: 0.999695] 
[Epoch 9/22] [Batch 2100/2300] [D loss: 0.347280] [G loss: 0.206615] [ema: 0.999696] 
[Epoch 9/22] [Batch 2200/2300] [D loss: 0.368569] [G loss: 0.213875] [ema: 0.999697] 
[Epoch 10/22] [Batch 0/2300] [D loss: 0.376867] [G loss: 0.212825] [ema: 0.999699] 
[Epoch 10/22] [Batch 100/2300] [D loss: 0.373662] [G loss: 0.189404] [ema: 0.999700] 
[Epoch 10/22] [Batch 200/2300] [D loss: 0.360425] [G loss: 0.206017] [ema: 0.999701] 
[Epoch 10/22] [Batch 300/2300] [D loss: 0.373984] [G loss: 0.166287] [ema: 0.999703] 
[Epoch 10/22] [Batch 400/2300] [D loss: 0.390009] [G loss: 0.210708] [ema: 0.999704] 
[Epoch 10/22] [Batch 500/2300] [D loss: 0.361392] [G loss: 0.208622] [ema: 0.999705] 
[Epoch 10/22] [Batch 600/2300] [D loss: 0.392709] [G loss: 0.195200] [ema: 0.999706] 
[Epoch 10/22] [Batch 700/2300] [D loss: 0.406673] [G loss: 0.200372] [ema: 0.999708] 
[Epoch 10/22] [Batch 800/2300] [D loss: 0.382058] [G loss: 0.195854] [ema: 0.999709] 
[Epoch 10/22] [Batch 900/2300] [D loss: 0.376260] [G loss: 0.185909] [ema: 0.999710] 
[Epoch 10/22] [Batch 1000/2300] [D loss: 0.351631] [G loss: 0.179009] [ema: 0.999711] 
[Epoch 10/22] [Batch 1100/2300] [D loss: 0.370374] [G loss: 0.188755] [ema: 0.999712] 
[Epoch 10/22] [Batch 1200/2300] [D loss: 0.365414] [G loss: 0.191265] [ema: 0.999714] 
[Epoch 10/22] [Batch 1300/2300] [D loss: 0.380773] [G loss: 0.196886] [ema: 0.999715] 
[Epoch 10/22] [Batch 1400/2300] [D loss: 0.406391] [G loss: 0.199809] [ema: 0.999716] 
[Epoch 10/22] [Batch 1500/2300] [D loss: 0.345044] [G loss: 0.204986] [ema: 0.999717] 
[Epoch 10/22] [Batch 1600/2300] [D loss: 0.371513] [G loss: 0.205364] [ema: 0.999718] 
[Epoch 10/22] [Batch 1700/2300] [D loss: 0.411582] [G loss: 0.187107] [ema: 0.999719] 
[Epoch 10/22] [Batch 1800/2300] [D loss: 0.383032] [G loss: 0.179619] [ema: 0.999721] 
[Epoch 10/22] [Batch 1900/2300] [D loss: 0.426760] [G loss: 0.203154] [ema: 0.999722] 
[Epoch 10/22] [Batch 2000/2300] [D loss: 0.428046] [G loss: 0.176561] [ema: 0.999723] 
[Epoch 10/22] [Batch 2100/2300] [D loss: 0.416223] [G loss: 0.166539] [ema: 0.999724] 
[Epoch 10/22] [Batch 2200/2300] [D loss: 0.417534] [G loss: 0.190086] [ema: 0.999725] 
[Epoch 11/22] [Batch 0/2300] [D loss: 0.427938] [G loss: 0.190626] [ema: 0.999726] 
[Epoch 11/22] [Batch 100/2300] [D loss: 0.408082] [G loss: 0.184345] [ema: 0.999727] 
[Epoch 11/22] [Batch 200/2300] [D loss: 0.355228] [G loss: 0.187070] [ema: 0.999728] 
[Epoch 11/22] [Batch 300/2300] [D loss: 0.406407] [G loss: 0.176882] [ema: 0.999729] 
[Epoch 11/22] [Batch 400/2300] [D loss: 0.352188] [G loss: 0.182800] [ema: 0.999730] 
[Epoch 11/22] [Batch 500/2300] [D loss: 0.375889] [G loss: 0.198836] [ema: 0.999731] 
[Epoch 11/22] [Batch 600/2300] [D loss: 0.394274] [G loss: 0.175669] [ema: 0.999732] 
[Epoch 11/22] [Batch 700/2300] [D loss: 0.358642] [G loss: 0.187596] [ema: 0.999733] 
[Epoch 11/22] [Batch 800/2300] [D loss: 0.413031] [G loss: 0.176526] [ema: 0.999734] 
[Epoch 11/22] [Batch 900/2300] [D loss: 0.439269] [G loss: 0.179735] [ema: 0.999735] 
[Epoch 11/22] [Batch 1000/2300] [D loss: 0.403595] [G loss: 0.163034] [ema: 0.999736] 
[Epoch 11/22] [Batch 1100/2300] [D loss: 0.369514] [G loss: 0.178661] [ema: 0.999737] 
[Epoch 11/22] [Batch 1200/2300] [D loss: 0.382558] [G loss: 0.184769] [ema: 0.999738] 
[Epoch 11/22] [Batch 1300/2300] [D loss: 0.403500] [G loss: 0.186605] [ema: 0.999739] 
[Epoch 11/22] [Batch 1400/2300] [D loss: 0.385371] [G loss: 0.186638] [ema: 0.999740] 
[Epoch 11/22] [Batch 1500/2300] [D loss: 0.396267] [G loss: 0.176317] [ema: 0.999741] 
[Epoch 11/22] [Batch 1600/2300] [D loss: 0.414224] [G loss: 0.164912] [ema: 0.999742] 
[Epoch 11/22] [Batch 1700/2300] [D loss: 0.413817] [G loss: 0.172652] [ema: 0.999743] 
[Epoch 11/22] [Batch 1800/2300] [D loss: 0.394709] [G loss: 0.167224] [ema: 0.999744] 
[Epoch 11/22] [Batch 1900/2300] [D loss: 0.401882] [G loss: 0.159286] [ema: 0.999745] 
[Epoch 11/22] [Batch 2000/2300] [D loss: 0.427185] [G loss: 0.164180] [ema: 0.999746] 
[Epoch 11/22] [Batch 2100/2300] [D loss: 0.438034] [G loss: 0.174012] [ema: 0.999747] 
[Epoch 11/22] [Batch 2200/2300] [D loss: 0.404218] [G loss: 0.183016] [ema: 0.999748] 



Saving checkpoint 3 in logs/daghar_all_50000_3axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_3axis_2024_10_30_18_51_41/Model



[Epoch 12/22] [Batch 0/2300] [D loss: 0.414245] [G loss: 0.151527] [ema: 0.999749] 
[Epoch 12/22] [Batch 100/2300] [D loss: 0.410433] [G loss: 0.164538] [ema: 0.999750] 
[Epoch 12/22] [Batch 200/2300] [D loss: 0.426016] [G loss: 0.179474] [ema: 0.999751] 
[Epoch 12/22] [Batch 300/2300] [D loss: 0.422357] [G loss: 0.174954] [ema: 0.999752] 
[Epoch 12/22] [Batch 400/2300] [D loss: 0.369932] [G loss: 0.181370] [ema: 0.999752] 
[Epoch 12/22] [Batch 500/2300] [D loss: 0.405202] [G loss: 0.168070] [ema: 0.999753] 
[Epoch 12/22] [Batch 600/2300] [D loss: 0.433966] [G loss: 0.172115] [ema: 0.999754] 
[Epoch 12/22] [Batch 700/2300] [D loss: 0.381393] [G loss: 0.185571] [ema: 0.999755] 
[Epoch 12/22] [Batch 800/2300] [D loss: 0.377037] [G loss: 0.172304] [ema: 0.999756] 
[Epoch 12/22] [Batch 900/2300] [D loss: 0.419359] [G loss: 0.175957] [ema: 0.999757] 
[Epoch 12/22] [Batch 1000/2300] [D loss: 0.433449] [G loss: 0.168940] [ema: 0.999758] 
[Epoch 12/22] [Batch 1100/2300] [D loss: 0.439033] [G loss: 0.188557] [ema: 0.999759] 
[Epoch 12/22] [Batch 1200/2300] [D loss: 0.413000] [G loss: 0.174688] [ema: 0.999759] 
[Epoch 12/22] [Batch 1300/2300] [D loss: 0.406623] [G loss: 0.183900] [ema: 0.999760] 
[Epoch 12/22] [Batch 1400/2300] [D loss: 0.388903] [G loss: 0.181192] [ema: 0.999761] 
[Epoch 12/22] [Batch 1500/2300] [D loss: 0.444442] [G loss: 0.169384] [ema: 0.999762] 
[Epoch 12/22] [Batch 1600/2300] [D loss: 0.373187] [G loss: 0.180954] [ema: 0.999763] 
[Epoch 12/22] [Batch 1700/2300] [D loss: 0.395556] [G loss: 0.180356] [ema: 0.999763] 
[Epoch 12/22] [Batch 1800/2300] [D loss: 0.444163] [G loss: 0.164340] [ema: 0.999764] 
[Epoch 12/22] [Batch 1900/2300] [D loss: 0.375478] [G loss: 0.184812] [ema: 0.999765] 
[Epoch 12/22] [Batch 2000/2300] [D loss: 0.388387] [G loss: 0.177100] [ema: 0.999766] 
[Epoch 12/22] [Batch 2100/2300] [D loss: 0.445932] [G loss: 0.163534] [ema: 0.999767] 
[Epoch 12/22] [Batch 2200/2300] [D loss: 0.408615] [G loss: 0.179015] [ema: 0.999767] 
[Epoch 13/22] [Batch 0/2300] [D loss: 0.406903] [G loss: 0.180040] [ema: 0.999768] 
[Epoch 13/22] [Batch 100/2300] [D loss: 0.367444] [G loss: 0.179495] [ema: 0.999769] 
[Epoch 13/22] [Batch 200/2300] [D loss: 0.388773] [G loss: 0.188901] [ema: 0.999770] 
[Epoch 13/22] [Batch 300/2300] [D loss: 0.425867] [G loss: 0.180543] [ema: 0.999771] 
[Epoch 13/22] [Batch 400/2300] [D loss: 0.408456] [G loss: 0.184231] [ema: 0.999771] 
[Epoch 13/22] [Batch 500/2300] [D loss: 0.385279] [G loss: 0.181687] [ema: 0.999772] 
[Epoch 13/22] [Batch 600/2300] [D loss: 0.412568] [G loss: 0.175025] [ema: 0.999773] 
[Epoch 13/22] [Batch 700/2300] [D loss: 0.421809] [G loss: 0.176017] [ema: 0.999774] 
[Epoch 13/22] [Batch 800/2300] [D loss: 0.419778] [G loss: 0.171308] [ema: 0.999774] 
[Epoch 13/22] [Batch 900/2300] [D loss: 0.387977] [G loss: 0.162924] [ema: 0.999775] 
[Epoch 13/22] [Batch 1000/2300] [D loss: 0.365515] [G loss: 0.177071] [ema: 0.999776] 
[Epoch 13/22] [Batch 1100/2300] [D loss: 0.396081] [G loss: 0.170520] [ema: 0.999776] 
[Epoch 13/22] [Batch 1200/2300] [D loss: 0.404529] [G loss: 0.187514] [ema: 0.999777] 
[Epoch 13/22] [Batch 1300/2300] [D loss: 0.401636] [G loss: 0.190709] [ema: 0.999778] 
[Epoch 13/22] [Batch 1400/2300] [D loss: 0.398641] [G loss: 0.192131] [ema: 0.999779] 
[Epoch 13/22] [Batch 1500/2300] [D loss: 0.404501] [G loss: 0.159422] [ema: 0.999779] 
[Epoch 13/22] [Batch 1600/2300] [D loss: 0.450504] [G loss: 0.156977] [ema: 0.999780] 
[Epoch 13/22] [Batch 1700/2300] [D loss: 0.362852] [G loss: 0.192412] [ema: 0.999781] 
[Epoch 13/22] [Batch 1800/2300] [D loss: 0.379867] [G loss: 0.187038] [ema: 0.999781] 
[Epoch 13/22] [Batch 1900/2300] [D loss: 0.373977] [G loss: 0.170451] [ema: 0.999782] 
[Epoch 13/22] [Batch 2000/2300] [D loss: 0.384736] [G loss: 0.165477] [ema: 0.999783] 
[Epoch 13/22] [Batch 2100/2300] [D loss: 0.373941] [G loss: 0.200131] [ema: 0.999783] 
[Epoch 13/22] [Batch 2200/2300] [D loss: 0.383430] [G loss: 0.190738] [ema: 0.999784] 
[Epoch 14/22] [Batch 0/2300] [D loss: 0.360096] [G loss: 0.186323] [ema: 0.999785] 
[Epoch 14/22] [Batch 100/2300] [D loss: 0.413641] [G loss: 0.178500] [ema: 0.999785] 
[Epoch 14/22] [Batch 200/2300] [D loss: 0.414575] [G loss: 0.178321] [ema: 0.999786] 
[Epoch 14/22] [Batch 300/2300] [D loss: 0.393935] [G loss: 0.180119] [ema: 0.999787] 
[Epoch 14/22] [Batch 400/2300] [D loss: 0.442760] [G loss: 0.183941] [ema: 0.999787] 
[Epoch 14/22] [Batch 500/2300] [D loss: 0.364885] [G loss: 0.168049] [ema: 0.999788] 
[Epoch 14/22] [Batch 600/2300] [D loss: 0.372182] [G loss: 0.172104] [ema: 0.999789] 
[Epoch 14/22] [Batch 700/2300] [D loss: 0.432740] [G loss: 0.174301] [ema: 0.999789] 
[Epoch 14/22] [Batch 800/2300] [D loss: 0.402136] [G loss: 0.169259] [ema: 0.999790] 
[Epoch 14/22] [Batch 900/2300] [D loss: 0.419217] [G loss: 0.176865] [ema: 0.999791] 
[Epoch 14/22] [Batch 1000/2300] [D loss: 0.386623] [G loss: 0.181688] [ema: 0.999791] 
[Epoch 14/22] [Batch 1100/2300] [D loss: 0.381665] [G loss: 0.191111] [ema: 0.999792] 
[Epoch 14/22] [Batch 1200/2300] [D loss: 0.390017] [G loss: 0.184734] [ema: 0.999792] 
[Epoch 14/22] [Batch 1300/2300] [D loss: 0.425081] [G loss: 0.182287] [ema: 0.999793] 
[Epoch 14/22] [Batch 1400/2300] [D loss: 0.381763] [G loss: 0.174342] [ema: 0.999794] 
[Epoch 14/22] [Batch 1500/2300] [D loss: 0.422377] [G loss: 0.196131] [ema: 0.999794] 
[Epoch 14/22] [Batch 1600/2300] [D loss: 0.421774] [G loss: 0.181399] [ema: 0.999795] 
[Epoch 14/22] [Batch 1700/2300] [D loss: 0.333230] [G loss: 0.181633] [ema: 0.999796] 
[Epoch 14/22] [Batch 1800/2300] [D loss: 0.399827] [G loss: 0.180451] [ema: 0.999796] 
[Epoch 14/22] [Batch 1900/2300] [D loss: 0.427215] [G loss: 0.168560] [ema: 0.999797] 
[Epoch 14/22] [Batch 2000/2300] [D loss: 0.378818] [G loss: 0.172376] [ema: 0.999797] 
[Epoch 14/22] [Batch 2100/2300] [D loss: 0.413762] [G loss: 0.183042] [ema: 0.999798] 
[Epoch 14/22] [Batch 2200/2300] [D loss: 0.431076] [G loss: 0.183710] [ema: 0.999799] 
[Epoch 15/22] [Batch 0/2300] [D loss: 0.441762] [G loss: 0.192442] [ema: 0.999799] 
[Epoch 15/22] [Batch 100/2300] [D loss: 0.428492] [G loss: 0.160604] [ema: 0.999800] 
[Epoch 15/22] [Batch 200/2300] [D loss: 0.423546] [G loss: 0.181489] [ema: 0.999800] 
[Epoch 15/22] [Batch 300/2300] [D loss: 0.429880] [G loss: 0.183993] [ema: 0.999801] 
[Epoch 15/22] [Batch 400/2300] [D loss: 0.388385] [G loss: 0.183583] [ema: 0.999801] 
[Epoch 15/22] [Batch 500/2300] [D loss: 0.365561] [G loss: 0.186960] [ema: 0.999802] 
[Epoch 15/22] [Batch 600/2300] [D loss: 0.377655] [G loss: 0.180503] [ema: 0.999803] 
[Epoch 15/22] [Batch 700/2300] [D loss: 0.384357] [G loss: 0.182859] [ema: 0.999803] 
[Epoch 15/22] [Batch 800/2300] [D loss: 0.415870] [G loss: 0.183386] [ema: 0.999804] 
[Epoch 15/22] [Batch 900/2300] [D loss: 0.385955] [G loss: 0.179190] [ema: 0.999804] 
[Epoch 15/22] [Batch 1000/2300] [D loss: 0.377976] [G loss: 0.186548] [ema: 0.999805] 
[Epoch 15/22] [Batch 1100/2300] [D loss: 0.418748] [G loss: 0.165725] [ema: 0.999805] 
[Epoch 15/22] [Batch 1200/2300] [D loss: 0.376742] [G loss: 0.180527] [ema: 0.999806] 
[Epoch 15/22] [Batch 1300/2300] [D loss: 0.440313] [G loss: 0.172972] [ema: 0.999806] 
[Epoch 15/22] [Batch 1400/2300] [D loss: 0.424879] [G loss: 0.187464] [ema: 0.999807] 
[Epoch 15/22] [Batch 1500/2300] [D loss: 0.436789] [G loss: 0.174912] [ema: 0.999807] 
[Epoch 15/22] [Batch 1600/2300] [D loss: 0.395364] [G loss: 0.174735] [ema: 0.999808] 
[Epoch 15/22] [Batch 1700/2300] [D loss: 0.354547] [G loss: 0.184381] [ema: 0.999809] 
[Epoch 15/22] [Batch 1800/2300] [D loss: 0.347355] [G loss: 0.202606] [ema: 0.999809] 
[Epoch 15/22] [Batch 1900/2300] [D loss: 0.373017] [G loss: 0.180753] [ema: 0.999810] 
[Epoch 15/22] [Batch 2000/2300] [D loss: 0.369530] [G loss: 0.186664] [ema: 0.999810] 
[Epoch 15/22] [Batch 2100/2300] [D loss: 0.413618] [G loss: 0.172851] [ema: 0.999811] 
[Epoch 15/22] [Batch 2200/2300] [D loss: 0.425219] [G loss: 0.169998] [ema: 0.999811] 
[Epoch 16/22] [Batch 0/2300] [D loss: 0.449807] [G loss: 0.184118] [ema: 0.999812] 
[Epoch 16/22] [Batch 100/2300] [D loss: 0.387088] [G loss: 0.186060] [ema: 0.999812] 
[Epoch 16/22] [Batch 200/2300] [D loss: 0.445726] [G loss: 0.160537] [ema: 0.999813] 
[Epoch 16/22] [Batch 300/2300] [D loss: 0.396159] [G loss: 0.156771] [ema: 0.999813] 
[Epoch 16/22] [Batch 400/2300] [D loss: 0.410514] [G loss: 0.195482] [ema: 0.999814] 
[Epoch 16/22] [Batch 500/2300] [D loss: 0.398385] [G loss: 0.176894] [ema: 0.999814] 
[Epoch 16/22] [Batch 600/2300] [D loss: 0.405784] [G loss: 0.169053] [ema: 0.999815] 
[Epoch 16/22] [Batch 700/2300] [D loss: 0.419010] [G loss: 0.180996] [ema: 0.999815] 
[Epoch 16/22] [Batch 800/2300] [D loss: 0.414141] [G loss: 0.177017] [ema: 0.999816] 
[Epoch 16/22] [Batch 900/2300] [D loss: 0.404377] [G loss: 0.189319] [ema: 0.999816] 
[Epoch 16/22] [Batch 1000/2300] [D loss: 0.388699] [G loss: 0.175513] [ema: 0.999817] 
[Epoch 16/22] [Batch 1100/2300] [D loss: 0.393313] [G loss: 0.180648] [ema: 0.999817] 
[Epoch 16/22] [Batch 1200/2300] [D loss: 0.355363] [G loss: 0.179149] [ema: 0.999818] 
[Epoch 16/22] [Batch 1300/2300] [D loss: 0.398646] [G loss: 0.182824] [ema: 0.999818] 
[Epoch 16/22] [Batch 1400/2300] [D loss: 0.430701] [G loss: 0.189215] [ema: 0.999819] 
[Epoch 16/22] [Batch 1500/2300] [D loss: 0.360253] [G loss: 0.186448] [ema: 0.999819] 
[Epoch 16/22] [Batch 1600/2300] [D loss: 0.393438] [G loss: 0.177415] [ema: 0.999820] 
[Epoch 16/22] [Batch 1700/2300] [D loss: 0.394517] [G loss: 0.181538] [ema: 0.999820] 
[Epoch 16/22] [Batch 1800/2300] [D loss: 0.375502] [G loss: 0.175778] [ema: 0.999820] 
[Epoch 16/22] [Batch 1900/2300] [D loss: 0.388128] [G loss: 0.189854] [ema: 0.999821] 
[Epoch 16/22] [Batch 2000/2300] [D loss: 0.342693] [G loss: 0.191822] [ema: 0.999821] 
[Epoch 16/22] [Batch 2100/2300] [D loss: 0.420387] [G loss: 0.189967] [ema: 0.999822] 
[Epoch 16/22] [Batch 2200/2300] [D loss: 0.403120] [G loss: 0.169710] [ema: 0.999822] 
[Epoch 17/22] [Batch 0/2300] [D loss: 0.421531] [G loss: 0.183649] [ema: 0.999823] 
[Epoch 17/22] [Batch 100/2300] [D loss: 0.437677] [G loss: 0.176528] [ema: 0.999823] 
[Epoch 17/22] [Batch 200/2300] [D loss: 0.445496] [G loss: 0.161609] [ema: 0.999824] 
[Epoch 17/22] [Batch 300/2300] [D loss: 0.387598] [G loss: 0.182209] [ema: 0.999824] 
[Epoch 17/22] [Batch 400/2300] [D loss: 0.335506] [G loss: 0.179492] [ema: 0.999825] 
[Epoch 17/22] [Batch 500/2300] [D loss: 0.412162] [G loss: 0.179996] [ema: 0.999825] 
[Epoch 17/22] [Batch 600/2300] [D loss: 0.392287] [G loss: 0.163957] [ema: 0.999825] 
[Epoch 17/22] [Batch 700/2300] [D loss: 0.379508] [G loss: 0.171305] [ema: 0.999826] 
[Epoch 17/22] [Batch 800/2300] [D loss: 0.368036] [G loss: 0.199280] [ema: 0.999826] 
[Epoch 17/22] [Batch 900/2300] [D loss: 0.401099] [G loss: 0.164765] [ema: 0.999827] 
[Epoch 17/22] [Batch 1000/2300] [D loss: 0.398796] [G loss: 0.184306] [ema: 0.999827] 
[Epoch 17/22] [Batch 1100/2300] [D loss: 0.371542] [G loss: 0.177666] [ema: 0.999828] 
[Epoch 17/22] [Batch 1200/2300] [D loss: 0.410977] [G loss: 0.181152] [ema: 0.999828] 
[Epoch 17/22] [Batch 1300/2300] [D loss: 0.376784] [G loss: 0.172828] [ema: 0.999828] 
[Epoch 17/22] [Batch 1400/2300] [D loss: 0.459368] [G loss: 0.169462] [ema: 0.999829] 
[Epoch 17/22] [Batch 1500/2300] [D loss: 0.394466] [G loss: 0.189202] [ema: 0.999829] 
[Epoch 17/22] [Batch 1600/2300] [D loss: 0.407693] [G loss: 0.166362] [ema: 0.999830] 
[Epoch 17/22] [Batch 1700/2300] [D loss: 0.434669] [G loss: 0.171400] [ema: 0.999830] 
[Epoch 17/22] [Batch 1800/2300] [D loss: 0.407582] [G loss: 0.188822] [ema: 0.999831] 
[Epoch 17/22] [Batch 1900/2300] [D loss: 0.405311] [G loss: 0.162073] [ema: 0.999831] 
[Epoch 17/22] [Batch 2000/2300] [D loss: 0.379049] [G loss: 0.193775] [ema: 0.999831] 
[Epoch 17/22] [Batch 2100/2300] [D loss: 0.387248] [G loss: 0.179438] [ema: 0.999832] 
[Epoch 17/22] [Batch 2200/2300] [D loss: 0.398531] [G loss: 0.167883] [ema: 0.999832] 



Saving checkpoint 4 in logs/daghar_all_50000_3axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_3axis_2024_10_30_18_51_41/Model



[Epoch 18/22] [Batch 0/2300] [D loss: 0.407107] [G loss: 0.181101] [ema: 0.999833] 
[Epoch 18/22] [Batch 100/2300] [D loss: 0.390464] [G loss: 0.186154] [ema: 0.999833] 
[Epoch 18/22] [Batch 200/2300] [D loss: 0.402606] [G loss: 0.174575] [ema: 0.999833] 
[Epoch 18/22] [Batch 300/2300] [D loss: 0.361594] [G loss: 0.181808] [ema: 0.999834] 
[Epoch 18/22] [Batch 400/2300] [D loss: 0.366368] [G loss: 0.179785] [ema: 0.999834] 
[Epoch 18/22] [Batch 500/2300] [D loss: 0.441923] [G loss: 0.161330] [ema: 0.999835] 
[Epoch 18/22] [Batch 600/2300] [D loss: 0.413692] [G loss: 0.179256] [ema: 0.999835] 
[Epoch 18/22] [Batch 700/2300] [D loss: 0.400694] [G loss: 0.183515] [ema: 0.999835] 
[Epoch 18/22] [Batch 800/2300] [D loss: 0.454120] [G loss: 0.176908] [ema: 0.999836] 
[Epoch 18/22] [Batch 900/2300] [D loss: 0.424225] [G loss: 0.182189] [ema: 0.999836] 
[Epoch 18/22] [Batch 1000/2300] [D loss: 0.444108] [G loss: 0.174209] [ema: 0.999837] 
[Epoch 18/22] [Batch 1100/2300] [D loss: 0.378669] [G loss: 0.185941] [ema: 0.999837] 
[Epoch 18/22] [Batch 1200/2300] [D loss: 0.416493] [G loss: 0.187954] [ema: 0.999837] 
[Epoch 18/22] [Batch 1300/2300] [D loss: 0.411135] [G loss: 0.187606] [ema: 0.999838] 
[Epoch 18/22] [Batch 1400/2300] [D loss: 0.407160] [G loss: 0.185007] [ema: 0.999838] 
[Epoch 18/22] [Batch 1500/2300] [D loss: 0.410864] [G loss: 0.175663] [ema: 0.999838] 
[Epoch 18/22] [Batch 1600/2300] [D loss: 0.371421] [G loss: 0.188797] [ema: 0.999839] 
[Epoch 18/22] [Batch 1700/2300] [D loss: 0.402881] [G loss: 0.173649] [ema: 0.999839] 
[Epoch 18/22] [Batch 1800/2300] [D loss: 0.394231] [G loss: 0.183509] [ema: 0.999840] 
[Epoch 18/22] [Batch 1900/2300] [D loss: 0.382629] [G loss: 0.183774] [ema: 0.999840] 
[Epoch 18/22] [Batch 2000/2300] [D loss: 0.411229] [G loss: 0.193258] [ema: 0.999840] 
[Epoch 18/22] [Batch 2100/2300] [D loss: 0.425900] [G loss: 0.172229] [ema: 0.999841] 
[Epoch 18/22] [Batch 2200/2300] [D loss: 0.474215] [G loss: 0.180775] [ema: 0.999841] 
[Epoch 19/22] [Batch 0/2300] [D loss: 0.415681] [G loss: 0.183465] [ema: 0.999841] 
[Epoch 19/22] [Batch 100/2300] [D loss: 0.379593] [G loss: 0.174909] [ema: 0.999842] 
[Epoch 19/22] [Batch 200/2300] [D loss: 0.369741] [G loss: 0.174226] [ema: 0.999842] 
[Epoch 19/22] [Batch 300/2300] [D loss: 0.406431] [G loss: 0.166389] [ema: 0.999842] 
[Epoch 19/22] [Batch 400/2300] [D loss: 0.463582] [G loss: 0.167575] [ema: 0.999843] 
[Epoch 19/22] [Batch 500/2300] [D loss: 0.415214] [G loss: 0.180676] [ema: 0.999843] 
[Epoch 19/22] [Batch 600/2300] [D loss: 0.419682] [G loss: 0.172308] [ema: 0.999844] 
[Epoch 19/22] [Batch 700/2300] [D loss: 0.408265] [G loss: 0.183861] [ema: 0.999844] 
[Epoch 19/22] [Batch 800/2300] [D loss: 0.366222] [G loss: 0.171901] [ema: 0.999844] 
[Epoch 19/22] [Batch 900/2300] [D loss: 0.403551] [G loss: 0.177450] [ema: 0.999845] 
[Epoch 19/22] [Batch 1000/2300] [D loss: 0.407473] [G loss: 0.184126] [ema: 0.999845] 
[Epoch 19/22] [Batch 1100/2300] [D loss: 0.384720] [G loss: 0.181516] [ema: 0.999845] 
[Epoch 19/22] [Batch 1200/2300] [D loss: 0.383203] [G loss: 0.180637] [ema: 0.999846] 
[Epoch 19/22] [Batch 1300/2300] [D loss: 0.453510] [G loss: 0.167453] [ema: 0.999846] 
[Epoch 19/22] [Batch 1400/2300] [D loss: 0.388310] [G loss: 0.180826] [ema: 0.999846] 
[Epoch 19/22] [Batch 1500/2300] [D loss: 0.436358] [G loss: 0.177036] [ema: 0.999847] 
[Epoch 19/22] [Batch 1600/2300] [D loss: 0.362297] [G loss: 0.167304] [ema: 0.999847] 
[Epoch 19/22] [Batch 1700/2300] [D loss: 0.419745] [G loss: 0.179039] [ema: 0.999847] 
[Epoch 19/22] [Batch 1800/2300] [D loss: 0.418013] [G loss: 0.168634] [ema: 0.999848] 
[Epoch 19/22] [Batch 1900/2300] [D loss: 0.405392] [G loss: 0.176486] [ema: 0.999848] 
[Epoch 19/22] [Batch 2000/2300] [D loss: 0.435831] [G loss: 0.181458] [ema: 0.999848] 
[Epoch 19/22] [Batch 2100/2300] [D loss: 0.398304] [G loss: 0.175355] [ema: 0.999849] 
[Epoch 19/22] [Batch 2200/2300] [D loss: 0.404770] [G loss: 0.175505] [ema: 0.999849] 
[Epoch 20/22] [Batch 0/2300] [D loss: 0.404491] [G loss: 0.172869] [ema: 0.999849] 
[Epoch 20/22] [Batch 100/2300] [D loss: 0.412505] [G loss: 0.171206] [ema: 0.999850] 
[Epoch 20/22] [Batch 200/2300] [D loss: 0.410301] [G loss: 0.173413] [ema: 0.999850] 
[Epoch 20/22] [Batch 300/2300] [D loss: 0.373768] [G loss: 0.181197] [ema: 0.999850] 
[Epoch 20/22] [Batch 400/2300] [D loss: 0.390423] [G loss: 0.184569] [ema: 0.999851] 
[Epoch 20/22] [Batch 500/2300] [D loss: 0.389166] [G loss: 0.187689] [ema: 0.999851] 
[Epoch 20/22] [Batch 600/2300] [D loss: 0.369705] [G loss: 0.190268] [ema: 0.999851] 
[Epoch 20/22] [Batch 700/2300] [D loss: 0.446520] [G loss: 0.160984] [ema: 0.999852] 
[Epoch 20/22] [Batch 800/2300] [D loss: 0.389088] [G loss: 0.182890] [ema: 0.999852] 
[Epoch 20/22] [Batch 900/2300] [D loss: 0.398361] [G loss: 0.188555] [ema: 0.999852] 
[Epoch 20/22] [Batch 1000/2300] [D loss: 0.420110] [G loss: 0.172402] [ema: 0.999853] 
[Epoch 20/22] [Batch 1100/2300] [D loss: 0.395114] [G loss: 0.167475] [ema: 0.999853] 
[Epoch 20/22] [Batch 1200/2300] [D loss: 0.386719] [G loss: 0.173983] [ema: 0.999853] 
[Epoch 20/22] [Batch 1300/2300] [D loss: 0.402692] [G loss: 0.173311] [ema: 0.999853] 
[Epoch 20/22] [Batch 1400/2300] [D loss: 0.470422] [G loss: 0.174364] [ema: 0.999854] 
[Epoch 20/22] [Batch 1500/2300] [D loss: 0.434546] [G loss: 0.177449] [ema: 0.999854] 
[Epoch 20/22] [Batch 1600/2300] [D loss: 0.413722] [G loss: 0.186360] [ema: 0.999854] 
[Epoch 20/22] [Batch 1700/2300] [D loss: 0.408324] [G loss: 0.166342] [ema: 0.999855] 
[Epoch 20/22] [Batch 1800/2300] [D loss: 0.380856] [G loss: 0.185573] [ema: 0.999855] 
[Epoch 20/22] [Batch 1900/2300] [D loss: 0.380403] [G loss: 0.188092] [ema: 0.999855] 
[Epoch 20/22] [Batch 2000/2300] [D loss: 0.393480] [G loss: 0.179584] [ema: 0.999856] 
[Epoch 20/22] [Batch 2100/2300] [D loss: 0.446326] [G loss: 0.149499] [ema: 0.999856] 
[Epoch 20/22] [Batch 2200/2300] [D loss: 0.437775] [G loss: 0.170903] [ema: 0.999856] 
[Epoch 21/22] [Batch 0/2300] [D loss: 0.430491] [G loss: 0.181536] [ema: 0.999857] 
[Epoch 21/22] [Batch 100/2300] [D loss: 0.389100] [G loss: 0.166034] [ema: 0.999857] 
[Epoch 21/22] [Batch 200/2300] [D loss: 0.395340] [G loss: 0.172591] [ema: 0.999857] 
[Epoch 21/22] [Batch 300/2300] [D loss: 0.410162] [G loss: 0.175819] [ema: 0.999857] 
[Epoch 21/22] [Batch 400/2300] [D loss: 0.391182] [G loss: 0.180077] [ema: 0.999858] 
[Epoch 21/22] [Batch 500/2300] [D loss: 0.386532] [G loss: 0.200751] [ema: 0.999858] 
[Epoch 21/22] [Batch 600/2300] [D loss: 0.396085] [G loss: 0.180344] [ema: 0.999858] 
[Epoch 21/22] [Batch 700/2300] [D loss: 0.384575] [G loss: 0.177303] [ema: 0.999859] 
[Epoch 21/22] [Batch 800/2300] [D loss: 0.380112] [G loss: 0.189015] [ema: 0.999859] 
[Epoch 21/22] [Batch 900/2300] [D loss: 0.409582] [G loss: 0.170013] [ema: 0.999859] 
[Epoch 21/22] [Batch 1000/2300] [D loss: 0.376607] [G loss: 0.186118] [ema: 0.999859] 
[Epoch 21/22] [Batch 1100/2300] [D loss: 0.407123] [G loss: 0.147488] [ema: 0.999860] 
[Epoch 21/22] [Batch 1200/2300] [D loss: 0.395992] [G loss: 0.174900] [ema: 0.999860] 
[Epoch 21/22] [Batch 1300/2300] [D loss: 0.378903] [G loss: 0.179552] [ema: 0.999860] 
[Epoch 21/22] [Batch 1400/2300] [D loss: 0.350417] [G loss: 0.175329] [ema: 0.999861] 
[Epoch 21/22] [Batch 1500/2300] [D loss: 0.423621] [G loss: 0.160663] [ema: 0.999861] 
[Epoch 21/22] [Batch 1600/2300] [D loss: 0.410179] [G loss: 0.165810] [ema: 0.999861] 
[Epoch 21/22] [Batch 1700/2300] [D loss: 0.428328] [G loss: 0.183547] [ema: 0.999861] 
[Epoch 21/22] [Batch 1800/2300] [D loss: 0.386990] [G loss: 0.163960] [ema: 0.999862] 
[Epoch 21/22] [Batch 1900/2300] [D loss: 0.393054] [G loss: 0.183848] [ema: 0.999862] 
[Epoch 21/22] [Batch 2000/2300] [D loss: 0.454948] [G loss: 0.164853] [ema: 0.999862] 
[Epoch 21/22] [Batch 2100/2300] [D loss: 0.424213] [G loss: 0.177900] [ema: 0.999862] 
[Epoch 21/22] [Batch 2200/2300] [D loss: 0.373679] [G loss: 0.167688] [ema: 0.999863] 
