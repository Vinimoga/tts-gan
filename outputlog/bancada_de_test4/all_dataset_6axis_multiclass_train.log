
 Starting training
Total of classes being trained: 1

['UCI_DAGHAR_Multiclass.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
UCI_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=300, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
UCI_DAGHAR_Multiclass
daghar
return single class data and labels, class is UCI_DAGHAR_Multiclass
data shape is (73576, 6, 1, 30)
label shape is (73576,)
4599
Epochs between checkpoint: 3



Saving checkpoint 1 in logs/daghar_all_50000_6axis_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_29_19_14_34/Model



[Epoch 0/11] [Batch 0/4599] [D loss: 4.785041] [G loss: 2.371347] [ema: 0.000000] 
[Epoch 0/11] [Batch 100/4599] [D loss: 0.379043] [G loss: 0.260091] [ema: 0.933033] 
[Epoch 0/11] [Batch 200/4599] [D loss: 0.602925] [G loss: 0.132750] [ema: 0.965936] 
[Epoch 0/11] [Batch 300/4599] [D loss: 0.435116] [G loss: 0.182221] [ema: 0.977160] 
[Epoch 0/11] [Batch 400/4599] [D loss: 0.477768] [G loss: 0.092567] [ema: 0.982821] 
[Epoch 0/11] [Batch 500/4599] [D loss: 0.419105] [G loss: 0.176563] [ema: 0.986233] 
[Epoch 0/11] [Batch 600/4599] [D loss: 0.473978] [G loss: 0.126080] [ema: 0.988514] 
[Epoch 0/11] [Batch 700/4599] [D loss: 0.509502] [G loss: 0.156474] [ema: 0.990147] 
[Epoch 0/11] [Batch 800/4599] [D loss: 0.436226] [G loss: 0.183812] [ema: 0.991373] 
[Epoch 0/11] [Batch 900/4599] [D loss: 0.432453] [G loss: 0.147125] [ema: 0.992328] 
[Epoch 0/11] [Batch 1000/4599] [D loss: 0.489716] [G loss: 0.152174] [ema: 0.993092] 
[Epoch 0/11] [Batch 1100/4599] [D loss: 0.454946] [G loss: 0.152843] [ema: 0.993718] 
[Epoch 0/11] [Batch 1200/4599] [D loss: 0.447314] [G loss: 0.177112] [ema: 0.994240] 
[Epoch 0/11] [Batch 1300/4599] [D loss: 0.448971] [G loss: 0.164688] [ema: 0.994682] 
[Epoch 0/11] [Batch 1400/4599] [D loss: 0.495052] [G loss: 0.145083] [ema: 0.995061] 
[Epoch 0/11] [Batch 1500/4599] [D loss: 0.457748] [G loss: 0.173994] [ema: 0.995390] 
[Epoch 0/11] [Batch 1600/4599] [D loss: 0.450279] [G loss: 0.146010] [ema: 0.995677] 
[Epoch 0/11] [Batch 1700/4599] [D loss: 0.477735] [G loss: 0.165942] [ema: 0.995931] 
[Epoch 0/11] [Batch 1800/4599] [D loss: 0.462005] [G loss: 0.149928] [ema: 0.996157] 
[Epoch 0/11] [Batch 1900/4599] [D loss: 0.499092] [G loss: 0.136417] [ema: 0.996359] 
[Epoch 0/11] [Batch 2000/4599] [D loss: 0.482484] [G loss: 0.169377] [ema: 0.996540] 
[Epoch 0/11] [Batch 2100/4599] [D loss: 0.472668] [G loss: 0.204147] [ema: 0.996705] 
[Epoch 0/11] [Batch 2200/4599] [D loss: 0.357023] [G loss: 0.178346] [ema: 0.996854] 
[Epoch 0/11] [Batch 2300/4599] [D loss: 0.431727] [G loss: 0.165315] [ema: 0.996991] 
[Epoch 0/11] [Batch 2400/4599] [D loss: 0.468891] [G loss: 0.149761] [ema: 0.997116] 
[Epoch 0/11] [Batch 2500/4599] [D loss: 0.364597] [G loss: 0.198360] [ema: 0.997231] 
[Epoch 0/11] [Batch 2600/4599] [D loss: 0.384910] [G loss: 0.191065] [ema: 0.997338] 
[Epoch 0/11] [Batch 2700/4599] [D loss: 0.417259] [G loss: 0.208824] [ema: 0.997436] 
[Epoch 0/11] [Batch 2800/4599] [D loss: 0.367147] [G loss: 0.165013] [ema: 0.997528] 
[Epoch 0/11] [Batch 2900/4599] [D loss: 0.401329] [G loss: 0.189151] [ema: 0.997613] 
[Epoch 0/11] [Batch 3000/4599] [D loss: 0.347561] [G loss: 0.194306] [ema: 0.997692] 
[Epoch 0/11] [Batch 3100/4599] [D loss: 0.344400] [G loss: 0.214000] [ema: 0.997767] 
[Epoch 0/11] [Batch 3200/4599] [D loss: 0.409120] [G loss: 0.173053] [ema: 0.997836] 
[Epoch 0/11] [Batch 3300/4599] [D loss: 0.412755] [G loss: 0.179337] [ema: 0.997902] 
[Epoch 0/11] [Batch 3400/4599] [D loss: 0.466331] [G loss: 0.187256] [ema: 0.997963] 
[Epoch 0/11] [Batch 3500/4599] [D loss: 0.371907] [G loss: 0.165335] [ema: 0.998022] 
[Epoch 0/11] [Batch 3600/4599] [D loss: 0.349811] [G loss: 0.188755] [ema: 0.998076] 
[Epoch 0/11] [Batch 3700/4599] [D loss: 0.438805] [G loss: 0.203224] [ema: 0.998128] 
[Epoch 0/11] [Batch 3800/4599] [D loss: 0.405598] [G loss: 0.192140] [ema: 0.998178] 
[Epoch 0/11] [Batch 3900/4599] [D loss: 0.441895] [G loss: 0.166037] [ema: 0.998224] 
[Epoch 0/11] [Batch 4000/4599] [D loss: 0.421726] [G loss: 0.189669] [ema: 0.998269] 
[Epoch 0/11] [Batch 4100/4599] [D loss: 0.434351] [G loss: 0.167339] [ema: 0.998311] 
[Epoch 0/11] [Batch 4200/4599] [D loss: 0.432508] [G loss: 0.165954] [ema: 0.998351] 
[Epoch 0/11] [Batch 4300/4599] [D loss: 0.399727] [G loss: 0.167966] [ema: 0.998389] 
[Epoch 0/11] [Batch 4400/4599] [D loss: 0.400353] [G loss: 0.170984] [ema: 0.998426] 
[Epoch 0/11] [Batch 4500/4599] [D loss: 0.452174] [G loss: 0.170580] [ema: 0.998461] 
[Epoch 1/11] [Batch 0/4599] [D loss: 0.389331] [G loss: 0.192491] [ema: 0.998494] 
[Epoch 1/11] [Batch 100/4599] [D loss: 0.421230] [G loss: 0.174823] [ema: 0.998526] 
[Epoch 1/11] [Batch 200/4599] [D loss: 0.401434] [G loss: 0.198410] [ema: 0.998557] 
[Epoch 1/11] [Batch 300/4599] [D loss: 0.431636] [G loss: 0.167853] [ema: 0.998586] 
[Epoch 1/11] [Batch 400/4599] [D loss: 0.425223] [G loss: 0.180891] [ema: 0.998614] 
[Epoch 1/11] [Batch 500/4599] [D loss: 0.373444] [G loss: 0.180869] [ema: 0.998642] 
[Epoch 1/11] [Batch 600/4599] [D loss: 0.376675] [G loss: 0.167036] [ema: 0.998668] 
[Epoch 1/11] [Batch 700/4599] [D loss: 0.429680] [G loss: 0.174692] [ema: 0.998693] 
[Epoch 1/11] [Batch 800/4599] [D loss: 0.424561] [G loss: 0.176432] [ema: 0.998717] 
[Epoch 1/11] [Batch 900/4599] [D loss: 0.400926] [G loss: 0.185105] [ema: 0.998740] 
[Epoch 1/11] [Batch 1000/4599] [D loss: 0.369420] [G loss: 0.186235] [ema: 0.998763] 
[Epoch 1/11] [Batch 1100/4599] [D loss: 0.445566] [G loss: 0.169888] [ema: 0.998784] 
[Epoch 1/11] [Batch 1200/4599] [D loss: 0.430864] [G loss: 0.161658] [ema: 0.998805] 
[Epoch 1/11] [Batch 1300/4599] [D loss: 0.362206] [G loss: 0.175602] [ema: 0.998826] 
[Epoch 1/11] [Batch 1400/4599] [D loss: 0.383589] [G loss: 0.182148] [ema: 0.998845] 
[Epoch 1/11] [Batch 1500/4599] [D loss: 0.372635] [G loss: 0.177858] [ema: 0.998864] 
[Epoch 1/11] [Batch 1600/4599] [D loss: 0.365996] [G loss: 0.168022] [ema: 0.998882] 
[Epoch 1/11] [Batch 1700/4599] [D loss: 0.418438] [G loss: 0.176181] [ema: 0.998900] 
[Epoch 1/11] [Batch 1800/4599] [D loss: 0.412202] [G loss: 0.163340] [ema: 0.998917] 
[Epoch 1/11] [Batch 1900/4599] [D loss: 0.418651] [G loss: 0.174641] [ema: 0.998934] 
[Epoch 1/11] [Batch 2000/4599] [D loss: 0.382217] [G loss: 0.195918] [ema: 0.998950] 
[Epoch 1/11] [Batch 2100/4599] [D loss: 0.401182] [G loss: 0.164088] [ema: 0.998966] 
[Epoch 1/11] [Batch 2200/4599] [D loss: 0.386803] [G loss: 0.186018] [ema: 0.998981] 
[Epoch 1/11] [Batch 2300/4599] [D loss: 0.393762] [G loss: 0.169048] [ema: 0.998996] 
[Epoch 1/11] [Batch 2400/4599] [D loss: 0.388521] [G loss: 0.174232] [ema: 0.999010] 
[Epoch 1/11] [Batch 2500/4599] [D loss: 0.425084] [G loss: 0.168629] [ema: 0.999024] 
[Epoch 1/11] [Batch 2600/4599] [D loss: 0.438726] [G loss: 0.180356] [ema: 0.999038] 
[Epoch 1/11] [Batch 2700/4599] [D loss: 0.396419] [G loss: 0.138906] [ema: 0.999051] 
[Epoch 1/11] [Batch 2800/4599] [D loss: 0.364941] [G loss: 0.175366] [ema: 0.999064] 
[Epoch 1/11] [Batch 2900/4599] [D loss: 0.355674] [G loss: 0.188180] [ema: 0.999076] 
[Epoch 1/11] [Batch 3000/4599] [D loss: 0.487336] [G loss: 0.169007] [ema: 0.999088] 
[Epoch 1/11] [Batch 3100/4599] [D loss: 0.368845] [G loss: 0.181060] [ema: 0.999100] 
[Epoch 1/11] [Batch 3200/4599] [D loss: 0.422605] [G loss: 0.172304] [ema: 0.999112] 
[Epoch 1/11] [Batch 3300/4599] [D loss: 0.406287] [G loss: 0.176172] [ema: 0.999123] 
[Epoch 1/11] [Batch 3400/4599] [D loss: 0.394921] [G loss: 0.174653] [ema: 0.999134] 
[Epoch 1/11] [Batch 3500/4599] [D loss: 0.419104] [G loss: 0.180233] [ema: 0.999145] 
[Epoch 1/11] [Batch 3600/4599] [D loss: 0.368144] [G loss: 0.188476] [ema: 0.999155] 
[Epoch 1/11] [Batch 3700/4599] [D loss: 0.429213] [G loss: 0.160905] [ema: 0.999165] 
[Epoch 1/11] [Batch 3800/4599] [D loss: 0.389031] [G loss: 0.169842] [ema: 0.999175] 
[Epoch 1/11] [Batch 3900/4599] [D loss: 0.409448] [G loss: 0.194766] [ema: 0.999185] 
[Epoch 1/11] [Batch 4000/4599] [D loss: 0.388624] [G loss: 0.167625] [ema: 0.999194] 
[Epoch 1/11] [Batch 4100/4599] [D loss: 0.393159] [G loss: 0.172762] [ema: 0.999204] 
[Epoch 1/11] [Batch 4200/4599] [D loss: 0.398160] [G loss: 0.180368] [ema: 0.999213] 
[Epoch 1/11] [Batch 4300/4599] [D loss: 0.391963] [G loss: 0.174147] [ema: 0.999221] 
[Epoch 1/11] [Batch 4400/4599] [D loss: 0.409818] [G loss: 0.176975] [ema: 0.999230] 
[Epoch 1/11] [Batch 4500/4599] [D loss: 0.420443] [G loss: 0.163632] [ema: 0.999239] 
[Epoch 2/11] [Batch 0/4599] [D loss: 0.365661] [G loss: 0.193550] [ema: 0.999247] 
[Epoch 2/11] [Batch 100/4599] [D loss: 0.404755] [G loss: 0.167129] [ema: 0.999255] 
[Epoch 2/11] [Batch 200/4599] [D loss: 0.380320] [G loss: 0.183086] [ema: 0.999263] 
[Epoch 2/11] [Batch 300/4599] [D loss: 0.379913] [G loss: 0.168038] [ema: 0.999270] 
[Epoch 2/11] [Batch 400/4599] [D loss: 0.415640] [G loss: 0.179436] [ema: 0.999278] 
[Epoch 2/11] [Batch 500/4599] [D loss: 0.421384] [G loss: 0.155759] [ema: 0.999286] 
[Epoch 2/11] [Batch 600/4599] [D loss: 0.412308] [G loss: 0.172378] [ema: 0.999293] 
[Epoch 2/11] [Batch 700/4599] [D loss: 0.424082] [G loss: 0.183426] [ema: 0.999300] 
[Epoch 2/11] [Batch 800/4599] [D loss: 0.402456] [G loss: 0.177802] [ema: 0.999307] 
[Epoch 2/11] [Batch 900/4599] [D loss: 0.402860] [G loss: 0.180821] [ema: 0.999314] 
[Epoch 2/11] [Batch 1000/4599] [D loss: 0.394169] [G loss: 0.173737] [ema: 0.999321] 
[Epoch 2/11] [Batch 1100/4599] [D loss: 0.376046] [G loss: 0.169184] [ema: 0.999327] 
[Epoch 2/11] [Batch 1200/4599] [D loss: 0.397743] [G loss: 0.175593] [ema: 0.999334] 
[Epoch 2/11] [Batch 1300/4599] [D loss: 0.396112] [G loss: 0.171636] [ema: 0.999340] 
[Epoch 2/11] [Batch 1400/4599] [D loss: 0.399476] [G loss: 0.175325] [ema: 0.999346] 
[Epoch 2/11] [Batch 1500/4599] [D loss: 0.411293] [G loss: 0.165702] [ema: 0.999352] 
[Epoch 2/11] [Batch 1600/4599] [D loss: 0.390679] [G loss: 0.173071] [ema: 0.999358] 
[Epoch 2/11] [Batch 1700/4599] [D loss: 0.391072] [G loss: 0.191645] [ema: 0.999364] 
[Epoch 2/11] [Batch 1800/4599] [D loss: 0.406534] [G loss: 0.172637] [ema: 0.999370] 
[Epoch 2/11] [Batch 1900/4599] [D loss: 0.431682] [G loss: 0.183616] [ema: 0.999376] 
[Epoch 2/11] [Batch 2000/4599] [D loss: 0.410380] [G loss: 0.180146] [ema: 0.999381] 
[Epoch 2/11] [Batch 2100/4599] [D loss: 0.406730] [G loss: 0.177405] [ema: 0.999387] 
[Epoch 2/11] [Batch 2200/4599] [D loss: 0.390661] [G loss: 0.172466] [ema: 0.999392] 
[Epoch 2/11] [Batch 2300/4599] [D loss: 0.395069] [G loss: 0.168265] [ema: 0.999397] 
[Epoch 2/11] [Batch 2400/4599] [D loss: 0.394617] [G loss: 0.169617] [ema: 0.999403] 
[Epoch 2/11] [Batch 2500/4599] [D loss: 0.354891] [G loss: 0.192001] [ema: 0.999408] 
[Epoch 2/11] [Batch 2600/4599] [D loss: 0.433226] [G loss: 0.168161] [ema: 0.999413] 
[Epoch 2/11] [Batch 2700/4599] [D loss: 0.410356] [G loss: 0.177216] [ema: 0.999418] 
[Epoch 2/11] [Batch 2800/4599] [D loss: 0.400618] [G loss: 0.175626] [ema: 0.999422] 
[Epoch 2/11] [Batch 2900/4599] [D loss: 0.409405] [G loss: 0.171329] [ema: 0.999427] 
[Epoch 2/11] [Batch 3000/4599] [D loss: 0.378820] [G loss: 0.185701] [ema: 0.999432] 
[Epoch 2/11] [Batch 3100/4599] [D loss: 0.414121] [G loss: 0.162604] [ema: 0.999437] 
[Epoch 2/11] [Batch 3200/4599] [D loss: 0.414013] [G loss: 0.168468] [ema: 0.999441] 
[Epoch 2/11] [Batch 3300/4599] [D loss: 0.396394] [G loss: 0.182775] [ema: 0.999446] 
[Epoch 2/11] [Batch 3400/4599] [D loss: 0.449332] [G loss: 0.182423] [ema: 0.999450] 
[Epoch 2/11] [Batch 3500/4599] [D loss: 0.333750] [G loss: 0.196636] [ema: 0.999454] 
[Epoch 2/11] [Batch 3600/4599] [D loss: 0.403693] [G loss: 0.173957] [ema: 0.999459] 
[Epoch 2/11] [Batch 3700/4599] [D loss: 0.374174] [G loss: 0.190403] [ema: 0.999463] 
[Epoch 2/11] [Batch 3800/4599] [D loss: 0.454286] [G loss: 0.176352] [ema: 0.999467] 
[Epoch 2/11] [Batch 3900/4599] [D loss: 0.423139] [G loss: 0.161939] [ema: 0.999471] 
[Epoch 2/11] [Batch 4000/4599] [D loss: 0.392184] [G loss: 0.180886] [ema: 0.999475] 
[Epoch 2/11] [Batch 4100/4599] [D loss: 0.377348] [G loss: 0.186792] [ema: 0.999479] 
[Epoch 2/11] [Batch 4200/4599] [D loss: 0.435269] [G loss: 0.161725] [ema: 0.999483] 
[Epoch 2/11] [Batch 4300/4599] [D loss: 0.375838] [G loss: 0.176276] [ema: 0.999487] 
[Epoch 2/11] [Batch 4400/4599] [D loss: 0.406519] [G loss: 0.192767] [ema: 0.999490] 
[Epoch 2/11] [Batch 4500/4599] [D loss: 0.441712] [G loss: 0.171211] [ema: 0.999494] 



Saving checkpoint 2 in logs/daghar_all_50000_6axis_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_29_19_14_34/Model



[Epoch 3/11] [Batch 0/4599] [D loss: 0.416917] [G loss: 0.169515] [ema: 0.999498] 
[Epoch 3/11] [Batch 100/4599] [D loss: 0.398802] [G loss: 0.191692] [ema: 0.999501] 
[Epoch 3/11] [Batch 200/4599] [D loss: 0.398392] [G loss: 0.174346] [ema: 0.999505] 
[Epoch 3/11] [Batch 300/4599] [D loss: 0.355602] [G loss: 0.201500] [ema: 0.999508] 
[Epoch 3/11] [Batch 400/4599] [D loss: 0.408210] [G loss: 0.177853] [ema: 0.999512] 
[Epoch 3/11] [Batch 500/4599] [D loss: 0.406124] [G loss: 0.184749] [ema: 0.999515] 
[Epoch 3/11] [Batch 600/4599] [D loss: 0.412249] [G loss: 0.170537] [ema: 0.999519] 
[Epoch 3/11] [Batch 700/4599] [D loss: 0.409267] [G loss: 0.176327] [ema: 0.999522] 
[Epoch 3/11] [Batch 800/4599] [D loss: 0.395761] [G loss: 0.171818] [ema: 0.999525] 
[Epoch 3/11] [Batch 900/4599] [D loss: 0.391772] [G loss: 0.184427] [ema: 0.999528] 
[Epoch 3/11] [Batch 1000/4599] [D loss: 0.366887] [G loss: 0.190579] [ema: 0.999532] 
[Epoch 3/11] [Batch 1100/4599] [D loss: 0.378825] [G loss: 0.172143] [ema: 0.999535] 
[Epoch 3/11] [Batch 1200/4599] [D loss: 0.385448] [G loss: 0.184292] [ema: 0.999538] 
[Epoch 3/11] [Batch 1300/4599] [D loss: 0.430395] [G loss: 0.171030] [ema: 0.999541] 
[Epoch 3/11] [Batch 1400/4599] [D loss: 0.445958] [G loss: 0.181173] [ema: 0.999544] 
[Epoch 3/11] [Batch 1500/4599] [D loss: 0.381773] [G loss: 0.165833] [ema: 0.999547] 
[Epoch 3/11] [Batch 1600/4599] [D loss: 0.425203] [G loss: 0.185985] [ema: 0.999550] 
[Epoch 3/11] [Batch 1700/4599] [D loss: 0.422612] [G loss: 0.171030] [ema: 0.999553] 
[Epoch 3/11] [Batch 1800/4599] [D loss: 0.389519] [G loss: 0.170703] [ema: 0.999556] 
[Epoch 3/11] [Batch 1900/4599] [D loss: 0.413230] [G loss: 0.168398] [ema: 0.999559] 
[Epoch 3/11] [Batch 2000/4599] [D loss: 0.371933] [G loss: 0.172747] [ema: 0.999561] 
[Epoch 3/11] [Batch 2100/4599] [D loss: 0.372772] [G loss: 0.180075] [ema: 0.999564] 
[Epoch 3/11] [Batch 2200/4599] [D loss: 0.431291] [G loss: 0.174356] [ema: 0.999567] 
[Epoch 3/11] [Batch 2300/4599] [D loss: 0.394815] [G loss: 0.173174] [ema: 0.999569] 
[Epoch 3/11] [Batch 2400/4599] [D loss: 0.388276] [G loss: 0.181380] [ema: 0.999572] 
[Epoch 3/11] [Batch 2500/4599] [D loss: 0.418308] [G loss: 0.183320] [ema: 0.999575] 
[Epoch 3/11] [Batch 2600/4599] [D loss: 0.404223] [G loss: 0.177097] [ema: 0.999577] 
[Epoch 3/11] [Batch 2700/4599] [D loss: 0.420042] [G loss: 0.192443] [ema: 0.999580] 
[Epoch 3/11] [Batch 2800/4599] [D loss: 0.385658] [G loss: 0.185973] [ema: 0.999582] 
[Epoch 3/11] [Batch 2900/4599] [D loss: 0.380879] [G loss: 0.171496] [ema: 0.999585] 
[Epoch 3/11] [Batch 3000/4599] [D loss: 0.399115] [G loss: 0.162507] [ema: 0.999587] 
[Epoch 3/11] [Batch 3100/4599] [D loss: 0.455602] [G loss: 0.176146] [ema: 0.999590] 
[Epoch 3/11] [Batch 3200/4599] [D loss: 0.406715] [G loss: 0.177866] [ema: 0.999592] 
[Epoch 3/11] [Batch 3300/4599] [D loss: 0.375750] [G loss: 0.186264] [ema: 0.999595] 
[Epoch 3/11] [Batch 3400/4599] [D loss: 0.421352] [G loss: 0.179191] [ema: 0.999597] 
[Epoch 3/11] [Batch 3500/4599] [D loss: 0.281449] [G loss: 0.242949] [ema: 0.999599] 
[Epoch 3/11] [Batch 3600/4599] [D loss: 0.381428] [G loss: 0.189435] [ema: 0.999602] 
[Epoch 3/11] [Batch 3700/4599] [D loss: 0.385931] [G loss: 0.193503] [ema: 0.999604] 
[Epoch 3/11] [Batch 3800/4599] [D loss: 0.420952] [G loss: 0.168361] [ema: 0.999606] 
[Epoch 3/11] [Batch 3900/4599] [D loss: 0.404661] [G loss: 0.180914] [ema: 0.999608] 
[Epoch 3/11] [Batch 4000/4599] [D loss: 0.444831] [G loss: 0.185254] [ema: 0.999611] 
[Epoch 3/11] [Batch 4100/4599] [D loss: 0.427114] [G loss: 0.185094] [ema: 0.999613] 
[Epoch 3/11] [Batch 4200/4599] [D loss: 0.426343] [G loss: 0.183391] [ema: 0.999615] 
[Epoch 3/11] [Batch 4300/4599] [D loss: 0.396287] [G loss: 0.188527] [ema: 0.999617] 
[Epoch 3/11] [Batch 4400/4599] [D loss: 0.412942] [G loss: 0.181303] [ema: 0.999619] 
[Epoch 3/11] [Batch 4500/4599] [D loss: 0.391321] [G loss: 0.182222] [ema: 0.999621] 
[Epoch 4/11] [Batch 0/4599] [D loss: 0.374769] [G loss: 0.203092] [ema: 0.999623] 
[Epoch 4/11] [Batch 100/4599] [D loss: 0.374583] [G loss: 0.188112] [ema: 0.999625] 
[Epoch 4/11] [Batch 200/4599] [D loss: 0.362014] [G loss: 0.161622] [ema: 0.999627] 
[Epoch 4/11] [Batch 300/4599] [D loss: 0.374596] [G loss: 0.190067] [ema: 0.999629] 
[Epoch 4/11] [Batch 400/4599] [D loss: 0.362410] [G loss: 0.184360] [ema: 0.999631] 
[Epoch 4/11] [Batch 500/4599] [D loss: 0.403886] [G loss: 0.187440] [ema: 0.999633] 
[Epoch 4/11] [Batch 600/4599] [D loss: 0.403646] [G loss: 0.156363] [ema: 0.999635] 
[Epoch 4/11] [Batch 700/4599] [D loss: 0.375266] [G loss: 0.178530] [ema: 0.999637] 
[Epoch 4/11] [Batch 800/4599] [D loss: 0.373651] [G loss: 0.200245] [ema: 0.999639] 
[Epoch 4/11] [Batch 900/4599] [D loss: 0.387972] [G loss: 0.172587] [ema: 0.999641] 
[Epoch 4/11] [Batch 1000/4599] [D loss: 0.435118] [G loss: 0.183467] [ema: 0.999643] 
[Epoch 4/11] [Batch 1100/4599] [D loss: 0.345749] [G loss: 0.186184] [ema: 0.999645] 
[Epoch 4/11] [Batch 1200/4599] [D loss: 0.425443] [G loss: 0.185534] [ema: 0.999646] 
[Epoch 4/11] [Batch 1300/4599] [D loss: 0.393030] [G loss: 0.172510] [ema: 0.999648] 
[Epoch 4/11] [Batch 1400/4599] [D loss: 0.353602] [G loss: 0.186799] [ema: 0.999650] 
[Epoch 4/11] [Batch 1500/4599] [D loss: 0.402526] [G loss: 0.181044] [ema: 0.999652] 
[Epoch 4/11] [Batch 1600/4599] [D loss: 0.392111] [G loss: 0.184518] [ema: 0.999653] 
[Epoch 4/11] [Batch 1700/4599] [D loss: 0.374262] [G loss: 0.189682] [ema: 0.999655] 
[Epoch 4/11] [Batch 1800/4599] [D loss: 0.383973] [G loss: 0.185409] [ema: 0.999657] 
[Epoch 4/11] [Batch 1900/4599] [D loss: 0.397742] [G loss: 0.203549] [ema: 0.999659] 
[Epoch 4/11] [Batch 2000/4599] [D loss: 0.397704] [G loss: 0.192349] [ema: 0.999660] 
[Epoch 4/11] [Batch 2100/4599] [D loss: 0.436042] [G loss: 0.171896] [ema: 0.999662] 
[Epoch 4/11] [Batch 2200/4599] [D loss: 0.425790] [G loss: 0.183064] [ema: 0.999664] 
[Epoch 4/11] [Batch 2300/4599] [D loss: 0.418287] [G loss: 0.166536] [ema: 0.999665] 
[Epoch 4/11] [Batch 2400/4599] [D loss: 0.446609] [G loss: 0.158348] [ema: 0.999667] 
[Epoch 4/11] [Batch 2500/4599] [D loss: 0.405893] [G loss: 0.177814] [ema: 0.999668] 
[Epoch 4/11] [Batch 2600/4599] [D loss: 0.384380] [G loss: 0.179861] [ema: 0.999670] 
[Epoch 4/11] [Batch 2700/4599] [D loss: 0.377774] [G loss: 0.182190] [ema: 0.999671] 
[Epoch 4/11] [Batch 2800/4599] [D loss: 0.334264] [G loss: 0.183624] [ema: 0.999673] 
[Epoch 4/11] [Batch 2900/4599] [D loss: 0.359133] [G loss: 0.175392] [ema: 0.999675] 
[Epoch 4/11] [Batch 3000/4599] [D loss: 0.385271] [G loss: 0.181036] [ema: 0.999676] 
[Epoch 4/11] [Batch 3100/4599] [D loss: 0.392939] [G loss: 0.177614] [ema: 0.999678] 
[Epoch 4/11] [Batch 3200/4599] [D loss: 0.392989] [G loss: 0.183190] [ema: 0.999679] 
[Epoch 4/11] [Batch 3300/4599] [D loss: 0.391294] [G loss: 0.185380] [ema: 0.999681] 
[Epoch 4/11] [Batch 3400/4599] [D loss: 0.421255] [G loss: 0.160162] [ema: 0.999682] 
[Epoch 4/11] [Batch 3500/4599] [D loss: 0.376504] [G loss: 0.195017] [ema: 0.999683] 
[Epoch 4/11] [Batch 3600/4599] [D loss: 0.365488] [G loss: 0.195607] [ema: 0.999685] 
[Epoch 4/11] [Batch 3700/4599] [D loss: 0.360009] [G loss: 0.166133] [ema: 0.999686] 
[Epoch 4/11] [Batch 3800/4599] [D loss: 0.406492] [G loss: 0.181245] [ema: 0.999688] 
[Epoch 4/11] [Batch 3900/4599] [D loss: 0.394856] [G loss: 0.174334] [ema: 0.999689] 
[Epoch 4/11] [Batch 4000/4599] [D loss: 0.425944] [G loss: 0.181976] [ema: 0.999691] 
[Epoch 4/11] [Batch 4100/4599] [D loss: 0.351670] [G loss: 0.176298] [ema: 0.999692] 
[Epoch 4/11] [Batch 4200/4599] [D loss: 0.335373] [G loss: 0.189832] [ema: 0.999693] 
[Epoch 4/11] [Batch 4300/4599] [D loss: 0.401073] [G loss: 0.179939] [ema: 0.999695] 
[Epoch 4/11] [Batch 4400/4599] [D loss: 0.354491] [G loss: 0.190839] [ema: 0.999696] 
[Epoch 4/11] [Batch 4500/4599] [D loss: 0.493602] [G loss: 0.171722] [ema: 0.999697] 
[Epoch 5/11] [Batch 0/4599] [D loss: 0.404720] [G loss: 0.188083] [ema: 0.999699] 
[Epoch 5/11] [Batch 100/4599] [D loss: 0.369524] [G loss: 0.175442] [ema: 0.999700] 
[Epoch 5/11] [Batch 200/4599] [D loss: 0.404900] [G loss: 0.177956] [ema: 0.999701] 
[Epoch 5/11] [Batch 300/4599] [D loss: 0.401813] [G loss: 0.177149] [ema: 0.999702] 
[Epoch 5/11] [Batch 400/4599] [D loss: 0.390350] [G loss: 0.175635] [ema: 0.999704] 
[Epoch 5/11] [Batch 500/4599] [D loss: 0.405879] [G loss: 0.178060] [ema: 0.999705] 
[Epoch 5/11] [Batch 600/4599] [D loss: 0.401453] [G loss: 0.186321] [ema: 0.999706] 
[Epoch 5/11] [Batch 700/4599] [D loss: 0.445663] [G loss: 0.178736] [ema: 0.999708] 
[Epoch 5/11] [Batch 800/4599] [D loss: 0.400955] [G loss: 0.174335] [ema: 0.999709] 
[Epoch 5/11] [Batch 900/4599] [D loss: 0.396127] [G loss: 0.173519] [ema: 0.999710] 
[Epoch 5/11] [Batch 1000/4599] [D loss: 0.389944] [G loss: 0.170585] [ema: 0.999711] 
[Epoch 5/11] [Batch 1100/4599] [D loss: 0.420592] [G loss: 0.180062] [ema: 0.999712] 
[Epoch 5/11] [Batch 1200/4599] [D loss: 0.388124] [G loss: 0.186695] [ema: 0.999714] 
[Epoch 5/11] [Batch 1300/4599] [D loss: 0.416469] [G loss: 0.173397] [ema: 0.999715] 
[Epoch 5/11] [Batch 1400/4599] [D loss: 0.389201] [G loss: 0.170458] [ema: 0.999716] 
[Epoch 5/11] [Batch 1500/4599] [D loss: 0.392687] [G loss: 0.177636] [ema: 0.999717] 
[Epoch 5/11] [Batch 1600/4599] [D loss: 0.435520] [G loss: 0.183850] [ema: 0.999718] 
[Epoch 5/11] [Batch 1700/4599] [D loss: 0.403804] [G loss: 0.189442] [ema: 0.999719] 
[Epoch 5/11] [Batch 1800/4599] [D loss: 0.436571] [G loss: 0.178828] [ema: 0.999720] 
[Epoch 5/11] [Batch 1900/4599] [D loss: 0.417661] [G loss: 0.181398] [ema: 0.999722] 
[Epoch 5/11] [Batch 2000/4599] [D loss: 0.410967] [G loss: 0.164014] [ema: 0.999723] 
[Epoch 5/11] [Batch 2100/4599] [D loss: 0.393130] [G loss: 0.197453] [ema: 0.999724] 
[Epoch 5/11] [Batch 2200/4599] [D loss: 0.390379] [G loss: 0.189215] [ema: 0.999725] 
[Epoch 5/11] [Batch 2300/4599] [D loss: 0.385825] [G loss: 0.177248] [ema: 0.999726] 
[Epoch 5/11] [Batch 2400/4599] [D loss: 0.402551] [G loss: 0.167769] [ema: 0.999727] 
[Epoch 5/11] [Batch 2500/4599] [D loss: 0.385507] [G loss: 0.182772] [ema: 0.999728] 
[Epoch 5/11] [Batch 2600/4599] [D loss: 0.432551] [G loss: 0.181993] [ema: 0.999729] 
[Epoch 5/11] [Batch 2700/4599] [D loss: 0.387507] [G loss: 0.186624] [ema: 0.999730] 
[Epoch 5/11] [Batch 2800/4599] [D loss: 0.433902] [G loss: 0.186842] [ema: 0.999731] 
[Epoch 5/11] [Batch 2900/4599] [D loss: 0.399544] [G loss: 0.176336] [ema: 0.999732] 
[Epoch 5/11] [Batch 3000/4599] [D loss: 0.394228] [G loss: 0.188639] [ema: 0.999733] 
[Epoch 5/11] [Batch 3100/4599] [D loss: 0.379698] [G loss: 0.176732] [ema: 0.999734] 
[Epoch 5/11] [Batch 3200/4599] [D loss: 0.406848] [G loss: 0.174051] [ema: 0.999735] 
[Epoch 5/11] [Batch 3300/4599] [D loss: 0.424421] [G loss: 0.182076] [ema: 0.999736] 
[Epoch 5/11] [Batch 3400/4599] [D loss: 0.394600] [G loss: 0.177412] [ema: 0.999737] 
[Epoch 5/11] [Batch 3500/4599] [D loss: 0.431861] [G loss: 0.179570] [ema: 0.999738] 
[Epoch 5/11] [Batch 3600/4599] [D loss: 0.395659] [G loss: 0.180766] [ema: 0.999739] 
[Epoch 5/11] [Batch 3700/4599] [D loss: 0.422524] [G loss: 0.197146] [ema: 0.999740] 
[Epoch 5/11] [Batch 3800/4599] [D loss: 0.403782] [G loss: 0.192138] [ema: 0.999741] 
[Epoch 5/11] [Batch 3900/4599] [D loss: 0.431841] [G loss: 0.182855] [ema: 0.999742] 
[Epoch 5/11] [Batch 4000/4599] [D loss: 0.383204] [G loss: 0.178047] [ema: 0.999743] 
[Epoch 5/11] [Batch 4100/4599] [D loss: 0.414436] [G loss: 0.175806] [ema: 0.999744] 
[Epoch 5/11] [Batch 4200/4599] [D loss: 0.336493] [G loss: 0.202913] [ema: 0.999745] 
[Epoch 5/11] [Batch 4300/4599] [D loss: 0.448536] [G loss: 0.167360] [ema: 0.999746] 
[Epoch 5/11] [Batch 4400/4599] [D loss: 0.384250] [G loss: 0.184806] [ema: 0.999747] 
[Epoch 5/11] [Batch 4500/4599] [D loss: 0.413390] [G loss: 0.206494] [ema: 0.999748] 



Saving checkpoint 3 in logs/daghar_all_50000_6axis_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_29_19_14_34/Model



[Epoch 6/11] [Batch 0/4599] [D loss: 0.375638] [G loss: 0.191825] [ema: 0.999749] 
[Epoch 6/11] [Batch 100/4599] [D loss: 0.330194] [G loss: 0.191775] [ema: 0.999750] 
[Epoch 6/11] [Batch 200/4599] [D loss: 0.413428] [G loss: 0.164015] [ema: 0.999751] 
[Epoch 6/11] [Batch 300/4599] [D loss: 0.403466] [G loss: 0.175612] [ema: 0.999752] 
[Epoch 6/11] [Batch 400/4599] [D loss: 0.442243] [G loss: 0.192637] [ema: 0.999752] 
[Epoch 6/11] [Batch 500/4599] [D loss: 0.388959] [G loss: 0.182300] [ema: 0.999753] 
[Epoch 6/11] [Batch 600/4599] [D loss: 0.366035] [G loss: 0.198273] [ema: 0.999754] 
[Epoch 6/11] [Batch 700/4599] [D loss: 0.442693] [G loss: 0.180293] [ema: 0.999755] 
[Epoch 6/11] [Batch 800/4599] [D loss: 0.376030] [G loss: 0.189179] [ema: 0.999756] 
[Epoch 6/11] [Batch 900/4599] [D loss: 0.424987] [G loss: 0.182305] [ema: 0.999757] 
[Epoch 6/11] [Batch 1000/4599] [D loss: 0.416665] [G loss: 0.171558] [ema: 0.999758] 
[Epoch 6/11] [Batch 1100/4599] [D loss: 0.370007] [G loss: 0.175495] [ema: 0.999758] 
[Epoch 6/11] [Batch 1200/4599] [D loss: 0.409710] [G loss: 0.165014] [ema: 0.999759] 
[Epoch 6/11] [Batch 1300/4599] [D loss: 0.407334] [G loss: 0.177958] [ema: 0.999760] 
[Epoch 6/11] [Batch 1400/4599] [D loss: 0.404486] [G loss: 0.177925] [ema: 0.999761] 
[Epoch 6/11] [Batch 1500/4599] [D loss: 0.430104] [G loss: 0.180020] [ema: 0.999762] 
[Epoch 6/11] [Batch 1600/4599] [D loss: 0.397770] [G loss: 0.193504] [ema: 0.999763] 
[Epoch 6/11] [Batch 1700/4599] [D loss: 0.398015] [G loss: 0.180873] [ema: 0.999763] 
[Epoch 6/11] [Batch 1800/4599] [D loss: 0.376263] [G loss: 0.187033] [ema: 0.999764] 
[Epoch 6/11] [Batch 1900/4599] [D loss: 0.356037] [G loss: 0.175391] [ema: 0.999765] 
[Epoch 6/11] [Batch 2000/4599] [D loss: 0.373253] [G loss: 0.173127] [ema: 0.999766] 
[Epoch 6/11] [Batch 2100/4599] [D loss: 0.432877] [G loss: 0.191001] [ema: 0.999767] 
[Epoch 6/11] [Batch 2200/4599] [D loss: 0.408358] [G loss: 0.174102] [ema: 0.999767] 
[Epoch 6/11] [Batch 2300/4599] [D loss: 0.393096] [G loss: 0.179249] [ema: 0.999768] 
[Epoch 6/11] [Batch 2400/4599] [D loss: 0.380224] [G loss: 0.197020] [ema: 0.999769] 
[Epoch 6/11] [Batch 2500/4599] [D loss: 0.384977] [G loss: 0.179708] [ema: 0.999770] 
[Epoch 6/11] [Batch 2600/4599] [D loss: 0.426841] [G loss: 0.192425] [ema: 0.999770] 
[Epoch 6/11] [Batch 2700/4599] [D loss: 0.391070] [G loss: 0.177297] [ema: 0.999771] 
[Epoch 6/11] [Batch 2800/4599] [D loss: 0.391543] [G loss: 0.175388] [ema: 0.999772] 
[Epoch 6/11] [Batch 2900/4599] [D loss: 0.423385] [G loss: 0.181903] [ema: 0.999773] 
[Epoch 6/11] [Batch 3000/4599] [D loss: 0.392572] [G loss: 0.195681] [ema: 0.999773] 
[Epoch 6/11] [Batch 3100/4599] [D loss: 0.397677] [G loss: 0.185148] [ema: 0.999774] 
[Epoch 6/11] [Batch 3200/4599] [D loss: 0.367115] [G loss: 0.181924] [ema: 0.999775] 
[Epoch 6/11] [Batch 3300/4599] [D loss: 0.378612] [G loss: 0.182502] [ema: 0.999776] 
[Epoch 6/11] [Batch 3400/4599] [D loss: 0.410824] [G loss: 0.188529] [ema: 0.999776] 
[Epoch 6/11] [Batch 3500/4599] [D loss: 0.394080] [G loss: 0.188820] [ema: 0.999777] 
[Epoch 6/11] [Batch 3600/4599] [D loss: 0.364574] [G loss: 0.187359] [ema: 0.999778] 
[Epoch 6/11] [Batch 3700/4599] [D loss: 0.410993] [G loss: 0.170488] [ema: 0.999779] 
[Epoch 6/11] [Batch 3800/4599] [D loss: 0.377254] [G loss: 0.188917] [ema: 0.999779] 
[Epoch 6/11] [Batch 3900/4599] [D loss: 0.401316] [G loss: 0.177191] [ema: 0.999780] 
[Epoch 6/11] [Batch 4000/4599] [D loss: 0.414532] [G loss: 0.172700] [ema: 0.999781] 
[Epoch 6/11] [Batch 4100/4599] [D loss: 0.382963] [G loss: 0.184123] [ema: 0.999781] 
[Epoch 6/11] [Batch 4200/4599] [D loss: 0.403879] [G loss: 0.181500] [ema: 0.999782] 
[Epoch 6/11] [Batch 4300/4599] [D loss: 0.411629] [G loss: 0.173408] [ema: 0.999783] 
[Epoch 6/11] [Batch 4400/4599] [D loss: 0.402505] [G loss: 0.176457] [ema: 0.999783] 
[Epoch 6/11] [Batch 4500/4599] [D loss: 0.433793] [G loss: 0.184199] [ema: 0.999784] 
[Epoch 7/11] [Batch 0/4599] [D loss: 0.391182] [G loss: 0.183968] [ema: 0.999785] 
[Epoch 7/11] [Batch 100/4599] [D loss: 0.377062] [G loss: 0.187610] [ema: 0.999785] 
[Epoch 7/11] [Batch 200/4599] [D loss: 0.432481] [G loss: 0.188880] [ema: 0.999786] 
[Epoch 7/11] [Batch 300/4599] [D loss: 0.355549] [G loss: 0.181321] [ema: 0.999787] 
[Epoch 7/11] [Batch 400/4599] [D loss: 0.383974] [G loss: 0.190257] [ema: 0.999787] 
[Epoch 7/11] [Batch 500/4599] [D loss: 0.435923] [G loss: 0.196862] [ema: 0.999788] 
[Epoch 7/11] [Batch 600/4599] [D loss: 0.399820] [G loss: 0.181774] [ema: 0.999789] 
[Epoch 7/11] [Batch 700/4599] [D loss: 0.411010] [G loss: 0.175545] [ema: 0.999789] 
[Epoch 7/11] [Batch 800/4599] [D loss: 0.390577] [G loss: 0.176177] [ema: 0.999790] 
[Epoch 7/11] [Batch 900/4599] [D loss: 0.348266] [G loss: 0.168397] [ema: 0.999791] 
[Epoch 7/11] [Batch 1000/4599] [D loss: 0.379938] [G loss: 0.174295] [ema: 0.999791] 
[Epoch 7/11] [Batch 1100/4599] [D loss: 0.368922] [G loss: 0.175358] [ema: 0.999792] 
[Epoch 7/11] [Batch 1200/4599] [D loss: 0.396047] [G loss: 0.188443] [ema: 0.999792] 
[Epoch 7/11] [Batch 1300/4599] [D loss: 0.420103] [G loss: 0.187638] [ema: 0.999793] 
[Epoch 7/11] [Batch 1400/4599] [D loss: 0.397837] [G loss: 0.171754] [ema: 0.999794] 
[Epoch 7/11] [Batch 1500/4599] [D loss: 0.418623] [G loss: 0.184220] [ema: 0.999794] 
[Epoch 7/11] [Batch 1600/4599] [D loss: 0.418432] [G loss: 0.168642] [ema: 0.999795] 
[Epoch 7/11] [Batch 1700/4599] [D loss: 0.415803] [G loss: 0.168828] [ema: 0.999796] 
[Epoch 7/11] [Batch 1800/4599] [D loss: 0.378035] [G loss: 0.172912] [ema: 0.999796] 
[Epoch 7/11] [Batch 1900/4599] [D loss: 0.380991] [G loss: 0.196477] [ema: 0.999797] 
[Epoch 7/11] [Batch 2000/4599] [D loss: 0.367854] [G loss: 0.188594] [ema: 0.999797] 
[Epoch 7/11] [Batch 2100/4599] [D loss: 0.379408] [G loss: 0.166423] [ema: 0.999798] 
[Epoch 7/11] [Batch 2200/4599] [D loss: 0.354687] [G loss: 0.189176] [ema: 0.999798] 
[Epoch 7/11] [Batch 2300/4599] [D loss: 0.369924] [G loss: 0.184043] [ema: 0.999799] 
[Epoch 7/11] [Batch 2400/4599] [D loss: 0.414130] [G loss: 0.189743] [ema: 0.999800] 
[Epoch 7/11] [Batch 2500/4599] [D loss: 0.345511] [G loss: 0.188965] [ema: 0.999800] 
[Epoch 7/11] [Batch 2600/4599] [D loss: 0.409196] [G loss: 0.173343] [ema: 0.999801] 
[Epoch 7/11] [Batch 2700/4599] [D loss: 0.393311] [G loss: 0.170180] [ema: 0.999801] 
[Epoch 7/11] [Batch 2800/4599] [D loss: 0.419464] [G loss: 0.187509] [ema: 0.999802] 
[Epoch 7/11] [Batch 2900/4599] [D loss: 0.398625] [G loss: 0.191545] [ema: 0.999803] 
[Epoch 7/11] [Batch 3000/4599] [D loss: 0.396993] [G loss: 0.179487] [ema: 0.999803] 
[Epoch 7/11] [Batch 3100/4599] [D loss: 0.449702] [G loss: 0.184819] [ema: 0.999804] 
[Epoch 7/11] [Batch 3200/4599] [D loss: 0.349638] [G loss: 0.181966] [ema: 0.999804] 
[Epoch 7/11] [Batch 3300/4599] [D loss: 0.392686] [G loss: 0.183459] [ema: 0.999805] 
[Epoch 7/11] [Batch 3400/4599] [D loss: 0.429238] [G loss: 0.175609] [ema: 0.999805] 
[Epoch 7/11] [Batch 3500/4599] [D loss: 0.369375] [G loss: 0.187219] [ema: 0.999806] 
[Epoch 7/11] [Batch 3600/4599] [D loss: 0.449876] [G loss: 0.178638] [ema: 0.999806] 
[Epoch 7/11] [Batch 3700/4599] [D loss: 0.436492] [G loss: 0.174719] [ema: 0.999807] 
[Epoch 7/11] [Batch 3800/4599] [D loss: 0.380430] [G loss: 0.188623] [ema: 0.999807] 
[Epoch 7/11] [Batch 3900/4599] [D loss: 0.418643] [G loss: 0.175121] [ema: 0.999808] 
[Epoch 7/11] [Batch 4000/4599] [D loss: 0.386704] [G loss: 0.183492] [ema: 0.999809] 
[Epoch 7/11] [Batch 4100/4599] [D loss: 0.367274] [G loss: 0.196289] [ema: 0.999809] 
[Epoch 7/11] [Batch 4200/4599] [D loss: 0.379409] [G loss: 0.194644] [ema: 0.999810] 
[Epoch 7/11] [Batch 4300/4599] [D loss: 0.414889] [G loss: 0.159996] [ema: 0.999810] 
[Epoch 7/11] [Batch 4400/4599] [D loss: 0.384552] [G loss: 0.172978] [ema: 0.999811] 
[Epoch 7/11] [Batch 4500/4599] [D loss: 0.369955] [G loss: 0.178574] [ema: 0.999811] 
[Epoch 8/11] [Batch 0/4599] [D loss: 0.408001] [G loss: 0.181955] [ema: 0.999812] 
[Epoch 8/11] [Batch 100/4599] [D loss: 0.411216] [G loss: 0.180444] [ema: 0.999812] 
[Epoch 8/11] [Batch 200/4599] [D loss: 0.386739] [G loss: 0.194468] [ema: 0.999813] 
[Epoch 8/11] [Batch 300/4599] [D loss: 0.413725] [G loss: 0.178346] [ema: 0.999813] 
[Epoch 8/11] [Batch 400/4599] [D loss: 0.390756] [G loss: 0.178148] [ema: 0.999814] 
[Epoch 8/11] [Batch 500/4599] [D loss: 0.410339] [G loss: 0.172052] [ema: 0.999814] 
[Epoch 8/11] [Batch 600/4599] [D loss: 0.398540] [G loss: 0.186872] [ema: 0.999815] 
[Epoch 8/11] [Batch 700/4599] [D loss: 0.375166] [G loss: 0.181577] [ema: 0.999815] 
[Epoch 8/11] [Batch 800/4599] [D loss: 0.389797] [G loss: 0.176532] [ema: 0.999816] 
[Epoch 8/11] [Batch 900/4599] [D loss: 0.384404] [G loss: 0.189478] [ema: 0.999816] 
[Epoch 8/11] [Batch 1000/4599] [D loss: 0.374947] [G loss: 0.182095] [ema: 0.999817] 
[Epoch 8/11] [Batch 1100/4599] [D loss: 0.430428] [G loss: 0.164773] [ema: 0.999817] 
[Epoch 8/11] [Batch 1200/4599] [D loss: 0.379779] [G loss: 0.189092] [ema: 0.999818] 
[Epoch 8/11] [Batch 1300/4599] [D loss: 0.413004] [G loss: 0.189746] [ema: 0.999818] 
[Epoch 8/11] [Batch 1400/4599] [D loss: 0.392276] [G loss: 0.187669] [ema: 0.999819] 
[Epoch 8/11] [Batch 1500/4599] [D loss: 0.398973] [G loss: 0.188016] [ema: 0.999819] 
[Epoch 8/11] [Batch 1600/4599] [D loss: 0.406519] [G loss: 0.180313] [ema: 0.999819] 
[Epoch 8/11] [Batch 1700/4599] [D loss: 0.392016] [G loss: 0.184680] [ema: 0.999820] 
[Epoch 8/11] [Batch 1800/4599] [D loss: 0.411374] [G loss: 0.186222] [ema: 0.999820] 
[Epoch 8/11] [Batch 1900/4599] [D loss: 0.378926] [G loss: 0.177372] [ema: 0.999821] 
[Epoch 8/11] [Batch 2000/4599] [D loss: 0.413786] [G loss: 0.178640] [ema: 0.999821] 
[Epoch 8/11] [Batch 2100/4599] [D loss: 0.401515] [G loss: 0.181313] [ema: 0.999822] 
[Epoch 8/11] [Batch 2200/4599] [D loss: 0.385768] [G loss: 0.180941] [ema: 0.999822] 
[Epoch 8/11] [Batch 2300/4599] [D loss: 0.428084] [G loss: 0.170482] [ema: 0.999823] 
[Epoch 8/11] [Batch 2400/4599] [D loss: 0.371254] [G loss: 0.181438] [ema: 0.999823] 
[Epoch 8/11] [Batch 2500/4599] [D loss: 0.419491] [G loss: 0.187645] [ema: 0.999824] 
[Epoch 8/11] [Batch 2600/4599] [D loss: 0.367709] [G loss: 0.171071] [ema: 0.999824] 
[Epoch 8/11] [Batch 2700/4599] [D loss: 0.382067] [G loss: 0.186577] [ema: 0.999824] 
[Epoch 8/11] [Batch 2800/4599] [D loss: 0.473804] [G loss: 0.177993] [ema: 0.999825] 
[Epoch 8/11] [Batch 2900/4599] [D loss: 0.387013] [G loss: 0.192193] [ema: 0.999825] 
[Epoch 8/11] [Batch 3000/4599] [D loss: 0.418126] [G loss: 0.183844] [ema: 0.999826] 
[Epoch 8/11] [Batch 3100/4599] [D loss: 0.362095] [G loss: 0.191889] [ema: 0.999826] 
[Epoch 8/11] [Batch 3200/4599] [D loss: 0.365752] [G loss: 0.191522] [ema: 0.999827] 
[Epoch 8/11] [Batch 3300/4599] [D loss: 0.403890] [G loss: 0.180002] [ema: 0.999827] 
[Epoch 8/11] [Batch 3400/4599] [D loss: 0.391150] [G loss: 0.185150] [ema: 0.999828] 
[Epoch 8/11] [Batch 3500/4599] [D loss: 0.395442] [G loss: 0.175247] [ema: 0.999828] 
[Epoch 8/11] [Batch 3600/4599] [D loss: 0.390648] [G loss: 0.179120] [ema: 0.999828] 
[Epoch 8/11] [Batch 3700/4599] [D loss: 0.387784] [G loss: 0.178190] [ema: 0.999829] 
[Epoch 8/11] [Batch 3800/4599] [D loss: 0.389541] [G loss: 0.178915] [ema: 0.999829] 
[Epoch 8/11] [Batch 3900/4599] [D loss: 0.444001] [G loss: 0.173760] [ema: 0.999830] 
[Epoch 8/11] [Batch 4000/4599] [D loss: 0.447511] [G loss: 0.173153] [ema: 0.999830] 
[Epoch 8/11] [Batch 4100/4599] [D loss: 0.372300] [G loss: 0.178364] [ema: 0.999831] 
[Epoch 8/11] [Batch 4200/4599] [D loss: 0.370895] [G loss: 0.173728] [ema: 0.999831] 
[Epoch 8/11] [Batch 4300/4599] [D loss: 0.403310] [G loss: 0.174085] [ema: 0.999831] 
[Epoch 8/11] [Batch 4400/4599] [D loss: 0.412389] [G loss: 0.184302] [ema: 0.999832] 
[Epoch 8/11] [Batch 4500/4599] [D loss: 0.410792] [G loss: 0.167343] [ema: 0.999832] 



Saving checkpoint 4 in logs/daghar_all_50000_6axis_30_100/UCI_DAGHAR_Multiclass_50000_D_30_2024_10_29_19_14_34/Model



[Epoch 9/11] [Batch 0/4599] [D loss: 0.398894] [G loss: 0.192357] [ema: 0.999833] 
[Epoch 9/11] [Batch 100/4599] [D loss: 0.425517] [G loss: 0.179750] [ema: 0.999833] 
[Epoch 9/11] [Batch 200/4599] [D loss: 0.427321] [G loss: 0.184221] [ema: 0.999833] 
[Epoch 9/11] [Batch 300/4599] [D loss: 0.371381] [G loss: 0.178861] [ema: 0.999834] 
[Epoch 9/11] [Batch 400/4599] [D loss: 0.403587] [G loss: 0.165147] [ema: 0.999834] 
[Epoch 9/11] [Batch 500/4599] [D loss: 0.448367] [G loss: 0.175938] [ema: 0.999835] 
[Epoch 9/11] [Batch 600/4599] [D loss: 0.443050] [G loss: 0.184634] [ema: 0.999835] 
[Epoch 9/11] [Batch 700/4599] [D loss: 0.383315] [G loss: 0.183465] [ema: 0.999835] 
[Epoch 9/11] [Batch 800/4599] [D loss: 0.407368] [G loss: 0.177597] [ema: 0.999836] 
[Epoch 9/11] [Batch 900/4599] [D loss: 0.417465] [G loss: 0.161480] [ema: 0.999836] 
[Epoch 9/11] [Batch 1000/4599] [D loss: 0.406693] [G loss: 0.168854] [ema: 0.999837] 
[Epoch 9/11] [Batch 1100/4599] [D loss: 0.391032] [G loss: 0.167734] [ema: 0.999837] 
[Epoch 9/11] [Batch 1200/4599] [D loss: 0.422238] [G loss: 0.170692] [ema: 0.999837] 
[Epoch 9/11] [Batch 1300/4599] [D loss: 0.479136] [G loss: 0.173431] [ema: 0.999838] 
[Epoch 9/11] [Batch 1400/4599] [D loss: 0.416700] [G loss: 0.151583] [ema: 0.999838] 
[Epoch 9/11] [Batch 1500/4599] [D loss: 0.395313] [G loss: 0.166825] [ema: 0.999838] 
[Epoch 9/11] [Batch 1600/4599] [D loss: 0.400555] [G loss: 0.168441] [ema: 0.999839] 
[Epoch 9/11] [Batch 1700/4599] [D loss: 0.472718] [G loss: 0.163616] [ema: 0.999839] 
[Epoch 9/11] [Batch 1800/4599] [D loss: 0.427288] [G loss: 0.172709] [ema: 0.999840] 
[Epoch 9/11] [Batch 1900/4599] [D loss: 0.393670] [G loss: 0.183305] [ema: 0.999840] 
[Epoch 9/11] [Batch 2000/4599] [D loss: 0.396715] [G loss: 0.177083] [ema: 0.999840] 
[Epoch 9/11] [Batch 2100/4599] [D loss: 0.403539] [G loss: 0.163600] [ema: 0.999841] 
[Epoch 9/11] [Batch 2200/4599] [D loss: 0.389675] [G loss: 0.183238] [ema: 0.999841] 
[Epoch 9/11] [Batch 2300/4599] [D loss: 0.377478] [G loss: 0.172556] [ema: 0.999841] 
[Epoch 9/11] [Batch 2400/4599] [D loss: 0.399651] [G loss: 0.164224] [ema: 0.999842] 
[Epoch 9/11] [Batch 2500/4599] [D loss: 0.368873] [G loss: 0.192391] [ema: 0.999842] 
[Epoch 9/11] [Batch 2600/4599] [D loss: 0.445365] [G loss: 0.179565] [ema: 0.999842] 
[Epoch 9/11] [Batch 2700/4599] [D loss: 0.388911] [G loss: 0.175171] [ema: 0.999843] 
[Epoch 9/11] [Batch 2800/4599] [D loss: 0.407285] [G loss: 0.164607] [ema: 0.999843] 
[Epoch 9/11] [Batch 2900/4599] [D loss: 0.465470] [G loss: 0.167112] [ema: 0.999844] 
[Epoch 9/11] [Batch 3000/4599] [D loss: 0.405048] [G loss: 0.170723] [ema: 0.999844] 
[Epoch 9/11] [Batch 3100/4599] [D loss: 0.401471] [G loss: 0.175906] [ema: 0.999844] 
[Epoch 9/11] [Batch 3200/4599] [D loss: 0.404837] [G loss: 0.175036] [ema: 0.999845] 
[Epoch 9/11] [Batch 3300/4599] [D loss: 0.422460] [G loss: 0.167292] [ema: 0.999845] 
[Epoch 9/11] [Batch 3400/4599] [D loss: 0.420793] [G loss: 0.155990] [ema: 0.999845] 
[Epoch 9/11] [Batch 3500/4599] [D loss: 0.418896] [G loss: 0.181020] [ema: 0.999846] 
[Epoch 9/11] [Batch 3600/4599] [D loss: 0.417181] [G loss: 0.180764] [ema: 0.999846] 
[Epoch 9/11] [Batch 3700/4599] [D loss: 0.400518] [G loss: 0.166481] [ema: 0.999846] 
[Epoch 9/11] [Batch 3800/4599] [D loss: 0.453797] [G loss: 0.159624] [ema: 0.999847] 
[Epoch 9/11] [Batch 3900/4599] [D loss: 0.392016] [G loss: 0.173197] [ema: 0.999847] 
[Epoch 9/11] [Batch 4000/4599] [D loss: 0.379211] [G loss: 0.177378] [ema: 0.999847] 
[Epoch 9/11] [Batch 4100/4599] [D loss: 0.390908] [G loss: 0.180143] [ema: 0.999848] 
[Epoch 9/11] [Batch 4200/4599] [D loss: 0.425880] [G loss: 0.176167] [ema: 0.999848] 
[Epoch 9/11] [Batch 4300/4599] [D loss: 0.383650] [G loss: 0.175047] [ema: 0.999848] 
[Epoch 9/11] [Batch 4400/4599] [D loss: 0.409118] [G loss: 0.183678] [ema: 0.999849] 
[Epoch 9/11] [Batch 4500/4599] [D loss: 0.433692] [G loss: 0.167136] [ema: 0.999849] 
[Epoch 10/11] [Batch 0/4599] [D loss: 0.418199] [G loss: 0.179627] [ema: 0.999849] 
[Epoch 10/11] [Batch 100/4599] [D loss: 0.414714] [G loss: 0.188067] [ema: 0.999850] 
[Epoch 10/11] [Batch 200/4599] [D loss: 0.422595] [G loss: 0.172163] [ema: 0.999850] 
[Epoch 10/11] [Batch 300/4599] [D loss: 0.420340] [G loss: 0.174482] [ema: 0.999850] 
[Epoch 10/11] [Batch 400/4599] [D loss: 0.383539] [G loss: 0.176998] [ema: 0.999851] 
[Epoch 10/11] [Batch 500/4599] [D loss: 0.391775] [G loss: 0.150433] [ema: 0.999851] 
[Epoch 10/11] [Batch 600/4599] [D loss: 0.419738] [G loss: 0.169649] [ema: 0.999851] 
[Epoch 10/11] [Batch 700/4599] [D loss: 0.429882] [G loss: 0.161367] [ema: 0.999852] 
[Epoch 10/11] [Batch 800/4599] [D loss: 0.411439] [G loss: 0.171992] [ema: 0.999852] 
[Epoch 10/11] [Batch 900/4599] [D loss: 0.414319] [G loss: 0.175607] [ema: 0.999852] 
[Epoch 10/11] [Batch 1000/4599] [D loss: 0.378505] [G loss: 0.174199] [ema: 0.999853] 
[Epoch 10/11] [Batch 1100/4599] [D loss: 0.416733] [G loss: 0.174351] [ema: 0.999853] 
[Epoch 10/11] [Batch 1200/4599] [D loss: 0.375844] [G loss: 0.186952] [ema: 0.999853] 
[Epoch 10/11] [Batch 1300/4599] [D loss: 0.402539] [G loss: 0.166378] [ema: 0.999853] 
[Epoch 10/11] [Batch 1400/4599] [D loss: 0.437585] [G loss: 0.158906] [ema: 0.999854] 
[Epoch 10/11] [Batch 1500/4599] [D loss: 0.432733] [G loss: 0.165658] [ema: 0.999854] 
[Epoch 10/11] [Batch 1600/4599] [D loss: 0.402271] [G loss: 0.194817] [ema: 0.999854] 
[Epoch 10/11] [Batch 1700/4599] [D loss: 0.396476] [G loss: 0.147431] [ema: 0.999855] 
[Epoch 10/11] [Batch 1800/4599] [D loss: 0.419644] [G loss: 0.160520] [ema: 0.999855] 
[Epoch 10/11] [Batch 1900/4599] [D loss: 0.385485] [G loss: 0.171017] [ema: 0.999855] 
[Epoch 10/11] [Batch 2000/4599] [D loss: 0.416137] [G loss: 0.172545] [ema: 0.999856] 
[Epoch 10/11] [Batch 2100/4599] [D loss: 0.451926] [G loss: 0.170965] [ema: 0.999856] 
[Epoch 10/11] [Batch 2200/4599] [D loss: 0.405575] [G loss: 0.179147] [ema: 0.999856] 
[Epoch 10/11] [Batch 2300/4599] [D loss: 0.381660] [G loss: 0.182689] [ema: 0.999856] 
[Epoch 10/11] [Batch 2400/4599] [D loss: 0.393249] [G loss: 0.177881] [ema: 0.999857] 
[Epoch 10/11] [Batch 2500/4599] [D loss: 0.421450] [G loss: 0.170710] [ema: 0.999857] 
[Epoch 10/11] [Batch 2600/4599] [D loss: 0.368866] [G loss: 0.181080] [ema: 0.999857] 
[Epoch 10/11] [Batch 2700/4599] [D loss: 0.439563] [G loss: 0.162582] [ema: 0.999858] 
[Epoch 10/11] [Batch 2800/4599] [D loss: 0.415477] [G loss: 0.185310] [ema: 0.999858] 
[Epoch 10/11] [Batch 2900/4599] [D loss: 0.394724] [G loss: 0.191448] [ema: 0.999858] 
[Epoch 10/11] [Batch 3000/4599] [D loss: 0.415341] [G loss: 0.172670] [ema: 0.999859] 
[Epoch 10/11] [Batch 3100/4599] [D loss: 0.415126] [G loss: 0.181068] [ema: 0.999859] 
[Epoch 10/11] [Batch 3200/4599] [D loss: 0.455209] [G loss: 0.180603] [ema: 0.999859] 
[Epoch 10/11] [Batch 3300/4599] [D loss: 0.484028] [G loss: 0.157288] [ema: 0.999859] 
[Epoch 10/11] [Batch 3400/4599] [D loss: 0.449240] [G loss: 0.169788] [ema: 0.999860] 
[Epoch 10/11] [Batch 3500/4599] [D loss: 0.402791] [G loss: 0.171839] [ema: 0.999860] 
[Epoch 10/11] [Batch 3600/4599] [D loss: 0.384675] [G loss: 0.169071] [ema: 0.999860] 
[Epoch 10/11] [Batch 3700/4599] [D loss: 0.388689] [G loss: 0.174089] [ema: 0.999861] 
[Epoch 10/11] [Batch 3800/4599] [D loss: 0.418830] [G loss: 0.173283] [ema: 0.999861] 
[Epoch 10/11] [Batch 3900/4599] [D loss: 0.376675] [G loss: 0.174667] [ema: 0.999861] 
[Epoch 10/11] [Batch 4000/4599] [D loss: 0.434587] [G loss: 0.164193] [ema: 0.999861] 
[Epoch 10/11] [Batch 4100/4599] [D loss: 0.378696] [G loss: 0.180126] [ema: 0.999862] 
[Epoch 10/11] [Batch 4200/4599] [D loss: 0.408080] [G loss: 0.171276] [ema: 0.999862] 
[Epoch 10/11] [Batch 4300/4599] [D loss: 0.438072] [G loss: 0.174026] [ema: 0.999862] 
[Epoch 10/11] [Batch 4400/4599] [D loss: 0.409727] [G loss: 0.180712] [ema: 0.999862] 
[Epoch 10/11] [Batch 4500/4599] [D loss: 0.389012] [G loss: 0.176638] [ema: 0.999863] 
