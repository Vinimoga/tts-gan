Generator(
  (l1): Linear(in_features=100, out_features=1500, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=45, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=45, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
x_train shape is (6055, 3, 1, 150), x_test shape is (1524, 3, 1, 150)
y_train shape is (6055,), y_test shape is (1524,)
return single class data and labels, class is Jumping
train_data shape is (600, 3, 1, 150), test_data shape is (146, 3, 1, 150)
train label shape is (600,), test data shape is (146,)
x_train shape is (6055, 3, 1, 150), x_test shape is (1524, 3, 1, 150)
y_train shape is (6055,), y_test shape is (1524,)
return single class data and labels, class is Jumping
train_data shape is (600, 3, 1, 150), test_data shape is (146, 3, 1, 150)
train label shape is (600,), test data shape is (146,)
38
Epochs between ckechpoint: 2632




Saving checkpoint 1 in logs/Jumping_2024_09_24_21_34_31/Model




[Epoch 0/13158] [Batch 0/38] [D loss: 1.171687] [G loss: 0.745932] [ema: 0.000000] 
[Epoch 1/13158] [Batch 0/38] [D loss: 0.347481] [G loss: 0.370370] [ema: 0.833262] 
[Epoch 2/13158] [Batch 0/38] [D loss: 0.633231] [G loss: 0.141949] [ema: 0.912832] 
[Epoch 3/13158] [Batch 0/38] [D loss: 0.488407] [G loss: 0.138334] [ema: 0.941009] 
[Epoch 4/13158] [Batch 0/38] [D loss: 0.397249] [G loss: 0.205023] [ema: 0.955422] 
[Epoch 5/13158] [Batch 0/38] [D loss: 0.493774] [G loss: 0.168401] [ema: 0.964176] 
[Epoch 6/13158] [Batch 0/38] [D loss: 0.384908] [G loss: 0.232053] [ema: 0.970056] 
[Epoch 7/13158] [Batch 0/38] [D loss: 0.388574] [G loss: 0.223005] [ema: 0.974278] 
[Epoch 8/13158] [Batch 0/38] [D loss: 0.361521] [G loss: 0.270278] [ema: 0.977457] 
[Epoch 9/13158] [Batch 0/38] [D loss: 0.252801] [G loss: 0.233867] [ema: 0.979937] 
[Epoch 10/13158] [Batch 0/38] [D loss: 0.311875] [G loss: 0.273648] [ema: 0.981925] 
[Epoch 11/13158] [Batch 0/38] [D loss: 0.357811] [G loss: 0.223197] [ema: 0.983554] 
[Epoch 12/13158] [Batch 0/38] [D loss: 0.478019] [G loss: 0.170546] [ema: 0.984914] 
[Epoch 13/13158] [Batch 0/38] [D loss: 0.290105] [G loss: 0.230024] [ema: 0.986067] 
[Epoch 14/13158] [Batch 0/38] [D loss: 0.278203] [G loss: 0.271643] [ema: 0.987055] 
[Epoch 15/13158] [Batch 0/38] [D loss: 0.303287] [G loss: 0.260105] [ema: 0.987913] 
[Epoch 16/13158] [Batch 0/38] [D loss: 0.304661] [G loss: 0.183699] [ema: 0.988664] 
[Epoch 17/13158] [Batch 0/38] [D loss: 0.381871] [G loss: 0.187343] [ema: 0.989328] 
[Epoch 18/13158] [Batch 0/38] [D loss: 0.365097] [G loss: 0.231236] [ema: 0.989917] 
[Epoch 19/13158] [Batch 0/38] [D loss: 0.334518] [G loss: 0.228222] [ema: 0.990446] 
[Epoch 20/13158] [Batch 0/38] [D loss: 0.398253] [G loss: 0.201671] [ema: 0.990921] 
[Epoch 21/13158] [Batch 0/38] [D loss: 0.457390] [G loss: 0.162090] [ema: 0.991352] 
[Epoch 22/13158] [Batch 0/38] [D loss: 0.340510] [G loss: 0.255229] [ema: 0.991743] 
[Epoch 23/13158] [Batch 0/38] [D loss: 0.395965] [G loss: 0.199796] [ema: 0.992101] 
[Epoch 24/13158] [Batch 0/38] [D loss: 0.398336] [G loss: 0.186538] [ema: 0.992429] 
[Epoch 25/13158] [Batch 0/38] [D loss: 0.342652] [G loss: 0.246275] [ema: 0.992730] 
[Epoch 26/13158] [Batch 0/38] [D loss: 0.363180] [G loss: 0.192408] [ema: 0.993009] 
[Epoch 27/13158] [Batch 0/38] [D loss: 0.375363] [G loss: 0.232302] [ema: 0.993267] 
[Epoch 28/13158] [Batch 0/38] [D loss: 0.369088] [G loss: 0.215036] [ema: 0.993507] 
[Epoch 29/13158] [Batch 0/38] [D loss: 0.336410] [G loss: 0.202339] [ema: 0.993730] 
[Epoch 30/13158] [Batch 0/38] [D loss: 0.344988] [G loss: 0.232019] [ema: 0.993938] 
[Epoch 31/13158] [Batch 0/38] [D loss: 0.393552] [G loss: 0.241479] [ema: 0.994133] 
[Epoch 32/13158] [Batch 0/38] [D loss: 0.402575] [G loss: 0.238144] [ema: 0.994316] 
[Epoch 33/13158] [Batch 0/38] [D loss: 0.445455] [G loss: 0.204812] [ema: 0.994488] 
[Epoch 34/13158] [Batch 0/38] [D loss: 0.381475] [G loss: 0.159225] [ema: 0.994649] 
[Epoch 35/13158] [Batch 0/38] [D loss: 0.450809] [G loss: 0.168666] [ema: 0.994802] 
[Epoch 36/13158] [Batch 0/38] [D loss: 0.441485] [G loss: 0.151223] [ema: 0.994946] 
[Epoch 37/13158] [Batch 0/38] [D loss: 0.410378] [G loss: 0.180306] [ema: 0.995082] 
[Epoch 38/13158] [Batch 0/38] [D loss: 0.459620] [G loss: 0.179543] [ema: 0.995211] 
[Epoch 39/13158] [Batch 0/38] [D loss: 0.492325] [G loss: 0.205358] [ema: 0.995334] 
[Epoch 40/13158] [Batch 0/38] [D loss: 0.495324] [G loss: 0.164059] [ema: 0.995450] 
[Epoch 41/13158] [Batch 0/38] [D loss: 0.450360] [G loss: 0.231619] [ema: 0.995561] 
[Epoch 42/13158] [Batch 0/38] [D loss: 0.496002] [G loss: 0.175716] [ema: 0.995666] 
[Epoch 43/13158] [Batch 0/38] [D loss: 0.393045] [G loss: 0.208899] [ema: 0.995767] 
[Epoch 44/13158] [Batch 0/38] [D loss: 0.457650] [G loss: 0.202598] [ema: 0.995863] 
[Epoch 45/13158] [Batch 0/38] [D loss: 0.387737] [G loss: 0.198090] [ema: 0.995955] 
[Epoch 46/13158] [Batch 0/38] [D loss: 0.542862] [G loss: 0.172148] [ema: 0.996042] 
[Epoch 47/13158] [Batch 0/38] [D loss: 0.389340] [G loss: 0.163463] [ema: 0.996127] 
[Epoch 48/13158] [Batch 0/38] [D loss: 0.455619] [G loss: 0.182786] [ema: 0.996207] 
[Epoch 49/13158] [Batch 0/38] [D loss: 0.393133] [G loss: 0.216747] [ema: 0.996284] 
[Epoch 50/13158] [Batch 0/38] [D loss: 0.475973] [G loss: 0.212567] [ema: 0.996359] 
[Epoch 51/13158] [Batch 0/38] [D loss: 0.319775] [G loss: 0.249106] [ema: 0.996430] 
[Epoch 52/13158] [Batch 0/38] [D loss: 0.422532] [G loss: 0.167453] [ema: 0.996498] 
[Epoch 53/13158] [Batch 0/38] [D loss: 0.352223] [G loss: 0.164564] [ema: 0.996564] 
[Epoch 54/13158] [Batch 0/38] [D loss: 0.362771] [G loss: 0.226480] [ema: 0.996628] 
[Epoch 55/13158] [Batch 0/38] [D loss: 0.445088] [G loss: 0.209011] [ema: 0.996689] 
[Epoch 56/13158] [Batch 0/38] [D loss: 0.547505] [G loss: 0.149245] [ema: 0.996748] 
[Epoch 57/13158] [Batch 0/38] [D loss: 0.393527] [G loss: 0.226640] [ema: 0.996805] 
[Epoch 58/13158] [Batch 0/38] [D loss: 0.397949] [G loss: 0.170911] [ema: 0.996860] 
[Epoch 59/13158] [Batch 0/38] [D loss: 0.507110] [G loss: 0.148638] [ema: 0.996913] 
[Epoch 60/13158] [Batch 0/38] [D loss: 0.360586] [G loss: 0.231088] [ema: 0.996964] 
[Epoch 61/13158] [Batch 0/38] [D loss: 0.378651] [G loss: 0.223321] [ema: 0.997014] 
[Epoch 62/13158] [Batch 0/38] [D loss: 0.436624] [G loss: 0.193853] [ema: 0.997062] 
[Epoch 63/13158] [Batch 0/38] [D loss: 0.384376] [G loss: 0.235787] [ema: 0.997109] 
[Epoch 64/13158] [Batch 0/38] [D loss: 0.418175] [G loss: 0.198416] [ema: 0.997154] 
[Epoch 65/13158] [Batch 0/38] [D loss: 0.399810] [G loss: 0.194604] [ema: 0.997198] 
[Epoch 66/13158] [Batch 0/38] [D loss: 0.392224] [G loss: 0.157753] [ema: 0.997240] 
[Epoch 67/13158] [Batch 0/38] [D loss: 0.326954] [G loss: 0.244704] [ema: 0.997281] 
