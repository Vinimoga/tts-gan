
 Starting training
Total of classes being trained: 1

['UCI_DAGHAR_Multiclass.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
UCI_DAGHAR_Multiclass training
Data path: DAGHAR_split_25_10_all/train/data/UCI_DAGHAR_Multiclass.csv
Label path: DAGHAR_split_25_10_all/train/label/UCI_Label_Multiclass.csv
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): RearrangeLayer()
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): ReduceLayer()
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): RearrangeLayer()
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): ReduceLayer()
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
UCI_DAGHAR_Multiclass
daghar
return single class data and labels, class is UCI_DAGHAR_Multiclass
data shape is (36788, 6, 1, 60)
label shape is (36788,)
2300
Epochs between checkpoint: 6



Saving checkpoint 1 in logs/daghar_all_50000_6axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_6axis_2024_12_02_23_01_31/Model



lsgan
[Epoch 0/22] [Batch 0/2300] [D loss: 2.050287] [G loss: 0.614858] [ema: 0.000000] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 100/2300] [D loss: 0.449725] [G loss: 0.157625] [ema: 0.933033] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 200/2300] [D loss: 0.508764] [G loss: 0.186840] [ema: 0.965936] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 300/2300] [D loss: 0.510328] [G loss: 0.160112] [ema: 0.977160] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 400/2300] [D loss: 0.519461] [G loss: 0.130978] [ema: 0.982821] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 500/2300] [D loss: 0.549101] [G loss: 0.172424] [ema: 0.986233] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 600/2300] [D loss: 0.426616] [G loss: 0.159634] [ema: 0.988514] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 700/2300] [D loss: 0.424991] [G loss: 0.150416] [ema: 0.990147] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 800/2300] [D loss: 0.457013] [G loss: 0.154592] [ema: 0.991373] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 900/2300] [D loss: 0.375618] [G loss: 0.175270] [ema: 0.992328] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1000/2300] [D loss: 0.476070] [G loss: 0.128207] [ema: 0.993092] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1100/2300] [D loss: 0.392024] [G loss: 0.176671] [ema: 0.993718] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1200/2300] [D loss: 0.487419] [G loss: 0.139316] [ema: 0.994240] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1300/2300] [D loss: 0.478691] [G loss: 0.131491] [ema: 0.994682] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1400/2300] [D loss: 0.504347] [G loss: 0.144728] [ema: 0.995061] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1500/2300] [D loss: 0.441530] [G loss: 0.178970] [ema: 0.995390] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1600/2300] [D loss: 0.421127] [G loss: 0.166580] [ema: 0.995677] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1700/2300] [D loss: 0.452206] [G loss: 0.140890] [ema: 0.995931] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1800/2300] [D loss: 0.436252] [G loss: 0.174104] [ema: 0.996157] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 1900/2300] [D loss: 0.491750] [G loss: 0.173629] [ema: 0.996359] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 2000/2300] [D loss: 0.415139] [G loss: 0.180941] [ema: 0.996540] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 2100/2300] [D loss: 0.452300] [G loss: 0.157675] [ema: 0.996705] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 0/22] [Batch 2200/2300] [D loss: 0.374291] [G loss: 0.171358] [ema: 0.996854] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
[Epoch 1/22] [Batch 0/2300] [D loss: 0.460576] [G loss: 0.167030] [ema: 0.996991] 
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
lsgan
