
 Starting training
Total of classes being trained: 6

['MotionSense_DAGHAR_Multiclass.csv', 'RealWorld_thigh_DAGHAR_Multiclass.csv', 'WISDM_DAGHAR_Multiclass.csv', 'UCI_DAGHAR_Multiclass.csv', 'RealWorld_waist_DAGHAR_Multiclass.csv', 'KuHar_DAGHAR_Multiclass.csv']
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------

 Starting individual training
MotionSense_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
MotionSense_DAGHAR_Multiclass
daghar
return single class data and labels, class is MotionSense_DAGHAR_Multiclass
data shape is (3558, 6, 1, 60)
label shape is (3558,)
223
Epochs between checkpoint: 57



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_60_100/MotionSense_DAGHAR_Multiclass_50000_D_60_2024_10_30_00_07_49/Model



[Epoch 0/225] [Batch 0/223] [D loss: 1.087165] [G loss: 0.598000] [ema: 0.000000] 
[Epoch 0/225] [Batch 100/223] [D loss: 0.409917] [G loss: 0.188006] [ema: 0.933033] 
[Epoch 0/225] [Batch 200/223] [D loss: 0.528425] [G loss: 0.184281] [ema: 0.965936] 
[Epoch 1/225] [Batch 0/223] [D loss: 0.493824] [G loss: 0.200502] [ema: 0.969395] 
[Epoch 1/225] [Batch 100/223] [D loss: 0.452344] [G loss: 0.170815] [ema: 0.978769] 
[Epoch 1/225] [Batch 200/223] [D loss: 0.473828] [G loss: 0.152017] [ema: 0.983747] 
[Epoch 2/225] [Batch 0/223] [D loss: 0.481673] [G loss: 0.175771] [ema: 0.984579] 
[Epoch 2/225] [Batch 100/223] [D loss: 0.502265] [G loss: 0.167469] [ema: 0.987385] 
[Epoch 2/225] [Batch 200/223] [D loss: 0.433141] [G loss: 0.151987] [ema: 0.989328] 
[Epoch 3/225] [Batch 0/223] [D loss: 0.431321] [G loss: 0.171914] [ema: 0.989693] 
[Epoch 3/225] [Batch 100/223] [D loss: 0.478765] [G loss: 0.149457] [ema: 0.991027] 
[Epoch 3/225] [Batch 200/223] [D loss: 0.420262] [G loss: 0.162370] [ema: 0.992055] 
[Epoch 4/225] [Batch 0/223] [D loss: 0.389423] [G loss: 0.200391] [ema: 0.992259] 
[Epoch 4/225] [Batch 100/223] [D loss: 0.463511] [G loss: 0.158025] [ema: 0.993037] 
[Epoch 4/225] [Batch 200/223] [D loss: 0.400511] [G loss: 0.173060] [ema: 0.993673] 
[Epoch 5/225] [Batch 0/223] [D loss: 0.474284] [G loss: 0.169978] [ema: 0.993803] 
[Epoch 5/225] [Batch 100/223] [D loss: 0.384121] [G loss: 0.175183] [ema: 0.994311] 
[Epoch 5/225] [Batch 200/223] [D loss: 0.424631] [G loss: 0.149481] [ema: 0.994743] 
[Epoch 6/225] [Batch 0/223] [D loss: 0.503908] [G loss: 0.184901] [ema: 0.994833] 
[Epoch 6/225] [Batch 100/223] [D loss: 0.421508] [G loss: 0.173291] [ema: 0.995191] 
[Epoch 6/225] [Batch 200/223] [D loss: 0.445453] [G loss: 0.180315] [ema: 0.995503] 
[Epoch 7/225] [Batch 0/223] [D loss: 0.430825] [G loss: 0.163022] [ema: 0.995569] 
[Epoch 7/225] [Batch 100/223] [D loss: 0.479664] [G loss: 0.151023] [ema: 0.995836] 
[Epoch 7/225] [Batch 200/223] [D loss: 0.404795] [G loss: 0.165666] [ema: 0.996072] 
[Epoch 8/225] [Batch 0/223] [D loss: 0.455081] [G loss: 0.181091] [ema: 0.996122] 
[Epoch 8/225] [Batch 100/223] [D loss: 0.474677] [G loss: 0.135380] [ema: 0.996328] 
[Epoch 8/225] [Batch 200/223] [D loss: 0.367396] [G loss: 0.195809] [ema: 0.996512] 
[Epoch 9/225] [Batch 0/223] [D loss: 0.437383] [G loss: 0.201819] [ema: 0.996552] 
[Epoch 9/225] [Batch 100/223] [D loss: 0.358920] [G loss: 0.217980] [ema: 0.996716] 
[Epoch 9/225] [Batch 200/223] [D loss: 0.390448] [G loss: 0.160316] [ema: 0.996864] 
[Epoch 10/225] [Batch 0/223] [D loss: 0.395853] [G loss: 0.164066] [ema: 0.996897] 
[Epoch 10/225] [Batch 100/223] [D loss: 0.319190] [G loss: 0.214387] [ema: 0.997030] 
[Epoch 10/225] [Batch 200/223] [D loss: 0.376213] [G loss: 0.214829] [ema: 0.997152] 
[Epoch 11/225] [Batch 0/223] [D loss: 0.319878] [G loss: 0.242074] [ema: 0.997178] 
[Epoch 11/225] [Batch 100/223] [D loss: 0.328317] [G loss: 0.159302] [ema: 0.997289] 
[Epoch 11/225] [Batch 200/223] [D loss: 0.323694] [G loss: 0.258225] [ema: 0.997391] 
[Epoch 12/225] [Batch 0/223] [D loss: 0.404460] [G loss: 0.205149] [ema: 0.997413] 
[Epoch 12/225] [Batch 100/223] [D loss: 0.431590] [G loss: 0.182620] [ema: 0.997506] 
[Epoch 12/225] [Batch 200/223] [D loss: 0.362369] [G loss: 0.167426] [ema: 0.997593] 
[Epoch 13/225] [Batch 0/223] [D loss: 0.376384] [G loss: 0.187779] [ema: 0.997612] 
[Epoch 13/225] [Batch 100/223] [D loss: 0.355385] [G loss: 0.182153] [ema: 0.997691] 
[Epoch 13/225] [Batch 200/223] [D loss: 0.389382] [G loss: 0.198050] [ema: 0.997766] 
[Epoch 14/225] [Batch 0/223] [D loss: 0.338094] [G loss: 0.206157] [ema: 0.997782] 
[Epoch 14/225] [Batch 100/223] [D loss: 0.337597] [G loss: 0.246016] [ema: 0.997851] 
[Epoch 14/225] [Batch 200/223] [D loss: 0.406924] [G loss: 0.180812] [ema: 0.997916] 
[Epoch 15/225] [Batch 0/223] [D loss: 0.427824] [G loss: 0.236922] [ema: 0.997930] 
[Epoch 15/225] [Batch 100/223] [D loss: 0.358781] [G loss: 0.226485] [ema: 0.997990] 
[Epoch 15/225] [Batch 200/223] [D loss: 0.379476] [G loss: 0.193672] [ema: 0.998047] 
[Epoch 16/225] [Batch 0/223] [D loss: 0.361696] [G loss: 0.218592] [ema: 0.998059] 
[Epoch 16/225] [Batch 100/223] [D loss: 0.406229] [G loss: 0.194573] [ema: 0.998112] 
[Epoch 16/225] [Batch 200/223] [D loss: 0.395532] [G loss: 0.201338] [ema: 0.998162] 
[Epoch 17/225] [Batch 0/223] [D loss: 0.438917] [G loss: 0.178261] [ema: 0.998173] 
[Epoch 17/225] [Batch 100/223] [D loss: 0.369639] [G loss: 0.150558] [ema: 0.998220] 
[Epoch 17/225] [Batch 200/223] [D loss: 0.381173] [G loss: 0.182126] [ema: 0.998265] 
[Epoch 18/225] [Batch 0/223] [D loss: 0.311580] [G loss: 0.229751] [ema: 0.998275] 
[Epoch 18/225] [Batch 100/223] [D loss: 0.330807] [G loss: 0.203334] [ema: 0.998317] 
[Epoch 18/225] [Batch 200/223] [D loss: 0.342445] [G loss: 0.197799] [ema: 0.998356] 
[Epoch 19/225] [Batch 0/223] [D loss: 0.346888] [G loss: 0.181688] [ema: 0.998365] 
[Epoch 19/225] [Batch 100/223] [D loss: 0.325327] [G loss: 0.190280] [ema: 0.998403] 
[Epoch 19/225] [Batch 200/223] [D loss: 0.357925] [G loss: 0.217905] [ema: 0.998439] 
[Epoch 20/225] [Batch 0/223] [D loss: 0.326412] [G loss: 0.248801] [ema: 0.998447] 
[Epoch 20/225] [Batch 100/223] [D loss: 0.361709] [G loss: 0.243507] [ema: 0.998481] 
[Epoch 20/225] [Batch 200/223] [D loss: 0.441223] [G loss: 0.151719] [ema: 0.998514] 
[Epoch 21/225] [Batch 0/223] [D loss: 0.394354] [G loss: 0.203075] [ema: 0.998521] 
[Epoch 21/225] [Batch 100/223] [D loss: 0.361959] [G loss: 0.182817] [ema: 0.998552] 
[Epoch 21/225] [Batch 200/223] [D loss: 0.407380] [G loss: 0.192776] [ema: 0.998581] 
[Epoch 22/225] [Batch 0/223] [D loss: 0.409312] [G loss: 0.174578] [ema: 0.998588] 
[Epoch 22/225] [Batch 100/223] [D loss: 0.371717] [G loss: 0.177826] [ema: 0.998616] 
[Epoch 22/225] [Batch 200/223] [D loss: 0.422959] [G loss: 0.221229] [ema: 0.998643] 
[Epoch 23/225] [Batch 0/223] [D loss: 0.402994] [G loss: 0.210412] [ema: 0.998649] 
[Epoch 23/225] [Batch 100/223] [D loss: 0.407887] [G loss: 0.178231] [ema: 0.998675] 
[Epoch 23/225] [Batch 200/223] [D loss: 0.410160] [G loss: 0.177278] [ema: 0.998700] 
[Epoch 24/225] [Batch 0/223] [D loss: 0.378211] [G loss: 0.184084] [ema: 0.998706] 
[Epoch 24/225] [Batch 100/223] [D loss: 0.433967] [G loss: 0.166675] [ema: 0.998729] 
[Epoch 24/225] [Batch 200/223] [D loss: 0.340133] [G loss: 0.189321] [ema: 0.998752] 
[Epoch 25/225] [Batch 0/223] [D loss: 0.422417] [G loss: 0.198359] [ema: 0.998757] 
[Epoch 25/225] [Batch 100/223] [D loss: 0.366358] [G loss: 0.212082] [ema: 0.998779] 
[Epoch 25/225] [Batch 200/223] [D loss: 0.349564] [G loss: 0.171127] [ema: 0.998800] 
[Epoch 26/225] [Batch 0/223] [D loss: 0.343945] [G loss: 0.223341] [ema: 0.998805] 
[Epoch 26/225] [Batch 100/223] [D loss: 0.345041] [G loss: 0.218933] [ema: 0.998825] 
[Epoch 26/225] [Batch 200/223] [D loss: 0.357770] [G loss: 0.194632] [ema: 0.998845] 
[Epoch 27/225] [Batch 0/223] [D loss: 0.377085] [G loss: 0.229566] [ema: 0.998849] 
[Epoch 27/225] [Batch 100/223] [D loss: 0.371658] [G loss: 0.214810] [ema: 0.998868] 
[Epoch 27/225] [Batch 200/223] [D loss: 0.417621] [G loss: 0.176292] [ema: 0.998886] 
[Epoch 28/225] [Batch 0/223] [D loss: 0.346101] [G loss: 0.212002] [ema: 0.998891] 
[Epoch 28/225] [Batch 100/223] [D loss: 0.383225] [G loss: 0.201553] [ema: 0.998908] 
[Epoch 28/225] [Batch 200/223] [D loss: 0.365607] [G loss: 0.213876] [ema: 0.998925] 
[Epoch 29/225] [Batch 0/223] [D loss: 0.408422] [G loss: 0.217924] [ema: 0.998929] 
[Epoch 29/225] [Batch 100/223] [D loss: 0.350964] [G loss: 0.192472] [ema: 0.998945] 
[Epoch 29/225] [Batch 200/223] [D loss: 0.377413] [G loss: 0.212506] [ema: 0.998961] 
[Epoch 30/225] [Batch 0/223] [D loss: 0.347671] [G loss: 0.220130] [ema: 0.998964] 
[Epoch 30/225] [Batch 100/223] [D loss: 0.374638] [G loss: 0.201174] [ema: 0.998980] 
[Epoch 30/225] [Batch 200/223] [D loss: 0.421150] [G loss: 0.199642] [ema: 0.998994] 
[Epoch 31/225] [Batch 0/223] [D loss: 0.341972] [G loss: 0.210011] [ema: 0.998998] 
[Epoch 31/225] [Batch 100/223] [D loss: 0.362009] [G loss: 0.211048] [ema: 0.999012] 
[Epoch 31/225] [Batch 200/223] [D loss: 0.383576] [G loss: 0.190299] [ema: 0.999026] 
[Epoch 32/225] [Batch 0/223] [D loss: 0.397495] [G loss: 0.184577] [ema: 0.999029] 
[Epoch 32/225] [Batch 100/223] [D loss: 0.364048] [G loss: 0.172618] [ema: 0.999043] 
[Epoch 32/225] [Batch 200/223] [D loss: 0.392195] [G loss: 0.235917] [ema: 0.999056] 
[Epoch 33/225] [Batch 0/223] [D loss: 0.366459] [G loss: 0.212382] [ema: 0.999059] 
[Epoch 33/225] [Batch 100/223] [D loss: 0.302318] [G loss: 0.230484] [ema: 0.999071] 
[Epoch 33/225] [Batch 200/223] [D loss: 0.378518] [G loss: 0.206418] [ema: 0.999083] 
[Epoch 34/225] [Batch 0/223] [D loss: 0.406105] [G loss: 0.209990] [ema: 0.999086] 
[Epoch 34/225] [Batch 100/223] [D loss: 0.367557] [G loss: 0.194870] [ema: 0.999098] 
[Epoch 34/225] [Batch 200/223] [D loss: 0.414156] [G loss: 0.184962] [ema: 0.999110] 
[Epoch 35/225] [Batch 0/223] [D loss: 0.374518] [G loss: 0.214901] [ema: 0.999112] 
[Epoch 35/225] [Batch 100/223] [D loss: 0.334703] [G loss: 0.231121] [ema: 0.999124] 
[Epoch 35/225] [Batch 200/223] [D loss: 0.320783] [G loss: 0.194716] [ema: 0.999134] 
[Epoch 36/225] [Batch 0/223] [D loss: 0.314328] [G loss: 0.213272] [ema: 0.999137] 
[Epoch 36/225] [Batch 100/223] [D loss: 0.351755] [G loss: 0.237759] [ema: 0.999148] 
[Epoch 36/225] [Batch 200/223] [D loss: 0.305299] [G loss: 0.214182] [ema: 0.999158] 
[Epoch 37/225] [Batch 0/223] [D loss: 0.322193] [G loss: 0.240418] [ema: 0.999160] 
[Epoch 37/225] [Batch 100/223] [D loss: 0.317923] [G loss: 0.231406] [ema: 0.999170] 
[Epoch 37/225] [Batch 200/223] [D loss: 0.301868] [G loss: 0.231708] [ema: 0.999180] 
[Epoch 38/225] [Batch 0/223] [D loss: 0.267162] [G loss: 0.219104] [ema: 0.999182] 
[Epoch 38/225] [Batch 100/223] [D loss: 0.336195] [G loss: 0.224378] [ema: 0.999192] 
[Epoch 38/225] [Batch 200/223] [D loss: 0.296315] [G loss: 0.228452] [ema: 0.999201] 
[Epoch 39/225] [Batch 0/223] [D loss: 0.345356] [G loss: 0.223971] [ema: 0.999203] 
[Epoch 39/225] [Batch 100/223] [D loss: 0.318038] [G loss: 0.229158] [ema: 0.999212] 
[Epoch 39/225] [Batch 200/223] [D loss: 0.375706] [G loss: 0.217190] [ema: 0.999221] 
[Epoch 40/225] [Batch 0/223] [D loss: 0.329479] [G loss: 0.201474] [ema: 0.999223] 
[Epoch 40/225] [Batch 100/223] [D loss: 0.323321] [G loss: 0.221195] [ema: 0.999232] 
[Epoch 40/225] [Batch 200/223] [D loss: 0.326544] [G loss: 0.205467] [ema: 0.999240] 
[Epoch 41/225] [Batch 0/223] [D loss: 0.313788] [G loss: 0.227009] [ema: 0.999242] 
[Epoch 41/225] [Batch 100/223] [D loss: 0.345352] [G loss: 0.224655] [ema: 0.999250] 
[Epoch 41/225] [Batch 200/223] [D loss: 0.257572] [G loss: 0.239848] [ema: 0.999258] 
[Epoch 42/225] [Batch 0/223] [D loss: 0.339267] [G loss: 0.252501] [ema: 0.999260] 
[Epoch 42/225] [Batch 100/223] [D loss: 0.316417] [G loss: 0.228021] [ema: 0.999268] 
[Epoch 42/225] [Batch 200/223] [D loss: 0.301363] [G loss: 0.209653] [ema: 0.999276] 
[Epoch 43/225] [Batch 0/223] [D loss: 0.296099] [G loss: 0.199786] [ema: 0.999277] 
[Epoch 43/225] [Batch 100/223] [D loss: 0.313498] [G loss: 0.229439] [ema: 0.999285] 
[Epoch 43/225] [Batch 200/223] [D loss: 0.369225] [G loss: 0.229107] [ema: 0.999292] 
[Epoch 44/225] [Batch 0/223] [D loss: 0.318240] [G loss: 0.219437] [ema: 0.999294] 
[Epoch 44/225] [Batch 100/223] [D loss: 0.323384] [G loss: 0.218709] [ema: 0.999301] 
[Epoch 44/225] [Batch 200/223] [D loss: 0.306787] [G loss: 0.209824] [ema: 0.999308] 
[Epoch 45/225] [Batch 0/223] [D loss: 0.287826] [G loss: 0.227910] [ema: 0.999310] 
[Epoch 45/225] [Batch 100/223] [D loss: 0.314265] [G loss: 0.221086] [ema: 0.999316] 
[Epoch 45/225] [Batch 200/223] [D loss: 0.331068] [G loss: 0.225364] [ema: 0.999323] 
[Epoch 46/225] [Batch 0/223] [D loss: 0.304218] [G loss: 0.211999] [ema: 0.999325] 
[Epoch 46/225] [Batch 100/223] [D loss: 0.308416] [G loss: 0.190037] [ema: 0.999331] 
[Epoch 46/225] [Batch 200/223] [D loss: 0.312979] [G loss: 0.230683] [ema: 0.999337] 
[Epoch 47/225] [Batch 0/223] [D loss: 0.306674] [G loss: 0.229813] [ema: 0.999339] 
[Epoch 47/225] [Batch 100/223] [D loss: 0.343373] [G loss: 0.210739] [ema: 0.999345] 
[Epoch 47/225] [Batch 200/223] [D loss: 0.348931] [G loss: 0.216464] [ema: 0.999351] 
[Epoch 48/225] [Batch 0/223] [D loss: 0.289367] [G loss: 0.220089] [ema: 0.999353] 
[Epoch 48/225] [Batch 100/223] [D loss: 0.352457] [G loss: 0.204519] [ema: 0.999359] 
[Epoch 48/225] [Batch 200/223] [D loss: 0.312339] [G loss: 0.220154] [ema: 0.999365] 
[Epoch 49/225] [Batch 0/223] [D loss: 0.300942] [G loss: 0.243423] [ema: 0.999366] 
[Epoch 49/225] [Batch 100/223] [D loss: 0.317942] [G loss: 0.178329] [ema: 0.999372] 
[Epoch 49/225] [Batch 200/223] [D loss: 0.308541] [G loss: 0.221927] [ema: 0.999377] 
[Epoch 50/225] [Batch 0/223] [D loss: 0.296500] [G loss: 0.236382] [ema: 0.999379] 
[Epoch 50/225] [Batch 100/223] [D loss: 0.324293] [G loss: 0.197645] [ema: 0.999384] 
[Epoch 50/225] [Batch 200/223] [D loss: 0.328607] [G loss: 0.224235] [ema: 0.999389] 
[Epoch 51/225] [Batch 0/223] [D loss: 0.336986] [G loss: 0.204831] [ema: 0.999391] 
[Epoch 51/225] [Batch 100/223] [D loss: 0.364741] [G loss: 0.220502] [ema: 0.999396] 
[Epoch 51/225] [Batch 200/223] [D loss: 0.355330] [G loss: 0.204387] [ema: 0.999401] 
[Epoch 52/225] [Batch 0/223] [D loss: 0.302404] [G loss: 0.219798] [ema: 0.999402] 
[Epoch 52/225] [Batch 100/223] [D loss: 0.303230] [G loss: 0.233417] [ema: 0.999408] 
[Epoch 52/225] [Batch 200/223] [D loss: 0.299213] [G loss: 0.192039] [ema: 0.999413] 
[Epoch 53/225] [Batch 0/223] [D loss: 0.319025] [G loss: 0.213201] [ema: 0.999414] 
[Epoch 53/225] [Batch 100/223] [D loss: 0.368421] [G loss: 0.219619] [ema: 0.999419] 
[Epoch 53/225] [Batch 200/223] [D loss: 0.299813] [G loss: 0.251677] [ema: 0.999423] 
[Epoch 54/225] [Batch 0/223] [D loss: 0.316823] [G loss: 0.205334] [ema: 0.999425] 
[Epoch 54/225] [Batch 100/223] [D loss: 0.369968] [G loss: 0.194593] [ema: 0.999429] 
[Epoch 54/225] [Batch 200/223] [D loss: 0.367849] [G loss: 0.204416] [ema: 0.999434] 
[Epoch 55/225] [Batch 0/223] [D loss: 0.404464] [G loss: 0.194649] [ema: 0.999435] 
[Epoch 55/225] [Batch 100/223] [D loss: 0.404896] [G loss: 0.176705] [ema: 0.999440] 
[Epoch 55/225] [Batch 200/223] [D loss: 0.351437] [G loss: 0.196686] [ema: 0.999444] 
[Epoch 56/225] [Batch 0/223] [D loss: 0.398444] [G loss: 0.199027] [ema: 0.999445] 
[Epoch 56/225] [Batch 100/223] [D loss: 0.301459] [G loss: 0.200513] [ema: 0.999450] 
[Epoch 56/225] [Batch 200/223] [D loss: 0.284822] [G loss: 0.218929] [ema: 0.999454] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_60_100/MotionSense_DAGHAR_Multiclass_50000_D_60_2024_10_30_00_07_49/Model



[Epoch 57/225] [Batch 0/223] [D loss: 0.384860] [G loss: 0.237815] [ema: 0.999455] 
[Epoch 57/225] [Batch 100/223] [D loss: 0.327363] [G loss: 0.198945] [ema: 0.999459] 
[Epoch 57/225] [Batch 200/223] [D loss: 0.398108] [G loss: 0.175083] [ema: 0.999463] 
[Epoch 58/225] [Batch 0/223] [D loss: 0.312900] [G loss: 0.222055] [ema: 0.999464] 
[Epoch 58/225] [Batch 100/223] [D loss: 0.359090] [G loss: 0.187145] [ema: 0.999468] 
[Epoch 58/225] [Batch 200/223] [D loss: 0.421671] [G loss: 0.170222] [ema: 0.999472] 
[Epoch 59/225] [Batch 0/223] [D loss: 0.344358] [G loss: 0.239275] [ema: 0.999473] 
[Epoch 59/225] [Batch 100/223] [D loss: 0.287952] [G loss: 0.197293] [ema: 0.999477] 
[Epoch 59/225] [Batch 200/223] [D loss: 0.335171] [G loss: 0.197916] [ema: 0.999481] 
[Epoch 60/225] [Batch 0/223] [D loss: 0.328101] [G loss: 0.227717] [ema: 0.999482] 
[Epoch 60/225] [Batch 100/223] [D loss: 0.309566] [G loss: 0.238582] [ema: 0.999486] 
[Epoch 60/225] [Batch 200/223] [D loss: 0.294347] [G loss: 0.225023] [ema: 0.999490] 
[Epoch 61/225] [Batch 0/223] [D loss: 0.310669] [G loss: 0.240351] [ema: 0.999491] 
[Epoch 61/225] [Batch 100/223] [D loss: 0.327556] [G loss: 0.211561] [ema: 0.999494] 
[Epoch 61/225] [Batch 200/223] [D loss: 0.324320] [G loss: 0.226183] [ema: 0.999498] 
[Epoch 62/225] [Batch 0/223] [D loss: 0.295807] [G loss: 0.235526] [ema: 0.999499] 
[Epoch 62/225] [Batch 100/223] [D loss: 0.313137] [G loss: 0.200150] [ema: 0.999502] 
[Epoch 62/225] [Batch 200/223] [D loss: 0.352849] [G loss: 0.223824] [ema: 0.999506] 
[Epoch 63/225] [Batch 0/223] [D loss: 0.279175] [G loss: 0.229985] [ema: 0.999507] 
[Epoch 63/225] [Batch 100/223] [D loss: 0.338164] [G loss: 0.224908] [ema: 0.999510] 
[Epoch 63/225] [Batch 200/223] [D loss: 0.339216] [G loss: 0.188716] [ema: 0.999514] 
[Epoch 64/225] [Batch 0/223] [D loss: 0.341445] [G loss: 0.226411] [ema: 0.999514] 
[Epoch 64/225] [Batch 100/223] [D loss: 0.385136] [G loss: 0.213630] [ema: 0.999518] 
[Epoch 64/225] [Batch 200/223] [D loss: 0.403368] [G loss: 0.189814] [ema: 0.999521] 
[Epoch 65/225] [Batch 0/223] [D loss: 0.355363] [G loss: 0.221773] [ema: 0.999522] 
[Epoch 65/225] [Batch 100/223] [D loss: 0.437669] [G loss: 0.204956] [ema: 0.999525] 
[Epoch 65/225] [Batch 200/223] [D loss: 0.381923] [G loss: 0.212646] [ema: 0.999528] 
[Epoch 66/225] [Batch 0/223] [D loss: 0.366442] [G loss: 0.205479] [ema: 0.999529] 
[Epoch 66/225] [Batch 100/223] [D loss: 0.356336] [G loss: 0.204279] [ema: 0.999532] 
[Epoch 66/225] [Batch 200/223] [D loss: 0.378276] [G loss: 0.209464] [ema: 0.999535] 
[Epoch 67/225] [Batch 0/223] [D loss: 0.364914] [G loss: 0.196591] [ema: 0.999536] 
[Epoch 67/225] [Batch 100/223] [D loss: 0.344176] [G loss: 0.209738] [ema: 0.999539] 
[Epoch 67/225] [Batch 200/223] [D loss: 0.334858] [G loss: 0.190333] [ema: 0.999542] 
[Epoch 68/225] [Batch 0/223] [D loss: 0.328661] [G loss: 0.246177] [ema: 0.999543] 
[Epoch 68/225] [Batch 100/223] [D loss: 0.299264] [G loss: 0.233206] [ema: 0.999546] 
[Epoch 68/225] [Batch 200/223] [D loss: 0.350579] [G loss: 0.202870] [ema: 0.999549] 
[Epoch 69/225] [Batch 0/223] [D loss: 0.403793] [G loss: 0.221789] [ema: 0.999550] 
[Epoch 69/225] [Batch 100/223] [D loss: 0.447982] [G loss: 0.178772] [ema: 0.999553] 
[Epoch 69/225] [Batch 200/223] [D loss: 0.384659] [G loss: 0.205982] [ema: 0.999555] 
[Epoch 70/225] [Batch 0/223] [D loss: 0.332241] [G loss: 0.192771] [ema: 0.999556] 
[Epoch 70/225] [Batch 100/223] [D loss: 0.411213] [G loss: 0.186331] [ema: 0.999559] 
[Epoch 70/225] [Batch 200/223] [D loss: 0.326656] [G loss: 0.213093] [ema: 0.999562] 
[Epoch 71/225] [Batch 0/223] [D loss: 0.323608] [G loss: 0.196954] [ema: 0.999562] 
[Epoch 71/225] [Batch 100/223] [D loss: 0.361899] [G loss: 0.209189] [ema: 0.999565] 
[Epoch 71/225] [Batch 200/223] [D loss: 0.323673] [G loss: 0.207739] [ema: 0.999568] 
[Epoch 72/225] [Batch 0/223] [D loss: 0.291660] [G loss: 0.212367] [ema: 0.999568] 
[Epoch 72/225] [Batch 100/223] [D loss: 0.345120] [G loss: 0.228824] [ema: 0.999571] 
[Epoch 72/225] [Batch 200/223] [D loss: 0.295649] [G loss: 0.247268] [ema: 0.999574] 
[Epoch 73/225] [Batch 0/223] [D loss: 0.318651] [G loss: 0.185725] [ema: 0.999574] 
[Epoch 73/225] [Batch 100/223] [D loss: 0.354019] [G loss: 0.200885] [ema: 0.999577] 
[Epoch 73/225] [Batch 200/223] [D loss: 0.404011] [G loss: 0.195733] [ema: 0.999579] 
[Epoch 74/225] [Batch 0/223] [D loss: 0.362379] [G loss: 0.215325] [ema: 0.999580] 
[Epoch 74/225] [Batch 100/223] [D loss: 0.389005] [G loss: 0.204846] [ema: 0.999583] 
[Epoch 74/225] [Batch 200/223] [D loss: 0.353958] [G loss: 0.179075] [ema: 0.999585] 
[Epoch 75/225] [Batch 0/223] [D loss: 0.366688] [G loss: 0.225336] [ema: 0.999586] 
[Epoch 75/225] [Batch 100/223] [D loss: 0.347277] [G loss: 0.187502] [ema: 0.999588] 
[Epoch 75/225] [Batch 200/223] [D loss: 0.356300] [G loss: 0.196159] [ema: 0.999591] 
[Epoch 76/225] [Batch 0/223] [D loss: 0.329306] [G loss: 0.219829] [ema: 0.999591] 
[Epoch 76/225] [Batch 100/223] [D loss: 0.368241] [G loss: 0.172330] [ema: 0.999593] 
[Epoch 76/225] [Batch 200/223] [D loss: 0.315212] [G loss: 0.217987] [ema: 0.999596] 
[Epoch 77/225] [Batch 0/223] [D loss: 0.362489] [G loss: 0.226007] [ema: 0.999596] 
[Epoch 77/225] [Batch 100/223] [D loss: 0.362985] [G loss: 0.170879] [ema: 0.999599] 
[Epoch 77/225] [Batch 200/223] [D loss: 0.339828] [G loss: 0.190870] [ema: 0.999601] 
[Epoch 78/225] [Batch 0/223] [D loss: 0.337838] [G loss: 0.221437] [ema: 0.999602] 
[Epoch 78/225] [Batch 100/223] [D loss: 0.320818] [G loss: 0.209393] [ema: 0.999604] 
[Epoch 78/225] [Batch 200/223] [D loss: 0.340740] [G loss: 0.212913] [ema: 0.999606] 
[Epoch 79/225] [Batch 0/223] [D loss: 0.352765] [G loss: 0.222302] [ema: 0.999607] 
[Epoch 79/225] [Batch 100/223] [D loss: 0.338603] [G loss: 0.227674] [ema: 0.999609] 
[Epoch 79/225] [Batch 200/223] [D loss: 0.344259] [G loss: 0.184853] [ema: 0.999611] 
[Epoch 80/225] [Batch 0/223] [D loss: 0.354643] [G loss: 0.215498] [ema: 0.999612] 
[Epoch 80/225] [Batch 100/223] [D loss: 0.352278] [G loss: 0.202803] [ema: 0.999614] 
[Epoch 80/225] [Batch 200/223] [D loss: 0.319052] [G loss: 0.222066] [ema: 0.999616] 
[Epoch 81/225] [Batch 0/223] [D loss: 0.334464] [G loss: 0.196008] [ema: 0.999616] 
[Epoch 81/225] [Batch 100/223] [D loss: 0.326184] [G loss: 0.208311] [ema: 0.999618] 
[Epoch 81/225] [Batch 200/223] [D loss: 0.306278] [G loss: 0.217449] [ema: 0.999621] 
[Epoch 82/225] [Batch 0/223] [D loss: 0.296191] [G loss: 0.221198] [ema: 0.999621] 
[Epoch 82/225] [Batch 100/223] [D loss: 0.361114] [G loss: 0.214202] [ema: 0.999623] 
[Epoch 82/225] [Batch 200/223] [D loss: 0.354808] [G loss: 0.211072] [ema: 0.999625] 
[Epoch 83/225] [Batch 0/223] [D loss: 0.365145] [G loss: 0.206700] [ema: 0.999626] 
[Epoch 83/225] [Batch 100/223] [D loss: 0.280943] [G loss: 0.205572] [ema: 0.999628] 
[Epoch 83/225] [Batch 200/223] [D loss: 0.372307] [G loss: 0.191903] [ema: 0.999630] 
[Epoch 84/225] [Batch 0/223] [D loss: 0.383775] [G loss: 0.230142] [ema: 0.999630] 
[Epoch 84/225] [Batch 100/223] [D loss: 0.359803] [G loss: 0.224365] [ema: 0.999632] 
[Epoch 84/225] [Batch 200/223] [D loss: 0.329525] [G loss: 0.204587] [ema: 0.999634] 
[Epoch 85/225] [Batch 0/223] [D loss: 0.300828] [G loss: 0.247339] [ema: 0.999634] 
[Epoch 85/225] [Batch 100/223] [D loss: 0.310808] [G loss: 0.211158] [ema: 0.999636] 
[Epoch 85/225] [Batch 200/223] [D loss: 0.393900] [G loss: 0.220183] [ema: 0.999638] 
[Epoch 86/225] [Batch 0/223] [D loss: 0.390728] [G loss: 0.199472] [ema: 0.999639] 
[Epoch 86/225] [Batch 100/223] [D loss: 0.328383] [G loss: 0.200067] [ema: 0.999641] 
[Epoch 86/225] [Batch 200/223] [D loss: 0.335498] [G loss: 0.225074] [ema: 0.999642] 
[Epoch 87/225] [Batch 0/223] [D loss: 0.387159] [G loss: 0.234518] [ema: 0.999643] 
[Epoch 87/225] [Batch 100/223] [D loss: 0.385127] [G loss: 0.219199] [ema: 0.999645] 
[Epoch 87/225] [Batch 200/223] [D loss: 0.335799] [G loss: 0.238215] [ema: 0.999646] 
[Epoch 88/225] [Batch 0/223] [D loss: 0.339965] [G loss: 0.213562] [ema: 0.999647] 
[Epoch 88/225] [Batch 100/223] [D loss: 0.351366] [G loss: 0.230664] [ema: 0.999649] 
[Epoch 88/225] [Batch 200/223] [D loss: 0.396329] [G loss: 0.206524] [ema: 0.999650] 
[Epoch 89/225] [Batch 0/223] [D loss: 0.332240] [G loss: 0.204779] [ema: 0.999651] 
[Epoch 89/225] [Batch 100/223] [D loss: 0.323377] [G loss: 0.203013] [ema: 0.999653] 
[Epoch 89/225] [Batch 200/223] [D loss: 0.344874] [G loss: 0.225276] [ema: 0.999654] 
[Epoch 90/225] [Batch 0/223] [D loss: 0.376064] [G loss: 0.185459] [ema: 0.999655] 
[Epoch 90/225] [Batch 100/223] [D loss: 0.319771] [G loss: 0.218855] [ema: 0.999656] 
[Epoch 90/225] [Batch 200/223] [D loss: 0.364009] [G loss: 0.224237] [ema: 0.999658] 
[Epoch 91/225] [Batch 0/223] [D loss: 0.332734] [G loss: 0.196097] [ema: 0.999658] 
[Epoch 91/225] [Batch 100/223] [D loss: 0.323238] [G loss: 0.205803] [ema: 0.999660] 
[Epoch 91/225] [Batch 200/223] [D loss: 0.335069] [G loss: 0.223548] [ema: 0.999662] 
[Epoch 92/225] [Batch 0/223] [D loss: 0.376897] [G loss: 0.195467] [ema: 0.999662] 
[Epoch 92/225] [Batch 100/223] [D loss: 0.303799] [G loss: 0.214186] [ema: 0.999664] 
[Epoch 92/225] [Batch 200/223] [D loss: 0.302591] [G loss: 0.218133] [ema: 0.999665] 
[Epoch 93/225] [Batch 0/223] [D loss: 0.314479] [G loss: 0.218222] [ema: 0.999666] 
[Epoch 93/225] [Batch 100/223] [D loss: 0.310434] [G loss: 0.199662] [ema: 0.999667] 
[Epoch 93/225] [Batch 200/223] [D loss: 0.360795] [G loss: 0.207120] [ema: 0.999669] 
[Epoch 94/225] [Batch 0/223] [D loss: 0.352624] [G loss: 0.214266] [ema: 0.999669] 
[Epoch 94/225] [Batch 100/223] [D loss: 0.370631] [G loss: 0.209404] [ema: 0.999671] 
[Epoch 94/225] [Batch 200/223] [D loss: 0.310549] [G loss: 0.208934] [ema: 0.999673] 
[Epoch 95/225] [Batch 0/223] [D loss: 0.318647] [G loss: 0.238930] [ema: 0.999673] 
[Epoch 95/225] [Batch 100/223] [D loss: 0.345328] [G loss: 0.223087] [ema: 0.999674] 
[Epoch 95/225] [Batch 200/223] [D loss: 0.321247] [G loss: 0.225692] [ema: 0.999676] 
[Epoch 96/225] [Batch 0/223] [D loss: 0.337469] [G loss: 0.212774] [ema: 0.999676] 
[Epoch 96/225] [Batch 100/223] [D loss: 0.335919] [G loss: 0.207589] [ema: 0.999678] 
[Epoch 96/225] [Batch 200/223] [D loss: 0.365748] [G loss: 0.243578] [ema: 0.999679] 
[Epoch 97/225] [Batch 0/223] [D loss: 0.423867] [G loss: 0.213165] [ema: 0.999680] 
[Epoch 97/225] [Batch 100/223] [D loss: 0.321687] [G loss: 0.222834] [ema: 0.999681] 
[Epoch 97/225] [Batch 200/223] [D loss: 0.349475] [G loss: 0.213357] [ema: 0.999683] 
[Epoch 98/225] [Batch 0/223] [D loss: 0.308269] [G loss: 0.220581] [ema: 0.999683] 
[Epoch 98/225] [Batch 100/223] [D loss: 0.342827] [G loss: 0.232107] [ema: 0.999684] 
[Epoch 98/225] [Batch 200/223] [D loss: 0.377809] [G loss: 0.176090] [ema: 0.999686] 
[Epoch 99/225] [Batch 0/223] [D loss: 0.284672] [G loss: 0.241907] [ema: 0.999686] 
[Epoch 99/225] [Batch 100/223] [D loss: 0.341266] [G loss: 0.203354] [ema: 0.999687] 
[Epoch 99/225] [Batch 200/223] [D loss: 0.332557] [G loss: 0.200361] [ema: 0.999689] 
[Epoch 100/225] [Batch 0/223] [D loss: 0.385285] [G loss: 0.244259] [ema: 0.999689] 
[Epoch 100/225] [Batch 100/223] [D loss: 0.325794] [G loss: 0.214607] [ema: 0.999691] 
[Epoch 100/225] [Batch 200/223] [D loss: 0.334421] [G loss: 0.226825] [ema: 0.999692] 
[Epoch 101/225] [Batch 0/223] [D loss: 0.374287] [G loss: 0.176060] [ema: 0.999692] 
[Epoch 101/225] [Batch 100/223] [D loss: 0.306170] [G loss: 0.209629] [ema: 0.999694] 
[Epoch 101/225] [Batch 200/223] [D loss: 0.388680] [G loss: 0.225632] [ema: 0.999695] 
[Epoch 102/225] [Batch 0/223] [D loss: 0.383885] [G loss: 0.209078] [ema: 0.999695] 
[Epoch 102/225] [Batch 100/223] [D loss: 0.320640] [G loss: 0.194056] [ema: 0.999697] 
[Epoch 102/225] [Batch 200/223] [D loss: 0.284578] [G loss: 0.209040] [ema: 0.999698] 
[Epoch 103/225] [Batch 0/223] [D loss: 0.394538] [G loss: 0.233924] [ema: 0.999698] 
[Epoch 103/225] [Batch 100/223] [D loss: 0.289444] [G loss: 0.231802] [ema: 0.999700] 
[Epoch 103/225] [Batch 200/223] [D loss: 0.297085] [G loss: 0.246143] [ema: 0.999701] 
[Epoch 104/225] [Batch 0/223] [D loss: 0.380289] [G loss: 0.193867] [ema: 0.999701] 
[Epoch 104/225] [Batch 100/223] [D loss: 0.366940] [G loss: 0.226198] [ema: 0.999702] 
[Epoch 104/225] [Batch 200/223] [D loss: 0.314577] [G loss: 0.216649] [ema: 0.999704] 
[Epoch 105/225] [Batch 0/223] [D loss: 0.340283] [G loss: 0.208010] [ema: 0.999704] 
[Epoch 105/225] [Batch 100/223] [D loss: 0.279278] [G loss: 0.216983] [ema: 0.999705] 
[Epoch 105/225] [Batch 200/223] [D loss: 0.312415] [G loss: 0.204889] [ema: 0.999707] 
[Epoch 106/225] [Batch 0/223] [D loss: 0.290877] [G loss: 0.211488] [ema: 0.999707] 
[Epoch 106/225] [Batch 100/223] [D loss: 0.348278] [G loss: 0.208822] [ema: 0.999708] 
[Epoch 106/225] [Batch 200/223] [D loss: 0.331829] [G loss: 0.217242] [ema: 0.999709] 
[Epoch 107/225] [Batch 0/223] [D loss: 0.381492] [G loss: 0.232253] [ema: 0.999710] 
[Epoch 107/225] [Batch 100/223] [D loss: 0.316958] [G loss: 0.218927] [ema: 0.999711] 
[Epoch 107/225] [Batch 200/223] [D loss: 0.266621] [G loss: 0.216767] [ema: 0.999712] 
[Epoch 108/225] [Batch 0/223] [D loss: 0.364536] [G loss: 0.199935] [ema: 0.999712] 
[Epoch 108/225] [Batch 100/223] [D loss: 0.274339] [G loss: 0.273088] [ema: 0.999713] 
[Epoch 108/225] [Batch 200/223] [D loss: 0.400847] [G loss: 0.232497] [ema: 0.999715] 
[Epoch 109/225] [Batch 0/223] [D loss: 0.372714] [G loss: 0.231096] [ema: 0.999715] 
[Epoch 109/225] [Batch 100/223] [D loss: 0.291450] [G loss: 0.215959] [ema: 0.999716] 
[Epoch 109/225] [Batch 200/223] [D loss: 0.333007] [G loss: 0.227323] [ema: 0.999717] 
[Epoch 110/225] [Batch 0/223] [D loss: 0.315153] [G loss: 0.213678] [ema: 0.999717] 
[Epoch 110/225] [Batch 100/223] [D loss: 0.353612] [G loss: 0.200527] [ema: 0.999719] 
[Epoch 110/225] [Batch 200/223] [D loss: 0.322565] [G loss: 0.228685] [ema: 0.999720] 
[Epoch 111/225] [Batch 0/223] [D loss: 0.310449] [G loss: 0.207450] [ema: 0.999720] 
[Epoch 111/225] [Batch 100/223] [D loss: 0.379013] [G loss: 0.209103] [ema: 0.999721] 
[Epoch 111/225] [Batch 200/223] [D loss: 0.346402] [G loss: 0.225205] [ema: 0.999722] 
[Epoch 112/225] [Batch 0/223] [D loss: 0.304348] [G loss: 0.240639] [ema: 0.999723] 
[Epoch 112/225] [Batch 100/223] [D loss: 0.335285] [G loss: 0.244245] [ema: 0.999724] 
[Epoch 112/225] [Batch 200/223] [D loss: 0.349785] [G loss: 0.208678] [ema: 0.999725] 
[Epoch 113/225] [Batch 0/223] [D loss: 0.274748] [G loss: 0.249699] [ema: 0.999725] 
[Epoch 113/225] [Batch 100/223] [D loss: 0.318391] [G loss: 0.223203] [ema: 0.999726] 
[Epoch 113/225] [Batch 200/223] [D loss: 0.360372] [G loss: 0.193123] [ema: 0.999727] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_60_100/MotionSense_DAGHAR_Multiclass_50000_D_60_2024_10_30_00_07_49/Model



[Epoch 114/225] [Batch 0/223] [D loss: 0.337725] [G loss: 0.232512] [ema: 0.999727] 
[Epoch 114/225] [Batch 100/223] [D loss: 0.340017] [G loss: 0.224204] [ema: 0.999728] 
[Epoch 114/225] [Batch 200/223] [D loss: 0.300751] [G loss: 0.241227] [ema: 0.999730] 
[Epoch 115/225] [Batch 0/223] [D loss: 0.342065] [G loss: 0.190668] [ema: 0.999730] 
[Epoch 115/225] [Batch 100/223] [D loss: 0.293220] [G loss: 0.229217] [ema: 0.999731] 
[Epoch 115/225] [Batch 200/223] [D loss: 0.302535] [G loss: 0.211127] [ema: 0.999732] 
[Epoch 116/225] [Batch 0/223] [D loss: 0.315793] [G loss: 0.217901] [ema: 0.999732] 
[Epoch 116/225] [Batch 100/223] [D loss: 0.307841] [G loss: 0.212239] [ema: 0.999733] 
[Epoch 116/225] [Batch 200/223] [D loss: 0.308459] [G loss: 0.215703] [ema: 0.999734] 
[Epoch 117/225] [Batch 0/223] [D loss: 0.283479] [G loss: 0.224318] [ema: 0.999734] 
[Epoch 117/225] [Batch 100/223] [D loss: 0.349630] [G loss: 0.233095] [ema: 0.999735] 
[Epoch 117/225] [Batch 200/223] [D loss: 0.277959] [G loss: 0.224319] [ema: 0.999736] 
[Epoch 118/225] [Batch 0/223] [D loss: 0.312858] [G loss: 0.230774] [ema: 0.999737] 
[Epoch 118/225] [Batch 100/223] [D loss: 0.342631] [G loss: 0.218044] [ema: 0.999738] 
[Epoch 118/225] [Batch 200/223] [D loss: 0.359538] [G loss: 0.207083] [ema: 0.999739] 
[Epoch 119/225] [Batch 0/223] [D loss: 0.344977] [G loss: 0.216111] [ema: 0.999739] 
[Epoch 119/225] [Batch 100/223] [D loss: 0.336043] [G loss: 0.191753] [ema: 0.999740] 
[Epoch 119/225] [Batch 200/223] [D loss: 0.348056] [G loss: 0.221971] [ema: 0.999741] 
[Epoch 120/225] [Batch 0/223] [D loss: 0.312832] [G loss: 0.208098] [ema: 0.999741] 
[Epoch 120/225] [Batch 100/223] [D loss: 0.325154] [G loss: 0.242283] [ema: 0.999742] 
[Epoch 120/225] [Batch 200/223] [D loss: 0.298571] [G loss: 0.225763] [ema: 0.999743] 
[Epoch 121/225] [Batch 0/223] [D loss: 0.282041] [G loss: 0.223509] [ema: 0.999743] 
[Epoch 121/225] [Batch 100/223] [D loss: 0.349960] [G loss: 0.202242] [ema: 0.999744] 
[Epoch 121/225] [Batch 200/223] [D loss: 0.315940] [G loss: 0.212323] [ema: 0.999745] 
[Epoch 122/225] [Batch 0/223] [D loss: 0.375485] [G loss: 0.252130] [ema: 0.999745] 
[Epoch 122/225] [Batch 100/223] [D loss: 0.302376] [G loss: 0.226142] [ema: 0.999746] 
[Epoch 122/225] [Batch 200/223] [D loss: 0.353582] [G loss: 0.188980] [ema: 0.999747] 
[Epoch 123/225] [Batch 0/223] [D loss: 0.279651] [G loss: 0.218328] [ema: 0.999747] 
[Epoch 123/225] [Batch 100/223] [D loss: 0.315395] [G loss: 0.197045] [ema: 0.999748] 
[Epoch 123/225] [Batch 200/223] [D loss: 0.329284] [G loss: 0.195955] [ema: 0.999749] 
[Epoch 124/225] [Batch 0/223] [D loss: 0.283129] [G loss: 0.232990] [ema: 0.999749] 
[Epoch 124/225] [Batch 100/223] [D loss: 0.304506] [G loss: 0.239509] [ema: 0.999750] 
[Epoch 124/225] [Batch 200/223] [D loss: 0.325669] [G loss: 0.213093] [ema: 0.999751] 
[Epoch 125/225] [Batch 0/223] [D loss: 0.348943] [G loss: 0.243111] [ema: 0.999751] 
[Epoch 125/225] [Batch 100/223] [D loss: 0.340242] [G loss: 0.206690] [ema: 0.999752] 
[Epoch 125/225] [Batch 200/223] [D loss: 0.318358] [G loss: 0.243371] [ema: 0.999753] 
[Epoch 126/225] [Batch 0/223] [D loss: 0.337124] [G loss: 0.235268] [ema: 0.999753] 
[Epoch 126/225] [Batch 100/223] [D loss: 0.279316] [G loss: 0.215506] [ema: 0.999754] 
[Epoch 126/225] [Batch 200/223] [D loss: 0.329975] [G loss: 0.217858] [ema: 0.999755] 
[Epoch 127/225] [Batch 0/223] [D loss: 0.337306] [G loss: 0.230900] [ema: 0.999755] 
[Epoch 127/225] [Batch 100/223] [D loss: 0.297008] [G loss: 0.237823] [ema: 0.999756] 
[Epoch 127/225] [Batch 200/223] [D loss: 0.361069] [G loss: 0.211389] [ema: 0.999757] 
[Epoch 128/225] [Batch 0/223] [D loss: 0.322714] [G loss: 0.223166] [ema: 0.999757] 
[Epoch 128/225] [Batch 100/223] [D loss: 0.316590] [G loss: 0.209579] [ema: 0.999758] 
[Epoch 128/225] [Batch 200/223] [D loss: 0.356389] [G loss: 0.223006] [ema: 0.999759] 
[Epoch 129/225] [Batch 0/223] [D loss: 0.355359] [G loss: 0.211751] [ema: 0.999759] 
[Epoch 129/225] [Batch 100/223] [D loss: 0.331596] [G loss: 0.212376] [ema: 0.999760] 
[Epoch 129/225] [Batch 200/223] [D loss: 0.309022] [G loss: 0.251159] [ema: 0.999761] 
[Epoch 130/225] [Batch 0/223] [D loss: 0.302122] [G loss: 0.235876] [ema: 0.999761] 
[Epoch 130/225] [Batch 100/223] [D loss: 0.285824] [G loss: 0.208518] [ema: 0.999762] 
[Epoch 130/225] [Batch 200/223] [D loss: 0.322589] [G loss: 0.227156] [ema: 0.999763] 
[Epoch 131/225] [Batch 0/223] [D loss: 0.309685] [G loss: 0.222923] [ema: 0.999763] 
[Epoch 131/225] [Batch 100/223] [D loss: 0.368710] [G loss: 0.200829] [ema: 0.999764] 
[Epoch 131/225] [Batch 200/223] [D loss: 0.261224] [G loss: 0.249778] [ema: 0.999764] 
[Epoch 132/225] [Batch 0/223] [D loss: 0.329246] [G loss: 0.221243] [ema: 0.999765] 
[Epoch 132/225] [Batch 100/223] [D loss: 0.355094] [G loss: 0.238343] [ema: 0.999765] 
[Epoch 132/225] [Batch 200/223] [D loss: 0.316334] [G loss: 0.229813] [ema: 0.999766] 
[Epoch 133/225] [Batch 0/223] [D loss: 0.312095] [G loss: 0.230481] [ema: 0.999766] 
[Epoch 133/225] [Batch 100/223] [D loss: 0.286536] [G loss: 0.233282] [ema: 0.999767] 
[Epoch 133/225] [Batch 200/223] [D loss: 0.308858] [G loss: 0.218196] [ema: 0.999768] 
[Epoch 134/225] [Batch 0/223] [D loss: 0.342284] [G loss: 0.220515] [ema: 0.999768] 
[Epoch 134/225] [Batch 100/223] [D loss: 0.355688] [G loss: 0.212135] [ema: 0.999769] 
[Epoch 134/225] [Batch 200/223] [D loss: 0.310911] [G loss: 0.217023] [ema: 0.999770] 
[Epoch 135/225] [Batch 0/223] [D loss: 0.323223] [G loss: 0.214737] [ema: 0.999770] 
[Epoch 135/225] [Batch 100/223] [D loss: 0.292787] [G loss: 0.207671] [ema: 0.999771] 
[Epoch 135/225] [Batch 200/223] [D loss: 0.301520] [G loss: 0.221829] [ema: 0.999771] 
[Epoch 136/225] [Batch 0/223] [D loss: 0.319829] [G loss: 0.221415] [ema: 0.999771] 
[Epoch 136/225] [Batch 100/223] [D loss: 0.319980] [G loss: 0.221953] [ema: 0.999772] 
[Epoch 136/225] [Batch 200/223] [D loss: 0.373271] [G loss: 0.230864] [ema: 0.999773] 
[Epoch 137/225] [Batch 0/223] [D loss: 0.356512] [G loss: 0.213299] [ema: 0.999773] 
[Epoch 137/225] [Batch 100/223] [D loss: 0.358335] [G loss: 0.192357] [ema: 0.999774] 
[Epoch 137/225] [Batch 200/223] [D loss: 0.300222] [G loss: 0.233078] [ema: 0.999775] 
[Epoch 138/225] [Batch 0/223] [D loss: 0.298319] [G loss: 0.215330] [ema: 0.999775] 
[Epoch 138/225] [Batch 100/223] [D loss: 0.327817] [G loss: 0.211220] [ema: 0.999776] 
[Epoch 138/225] [Batch 200/223] [D loss: 0.284091] [G loss: 0.222251] [ema: 0.999776] 
[Epoch 139/225] [Batch 0/223] [D loss: 0.300160] [G loss: 0.222658] [ema: 0.999776] 
[Epoch 139/225] [Batch 100/223] [D loss: 0.326119] [G loss: 0.210044] [ema: 0.999777] 
[Epoch 139/225] [Batch 200/223] [D loss: 0.287319] [G loss: 0.211753] [ema: 0.999778] 
[Epoch 140/225] [Batch 0/223] [D loss: 0.342578] [G loss: 0.241953] [ema: 0.999778] 
[Epoch 140/225] [Batch 100/223] [D loss: 0.304623] [G loss: 0.235113] [ema: 0.999779] 
[Epoch 140/225] [Batch 200/223] [D loss: 0.296570] [G loss: 0.225159] [ema: 0.999779] 
[Epoch 141/225] [Batch 0/223] [D loss: 0.306756] [G loss: 0.222400] [ema: 0.999780] 
[Epoch 141/225] [Batch 100/223] [D loss: 0.302289] [G loss: 0.228019] [ema: 0.999780] 
[Epoch 141/225] [Batch 200/223] [D loss: 0.319513] [G loss: 0.191500] [ema: 0.999781] 
[Epoch 142/225] [Batch 0/223] [D loss: 0.321290] [G loss: 0.207751] [ema: 0.999781] 
[Epoch 142/225] [Batch 100/223] [D loss: 0.304252] [G loss: 0.219861] [ema: 0.999782] 
[Epoch 142/225] [Batch 200/223] [D loss: 0.288692] [G loss: 0.211189] [ema: 0.999783] 
[Epoch 143/225] [Batch 0/223] [D loss: 0.289848] [G loss: 0.242809] [ema: 0.999783] 
[Epoch 143/225] [Batch 100/223] [D loss: 0.316297] [G loss: 0.257606] [ema: 0.999783] 
[Epoch 143/225] [Batch 200/223] [D loss: 0.293808] [G loss: 0.222340] [ema: 0.999784] 
[Epoch 144/225] [Batch 0/223] [D loss: 0.311408] [G loss: 0.232064] [ema: 0.999784] 
[Epoch 144/225] [Batch 100/223] [D loss: 0.256586] [G loss: 0.206033] [ema: 0.999785] 
[Epoch 144/225] [Batch 200/223] [D loss: 0.292223] [G loss: 0.244766] [ema: 0.999786] 
[Epoch 145/225] [Batch 0/223] [D loss: 0.285504] [G loss: 0.257032] [ema: 0.999786] 
[Epoch 145/225] [Batch 100/223] [D loss: 0.319553] [G loss: 0.211964] [ema: 0.999786] 
[Epoch 145/225] [Batch 200/223] [D loss: 0.292499] [G loss: 0.207310] [ema: 0.999787] 
[Epoch 146/225] [Batch 0/223] [D loss: 0.292820] [G loss: 0.234241] [ema: 0.999787] 
[Epoch 146/225] [Batch 100/223] [D loss: 0.323002] [G loss: 0.200908] [ema: 0.999788] 
[Epoch 146/225] [Batch 200/223] [D loss: 0.343081] [G loss: 0.216349] [ema: 0.999788] 
[Epoch 147/225] [Batch 0/223] [D loss: 0.301677] [G loss: 0.202582] [ema: 0.999789] 
[Epoch 147/225] [Batch 100/223] [D loss: 0.318037] [G loss: 0.222647] [ema: 0.999789] 
[Epoch 147/225] [Batch 200/223] [D loss: 0.265589] [G loss: 0.210834] [ema: 0.999790] 
[Epoch 148/225] [Batch 0/223] [D loss: 0.284953] [G loss: 0.211162] [ema: 0.999790] 
[Epoch 148/225] [Batch 100/223] [D loss: 0.284231] [G loss: 0.235144] [ema: 0.999791] 
[Epoch 148/225] [Batch 200/223] [D loss: 0.262949] [G loss: 0.210898] [ema: 0.999791] 
[Epoch 149/225] [Batch 0/223] [D loss: 0.281637] [G loss: 0.223332] [ema: 0.999791] 
[Epoch 149/225] [Batch 100/223] [D loss: 0.283567] [G loss: 0.220199] [ema: 0.999792] 
[Epoch 149/225] [Batch 200/223] [D loss: 0.318389] [G loss: 0.228864] [ema: 0.999793] 
[Epoch 150/225] [Batch 0/223] [D loss: 0.333193] [G loss: 0.211889] [ema: 0.999793] 
[Epoch 150/225] [Batch 100/223] [D loss: 0.304969] [G loss: 0.205250] [ema: 0.999793] 
[Epoch 150/225] [Batch 200/223] [D loss: 0.289177] [G loss: 0.218590] [ema: 0.999794] 
[Epoch 151/225] [Batch 0/223] [D loss: 0.302866] [G loss: 0.226738] [ema: 0.999794] 
[Epoch 151/225] [Batch 100/223] [D loss: 0.275336] [G loss: 0.243015] [ema: 0.999795] 
[Epoch 151/225] [Batch 200/223] [D loss: 0.273703] [G loss: 0.218559] [ema: 0.999795] 
[Epoch 152/225] [Batch 0/223] [D loss: 0.295588] [G loss: 0.215397] [ema: 0.999796] 
[Epoch 152/225] [Batch 100/223] [D loss: 0.289251] [G loss: 0.240525] [ema: 0.999796] 
[Epoch 152/225] [Batch 200/223] [D loss: 0.273151] [G loss: 0.209912] [ema: 0.999797] 
[Epoch 153/225] [Batch 0/223] [D loss: 0.337150] [G loss: 0.222951] [ema: 0.999797] 
[Epoch 153/225] [Batch 100/223] [D loss: 0.279229] [G loss: 0.242973] [ema: 0.999797] 
[Epoch 153/225] [Batch 200/223] [D loss: 0.295056] [G loss: 0.205807] [ema: 0.999798] 
[Epoch 154/225] [Batch 0/223] [D loss: 0.287576] [G loss: 0.239693] [ema: 0.999798] 
[Epoch 154/225] [Batch 100/223] [D loss: 0.287150] [G loss: 0.232934] [ema: 0.999799] 
[Epoch 154/225] [Batch 200/223] [D loss: 0.363679] [G loss: 0.208115] [ema: 0.999799] 
[Epoch 155/225] [Batch 0/223] [D loss: 0.275929] [G loss: 0.205378] [ema: 0.999799] 
[Epoch 155/225] [Batch 100/223] [D loss: 0.297662] [G loss: 0.246703] [ema: 0.999800] 
[Epoch 155/225] [Batch 200/223] [D loss: 0.293309] [G loss: 0.218976] [ema: 0.999801] 
[Epoch 156/225] [Batch 0/223] [D loss: 0.278887] [G loss: 0.225514] [ema: 0.999801] 
[Epoch 156/225] [Batch 100/223] [D loss: 0.323187] [G loss: 0.231655] [ema: 0.999801] 
[Epoch 156/225] [Batch 200/223] [D loss: 0.288521] [G loss: 0.221801] [ema: 0.999802] 
[Epoch 157/225] [Batch 0/223] [D loss: 0.291481] [G loss: 0.226236] [ema: 0.999802] 
[Epoch 157/225] [Batch 100/223] [D loss: 0.327870] [G loss: 0.216610] [ema: 0.999803] 
[Epoch 157/225] [Batch 200/223] [D loss: 0.291709] [G loss: 0.241670] [ema: 0.999803] 
[Epoch 158/225] [Batch 0/223] [D loss: 0.348344] [G loss: 0.246281] [ema: 0.999803] 
[Epoch 158/225] [Batch 100/223] [D loss: 0.274569] [G loss: 0.233116] [ema: 0.999804] 
[Epoch 158/225] [Batch 200/223] [D loss: 0.312086] [G loss: 0.216521] [ema: 0.999804] 
[Epoch 159/225] [Batch 0/223] [D loss: 0.282216] [G loss: 0.260008] [ema: 0.999805] 
[Epoch 159/225] [Batch 100/223] [D loss: 0.316301] [G loss: 0.216259] [ema: 0.999805] 
[Epoch 159/225] [Batch 200/223] [D loss: 0.284352] [G loss: 0.263422] [ema: 0.999806] 
[Epoch 160/225] [Batch 0/223] [D loss: 0.308987] [G loss: 0.226496] [ema: 0.999806] 
[Epoch 160/225] [Batch 100/223] [D loss: 0.316931] [G loss: 0.249423] [ema: 0.999806] 
[Epoch 160/225] [Batch 200/223] [D loss: 0.339872] [G loss: 0.183375] [ema: 0.999807] 
[Epoch 161/225] [Batch 0/223] [D loss: 0.347505] [G loss: 0.233790] [ema: 0.999807] 
[Epoch 161/225] [Batch 100/223] [D loss: 0.297746] [G loss: 0.251393] [ema: 0.999807] 
[Epoch 161/225] [Batch 200/223] [D loss: 0.318195] [G loss: 0.216130] [ema: 0.999808] 
[Epoch 162/225] [Batch 0/223] [D loss: 0.296509] [G loss: 0.220426] [ema: 0.999808] 
[Epoch 162/225] [Batch 100/223] [D loss: 0.294209] [G loss: 0.246836] [ema: 0.999809] 
[Epoch 162/225] [Batch 200/223] [D loss: 0.270970] [G loss: 0.242185] [ema: 0.999809] 
[Epoch 163/225] [Batch 0/223] [D loss: 0.327883] [G loss: 0.203505] [ema: 0.999809] 
[Epoch 163/225] [Batch 100/223] [D loss: 0.317943] [G loss: 0.221854] [ema: 0.999810] 
[Epoch 163/225] [Batch 200/223] [D loss: 0.303229] [G loss: 0.224525] [ema: 0.999810] 
[Epoch 164/225] [Batch 0/223] [D loss: 0.308264] [G loss: 0.233085] [ema: 0.999810] 
[Epoch 164/225] [Batch 100/223] [D loss: 0.315145] [G loss: 0.233839] [ema: 0.999811] 
[Epoch 164/225] [Batch 200/223] [D loss: 0.285937] [G loss: 0.237933] [ema: 0.999812] 
[Epoch 165/225] [Batch 0/223] [D loss: 0.323694] [G loss: 0.225751] [ema: 0.999812] 
[Epoch 165/225] [Batch 100/223] [D loss: 0.314854] [G loss: 0.206630] [ema: 0.999812] 
[Epoch 165/225] [Batch 200/223] [D loss: 0.274881] [G loss: 0.224329] [ema: 0.999813] 
[Epoch 166/225] [Batch 0/223] [D loss: 0.298781] [G loss: 0.255390] [ema: 0.999813] 
[Epoch 166/225] [Batch 100/223] [D loss: 0.284166] [G loss: 0.212228] [ema: 0.999813] 
[Epoch 166/225] [Batch 200/223] [D loss: 0.314708] [G loss: 0.233689] [ema: 0.999814] 
[Epoch 167/225] [Batch 0/223] [D loss: 0.308343] [G loss: 0.217187] [ema: 0.999814] 
[Epoch 167/225] [Batch 100/223] [D loss: 0.306410] [G loss: 0.223790] [ema: 0.999814] 
[Epoch 167/225] [Batch 200/223] [D loss: 0.300402] [G loss: 0.221443] [ema: 0.999815] 
[Epoch 168/225] [Batch 0/223] [D loss: 0.298894] [G loss: 0.242222] [ema: 0.999815] 
[Epoch 168/225] [Batch 100/223] [D loss: 0.295247] [G loss: 0.227006] [ema: 0.999815] 
[Epoch 168/225] [Batch 200/223] [D loss: 0.277727] [G loss: 0.236915] [ema: 0.999816] 
[Epoch 169/225] [Batch 0/223] [D loss: 0.317726] [G loss: 0.210852] [ema: 0.999816] 
[Epoch 169/225] [Batch 100/223] [D loss: 0.291110] [G loss: 0.207694] [ema: 0.999817] 
[Epoch 169/225] [Batch 200/223] [D loss: 0.273910] [G loss: 0.225919] [ema: 0.999817] 
[Epoch 170/225] [Batch 0/223] [D loss: 0.293011] [G loss: 0.229013] [ema: 0.999817] 
[Epoch 170/225] [Batch 100/223] [D loss: 0.285160] [G loss: 0.207491] [ema: 0.999818] 
[Epoch 170/225] [Batch 200/223] [D loss: 0.308423] [G loss: 0.222615] [ema: 0.999818] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_60_100/MotionSense_DAGHAR_Multiclass_50000_D_60_2024_10_30_00_07_49/Model



[Epoch 171/225] [Batch 0/223] [D loss: 0.322194] [G loss: 0.247394] [ema: 0.999818] 
[Epoch 171/225] [Batch 100/223] [D loss: 0.284154] [G loss: 0.225961] [ema: 0.999819] 
[Epoch 171/225] [Batch 200/223] [D loss: 0.334588] [G loss: 0.209049] [ema: 0.999819] 
[Epoch 172/225] [Batch 0/223] [D loss: 0.313240] [G loss: 0.219458] [ema: 0.999819] 
[Epoch 172/225] [Batch 100/223] [D loss: 0.302443] [G loss: 0.206182] [ema: 0.999820] 
[Epoch 172/225] [Batch 200/223] [D loss: 0.300564] [G loss: 0.238779] [ema: 0.999820] 
[Epoch 173/225] [Batch 0/223] [D loss: 0.296667] [G loss: 0.257322] [ema: 0.999820] 
[Epoch 173/225] [Batch 100/223] [D loss: 0.288720] [G loss: 0.216127] [ema: 0.999821] 
[Epoch 173/225] [Batch 200/223] [D loss: 0.314138] [G loss: 0.243369] [ema: 0.999821] 
[Epoch 174/225] [Batch 0/223] [D loss: 0.334153] [G loss: 0.238148] [ema: 0.999821] 
[Epoch 174/225] [Batch 100/223] [D loss: 0.371001] [G loss: 0.225442] [ema: 0.999822] 
[Epoch 174/225] [Batch 200/223] [D loss: 0.381178] [G loss: 0.230451] [ema: 0.999822] 
[Epoch 175/225] [Batch 0/223] [D loss: 0.293407] [G loss: 0.222409] [ema: 0.999822] 
[Epoch 175/225] [Batch 100/223] [D loss: 0.313574] [G loss: 0.226099] [ema: 0.999823] 
[Epoch 175/225] [Batch 200/223] [D loss: 0.296485] [G loss: 0.228612] [ema: 0.999823] 
[Epoch 176/225] [Batch 0/223] [D loss: 0.292833] [G loss: 0.237734] [ema: 0.999823] 
[Epoch 176/225] [Batch 100/223] [D loss: 0.287456] [G loss: 0.238665] [ema: 0.999824] 
[Epoch 176/225] [Batch 200/223] [D loss: 0.287432] [G loss: 0.227834] [ema: 0.999824] 
[Epoch 177/225] [Batch 0/223] [D loss: 0.266735] [G loss: 0.216847] [ema: 0.999824] 
[Epoch 177/225] [Batch 100/223] [D loss: 0.288942] [G loss: 0.226025] [ema: 0.999825] 
[Epoch 177/225] [Batch 200/223] [D loss: 0.300116] [G loss: 0.239118] [ema: 0.999825] 
[Epoch 178/225] [Batch 0/223] [D loss: 0.337633] [G loss: 0.216391] [ema: 0.999825] 
[Epoch 178/225] [Batch 100/223] [D loss: 0.261055] [G loss: 0.248452] [ema: 0.999826] 
[Epoch 178/225] [Batch 200/223] [D loss: 0.303759] [G loss: 0.236613] [ema: 0.999826] 
[Epoch 179/225] [Batch 0/223] [D loss: 0.286037] [G loss: 0.248428] [ema: 0.999826] 
[Epoch 179/225] [Batch 100/223] [D loss: 0.344496] [G loss: 0.209615] [ema: 0.999827] 
[Epoch 179/225] [Batch 200/223] [D loss: 0.289795] [G loss: 0.216414] [ema: 0.999827] 
[Epoch 180/225] [Batch 0/223] [D loss: 0.302437] [G loss: 0.249744] [ema: 0.999827] 
[Epoch 180/225] [Batch 100/223] [D loss: 0.296688] [G loss: 0.257496] [ema: 0.999828] 
[Epoch 180/225] [Batch 200/223] [D loss: 0.308805] [G loss: 0.226857] [ema: 0.999828] 
[Epoch 181/225] [Batch 0/223] [D loss: 0.292809] [G loss: 0.237402] [ema: 0.999828] 
[Epoch 181/225] [Batch 100/223] [D loss: 0.298232] [G loss: 0.224169] [ema: 0.999829] 
[Epoch 181/225] [Batch 200/223] [D loss: 0.276780] [G loss: 0.225195] [ema: 0.999829] 
[Epoch 182/225] [Batch 0/223] [D loss: 0.309068] [G loss: 0.231037] [ema: 0.999829] 
[Epoch 182/225] [Batch 100/223] [D loss: 0.288062] [G loss: 0.213147] [ema: 0.999830] 
[Epoch 182/225] [Batch 200/223] [D loss: 0.309033] [G loss: 0.227359] [ema: 0.999830] 
[Epoch 183/225] [Batch 0/223] [D loss: 0.278717] [G loss: 0.227573] [ema: 0.999830] 
[Epoch 183/225] [Batch 100/223] [D loss: 0.263554] [G loss: 0.248503] [ema: 0.999831] 
[Epoch 183/225] [Batch 200/223] [D loss: 0.346027] [G loss: 0.233864] [ema: 0.999831] 
[Epoch 184/225] [Batch 0/223] [D loss: 0.308919] [G loss: 0.226756] [ema: 0.999831] 
[Epoch 184/225] [Batch 100/223] [D loss: 0.288797] [G loss: 0.218773] [ema: 0.999831] 
[Epoch 184/225] [Batch 200/223] [D loss: 0.300260] [G loss: 0.234934] [ema: 0.999832] 
[Epoch 185/225] [Batch 0/223] [D loss: 0.278334] [G loss: 0.235132] [ema: 0.999832] 
[Epoch 185/225] [Batch 100/223] [D loss: 0.307295] [G loss: 0.223585] [ema: 0.999832] 
[Epoch 185/225] [Batch 200/223] [D loss: 0.315654] [G loss: 0.226839] [ema: 0.999833] 
[Epoch 186/225] [Batch 0/223] [D loss: 0.270074] [G loss: 0.241306] [ema: 0.999833] 
[Epoch 186/225] [Batch 100/223] [D loss: 0.347820] [G loss: 0.219628] [ema: 0.999833] 
[Epoch 186/225] [Batch 200/223] [D loss: 0.250602] [G loss: 0.254050] [ema: 0.999834] 
[Epoch 187/225] [Batch 0/223] [D loss: 0.303442] [G loss: 0.246289] [ema: 0.999834] 
[Epoch 187/225] [Batch 100/223] [D loss: 0.294096] [G loss: 0.205462] [ema: 0.999834] 
[Epoch 187/225] [Batch 200/223] [D loss: 0.327631] [G loss: 0.208993] [ema: 0.999835] 
[Epoch 188/225] [Batch 0/223] [D loss: 0.367635] [G loss: 0.229251] [ema: 0.999835] 
[Epoch 188/225] [Batch 100/223] [D loss: 0.280234] [G loss: 0.234725] [ema: 0.999835] 
[Epoch 188/225] [Batch 200/223] [D loss: 0.311918] [G loss: 0.239453] [ema: 0.999835] 
[Epoch 189/225] [Batch 0/223] [D loss: 0.306467] [G loss: 0.232748] [ema: 0.999836] 
[Epoch 189/225] [Batch 100/223] [D loss: 0.299741] [G loss: 0.235050] [ema: 0.999836] 
[Epoch 189/225] [Batch 200/223] [D loss: 0.298981] [G loss: 0.228797] [ema: 0.999836] 
[Epoch 190/225] [Batch 0/223] [D loss: 0.266434] [G loss: 0.248978] [ema: 0.999836] 
[Epoch 190/225] [Batch 100/223] [D loss: 0.274464] [G loss: 0.239645] [ema: 0.999837] 
[Epoch 190/225] [Batch 200/223] [D loss: 0.318736] [G loss: 0.232833] [ema: 0.999837] 
[Epoch 191/225] [Batch 0/223] [D loss: 0.300444] [G loss: 0.219640] [ema: 0.999837] 
[Epoch 191/225] [Batch 100/223] [D loss: 0.303693] [G loss: 0.224330] [ema: 0.999838] 
[Epoch 191/225] [Batch 200/223] [D loss: 0.329973] [G loss: 0.215954] [ema: 0.999838] 
[Epoch 192/225] [Batch 0/223] [D loss: 0.295102] [G loss: 0.238944] [ema: 0.999838] 
[Epoch 192/225] [Batch 100/223] [D loss: 0.284939] [G loss: 0.211754] [ema: 0.999839] 
[Epoch 192/225] [Batch 200/223] [D loss: 0.285801] [G loss: 0.223492] [ema: 0.999839] 
[Epoch 193/225] [Batch 0/223] [D loss: 0.341676] [G loss: 0.251193] [ema: 0.999839] 
[Epoch 193/225] [Batch 100/223] [D loss: 0.286076] [G loss: 0.217571] [ema: 0.999839] 
[Epoch 193/225] [Batch 200/223] [D loss: 0.276694] [G loss: 0.223634] [ema: 0.999840] 
[Epoch 194/225] [Batch 0/223] [D loss: 0.332194] [G loss: 0.223718] [ema: 0.999840] 
[Epoch 194/225] [Batch 100/223] [D loss: 0.302740] [G loss: 0.232705] [ema: 0.999840] 
[Epoch 194/225] [Batch 200/223] [D loss: 0.295487] [G loss: 0.235627] [ema: 0.999841] 
[Epoch 195/225] [Batch 0/223] [D loss: 0.323629] [G loss: 0.244868] [ema: 0.999841] 
[Epoch 195/225] [Batch 100/223] [D loss: 0.289856] [G loss: 0.248899] [ema: 0.999841] 
[Epoch 195/225] [Batch 200/223] [D loss: 0.280964] [G loss: 0.224209] [ema: 0.999841] 
[Epoch 196/225] [Batch 0/223] [D loss: 0.319916] [G loss: 0.253786] [ema: 0.999841] 
[Epoch 196/225] [Batch 100/223] [D loss: 0.266339] [G loss: 0.240843] [ema: 0.999842] 
[Epoch 196/225] [Batch 200/223] [D loss: 0.297426] [G loss: 0.209536] [ema: 0.999842] 
[Epoch 197/225] [Batch 0/223] [D loss: 0.295554] [G loss: 0.228826] [ema: 0.999842] 
[Epoch 197/225] [Batch 100/223] [D loss: 0.335862] [G loss: 0.199587] [ema: 0.999843] 
[Epoch 197/225] [Batch 200/223] [D loss: 0.267608] [G loss: 0.231447] [ema: 0.999843] 
[Epoch 198/225] [Batch 0/223] [D loss: 0.248547] [G loss: 0.257227] [ema: 0.999843] 
[Epoch 198/225] [Batch 100/223] [D loss: 0.257854] [G loss: 0.213501] [ema: 0.999843] 
[Epoch 198/225] [Batch 200/223] [D loss: 0.290284] [G loss: 0.232373] [ema: 0.999844] 
[Epoch 199/225] [Batch 0/223] [D loss: 0.301854] [G loss: 0.252689] [ema: 0.999844] 
[Epoch 199/225] [Batch 100/223] [D loss: 0.328228] [G loss: 0.223754] [ema: 0.999844] 
[Epoch 199/225] [Batch 200/223] [D loss: 0.268352] [G loss: 0.223074] [ema: 0.999845] 
[Epoch 200/225] [Batch 0/223] [D loss: 0.309954] [G loss: 0.243755] [ema: 0.999845] 
[Epoch 200/225] [Batch 100/223] [D loss: 0.268052] [G loss: 0.243553] [ema: 0.999845] 
[Epoch 200/225] [Batch 200/223] [D loss: 0.301858] [G loss: 0.224583] [ema: 0.999845] 
[Epoch 201/225] [Batch 0/223] [D loss: 0.279875] [G loss: 0.227583] [ema: 0.999845] 
[Epoch 201/225] [Batch 100/223] [D loss: 0.272372] [G loss: 0.228744] [ema: 0.999846] 
[Epoch 201/225] [Batch 200/223] [D loss: 0.373213] [G loss: 0.229009] [ema: 0.999846] 
[Epoch 202/225] [Batch 0/223] [D loss: 0.340190] [G loss: 0.195246] [ema: 0.999846] 
[Epoch 202/225] [Batch 100/223] [D loss: 0.279390] [G loss: 0.231164] [ema: 0.999846] 
[Epoch 202/225] [Batch 200/223] [D loss: 0.272945] [G loss: 0.231566] [ema: 0.999847] 
[Epoch 203/225] [Batch 0/223] [D loss: 0.273196] [G loss: 0.250171] [ema: 0.999847] 
[Epoch 203/225] [Batch 100/223] [D loss: 0.251627] [G loss: 0.251811] [ema: 0.999847] 
[Epoch 203/225] [Batch 200/223] [D loss: 0.302228] [G loss: 0.203796] [ema: 0.999848] 
[Epoch 204/225] [Batch 0/223] [D loss: 0.303120] [G loss: 0.252607] [ema: 0.999848] 
[Epoch 204/225] [Batch 100/223] [D loss: 0.299324] [G loss: 0.215439] [ema: 0.999848] 
[Epoch 204/225] [Batch 200/223] [D loss: 0.270781] [G loss: 0.225314] [ema: 0.999848] 
[Epoch 205/225] [Batch 0/223] [D loss: 0.287485] [G loss: 0.223116] [ema: 0.999848] 
[Epoch 205/225] [Batch 100/223] [D loss: 0.290940] [G loss: 0.240431] [ema: 0.999849] 
[Epoch 205/225] [Batch 200/223] [D loss: 0.294893] [G loss: 0.220684] [ema: 0.999849] 
[Epoch 206/225] [Batch 0/223] [D loss: 0.287199] [G loss: 0.235814] [ema: 0.999849] 
[Epoch 206/225] [Batch 100/223] [D loss: 0.276377] [G loss: 0.227217] [ema: 0.999849] 
[Epoch 206/225] [Batch 200/223] [D loss: 0.269614] [G loss: 0.225660] [ema: 0.999850] 
[Epoch 207/225] [Batch 0/223] [D loss: 0.301343] [G loss: 0.239082] [ema: 0.999850] 
[Epoch 207/225] [Batch 100/223] [D loss: 0.268278] [G loss: 0.239811] [ema: 0.999850] 
[Epoch 207/225] [Batch 200/223] [D loss: 0.292238] [G loss: 0.222522] [ema: 0.999851] 
[Epoch 208/225] [Batch 0/223] [D loss: 0.305286] [G loss: 0.226982] [ema: 0.999851] 
[Epoch 208/225] [Batch 100/223] [D loss: 0.277644] [G loss: 0.235450] [ema: 0.999851] 
[Epoch 208/225] [Batch 200/223] [D loss: 0.278621] [G loss: 0.230584] [ema: 0.999851] 
[Epoch 209/225] [Batch 0/223] [D loss: 0.295551] [G loss: 0.228087] [ema: 0.999851] 
[Epoch 209/225] [Batch 100/223] [D loss: 0.271774] [G loss: 0.226581] [ema: 0.999852] 
[Epoch 209/225] [Batch 200/223] [D loss: 0.315672] [G loss: 0.233323] [ema: 0.999852] 
[Epoch 210/225] [Batch 0/223] [D loss: 0.300980] [G loss: 0.247794] [ema: 0.999852] 
[Epoch 210/225] [Batch 100/223] [D loss: 0.279955] [G loss: 0.228351] [ema: 0.999852] 
[Epoch 210/225] [Batch 200/223] [D loss: 0.297627] [G loss: 0.256914] [ema: 0.999853] 
[Epoch 211/225] [Batch 0/223] [D loss: 0.281903] [G loss: 0.243641] [ema: 0.999853] 
[Epoch 211/225] [Batch 100/223] [D loss: 0.303096] [G loss: 0.205097] [ema: 0.999853] 
[Epoch 211/225] [Batch 200/223] [D loss: 0.302789] [G loss: 0.238810] [ema: 0.999853] 
[Epoch 212/225] [Batch 0/223] [D loss: 0.291284] [G loss: 0.239321] [ema: 0.999853] 
[Epoch 212/225] [Batch 100/223] [D loss: 0.272016] [G loss: 0.227819] [ema: 0.999854] 
[Epoch 212/225] [Batch 200/223] [D loss: 0.286993] [G loss: 0.244931] [ema: 0.999854] 
[Epoch 213/225] [Batch 0/223] [D loss: 0.295278] [G loss: 0.239055] [ema: 0.999854] 
[Epoch 213/225] [Batch 100/223] [D loss: 0.337223] [G loss: 0.233610] [ema: 0.999854] 
[Epoch 213/225] [Batch 200/223] [D loss: 0.271947] [G loss: 0.216427] [ema: 0.999855] 
[Epoch 214/225] [Batch 0/223] [D loss: 0.310739] [G loss: 0.250530] [ema: 0.999855] 
[Epoch 214/225] [Batch 100/223] [D loss: 0.326138] [G loss: 0.223439] [ema: 0.999855] 
[Epoch 214/225] [Batch 200/223] [D loss: 0.316648] [G loss: 0.218780] [ema: 0.999855] 
[Epoch 215/225] [Batch 0/223] [D loss: 0.347911] [G loss: 0.212295] [ema: 0.999855] 
[Epoch 215/225] [Batch 100/223] [D loss: 0.311542] [G loss: 0.195421] [ema: 0.999856] 
[Epoch 215/225] [Batch 200/223] [D loss: 0.287287] [G loss: 0.234020] [ema: 0.999856] 
[Epoch 216/225] [Batch 0/223] [D loss: 0.268373] [G loss: 0.245927] [ema: 0.999856] 
[Epoch 216/225] [Batch 100/223] [D loss: 0.270819] [G loss: 0.227853] [ema: 0.999856] 
[Epoch 216/225] [Batch 200/223] [D loss: 0.259529] [G loss: 0.229773] [ema: 0.999857] 
[Epoch 217/225] [Batch 0/223] [D loss: 0.294967] [G loss: 0.213509] [ema: 0.999857] 
[Epoch 217/225] [Batch 100/223] [D loss: 0.307009] [G loss: 0.229904] [ema: 0.999857] 
[Epoch 217/225] [Batch 200/223] [D loss: 0.250228] [G loss: 0.242579] [ema: 0.999857] 
[Epoch 218/225] [Batch 0/223] [D loss: 0.314543] [G loss: 0.229086] [ema: 0.999857] 
[Epoch 218/225] [Batch 100/223] [D loss: 0.315426] [G loss: 0.232739] [ema: 0.999858] 
[Epoch 218/225] [Batch 200/223] [D loss: 0.297386] [G loss: 0.224582] [ema: 0.999858] 
[Epoch 219/225] [Batch 0/223] [D loss: 0.329870] [G loss: 0.213557] [ema: 0.999858] 
[Epoch 219/225] [Batch 100/223] [D loss: 0.312749] [G loss: 0.208441] [ema: 0.999858] 
[Epoch 219/225] [Batch 200/223] [D loss: 0.373340] [G loss: 0.237036] [ema: 0.999859] 
[Epoch 220/225] [Batch 0/223] [D loss: 0.270312] [G loss: 0.238820] [ema: 0.999859] 
[Epoch 220/225] [Batch 100/223] [D loss: 0.297034] [G loss: 0.231665] [ema: 0.999859] 
[Epoch 220/225] [Batch 200/223] [D loss: 0.268937] [G loss: 0.241159] [ema: 0.999859] 
[Epoch 221/225] [Batch 0/223] [D loss: 0.267258] [G loss: 0.240443] [ema: 0.999859] 
[Epoch 221/225] [Batch 100/223] [D loss: 0.285010] [G loss: 0.230164] [ema: 0.999860] 
[Epoch 221/225] [Batch 200/223] [D loss: 0.294633] [G loss: 0.242925] [ema: 0.999860] 
[Epoch 222/225] [Batch 0/223] [D loss: 0.341607] [G loss: 0.265178] [ema: 0.999860] 
[Epoch 222/225] [Batch 100/223] [D loss: 0.291272] [G loss: 0.235799] [ema: 0.999860] 
[Epoch 222/225] [Batch 200/223] [D loss: 0.258903] [G loss: 0.230377] [ema: 0.999861] 
[Epoch 223/225] [Batch 0/223] [D loss: 0.288551] [G loss: 0.240494] [ema: 0.999861] 
[Epoch 223/225] [Batch 100/223] [D loss: 0.275473] [G loss: 0.234818] [ema: 0.999861] 
[Epoch 223/225] [Batch 200/223] [D loss: 0.260052] [G loss: 0.250466] [ema: 0.999861] 
[Epoch 224/225] [Batch 0/223] [D loss: 0.308930] [G loss: 0.242334] [ema: 0.999861] 
[Epoch 224/225] [Batch 100/223] [D loss: 0.288647] [G loss: 0.231810] [ema: 0.999862] 
[Epoch 224/225] [Batch 200/223] [D loss: 0.294003] [G loss: 0.218395] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
RealWorld_thigh_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
RealWorld_thigh_DAGHAR_Multiclass
daghar
return single class data and labels, class is RealWorld_thigh_DAGHAR_Multiclass
data shape is (10338, 6, 1, 60)
label shape is (10338,)
647
Epochs between checkpoint: 20



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_60_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_60_2024_10_30_00_43_14/Model



[Epoch 0/78] [Batch 0/647] [D loss: 1.304175] [G loss: 0.603372] [ema: 0.000000] 
[Epoch 0/78] [Batch 100/647] [D loss: 0.435277] [G loss: 0.144488] [ema: 0.933033] 
[Epoch 0/78] [Batch 200/647] [D loss: 0.478195] [G loss: 0.189042] [ema: 0.965936] 
[Epoch 0/78] [Batch 300/647] [D loss: 0.555572] [G loss: 0.134435] [ema: 0.977160] 
[Epoch 0/78] [Batch 400/647] [D loss: 0.465872] [G loss: 0.174809] [ema: 0.982821] 
[Epoch 0/78] [Batch 500/647] [D loss: 0.517989] [G loss: 0.171129] [ema: 0.986233] 
[Epoch 0/78] [Batch 600/647] [D loss: 0.414316] [G loss: 0.161125] [ema: 0.988514] 
[Epoch 1/78] [Batch 0/647] [D loss: 0.455701] [G loss: 0.182751] [ema: 0.989344] 
[Epoch 1/78] [Batch 100/647] [D loss: 0.444466] [G loss: 0.164949] [ema: 0.990764] 
[Epoch 1/78] [Batch 200/647] [D loss: 0.442976] [G loss: 0.176276] [ema: 0.991850] 
[Epoch 1/78] [Batch 300/647] [D loss: 0.438430] [G loss: 0.157648] [ema: 0.992707] 
[Epoch 1/78] [Batch 400/647] [D loss: 0.482945] [G loss: 0.124710] [ema: 0.993402] 
[Epoch 1/78] [Batch 500/647] [D loss: 0.404881] [G loss: 0.156052] [ema: 0.993975] 
[Epoch 1/78] [Batch 600/647] [D loss: 0.406729] [G loss: 0.136280] [ema: 0.994457] 
[Epoch 2/78] [Batch 0/647] [D loss: 0.433760] [G loss: 0.163099] [ema: 0.994658] 
[Epoch 2/78] [Batch 100/647] [D loss: 0.473496] [G loss: 0.135724] [ema: 0.995040] 
[Epoch 2/78] [Batch 200/647] [D loss: 0.405122] [G loss: 0.189327] [ema: 0.995371] 
[Epoch 2/78] [Batch 300/647] [D loss: 0.428751] [G loss: 0.146043] [ema: 0.995661] 
[Epoch 2/78] [Batch 400/647] [D loss: 0.417127] [G loss: 0.177496] [ema: 0.995917] 
[Epoch 2/78] [Batch 500/647] [D loss: 0.438734] [G loss: 0.145150] [ema: 0.996144] 
[Epoch 2/78] [Batch 600/647] [D loss: 0.439590] [G loss: 0.172600] [ema: 0.996347] 
[Epoch 3/78] [Batch 0/647] [D loss: 0.428348] [G loss: 0.172821] [ema: 0.996435] 
[Epoch 3/78] [Batch 100/647] [D loss: 0.449499] [G loss: 0.159897] [ema: 0.996610] 
[Epoch 3/78] [Batch 200/647] [D loss: 0.430885] [G loss: 0.167608] [ema: 0.996768] 
[Epoch 3/78] [Batch 300/647] [D loss: 0.358927] [G loss: 0.208537] [ema: 0.996912] 
[Epoch 3/78] [Batch 400/647] [D loss: 0.471789] [G loss: 0.142614] [ema: 0.997043] 
[Epoch 3/78] [Batch 500/647] [D loss: 0.412307] [G loss: 0.182216] [ema: 0.997164] 
[Epoch 3/78] [Batch 600/647] [D loss: 0.389990] [G loss: 0.155274] [ema: 0.997276] 
[Epoch 4/78] [Batch 0/647] [D loss: 0.361030] [G loss: 0.226613] [ema: 0.997325] 
[Epoch 4/78] [Batch 100/647] [D loss: 0.378272] [G loss: 0.172975] [ema: 0.997425] 
[Epoch 4/78] [Batch 200/647] [D loss: 0.350826] [G loss: 0.208245] [ema: 0.997517] 
[Epoch 4/78] [Batch 300/647] [D loss: 0.319964] [G loss: 0.209580] [ema: 0.997603] 
[Epoch 4/78] [Batch 400/647] [D loss: 0.340767] [G loss: 0.162962] [ema: 0.997683] 
[Epoch 4/78] [Batch 500/647] [D loss: 0.366061] [G loss: 0.238637] [ema: 0.997758] 
[Epoch 4/78] [Batch 600/647] [D loss: 0.370656] [G loss: 0.221450] [ema: 0.997828] 
[Epoch 5/78] [Batch 0/647] [D loss: 0.377724] [G loss: 0.189990] [ema: 0.997860] 
[Epoch 5/78] [Batch 100/647] [D loss: 0.389232] [G loss: 0.167060] [ema: 0.997924] 
[Epoch 5/78] [Batch 200/647] [D loss: 0.361288] [G loss: 0.178965] [ema: 0.997984] 
[Epoch 5/78] [Batch 300/647] [D loss: 0.425236] [G loss: 0.225911] [ema: 0.998041] 
[Epoch 5/78] [Batch 400/647] [D loss: 0.432716] [G loss: 0.170570] [ema: 0.998095] 
[Epoch 5/78] [Batch 500/647] [D loss: 0.416224] [G loss: 0.167919] [ema: 0.998146] 
[Epoch 5/78] [Batch 600/647] [D loss: 0.329774] [G loss: 0.191897] [ema: 0.998194] 
[Epoch 6/78] [Batch 0/647] [D loss: 0.431067] [G loss: 0.212486] [ema: 0.998216] 
[Epoch 6/78] [Batch 100/647] [D loss: 0.404385] [G loss: 0.185598] [ema: 0.998261] 
[Epoch 6/78] [Batch 200/647] [D loss: 0.405569] [G loss: 0.196223] [ema: 0.998303] 
[Epoch 6/78] [Batch 300/647] [D loss: 0.340023] [G loss: 0.210366] [ema: 0.998344] 
[Epoch 6/78] [Batch 400/647] [D loss: 0.379327] [G loss: 0.183666] [ema: 0.998383] 
[Epoch 6/78] [Batch 500/647] [D loss: 0.393156] [G loss: 0.210322] [ema: 0.998419] 
[Epoch 6/78] [Batch 600/647] [D loss: 0.394504] [G loss: 0.183554] [ema: 0.998455] 
[Epoch 7/78] [Batch 0/647] [D loss: 0.387546] [G loss: 0.198827] [ema: 0.998471] 
[Epoch 7/78] [Batch 100/647] [D loss: 0.426458] [G loss: 0.169862] [ema: 0.998504] 
[Epoch 7/78] [Batch 200/647] [D loss: 0.405018] [G loss: 0.178222] [ema: 0.998535] 
[Epoch 7/78] [Batch 300/647] [D loss: 0.362520] [G loss: 0.166396] [ema: 0.998566] 
[Epoch 7/78] [Batch 400/647] [D loss: 0.377990] [G loss: 0.199413] [ema: 0.998595] 
[Epoch 7/78] [Batch 500/647] [D loss: 0.408173] [G loss: 0.183519] [ema: 0.998623] 
[Epoch 7/78] [Batch 600/647] [D loss: 0.442059] [G loss: 0.171959] [ema: 0.998649] 
[Epoch 8/78] [Batch 0/647] [D loss: 0.419934] [G loss: 0.193407] [ema: 0.998662] 
[Epoch 8/78] [Batch 100/647] [D loss: 0.426631] [G loss: 0.169625] [ema: 0.998687] 
[Epoch 8/78] [Batch 200/647] [D loss: 0.417657] [G loss: 0.185227] [ema: 0.998711] 
[Epoch 8/78] [Batch 300/647] [D loss: 0.407399] [G loss: 0.175294] [ema: 0.998735] 
[Epoch 8/78] [Batch 400/647] [D loss: 0.414827] [G loss: 0.181381] [ema: 0.998758] 
[Epoch 8/78] [Batch 500/647] [D loss: 0.381726] [G loss: 0.187177] [ema: 0.998780] 
[Epoch 8/78] [Batch 600/647] [D loss: 0.391339] [G loss: 0.173770] [ema: 0.998801] 
[Epoch 9/78] [Batch 0/647] [D loss: 0.419678] [G loss: 0.172030] [ema: 0.998810] 
[Epoch 9/78] [Batch 100/647] [D loss: 0.414391] [G loss: 0.192584] [ema: 0.998830] 
[Epoch 9/78] [Batch 200/647] [D loss: 0.421746] [G loss: 0.167558] [ema: 0.998850] 
[Epoch 9/78] [Batch 300/647] [D loss: 0.398713] [G loss: 0.189885] [ema: 0.998869] 
[Epoch 9/78] [Batch 400/647] [D loss: 0.430708] [G loss: 0.183401] [ema: 0.998887] 
[Epoch 9/78] [Batch 500/647] [D loss: 0.397691] [G loss: 0.178151] [ema: 0.998904] 
[Epoch 9/78] [Batch 600/647] [D loss: 0.425216] [G loss: 0.178177] [ema: 0.998921] 
[Epoch 10/78] [Batch 0/647] [D loss: 0.412911] [G loss: 0.168232] [ema: 0.998929] 
[Epoch 10/78] [Batch 100/647] [D loss: 0.352500] [G loss: 0.180575] [ema: 0.998946] 
[Epoch 10/78] [Batch 200/647] [D loss: 0.391404] [G loss: 0.162112] [ema: 0.998961] 
[Epoch 10/78] [Batch 300/647] [D loss: 0.367968] [G loss: 0.193887] [ema: 0.998977] 
[Epoch 10/78] [Batch 400/647] [D loss: 0.374104] [G loss: 0.171436] [ema: 0.998992] 
[Epoch 10/78] [Batch 500/647] [D loss: 0.391839] [G loss: 0.167732] [ema: 0.999006] 
[Epoch 10/78] [Batch 600/647] [D loss: 0.377691] [G loss: 0.211186] [ema: 0.999020] 
[Epoch 11/78] [Batch 0/647] [D loss: 0.376594] [G loss: 0.190647] [ema: 0.999027] 
[Epoch 11/78] [Batch 100/647] [D loss: 0.412113] [G loss: 0.183530] [ema: 0.999040] 
[Epoch 11/78] [Batch 200/647] [D loss: 0.403814] [G loss: 0.181652] [ema: 0.999053] 
[Epoch 11/78] [Batch 300/647] [D loss: 0.373785] [G loss: 0.193000] [ema: 0.999066] 
[Epoch 11/78] [Batch 400/647] [D loss: 0.362052] [G loss: 0.192307] [ema: 0.999078] 
[Epoch 11/78] [Batch 500/647] [D loss: 0.358313] [G loss: 0.180962] [ema: 0.999090] 
[Epoch 11/78] [Batch 600/647] [D loss: 0.410440] [G loss: 0.176542] [ema: 0.999102] 
[Epoch 12/78] [Batch 0/647] [D loss: 0.452753] [G loss: 0.169339] [ema: 0.999108] 
[Epoch 12/78] [Batch 100/647] [D loss: 0.408769] [G loss: 0.168288] [ema: 0.999119] 
[Epoch 12/78] [Batch 200/647] [D loss: 0.402057] [G loss: 0.174426] [ema: 0.999130] 
[Epoch 12/78] [Batch 300/647] [D loss: 0.379493] [G loss: 0.182737] [ema: 0.999141] 
[Epoch 12/78] [Batch 400/647] [D loss: 0.422397] [G loss: 0.188348] [ema: 0.999151] 
[Epoch 12/78] [Batch 500/647] [D loss: 0.412423] [G loss: 0.188871] [ema: 0.999162] 
[Epoch 12/78] [Batch 600/647] [D loss: 0.410457] [G loss: 0.187897] [ema: 0.999172] 
[Epoch 13/78] [Batch 0/647] [D loss: 0.417186] [G loss: 0.176991] [ema: 0.999176] 
[Epoch 13/78] [Batch 100/647] [D loss: 0.365498] [G loss: 0.193441] [ema: 0.999186] 
[Epoch 13/78] [Batch 200/647] [D loss: 0.426263] [G loss: 0.193465] [ema: 0.999195] 
[Epoch 13/78] [Batch 300/647] [D loss: 0.351419] [G loss: 0.186370] [ema: 0.999205] 
[Epoch 13/78] [Batch 400/647] [D loss: 0.381838] [G loss: 0.184362] [ema: 0.999214] 
[Epoch 13/78] [Batch 500/647] [D loss: 0.418216] [G loss: 0.171222] [ema: 0.999222] 
[Epoch 13/78] [Batch 600/647] [D loss: 0.386183] [G loss: 0.170849] [ema: 0.999231] 
[Epoch 14/78] [Batch 0/647] [D loss: 0.389745] [G loss: 0.182831] [ema: 0.999235] 
[Epoch 14/78] [Batch 100/647] [D loss: 0.417350] [G loss: 0.179131] [ema: 0.999243] 
[Epoch 14/78] [Batch 200/647] [D loss: 0.419501] [G loss: 0.201394] [ema: 0.999252] 
[Epoch 14/78] [Batch 300/647] [D loss: 0.374774] [G loss: 0.170998] [ema: 0.999260] 
[Epoch 14/78] [Batch 400/647] [D loss: 0.376881] [G loss: 0.167373] [ema: 0.999267] 
[Epoch 14/78] [Batch 500/647] [D loss: 0.410154] [G loss: 0.181701] [ema: 0.999275] 
[Epoch 14/78] [Batch 600/647] [D loss: 0.417221] [G loss: 0.191200] [ema: 0.999283] 
[Epoch 15/78] [Batch 0/647] [D loss: 0.376812] [G loss: 0.186097] [ema: 0.999286] 
[Epoch 15/78] [Batch 100/647] [D loss: 0.359838] [G loss: 0.212845] [ema: 0.999293] 
[Epoch 15/78] [Batch 200/647] [D loss: 0.394622] [G loss: 0.172735] [ema: 0.999300] 
[Epoch 15/78] [Batch 300/647] [D loss: 0.399125] [G loss: 0.178076] [ema: 0.999307] 
[Epoch 15/78] [Batch 400/647] [D loss: 0.395425] [G loss: 0.198074] [ema: 0.999314] 
[Epoch 15/78] [Batch 500/647] [D loss: 0.413533] [G loss: 0.167697] [ema: 0.999321] 
[Epoch 15/78] [Batch 600/647] [D loss: 0.417468] [G loss: 0.135023] [ema: 0.999328] 
[Epoch 16/78] [Batch 0/647] [D loss: 0.389430] [G loss: 0.185610] [ema: 0.999331] 
[Epoch 16/78] [Batch 100/647] [D loss: 0.365452] [G loss: 0.190690] [ema: 0.999337] 
[Epoch 16/78] [Batch 200/647] [D loss: 0.383147] [G loss: 0.180896] [ema: 0.999343] 
[Epoch 16/78] [Batch 300/647] [D loss: 0.393776] [G loss: 0.177698] [ema: 0.999349] 
[Epoch 16/78] [Batch 400/647] [D loss: 0.401179] [G loss: 0.166455] [ema: 0.999356] 
[Epoch 16/78] [Batch 500/647] [D loss: 0.409799] [G loss: 0.192176] [ema: 0.999361] 
[Epoch 16/78] [Batch 600/647] [D loss: 0.378117] [G loss: 0.182788] [ema: 0.999367] 
[Epoch 17/78] [Batch 0/647] [D loss: 0.424933] [G loss: 0.179248] [ema: 0.999370] 
[Epoch 17/78] [Batch 100/647] [D loss: 0.427445] [G loss: 0.155946] [ema: 0.999376] 
[Epoch 17/78] [Batch 200/647] [D loss: 0.357872] [G loss: 0.194190] [ema: 0.999381] 
[Epoch 17/78] [Batch 300/647] [D loss: 0.395146] [G loss: 0.171742] [ema: 0.999387] 
[Epoch 17/78] [Batch 400/647] [D loss: 0.399314] [G loss: 0.188092] [ema: 0.999392] 
[Epoch 17/78] [Batch 500/647] [D loss: 0.383483] [G loss: 0.192803] [ema: 0.999397] 
[Epoch 17/78] [Batch 600/647] [D loss: 0.385802] [G loss: 0.180761] [ema: 0.999403] 
[Epoch 18/78] [Batch 0/647] [D loss: 0.397018] [G loss: 0.182212] [ema: 0.999405] 
[Epoch 18/78] [Batch 100/647] [D loss: 0.425069] [G loss: 0.182380] [ema: 0.999410] 
[Epoch 18/78] [Batch 200/647] [D loss: 0.397790] [G loss: 0.177661] [ema: 0.999415] 
[Epoch 18/78] [Batch 300/647] [D loss: 0.426413] [G loss: 0.170359] [ema: 0.999420] 
[Epoch 18/78] [Batch 400/647] [D loss: 0.321156] [G loss: 0.202414] [ema: 0.999425] 
[Epoch 18/78] [Batch 500/647] [D loss: 0.388661] [G loss: 0.179236] [ema: 0.999429] 
[Epoch 18/78] [Batch 600/647] [D loss: 0.343353] [G loss: 0.180334] [ema: 0.999434] 
[Epoch 19/78] [Batch 0/647] [D loss: 0.400614] [G loss: 0.186602] [ema: 0.999436] 
[Epoch 19/78] [Batch 100/647] [D loss: 0.391755] [G loss: 0.178501] [ema: 0.999441] 
[Epoch 19/78] [Batch 200/647] [D loss: 0.388560] [G loss: 0.175288] [ema: 0.999445] 
[Epoch 19/78] [Batch 300/647] [D loss: 0.360020] [G loss: 0.192192] [ema: 0.999450] 
[Epoch 19/78] [Batch 400/647] [D loss: 0.393069] [G loss: 0.177536] [ema: 0.999454] 
[Epoch 19/78] [Batch 500/647] [D loss: 0.409825] [G loss: 0.182583] [ema: 0.999458] 
[Epoch 19/78] [Batch 600/647] [D loss: 0.385013] [G loss: 0.181861] [ema: 0.999463] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_60_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_60_2024_10_30_00_43_14/Model



[Epoch 20/78] [Batch 0/647] [D loss: 0.381848] [G loss: 0.181586] [ema: 0.999464] 
[Epoch 20/78] [Batch 100/647] [D loss: 0.392650] [G loss: 0.182129] [ema: 0.999469] 
[Epoch 20/78] [Batch 200/647] [D loss: 0.321497] [G loss: 0.210132] [ema: 0.999473] 
[Epoch 20/78] [Batch 300/647] [D loss: 0.374477] [G loss: 0.191592] [ema: 0.999477] 
[Epoch 20/78] [Batch 400/647] [D loss: 0.347038] [G loss: 0.206264] [ema: 0.999481] 
[Epoch 20/78] [Batch 500/647] [D loss: 0.340153] [G loss: 0.194063] [ema: 0.999484] 
[Epoch 20/78] [Batch 600/647] [D loss: 0.369616] [G loss: 0.197676] [ema: 0.999488] 
[Epoch 21/78] [Batch 0/647] [D loss: 0.338875] [G loss: 0.209430] [ema: 0.999490] 
[Epoch 21/78] [Batch 100/647] [D loss: 0.395529] [G loss: 0.194759] [ema: 0.999494] 
[Epoch 21/78] [Batch 200/647] [D loss: 0.418843] [G loss: 0.180903] [ema: 0.999497] 
[Epoch 21/78] [Batch 300/647] [D loss: 0.381428] [G loss: 0.184323] [ema: 0.999501] 
[Epoch 21/78] [Batch 400/647] [D loss: 0.349906] [G loss: 0.191137] [ema: 0.999505] 
[Epoch 21/78] [Batch 500/647] [D loss: 0.396172] [G loss: 0.177065] [ema: 0.999508] 
[Epoch 21/78] [Batch 600/647] [D loss: 0.366032] [G loss: 0.192130] [ema: 0.999512] 
[Epoch 22/78] [Batch 0/647] [D loss: 0.387487] [G loss: 0.185605] [ema: 0.999513] 
[Epoch 22/78] [Batch 100/647] [D loss: 0.403046] [G loss: 0.177063] [ema: 0.999517] 
[Epoch 22/78] [Batch 200/647] [D loss: 0.393277] [G loss: 0.183646] [ema: 0.999520] 
[Epoch 22/78] [Batch 300/647] [D loss: 0.334440] [G loss: 0.187280] [ema: 0.999523] 
[Epoch 22/78] [Batch 400/647] [D loss: 0.376216] [G loss: 0.178366] [ema: 0.999526] 
[Epoch 22/78] [Batch 500/647] [D loss: 0.333973] [G loss: 0.195870] [ema: 0.999530] 
[Epoch 22/78] [Batch 600/647] [D loss: 0.373048] [G loss: 0.179606] [ema: 0.999533] 
[Epoch 23/78] [Batch 0/647] [D loss: 0.394692] [G loss: 0.204949] [ema: 0.999534] 
[Epoch 23/78] [Batch 100/647] [D loss: 0.368589] [G loss: 0.192254] [ema: 0.999537] 
[Epoch 23/78] [Batch 200/647] [D loss: 0.392985] [G loss: 0.190787] [ema: 0.999540] 
[Epoch 23/78] [Batch 300/647] [D loss: 0.361217] [G loss: 0.167463] [ema: 0.999544] 
[Epoch 23/78] [Batch 400/647] [D loss: 0.359832] [G loss: 0.189309] [ema: 0.999547] 
[Epoch 23/78] [Batch 500/647] [D loss: 0.365646] [G loss: 0.190508] [ema: 0.999549] 
[Epoch 23/78] [Batch 600/647] [D loss: 0.336086] [G loss: 0.190639] [ema: 0.999552] 
[Epoch 24/78] [Batch 0/647] [D loss: 0.389360] [G loss: 0.168924] [ema: 0.999554] 
[Epoch 24/78] [Batch 100/647] [D loss: 0.385026] [G loss: 0.183382] [ema: 0.999557] 
[Epoch 24/78] [Batch 200/647] [D loss: 0.387464] [G loss: 0.188115] [ema: 0.999559] 
[Epoch 24/78] [Batch 300/647] [D loss: 0.389899] [G loss: 0.172748] [ema: 0.999562] 
[Epoch 24/78] [Batch 400/647] [D loss: 0.391994] [G loss: 0.171304] [ema: 0.999565] 
[Epoch 24/78] [Batch 500/647] [D loss: 0.379117] [G loss: 0.183245] [ema: 0.999568] 
[Epoch 24/78] [Batch 600/647] [D loss: 0.416187] [G loss: 0.165803] [ema: 0.999570] 
[Epoch 25/78] [Batch 0/647] [D loss: 0.382744] [G loss: 0.191166] [ema: 0.999572] 
[Epoch 25/78] [Batch 100/647] [D loss: 0.396491] [G loss: 0.175601] [ema: 0.999574] 
[Epoch 25/78] [Batch 200/647] [D loss: 0.394117] [G loss: 0.205319] [ema: 0.999577] 
[Epoch 25/78] [Batch 300/647] [D loss: 0.359263] [G loss: 0.194289] [ema: 0.999579] 
[Epoch 25/78] [Batch 400/647] [D loss: 0.335743] [G loss: 0.204850] [ema: 0.999582] 
[Epoch 25/78] [Batch 500/647] [D loss: 0.384447] [G loss: 0.166614] [ema: 0.999584] 
[Epoch 25/78] [Batch 600/647] [D loss: 0.414883] [G loss: 0.182280] [ema: 0.999587] 
[Epoch 26/78] [Batch 0/647] [D loss: 0.370772] [G loss: 0.200065] [ema: 0.999588] 
[Epoch 26/78] [Batch 100/647] [D loss: 0.367721] [G loss: 0.191777] [ema: 0.999590] 
[Epoch 26/78] [Batch 200/647] [D loss: 0.369624] [G loss: 0.190679] [ema: 0.999593] 
[Epoch 26/78] [Batch 300/647] [D loss: 0.381796] [G loss: 0.181309] [ema: 0.999595] 
[Epoch 26/78] [Batch 400/647] [D loss: 0.401394] [G loss: 0.186789] [ema: 0.999598] 
[Epoch 26/78] [Batch 500/647] [D loss: 0.388637] [G loss: 0.200023] [ema: 0.999600] 
[Epoch 26/78] [Batch 600/647] [D loss: 0.412195] [G loss: 0.177403] [ema: 0.999602] 
[Epoch 27/78] [Batch 0/647] [D loss: 0.356143] [G loss: 0.189951] [ema: 0.999603] 
[Epoch 27/78] [Batch 100/647] [D loss: 0.420867] [G loss: 0.177382] [ema: 0.999606] 
[Epoch 27/78] [Batch 200/647] [D loss: 0.361296] [G loss: 0.203058] [ema: 0.999608] 
[Epoch 27/78] [Batch 300/647] [D loss: 0.357258] [G loss: 0.187979] [ema: 0.999610] 
[Epoch 27/78] [Batch 400/647] [D loss: 0.359082] [G loss: 0.199217] [ema: 0.999612] 
[Epoch 27/78] [Batch 500/647] [D loss: 0.404028] [G loss: 0.193185] [ema: 0.999614] 
[Epoch 27/78] [Batch 600/647] [D loss: 0.370134] [G loss: 0.189766] [ema: 0.999616] 
[Epoch 28/78] [Batch 0/647] [D loss: 0.397647] [G loss: 0.181142] [ema: 0.999617] 
[Epoch 28/78] [Batch 100/647] [D loss: 0.383181] [G loss: 0.191736] [ema: 0.999620] 
[Epoch 28/78] [Batch 200/647] [D loss: 0.418307] [G loss: 0.182802] [ema: 0.999622] 
[Epoch 28/78] [Batch 300/647] [D loss: 0.374358] [G loss: 0.182546] [ema: 0.999624] 
[Epoch 28/78] [Batch 400/647] [D loss: 0.427371] [G loss: 0.191822] [ema: 0.999626] 
[Epoch 28/78] [Batch 500/647] [D loss: 0.411809] [G loss: 0.173354] [ema: 0.999628] 
[Epoch 28/78] [Batch 600/647] [D loss: 0.363740] [G loss: 0.180324] [ema: 0.999630] 
[Epoch 29/78] [Batch 0/647] [D loss: 0.373740] [G loss: 0.171764] [ema: 0.999631] 
[Epoch 29/78] [Batch 100/647] [D loss: 0.398018] [G loss: 0.181964] [ema: 0.999633] 
[Epoch 29/78] [Batch 200/647] [D loss: 0.369382] [G loss: 0.194428] [ema: 0.999635] 
[Epoch 29/78] [Batch 300/647] [D loss: 0.374345] [G loss: 0.173571] [ema: 0.999636] 
[Epoch 29/78] [Batch 400/647] [D loss: 0.358716] [G loss: 0.196805] [ema: 0.999638] 
[Epoch 29/78] [Batch 500/647] [D loss: 0.339445] [G loss: 0.183010] [ema: 0.999640] 
[Epoch 29/78] [Batch 600/647] [D loss: 0.422484] [G loss: 0.168424] [ema: 0.999642] 
[Epoch 30/78] [Batch 0/647] [D loss: 0.383496] [G loss: 0.194664] [ema: 0.999643] 
[Epoch 30/78] [Batch 100/647] [D loss: 0.392439] [G loss: 0.182245] [ema: 0.999645] 
[Epoch 30/78] [Batch 200/647] [D loss: 0.380937] [G loss: 0.189625] [ema: 0.999647] 
[Epoch 30/78] [Batch 300/647] [D loss: 0.410697] [G loss: 0.194175] [ema: 0.999648] 
[Epoch 30/78] [Batch 400/647] [D loss: 0.399577] [G loss: 0.191693] [ema: 0.999650] 
[Epoch 30/78] [Batch 500/647] [D loss: 0.387617] [G loss: 0.179474] [ema: 0.999652] 
[Epoch 30/78] [Batch 600/647] [D loss: 0.351421] [G loss: 0.183964] [ema: 0.999654] 
[Epoch 31/78] [Batch 0/647] [D loss: 0.392417] [G loss: 0.185649] [ema: 0.999654] 
[Epoch 31/78] [Batch 100/647] [D loss: 0.369650] [G loss: 0.179916] [ema: 0.999656] 
[Epoch 31/78] [Batch 200/647] [D loss: 0.376129] [G loss: 0.181856] [ema: 0.999658] 
[Epoch 31/78] [Batch 300/647] [D loss: 0.382983] [G loss: 0.171243] [ema: 0.999660] 
[Epoch 31/78] [Batch 400/647] [D loss: 0.373436] [G loss: 0.194033] [ema: 0.999661] 
[Epoch 31/78] [Batch 500/647] [D loss: 0.368036] [G loss: 0.195063] [ema: 0.999663] 
[Epoch 31/78] [Batch 600/647] [D loss: 0.382806] [G loss: 0.192417] [ema: 0.999665] 
[Epoch 32/78] [Batch 0/647] [D loss: 0.383058] [G loss: 0.191098] [ema: 0.999665] 
[Epoch 32/78] [Batch 100/647] [D loss: 0.429983] [G loss: 0.178306] [ema: 0.999667] 
[Epoch 32/78] [Batch 200/647] [D loss: 0.400970] [G loss: 0.201207] [ema: 0.999668] 
[Epoch 32/78] [Batch 300/647] [D loss: 0.371652] [G loss: 0.185683] [ema: 0.999670] 
[Epoch 32/78] [Batch 400/647] [D loss: 0.410839] [G loss: 0.184678] [ema: 0.999672] 
[Epoch 32/78] [Batch 500/647] [D loss: 0.404184] [G loss: 0.177852] [ema: 0.999673] 
[Epoch 32/78] [Batch 600/647] [D loss: 0.431884] [G loss: 0.186718] [ema: 0.999675] 
[Epoch 33/78] [Batch 0/647] [D loss: 0.378918] [G loss: 0.193487] [ema: 0.999675] 
[Epoch 33/78] [Batch 100/647] [D loss: 0.446902] [G loss: 0.178799] [ema: 0.999677] 
[Epoch 33/78] [Batch 200/647] [D loss: 0.415184] [G loss: 0.168897] [ema: 0.999678] 
[Epoch 33/78] [Batch 300/647] [D loss: 0.429775] [G loss: 0.175845] [ema: 0.999680] 
[Epoch 33/78] [Batch 400/647] [D loss: 0.360398] [G loss: 0.195589] [ema: 0.999681] 
[Epoch 33/78] [Batch 500/647] [D loss: 0.396720] [G loss: 0.187341] [ema: 0.999683] 
[Epoch 33/78] [Batch 600/647] [D loss: 0.384162] [G loss: 0.179157] [ema: 0.999684] 
[Epoch 34/78] [Batch 0/647] [D loss: 0.416130] [G loss: 0.188637] [ema: 0.999685] 
[Epoch 34/78] [Batch 100/647] [D loss: 0.418519] [G loss: 0.177581] [ema: 0.999686] 
[Epoch 34/78] [Batch 200/647] [D loss: 0.360282] [G loss: 0.187700] [ema: 0.999688] 
[Epoch 34/78] [Batch 300/647] [D loss: 0.399699] [G loss: 0.193861] [ema: 0.999689] 
[Epoch 34/78] [Batch 400/647] [D loss: 0.363825] [G loss: 0.182045] [ema: 0.999691] 
[Epoch 34/78] [Batch 500/647] [D loss: 0.401307] [G loss: 0.189041] [ema: 0.999692] 
[Epoch 34/78] [Batch 600/647] [D loss: 0.371951] [G loss: 0.183383] [ema: 0.999693] 
[Epoch 35/78] [Batch 0/647] [D loss: 0.399532] [G loss: 0.193895] [ema: 0.999694] 
[Epoch 35/78] [Batch 100/647] [D loss: 0.376144] [G loss: 0.181228] [ema: 0.999695] 
[Epoch 35/78] [Batch 200/647] [D loss: 0.372515] [G loss: 0.198352] [ema: 0.999697] 
[Epoch 35/78] [Batch 300/647] [D loss: 0.371320] [G loss: 0.192307] [ema: 0.999698] 
[Epoch 35/78] [Batch 400/647] [D loss: 0.403620] [G loss: 0.175014] [ema: 0.999699] 
[Epoch 35/78] [Batch 500/647] [D loss: 0.398867] [G loss: 0.183957] [ema: 0.999701] 
[Epoch 35/78] [Batch 600/647] [D loss: 0.414339] [G loss: 0.172613] [ema: 0.999702] 
[Epoch 36/78] [Batch 0/647] [D loss: 0.354204] [G loss: 0.192301] [ema: 0.999702] 
[Epoch 36/78] [Batch 100/647] [D loss: 0.413041] [G loss: 0.184835] [ema: 0.999704] 
[Epoch 36/78] [Batch 200/647] [D loss: 0.404685] [G loss: 0.169918] [ema: 0.999705] 
[Epoch 36/78] [Batch 300/647] [D loss: 0.370634] [G loss: 0.185667] [ema: 0.999706] 
[Epoch 36/78] [Batch 400/647] [D loss: 0.392258] [G loss: 0.191346] [ema: 0.999707] 
[Epoch 36/78] [Batch 500/647] [D loss: 0.368926] [G loss: 0.166476] [ema: 0.999709] 
[Epoch 36/78] [Batch 600/647] [D loss: 0.383127] [G loss: 0.186920] [ema: 0.999710] 
[Epoch 37/78] [Batch 0/647] [D loss: 0.412201] [G loss: 0.183991] [ema: 0.999710] 
[Epoch 37/78] [Batch 100/647] [D loss: 0.412505] [G loss: 0.186253] [ema: 0.999712] 
[Epoch 37/78] [Batch 200/647] [D loss: 0.382924] [G loss: 0.180775] [ema: 0.999713] 
[Epoch 37/78] [Batch 300/647] [D loss: 0.365382] [G loss: 0.180270] [ema: 0.999714] 
[Epoch 37/78] [Batch 400/647] [D loss: 0.386238] [G loss: 0.192118] [ema: 0.999715] 
[Epoch 37/78] [Batch 500/647] [D loss: 0.404886] [G loss: 0.171644] [ema: 0.999716] 
[Epoch 37/78] [Batch 600/647] [D loss: 0.387069] [G loss: 0.182221] [ema: 0.999718] 
[Epoch 38/78] [Batch 0/647] [D loss: 0.365166] [G loss: 0.189466] [ema: 0.999718] 
[Epoch 38/78] [Batch 100/647] [D loss: 0.400205] [G loss: 0.185559] [ema: 0.999719] 
[Epoch 38/78] [Batch 200/647] [D loss: 0.400801] [G loss: 0.181828] [ema: 0.999720] 
[Epoch 38/78] [Batch 300/647] [D loss: 0.382552] [G loss: 0.186978] [ema: 0.999722] 
[Epoch 38/78] [Batch 400/647] [D loss: 0.387902] [G loss: 0.193653] [ema: 0.999723] 
[Epoch 38/78] [Batch 500/647] [D loss: 0.405725] [G loss: 0.193008] [ema: 0.999724] 
[Epoch 38/78] [Batch 600/647] [D loss: 0.381709] [G loss: 0.196174] [ema: 0.999725] 
[Epoch 39/78] [Batch 0/647] [D loss: 0.352050] [G loss: 0.192188] [ema: 0.999725] 
[Epoch 39/78] [Batch 100/647] [D loss: 0.410518] [G loss: 0.185006] [ema: 0.999726] 
[Epoch 39/78] [Batch 200/647] [D loss: 0.342089] [G loss: 0.201721] [ema: 0.999727] 
[Epoch 39/78] [Batch 300/647] [D loss: 0.337519] [G loss: 0.191156] [ema: 0.999729] 
[Epoch 39/78] [Batch 400/647] [D loss: 0.419117] [G loss: 0.162107] [ema: 0.999730] 
[Epoch 39/78] [Batch 500/647] [D loss: 0.401809] [G loss: 0.189191] [ema: 0.999731] 
[Epoch 39/78] [Batch 600/647] [D loss: 0.360630] [G loss: 0.180161] [ema: 0.999732] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_60_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_60_2024_10_30_00_43_14/Model



[Epoch 40/78] [Batch 0/647] [D loss: 0.454824] [G loss: 0.196729] [ema: 0.999732] 
[Epoch 40/78] [Batch 100/647] [D loss: 0.394665] [G loss: 0.173561] [ema: 0.999733] 
[Epoch 40/78] [Batch 200/647] [D loss: 0.394590] [G loss: 0.168762] [ema: 0.999734] 
[Epoch 40/78] [Batch 300/647] [D loss: 0.394581] [G loss: 0.186419] [ema: 0.999735] 
[Epoch 40/78] [Batch 400/647] [D loss: 0.414941] [G loss: 0.171833] [ema: 0.999736] 
[Epoch 40/78] [Batch 500/647] [D loss: 0.392615] [G loss: 0.182687] [ema: 0.999737] 
[Epoch 40/78] [Batch 600/647] [D loss: 0.365236] [G loss: 0.191012] [ema: 0.999738] 
[Epoch 41/78] [Batch 0/647] [D loss: 0.420877] [G loss: 0.181219] [ema: 0.999739] 
[Epoch 41/78] [Batch 100/647] [D loss: 0.386151] [G loss: 0.174067] [ema: 0.999740] 
[Epoch 41/78] [Batch 200/647] [D loss: 0.396082] [G loss: 0.202201] [ema: 0.999741] 
[Epoch 41/78] [Batch 300/647] [D loss: 0.436983] [G loss: 0.175882] [ema: 0.999742] 
[Epoch 41/78] [Batch 400/647] [D loss: 0.397034] [G loss: 0.186444] [ema: 0.999743] 
[Epoch 41/78] [Batch 500/647] [D loss: 0.376053] [G loss: 0.173796] [ema: 0.999744] 
[Epoch 41/78] [Batch 600/647] [D loss: 0.411550] [G loss: 0.176111] [ema: 0.999745] 
[Epoch 42/78] [Batch 0/647] [D loss: 0.385024] [G loss: 0.194168] [ema: 0.999745] 
[Epoch 42/78] [Batch 100/647] [D loss: 0.358970] [G loss: 0.189635] [ema: 0.999746] 
[Epoch 42/78] [Batch 200/647] [D loss: 0.365016] [G loss: 0.170532] [ema: 0.999747] 
[Epoch 42/78] [Batch 300/647] [D loss: 0.387201] [G loss: 0.178965] [ema: 0.999748] 
[Epoch 42/78] [Batch 400/647] [D loss: 0.425229] [G loss: 0.180599] [ema: 0.999749] 
[Epoch 42/78] [Batch 500/647] [D loss: 0.374873] [G loss: 0.180751] [ema: 0.999750] 
[Epoch 42/78] [Batch 600/647] [D loss: 0.379782] [G loss: 0.178822] [ema: 0.999750] 
[Epoch 43/78] [Batch 0/647] [D loss: 0.362544] [G loss: 0.199521] [ema: 0.999751] 
[Epoch 43/78] [Batch 100/647] [D loss: 0.370570] [G loss: 0.192613] [ema: 0.999752] 
[Epoch 43/78] [Batch 200/647] [D loss: 0.384466] [G loss: 0.177560] [ema: 0.999753] 
[Epoch 43/78] [Batch 300/647] [D loss: 0.377594] [G loss: 0.184285] [ema: 0.999754] 
[Epoch 43/78] [Batch 400/647] [D loss: 0.352893] [G loss: 0.183231] [ema: 0.999754] 
[Epoch 43/78] [Batch 500/647] [D loss: 0.419310] [G loss: 0.187757] [ema: 0.999755] 
[Epoch 43/78] [Batch 600/647] [D loss: 0.394713] [G loss: 0.176150] [ema: 0.999756] 
[Epoch 44/78] [Batch 0/647] [D loss: 0.367027] [G loss: 0.194352] [ema: 0.999757] 
[Epoch 44/78] [Batch 100/647] [D loss: 0.395057] [G loss: 0.178639] [ema: 0.999757] 
[Epoch 44/78] [Batch 200/647] [D loss: 0.395408] [G loss: 0.179034] [ema: 0.999758] 
[Epoch 44/78] [Batch 300/647] [D loss: 0.386468] [G loss: 0.182775] [ema: 0.999759] 
[Epoch 44/78] [Batch 400/647] [D loss: 0.417341] [G loss: 0.177278] [ema: 0.999760] 
[Epoch 44/78] [Batch 500/647] [D loss: 0.362147] [G loss: 0.185729] [ema: 0.999761] 
[Epoch 44/78] [Batch 600/647] [D loss: 0.362045] [G loss: 0.191588] [ema: 0.999762] 
[Epoch 45/78] [Batch 0/647] [D loss: 0.364448] [G loss: 0.186987] [ema: 0.999762] 
[Epoch 45/78] [Batch 100/647] [D loss: 0.387895] [G loss: 0.178035] [ema: 0.999763] 
[Epoch 45/78] [Batch 200/647] [D loss: 0.398649] [G loss: 0.172598] [ema: 0.999764] 
[Epoch 45/78] [Batch 300/647] [D loss: 0.427052] [G loss: 0.175366] [ema: 0.999764] 
[Epoch 45/78] [Batch 400/647] [D loss: 0.404039] [G loss: 0.173520] [ema: 0.999765] 
[Epoch 45/78] [Batch 500/647] [D loss: 0.402999] [G loss: 0.163638] [ema: 0.999766] 
[Epoch 45/78] [Batch 600/647] [D loss: 0.414141] [G loss: 0.176480] [ema: 0.999767] 
[Epoch 46/78] [Batch 0/647] [D loss: 0.396954] [G loss: 0.204386] [ema: 0.999767] 
[Epoch 46/78] [Batch 100/647] [D loss: 0.387817] [G loss: 0.185106] [ema: 0.999768] 
[Epoch 46/78] [Batch 200/647] [D loss: 0.370862] [G loss: 0.183690] [ema: 0.999769] 
[Epoch 46/78] [Batch 300/647] [D loss: 0.381340] [G loss: 0.183719] [ema: 0.999769] 
[Epoch 46/78] [Batch 400/647] [D loss: 0.396399] [G loss: 0.186227] [ema: 0.999770] 
[Epoch 46/78] [Batch 500/647] [D loss: 0.369947] [G loss: 0.188903] [ema: 0.999771] 
[Epoch 46/78] [Batch 600/647] [D loss: 0.415148] [G loss: 0.168279] [ema: 0.999772] 
[Epoch 47/78] [Batch 0/647] [D loss: 0.375471] [G loss: 0.196467] [ema: 0.999772] 
[Epoch 47/78] [Batch 100/647] [D loss: 0.396853] [G loss: 0.174744] [ema: 0.999773] 
[Epoch 47/78] [Batch 200/647] [D loss: 0.404817] [G loss: 0.193617] [ema: 0.999774] 
[Epoch 47/78] [Batch 300/647] [D loss: 0.392284] [G loss: 0.192660] [ema: 0.999774] 
[Epoch 47/78] [Batch 400/647] [D loss: 0.421128] [G loss: 0.181762] [ema: 0.999775] 
[Epoch 47/78] [Batch 500/647] [D loss: 0.429665] [G loss: 0.174025] [ema: 0.999776] 
[Epoch 47/78] [Batch 600/647] [D loss: 0.423852] [G loss: 0.179679] [ema: 0.999776] 
[Epoch 48/78] [Batch 0/647] [D loss: 0.350022] [G loss: 0.187280] [ema: 0.999777] 
[Epoch 48/78] [Batch 100/647] [D loss: 0.423504] [G loss: 0.178188] [ema: 0.999778] 
[Epoch 48/78] [Batch 200/647] [D loss: 0.361491] [G loss: 0.186948] [ema: 0.999778] 
[Epoch 48/78] [Batch 300/647] [D loss: 0.354240] [G loss: 0.177713] [ema: 0.999779] 
[Epoch 48/78] [Batch 400/647] [D loss: 0.371989] [G loss: 0.190966] [ema: 0.999780] 
[Epoch 48/78] [Batch 500/647] [D loss: 0.377619] [G loss: 0.179010] [ema: 0.999780] 
[Epoch 48/78] [Batch 600/647] [D loss: 0.356373] [G loss: 0.192566] [ema: 0.999781] 
[Epoch 49/78] [Batch 0/647] [D loss: 0.364538] [G loss: 0.183705] [ema: 0.999781] 
[Epoch 49/78] [Batch 100/647] [D loss: 0.392711] [G loss: 0.175019] [ema: 0.999782] 
[Epoch 49/78] [Batch 200/647] [D loss: 0.401170] [G loss: 0.181999] [ema: 0.999783] 
[Epoch 49/78] [Batch 300/647] [D loss: 0.372772] [G loss: 0.172812] [ema: 0.999783] 
[Epoch 49/78] [Batch 400/647] [D loss: 0.363435] [G loss: 0.195073] [ema: 0.999784] 
[Epoch 49/78] [Batch 500/647] [D loss: 0.403260] [G loss: 0.175664] [ema: 0.999785] 
[Epoch 49/78] [Batch 600/647] [D loss: 0.398505] [G loss: 0.176863] [ema: 0.999785] 
[Epoch 50/78] [Batch 0/647] [D loss: 0.390821] [G loss: 0.188848] [ema: 0.999786] 
[Epoch 50/78] [Batch 100/647] [D loss: 0.397346] [G loss: 0.186154] [ema: 0.999786] 
[Epoch 50/78] [Batch 200/647] [D loss: 0.380524] [G loss: 0.178147] [ema: 0.999787] 
[Epoch 50/78] [Batch 300/647] [D loss: 0.352661] [G loss: 0.191096] [ema: 0.999788] 
[Epoch 50/78] [Batch 400/647] [D loss: 0.371459] [G loss: 0.167060] [ema: 0.999788] 
[Epoch 50/78] [Batch 500/647] [D loss: 0.392826] [G loss: 0.182287] [ema: 0.999789] 
[Epoch 50/78] [Batch 600/647] [D loss: 0.417404] [G loss: 0.177383] [ema: 0.999790] 
[Epoch 51/78] [Batch 0/647] [D loss: 0.390697] [G loss: 0.187629] [ema: 0.999790] 
[Epoch 51/78] [Batch 100/647] [D loss: 0.381441] [G loss: 0.186048] [ema: 0.999791] 
[Epoch 51/78] [Batch 200/647] [D loss: 0.366477] [G loss: 0.196017] [ema: 0.999791] 
[Epoch 51/78] [Batch 300/647] [D loss: 0.365994] [G loss: 0.189155] [ema: 0.999792] 
[Epoch 51/78] [Batch 400/647] [D loss: 0.449841] [G loss: 0.183251] [ema: 0.999792] 
[Epoch 51/78] [Batch 500/647] [D loss: 0.377177] [G loss: 0.182024] [ema: 0.999793] 
[Epoch 51/78] [Batch 600/647] [D loss: 0.445877] [G loss: 0.153274] [ema: 0.999794] 
[Epoch 52/78] [Batch 0/647] [D loss: 0.370611] [G loss: 0.190307] [ema: 0.999794] 
[Epoch 52/78] [Batch 100/647] [D loss: 0.393867] [G loss: 0.174343] [ema: 0.999795] 
[Epoch 52/78] [Batch 200/647] [D loss: 0.379936] [G loss: 0.172343] [ema: 0.999795] 
[Epoch 52/78] [Batch 300/647] [D loss: 0.379053] [G loss: 0.171957] [ema: 0.999796] 
[Epoch 52/78] [Batch 400/647] [D loss: 0.400790] [G loss: 0.177874] [ema: 0.999796] 
[Epoch 52/78] [Batch 500/647] [D loss: 0.401382] [G loss: 0.176800] [ema: 0.999797] 
[Epoch 52/78] [Batch 600/647] [D loss: 0.406494] [G loss: 0.181934] [ema: 0.999798] 
[Epoch 53/78] [Batch 0/647] [D loss: 0.414990] [G loss: 0.177023] [ema: 0.999798] 
[Epoch 53/78] [Batch 100/647] [D loss: 0.357336] [G loss: 0.167712] [ema: 0.999798] 
[Epoch 53/78] [Batch 200/647] [D loss: 0.391313] [G loss: 0.178081] [ema: 0.999799] 
[Epoch 53/78] [Batch 300/647] [D loss: 0.393227] [G loss: 0.178865] [ema: 0.999800] 
[Epoch 53/78] [Batch 400/647] [D loss: 0.395725] [G loss: 0.184022] [ema: 0.999800] 
[Epoch 53/78] [Batch 500/647] [D loss: 0.457839] [G loss: 0.196236] [ema: 0.999801] 
[Epoch 53/78] [Batch 600/647] [D loss: 0.382723] [G loss: 0.196166] [ema: 0.999801] 
[Epoch 54/78] [Batch 0/647] [D loss: 0.433364] [G loss: 0.182191] [ema: 0.999802] 
[Epoch 54/78] [Batch 100/647] [D loss: 0.374380] [G loss: 0.164747] [ema: 0.999802] 
[Epoch 54/78] [Batch 200/647] [D loss: 0.405416] [G loss: 0.182797] [ema: 0.999803] 
[Epoch 54/78] [Batch 300/647] [D loss: 0.366429] [G loss: 0.192041] [ema: 0.999803] 
[Epoch 54/78] [Batch 400/647] [D loss: 0.434110] [G loss: 0.175123] [ema: 0.999804] 
[Epoch 54/78] [Batch 500/647] [D loss: 0.391783] [G loss: 0.173397] [ema: 0.999804] 
[Epoch 54/78] [Batch 600/647] [D loss: 0.373272] [G loss: 0.170627] [ema: 0.999805] 
[Epoch 55/78] [Batch 0/647] [D loss: 0.374980] [G loss: 0.192252] [ema: 0.999805] 
[Epoch 55/78] [Batch 100/647] [D loss: 0.416108] [G loss: 0.189688] [ema: 0.999806] 
[Epoch 55/78] [Batch 200/647] [D loss: 0.377830] [G loss: 0.184584] [ema: 0.999806] 
[Epoch 55/78] [Batch 300/647] [D loss: 0.404676] [G loss: 0.188748] [ema: 0.999807] 
[Epoch 55/78] [Batch 400/647] [D loss: 0.419026] [G loss: 0.185748] [ema: 0.999807] 
[Epoch 55/78] [Batch 500/647] [D loss: 0.439279] [G loss: 0.174711] [ema: 0.999808] 
[Epoch 55/78] [Batch 600/647] [D loss: 0.385685] [G loss: 0.179540] [ema: 0.999808] 
[Epoch 56/78] [Batch 0/647] [D loss: 0.372250] [G loss: 0.187396] [ema: 0.999809] 
[Epoch 56/78] [Batch 100/647] [D loss: 0.418312] [G loss: 0.182503] [ema: 0.999809] 
[Epoch 56/78] [Batch 200/647] [D loss: 0.369074] [G loss: 0.164278] [ema: 0.999810] 
[Epoch 56/78] [Batch 300/647] [D loss: 0.454162] [G loss: 0.176858] [ema: 0.999810] 
[Epoch 56/78] [Batch 400/647] [D loss: 0.412059] [G loss: 0.185448] [ema: 0.999811] 
[Epoch 56/78] [Batch 500/647] [D loss: 0.419647] [G loss: 0.175759] [ema: 0.999811] 
[Epoch 56/78] [Batch 600/647] [D loss: 0.427413] [G loss: 0.167912] [ema: 0.999812] 
[Epoch 57/78] [Batch 0/647] [D loss: 0.362009] [G loss: 0.203004] [ema: 0.999812] 
[Epoch 57/78] [Batch 100/647] [D loss: 0.403917] [G loss: 0.187629] [ema: 0.999813] 
[Epoch 57/78] [Batch 200/647] [D loss: 0.372846] [G loss: 0.182985] [ema: 0.999813] 
[Epoch 57/78] [Batch 300/647] [D loss: 0.400289] [G loss: 0.173584] [ema: 0.999814] 
[Epoch 57/78] [Batch 400/647] [D loss: 0.400242] [G loss: 0.175133] [ema: 0.999814] 
[Epoch 57/78] [Batch 500/647] [D loss: 0.376423] [G loss: 0.171905] [ema: 0.999815] 
[Epoch 57/78] [Batch 600/647] [D loss: 0.393947] [G loss: 0.188281] [ema: 0.999815] 
[Epoch 58/78] [Batch 0/647] [D loss: 0.403658] [G loss: 0.183561] [ema: 0.999815] 
[Epoch 58/78] [Batch 100/647] [D loss: 0.390047] [G loss: 0.170671] [ema: 0.999816] 
[Epoch 58/78] [Batch 200/647] [D loss: 0.344270] [G loss: 0.182211] [ema: 0.999816] 
[Epoch 58/78] [Batch 300/647] [D loss: 0.419344] [G loss: 0.176092] [ema: 0.999817] 
[Epoch 58/78] [Batch 400/647] [D loss: 0.360666] [G loss: 0.187089] [ema: 0.999817] 
[Epoch 58/78] [Batch 500/647] [D loss: 0.403717] [G loss: 0.191448] [ema: 0.999818] 
[Epoch 58/78] [Batch 600/647] [D loss: 0.437334] [G loss: 0.181905] [ema: 0.999818] 
[Epoch 59/78] [Batch 0/647] [D loss: 0.438971] [G loss: 0.173448] [ema: 0.999818] 
[Epoch 59/78] [Batch 100/647] [D loss: 0.374292] [G loss: 0.194664] [ema: 0.999819] 
[Epoch 59/78] [Batch 200/647] [D loss: 0.411230] [G loss: 0.178291] [ema: 0.999819] 
[Epoch 59/78] [Batch 300/647] [D loss: 0.377292] [G loss: 0.167431] [ema: 0.999820] 
[Epoch 59/78] [Batch 400/647] [D loss: 0.420184] [G loss: 0.179272] [ema: 0.999820] 
[Epoch 59/78] [Batch 500/647] [D loss: 0.429797] [G loss: 0.174771] [ema: 0.999821] 
[Epoch 59/78] [Batch 600/647] [D loss: 0.430056] [G loss: 0.172441] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_60_100/RealWorld_thigh_DAGHAR_Multiclass_50000_D_60_2024_10_30_00_43_14/Model



[Epoch 60/78] [Batch 0/647] [D loss: 0.412790] [G loss: 0.193833] [ema: 0.999821] 
[Epoch 60/78] [Batch 100/647] [D loss: 0.377474] [G loss: 0.186168] [ema: 0.999822] 
[Epoch 60/78] [Batch 200/647] [D loss: 0.363049] [G loss: 0.188799] [ema: 0.999822] 
[Epoch 60/78] [Batch 300/647] [D loss: 0.425433] [G loss: 0.179170] [ema: 0.999823] 
[Epoch 60/78] [Batch 400/647] [D loss: 0.352789] [G loss: 0.192549] [ema: 0.999823] 
[Epoch 60/78] [Batch 500/647] [D loss: 0.411643] [G loss: 0.181694] [ema: 0.999824] 
[Epoch 60/78] [Batch 600/647] [D loss: 0.404934] [G loss: 0.181668] [ema: 0.999824] 
[Epoch 61/78] [Batch 0/647] [D loss: 0.396149] [G loss: 0.174078] [ema: 0.999824] 
[Epoch 61/78] [Batch 100/647] [D loss: 0.428398] [G loss: 0.174886] [ema: 0.999825] 
[Epoch 61/78] [Batch 200/647] [D loss: 0.369218] [G loss: 0.173572] [ema: 0.999825] 
[Epoch 61/78] [Batch 300/647] [D loss: 0.410221] [G loss: 0.185746] [ema: 0.999826] 
[Epoch 61/78] [Batch 400/647] [D loss: 0.410007] [G loss: 0.178983] [ema: 0.999826] 
[Epoch 61/78] [Batch 500/647] [D loss: 0.360521] [G loss: 0.188719] [ema: 0.999827] 
[Epoch 61/78] [Batch 600/647] [D loss: 0.433506] [G loss: 0.183333] [ema: 0.999827] 
[Epoch 62/78] [Batch 0/647] [D loss: 0.398895] [G loss: 0.186482] [ema: 0.999827] 
[Epoch 62/78] [Batch 100/647] [D loss: 0.403632] [G loss: 0.169775] [ema: 0.999828] 
[Epoch 62/78] [Batch 200/647] [D loss: 0.399221] [G loss: 0.182032] [ema: 0.999828] 
[Epoch 62/78] [Batch 300/647] [D loss: 0.376097] [G loss: 0.183141] [ema: 0.999829] 
[Epoch 62/78] [Batch 400/647] [D loss: 0.381192] [G loss: 0.178588] [ema: 0.999829] 
[Epoch 62/78] [Batch 500/647] [D loss: 0.413751] [G loss: 0.165254] [ema: 0.999829] 
[Epoch 62/78] [Batch 600/647] [D loss: 0.433655] [G loss: 0.186837] [ema: 0.999830] 
[Epoch 63/78] [Batch 0/647] [D loss: 0.383944] [G loss: 0.187367] [ema: 0.999830] 
[Epoch 63/78] [Batch 100/647] [D loss: 0.432899] [G loss: 0.178230] [ema: 0.999830] 
[Epoch 63/78] [Batch 200/647] [D loss: 0.403218] [G loss: 0.178369] [ema: 0.999831] 
[Epoch 63/78] [Batch 300/647] [D loss: 0.430417] [G loss: 0.184857] [ema: 0.999831] 
[Epoch 63/78] [Batch 400/647] [D loss: 0.411362] [G loss: 0.182560] [ema: 0.999832] 
[Epoch 63/78] [Batch 500/647] [D loss: 0.378085] [G loss: 0.187030] [ema: 0.999832] 
[Epoch 63/78] [Batch 600/647] [D loss: 0.421050] [G loss: 0.180294] [ema: 0.999832] 
[Epoch 64/78] [Batch 0/647] [D loss: 0.404770] [G loss: 0.192279] [ema: 0.999833] 
[Epoch 64/78] [Batch 100/647] [D loss: 0.388832] [G loss: 0.172683] [ema: 0.999833] 
[Epoch 64/78] [Batch 200/647] [D loss: 0.462932] [G loss: 0.172901] [ema: 0.999833] 
[Epoch 64/78] [Batch 300/647] [D loss: 0.398270] [G loss: 0.165415] [ema: 0.999834] 
[Epoch 64/78] [Batch 400/647] [D loss: 0.405986] [G loss: 0.178787] [ema: 0.999834] 
[Epoch 64/78] [Batch 500/647] [D loss: 0.415663] [G loss: 0.179434] [ema: 0.999835] 
[Epoch 64/78] [Batch 600/647] [D loss: 0.387660] [G loss: 0.174188] [ema: 0.999835] 
[Epoch 65/78] [Batch 0/647] [D loss: 0.368912] [G loss: 0.188772] [ema: 0.999835] 
[Epoch 65/78] [Batch 100/647] [D loss: 0.390212] [G loss: 0.179108] [ema: 0.999836] 
[Epoch 65/78] [Batch 200/647] [D loss: 0.394949] [G loss: 0.173719] [ema: 0.999836] 
[Epoch 65/78] [Batch 300/647] [D loss: 0.389176] [G loss: 0.184658] [ema: 0.999836] 
[Epoch 65/78] [Batch 400/647] [D loss: 0.464004] [G loss: 0.159813] [ema: 0.999837] 
[Epoch 65/78] [Batch 500/647] [D loss: 0.409430] [G loss: 0.173980] [ema: 0.999837] 
[Epoch 65/78] [Batch 600/647] [D loss: 0.381800] [G loss: 0.169388] [ema: 0.999838] 
[Epoch 66/78] [Batch 0/647] [D loss: 0.429426] [G loss: 0.189761] [ema: 0.999838] 
[Epoch 66/78] [Batch 100/647] [D loss: 0.384688] [G loss: 0.169685] [ema: 0.999838] 
[Epoch 66/78] [Batch 200/647] [D loss: 0.370881] [G loss: 0.177161] [ema: 0.999838] 
[Epoch 66/78] [Batch 300/647] [D loss: 0.381717] [G loss: 0.173066] [ema: 0.999839] 
[Epoch 66/78] [Batch 400/647] [D loss: 0.399632] [G loss: 0.169915] [ema: 0.999839] 
[Epoch 66/78] [Batch 500/647] [D loss: 0.388174] [G loss: 0.178604] [ema: 0.999840] 
[Epoch 66/78] [Batch 600/647] [D loss: 0.421789] [G loss: 0.181433] [ema: 0.999840] 
[Epoch 67/78] [Batch 0/647] [D loss: 0.393536] [G loss: 0.197141] [ema: 0.999840] 
[Epoch 67/78] [Batch 100/647] [D loss: 0.425135] [G loss: 0.178053] [ema: 0.999840] 
[Epoch 67/78] [Batch 200/647] [D loss: 0.398954] [G loss: 0.177232] [ema: 0.999841] 
[Epoch 67/78] [Batch 300/647] [D loss: 0.395571] [G loss: 0.184267] [ema: 0.999841] 
[Epoch 67/78] [Batch 400/647] [D loss: 0.370614] [G loss: 0.189723] [ema: 0.999842] 
[Epoch 67/78] [Batch 500/647] [D loss: 0.374406] [G loss: 0.181051] [ema: 0.999842] 
[Epoch 67/78] [Batch 600/647] [D loss: 0.439020] [G loss: 0.184758] [ema: 0.999842] 
[Epoch 68/78] [Batch 0/647] [D loss: 0.379491] [G loss: 0.184137] [ema: 0.999842] 
[Epoch 68/78] [Batch 100/647] [D loss: 0.440483] [G loss: 0.175266] [ema: 0.999843] 
[Epoch 68/78] [Batch 200/647] [D loss: 0.358616] [G loss: 0.176791] [ema: 0.999843] 
[Epoch 68/78] [Batch 300/647] [D loss: 0.435452] [G loss: 0.181798] [ema: 0.999844] 
[Epoch 68/78] [Batch 400/647] [D loss: 0.406319] [G loss: 0.172769] [ema: 0.999844] 
[Epoch 68/78] [Batch 500/647] [D loss: 0.405164] [G loss: 0.186527] [ema: 0.999844] 
[Epoch 68/78] [Batch 600/647] [D loss: 0.415249] [G loss: 0.168513] [ema: 0.999845] 
[Epoch 69/78] [Batch 0/647] [D loss: 0.391900] [G loss: 0.186636] [ema: 0.999845] 
[Epoch 69/78] [Batch 100/647] [D loss: 0.394374] [G loss: 0.192202] [ema: 0.999845] 
[Epoch 69/78] [Batch 200/647] [D loss: 0.440144] [G loss: 0.178571] [ema: 0.999845] 
[Epoch 69/78] [Batch 300/647] [D loss: 0.385165] [G loss: 0.172423] [ema: 0.999846] 
[Epoch 69/78] [Batch 400/647] [D loss: 0.378120] [G loss: 0.181226] [ema: 0.999846] 
[Epoch 69/78] [Batch 500/647] [D loss: 0.366414] [G loss: 0.175563] [ema: 0.999846] 
[Epoch 69/78] [Batch 600/647] [D loss: 0.380495] [G loss: 0.179624] [ema: 0.999847] 
[Epoch 70/78] [Batch 0/647] [D loss: 0.388234] [G loss: 0.169471] [ema: 0.999847] 
[Epoch 70/78] [Batch 100/647] [D loss: 0.393970] [G loss: 0.180377] [ema: 0.999847] 
[Epoch 70/78] [Batch 200/647] [D loss: 0.401341] [G loss: 0.178353] [ema: 0.999848] 
[Epoch 70/78] [Batch 300/647] [D loss: 0.408594] [G loss: 0.178025] [ema: 0.999848] 
[Epoch 70/78] [Batch 400/647] [D loss: 0.388626] [G loss: 0.171967] [ema: 0.999848] 
[Epoch 70/78] [Batch 500/647] [D loss: 0.414040] [G loss: 0.187406] [ema: 0.999849] 
[Epoch 70/78] [Batch 600/647] [D loss: 0.422333] [G loss: 0.180335] [ema: 0.999849] 
[Epoch 71/78] [Batch 0/647] [D loss: 0.435184] [G loss: 0.176997] [ema: 0.999849] 
[Epoch 71/78] [Batch 100/647] [D loss: 0.384719] [G loss: 0.183311] [ema: 0.999849] 
[Epoch 71/78] [Batch 200/647] [D loss: 0.396596] [G loss: 0.177697] [ema: 0.999850] 
[Epoch 71/78] [Batch 300/647] [D loss: 0.379747] [G loss: 0.183308] [ema: 0.999850] 
[Epoch 71/78] [Batch 400/647] [D loss: 0.360992] [G loss: 0.176212] [ema: 0.999850] 
[Epoch 71/78] [Batch 500/647] [D loss: 0.349209] [G loss: 0.181545] [ema: 0.999851] 
[Epoch 71/78] [Batch 600/647] [D loss: 0.417117] [G loss: 0.179408] [ema: 0.999851] 
[Epoch 72/78] [Batch 0/647] [D loss: 0.358279] [G loss: 0.191651] [ema: 0.999851] 
[Epoch 72/78] [Batch 100/647] [D loss: 0.373569] [G loss: 0.194261] [ema: 0.999852] 
[Epoch 72/78] [Batch 200/647] [D loss: 0.383149] [G loss: 0.187574] [ema: 0.999852] 
[Epoch 72/78] [Batch 300/647] [D loss: 0.439569] [G loss: 0.175973] [ema: 0.999852] 
[Epoch 72/78] [Batch 400/647] [D loss: 0.384313] [G loss: 0.176654] [ema: 0.999852] 
[Epoch 72/78] [Batch 500/647] [D loss: 0.372952] [G loss: 0.187048] [ema: 0.999853] 
[Epoch 72/78] [Batch 600/647] [D loss: 0.389839] [G loss: 0.176569] [ema: 0.999853] 
[Epoch 73/78] [Batch 0/647] [D loss: 0.366902] [G loss: 0.177369] [ema: 0.999853] 
[Epoch 73/78] [Batch 100/647] [D loss: 0.401509] [G loss: 0.183903] [ema: 0.999854] 
[Epoch 73/78] [Batch 200/647] [D loss: 0.388594] [G loss: 0.190358] [ema: 0.999854] 
[Epoch 73/78] [Batch 300/647] [D loss: 0.428080] [G loss: 0.173923] [ema: 0.999854] 
[Epoch 73/78] [Batch 400/647] [D loss: 0.381938] [G loss: 0.188544] [ema: 0.999854] 
[Epoch 73/78] [Batch 500/647] [D loss: 0.449292] [G loss: 0.171541] [ema: 0.999855] 
[Epoch 73/78] [Batch 600/647] [D loss: 0.383570] [G loss: 0.179627] [ema: 0.999855] 
[Epoch 74/78] [Batch 0/647] [D loss: 0.373187] [G loss: 0.185282] [ema: 0.999855] 
[Epoch 74/78] [Batch 100/647] [D loss: 0.434932] [G loss: 0.183371] [ema: 0.999856] 
[Epoch 74/78] [Batch 200/647] [D loss: 0.426809] [G loss: 0.169893] [ema: 0.999856] 
[Epoch 74/78] [Batch 300/647] [D loss: 0.384563] [G loss: 0.182113] [ema: 0.999856] 
[Epoch 74/78] [Batch 400/647] [D loss: 0.402317] [G loss: 0.177736] [ema: 0.999856] 
[Epoch 74/78] [Batch 500/647] [D loss: 0.365873] [G loss: 0.177304] [ema: 0.999857] 
[Epoch 74/78] [Batch 600/647] [D loss: 0.401653] [G loss: 0.175189] [ema: 0.999857] 
[Epoch 75/78] [Batch 0/647] [D loss: 0.407218] [G loss: 0.198635] [ema: 0.999857] 
[Epoch 75/78] [Batch 100/647] [D loss: 0.391097] [G loss: 0.173322] [ema: 0.999857] 
[Epoch 75/78] [Batch 200/647] [D loss: 0.379801] [G loss: 0.181032] [ema: 0.999858] 
[Epoch 75/78] [Batch 300/647] [D loss: 0.389373] [G loss: 0.175411] [ema: 0.999858] 
[Epoch 75/78] [Batch 400/647] [D loss: 0.414717] [G loss: 0.177408] [ema: 0.999858] 
[Epoch 75/78] [Batch 500/647] [D loss: 0.416793] [G loss: 0.180645] [ema: 0.999859] 
[Epoch 75/78] [Batch 600/647] [D loss: 0.421336] [G loss: 0.175807] [ema: 0.999859] 
[Epoch 76/78] [Batch 0/647] [D loss: 0.412855] [G loss: 0.180378] [ema: 0.999859] 
[Epoch 76/78] [Batch 100/647] [D loss: 0.456576] [G loss: 0.166691] [ema: 0.999859] 
[Epoch 76/78] [Batch 200/647] [D loss: 0.373361] [G loss: 0.182883] [ema: 0.999860] 
[Epoch 76/78] [Batch 300/647] [D loss: 0.418900] [G loss: 0.172015] [ema: 0.999860] 
[Epoch 76/78] [Batch 400/647] [D loss: 0.407646] [G loss: 0.178963] [ema: 0.999860] 
[Epoch 76/78] [Batch 500/647] [D loss: 0.373121] [G loss: 0.183859] [ema: 0.999860] 
[Epoch 76/78] [Batch 600/647] [D loss: 0.380411] [G loss: 0.182288] [ema: 0.999861] 
[Epoch 77/78] [Batch 0/647] [D loss: 0.424966] [G loss: 0.163781] [ema: 0.999861] 
[Epoch 77/78] [Batch 100/647] [D loss: 0.422763] [G loss: 0.183306] [ema: 0.999861] 
[Epoch 77/78] [Batch 200/647] [D loss: 0.396620] [G loss: 0.184767] [ema: 0.999861] 
[Epoch 77/78] [Batch 300/647] [D loss: 0.392253] [G loss: 0.178777] [ema: 0.999862] 
[Epoch 77/78] [Batch 400/647] [D loss: 0.398367] [G loss: 0.182905] [ema: 0.999862] 
[Epoch 77/78] [Batch 500/647] [D loss: 0.359626] [G loss: 0.174250] [ema: 0.999862] 
[Epoch 77/78] [Batch 600/647] [D loss: 0.400153] [G loss: 0.179992] [ema: 0.999863] 

----------------------------------------------------------------------------------------------------

 Starting individual training
WISDM_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
WISDM_DAGHAR_Multiclass
daghar
return single class data and labels, class is WISDM_DAGHAR_Multiclass
data shape is (8748, 6, 1, 60)
label shape is (8748,)
547
Epochs between checkpoint: 23



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_60_100/WISDM_DAGHAR_Multiclass_50000_D_60_2024_10_30_01_16_54/Model



[Epoch 0/92] [Batch 0/547] [D loss: 1.626971] [G loss: 0.701184] [ema: 0.000000] 
[Epoch 0/92] [Batch 100/547] [D loss: 0.433149] [G loss: 0.169089] [ema: 0.933033] 
[Epoch 0/92] [Batch 200/547] [D loss: 0.549402] [G loss: 0.147335] [ema: 0.965936] 
[Epoch 0/92] [Batch 300/547] [D loss: 0.468608] [G loss: 0.165041] [ema: 0.977160] 
[Epoch 0/92] [Batch 400/547] [D loss: 0.518593] [G loss: 0.141337] [ema: 0.982821] 
[Epoch 0/92] [Batch 500/547] [D loss: 0.598178] [G loss: 0.136760] [ema: 0.986233] 
[Epoch 1/92] [Batch 0/547] [D loss: 0.522410] [G loss: 0.151177] [ema: 0.987408] 
[Epoch 1/92] [Batch 100/547] [D loss: 0.499930] [G loss: 0.147130] [ema: 0.989344] 
[Epoch 1/92] [Batch 200/547] [D loss: 0.513606] [G loss: 0.142941] [ema: 0.990764] 
[Epoch 1/92] [Batch 300/547] [D loss: 0.498639] [G loss: 0.143585] [ema: 0.991850] 
[Epoch 1/92] [Batch 400/547] [D loss: 0.461966] [G loss: 0.163438] [ema: 0.992707] 
[Epoch 1/92] [Batch 500/547] [D loss: 0.430087] [G loss: 0.155275] [ema: 0.993402] 
[Epoch 2/92] [Batch 0/547] [D loss: 0.502367] [G loss: 0.161811] [ema: 0.993684] 
[Epoch 2/92] [Batch 100/547] [D loss: 0.495180] [G loss: 0.151254] [ema: 0.994212] 
[Epoch 2/92] [Batch 200/547] [D loss: 0.523677] [G loss: 0.140356] [ema: 0.994658] 
[Epoch 2/92] [Batch 300/547] [D loss: 0.462348] [G loss: 0.130419] [ema: 0.995040] 
[Epoch 2/92] [Batch 400/547] [D loss: 0.489545] [G loss: 0.153056] [ema: 0.995371] 
[Epoch 2/92] [Batch 500/547] [D loss: 0.497464] [G loss: 0.160702] [ema: 0.995661] 
[Epoch 3/92] [Batch 0/547] [D loss: 0.488378] [G loss: 0.129562] [ema: 0.995785] 
[Epoch 3/92] [Batch 100/547] [D loss: 0.460622] [G loss: 0.152196] [ema: 0.996027] 
[Epoch 3/92] [Batch 200/547] [D loss: 0.500997] [G loss: 0.123188] [ema: 0.996242] 
[Epoch 3/92] [Batch 300/547] [D loss: 0.484763] [G loss: 0.132911] [ema: 0.996435] 
[Epoch 3/92] [Batch 400/547] [D loss: 0.477403] [G loss: 0.164607] [ema: 0.996610] 
[Epoch 3/92] [Batch 500/547] [D loss: 0.440592] [G loss: 0.159576] [ema: 0.996768] 
[Epoch 4/92] [Batch 0/547] [D loss: 0.469808] [G loss: 0.152571] [ema: 0.996837] 
[Epoch 4/92] [Batch 100/547] [D loss: 0.513931] [G loss: 0.133863] [ema: 0.996975] 
[Epoch 4/92] [Batch 200/547] [D loss: 0.451713] [G loss: 0.160306] [ema: 0.997102] 
[Epoch 4/92] [Batch 300/547] [D loss: 0.457936] [G loss: 0.165336] [ema: 0.997218] 
[Epoch 4/92] [Batch 400/547] [D loss: 0.448780] [G loss: 0.178461] [ema: 0.997325] 
[Epoch 4/92] [Batch 500/547] [D loss: 0.365474] [G loss: 0.196479] [ema: 0.997425] 
[Epoch 5/92] [Batch 0/547] [D loss: 0.532285] [G loss: 0.148150] [ema: 0.997469] 
[Epoch 5/92] [Batch 100/547] [D loss: 0.479617] [G loss: 0.132734] [ema: 0.997558] 
[Epoch 5/92] [Batch 200/547] [D loss: 0.410082] [G loss: 0.158750] [ema: 0.997641] 
[Epoch 5/92] [Batch 300/547] [D loss: 0.391369] [G loss: 0.199333] [ema: 0.997719] 
[Epoch 5/92] [Batch 400/547] [D loss: 0.433015] [G loss: 0.192974] [ema: 0.997791] 
[Epoch 5/92] [Batch 500/547] [D loss: 0.452018] [G loss: 0.166900] [ema: 0.997860] 
[Epoch 6/92] [Batch 0/547] [D loss: 0.374616] [G loss: 0.219174] [ema: 0.997890] 
[Epoch 6/92] [Batch 100/547] [D loss: 0.365213] [G loss: 0.224634] [ema: 0.997953] 
[Epoch 6/92] [Batch 200/547] [D loss: 0.433422] [G loss: 0.168678] [ema: 0.998011] 
[Epoch 6/92] [Batch 300/547] [D loss: 0.341656] [G loss: 0.193792] [ema: 0.998067] 
[Epoch 6/92] [Batch 400/547] [D loss: 0.365455] [G loss: 0.218138] [ema: 0.998119] 
[Epoch 6/92] [Batch 500/547] [D loss: 0.379960] [G loss: 0.211809] [ema: 0.998169] 
[Epoch 7/92] [Batch 0/547] [D loss: 0.379097] [G loss: 0.210779] [ema: 0.998191] 
[Epoch 7/92] [Batch 100/547] [D loss: 0.323086] [G loss: 0.187343] [ema: 0.998237] 
[Epoch 7/92] [Batch 200/547] [D loss: 0.364385] [G loss: 0.198926] [ema: 0.998281] 
[Epoch 7/92] [Batch 300/547] [D loss: 0.385325] [G loss: 0.173910] [ema: 0.998323] 
[Epoch 7/92] [Batch 400/547] [D loss: 0.314103] [G loss: 0.194855] [ema: 0.998362] 
[Epoch 7/92] [Batch 500/547] [D loss: 0.350925] [G loss: 0.196577] [ema: 0.998400] 
[Epoch 8/92] [Batch 0/547] [D loss: 0.372014] [G loss: 0.166031] [ema: 0.998417] 
[Epoch 8/92] [Batch 100/547] [D loss: 0.363718] [G loss: 0.178359] [ema: 0.998453] 
[Epoch 8/92] [Batch 200/547] [D loss: 0.344112] [G loss: 0.207234] [ema: 0.998486] 
[Epoch 8/92] [Batch 300/547] [D loss: 0.414349] [G loss: 0.165719] [ema: 0.998519] 
[Epoch 8/92] [Batch 400/547] [D loss: 0.289385] [G loss: 0.224877] [ema: 0.998550] 
[Epoch 8/92] [Batch 500/547] [D loss: 0.344371] [G loss: 0.202749] [ema: 0.998579] 
[Epoch 9/92] [Batch 0/547] [D loss: 0.354062] [G loss: 0.224988] [ema: 0.998593] 
[Epoch 9/92] [Batch 100/547] [D loss: 0.363467] [G loss: 0.215543] [ema: 0.998621] 
[Epoch 9/92] [Batch 200/547] [D loss: 0.284020] [G loss: 0.209857] [ema: 0.998648] 
[Epoch 9/92] [Batch 300/547] [D loss: 0.382189] [G loss: 0.188436] [ema: 0.998674] 
[Epoch 9/92] [Batch 400/547] [D loss: 0.304575] [G loss: 0.224278] [ema: 0.998699] 
[Epoch 9/92] [Batch 500/547] [D loss: 0.301174] [G loss: 0.227099] [ema: 0.998723] 
[Epoch 10/92] [Batch 0/547] [D loss: 0.391834] [G loss: 0.209696] [ema: 0.998734] 
[Epoch 10/92] [Batch 100/547] [D loss: 0.352001] [G loss: 0.197631] [ema: 0.998756] 
[Epoch 10/92] [Batch 200/547] [D loss: 0.309267] [G loss: 0.232610] [ema: 0.998778] 
[Epoch 10/92] [Batch 300/547] [D loss: 0.266006] [G loss: 0.234059] [ema: 0.998799] 
[Epoch 10/92] [Batch 400/547] [D loss: 0.279256] [G loss: 0.236551] [ema: 0.998820] 
[Epoch 10/92] [Batch 500/547] [D loss: 0.262850] [G loss: 0.250436] [ema: 0.998840] 
[Epoch 11/92] [Batch 0/547] [D loss: 0.277064] [G loss: 0.239924] [ema: 0.998849] 
[Epoch 11/92] [Batch 100/547] [D loss: 0.375259] [G loss: 0.236672] [ema: 0.998867] 
[Epoch 11/92] [Batch 200/547] [D loss: 0.280044] [G loss: 0.233823] [ema: 0.998886] 
[Epoch 11/92] [Batch 300/547] [D loss: 0.276429] [G loss: 0.226634] [ema: 0.998903] 
[Epoch 11/92] [Batch 400/547] [D loss: 0.262821] [G loss: 0.218624] [ema: 0.998920] 
[Epoch 11/92] [Batch 500/547] [D loss: 0.373642] [G loss: 0.182828] [ema: 0.998937] 
[Epoch 12/92] [Batch 0/547] [D loss: 0.314930] [G loss: 0.223571] [ema: 0.998945] 
[Epoch 12/92] [Batch 100/547] [D loss: 0.316063] [G loss: 0.173841] [ema: 0.998960] 
[Epoch 12/92] [Batch 200/547] [D loss: 0.299780] [G loss: 0.231679] [ema: 0.998976] 
[Epoch 12/92] [Batch 300/547] [D loss: 0.338026] [G loss: 0.198835] [ema: 0.998991] 
[Epoch 12/92] [Batch 400/547] [D loss: 0.298196] [G loss: 0.226029] [ema: 0.999005] 
[Epoch 12/92] [Batch 500/547] [D loss: 0.347096] [G loss: 0.210955] [ema: 0.999019] 
[Epoch 13/92] [Batch 0/547] [D loss: 0.378172] [G loss: 0.210236] [ema: 0.999026] 
[Epoch 13/92] [Batch 100/547] [D loss: 0.332832] [G loss: 0.234952] [ema: 0.999039] 
[Epoch 13/92] [Batch 200/547] [D loss: 0.354315] [G loss: 0.254415] [ema: 0.999052] 
[Epoch 13/92] [Batch 300/547] [D loss: 0.295044] [G loss: 0.209797] [ema: 0.999065] 
[Epoch 13/92] [Batch 400/547] [D loss: 0.303181] [G loss: 0.242340] [ema: 0.999078] 
[Epoch 13/92] [Batch 500/547] [D loss: 0.299174] [G loss: 0.214558] [ema: 0.999090] 
[Epoch 14/92] [Batch 0/547] [D loss: 0.284344] [G loss: 0.251423] [ema: 0.999095] 
[Epoch 14/92] [Batch 100/547] [D loss: 0.315315] [G loss: 0.215943] [ema: 0.999107] 
[Epoch 14/92] [Batch 200/547] [D loss: 0.270464] [G loss: 0.220040] [ema: 0.999118] 
[Epoch 14/92] [Batch 300/547] [D loss: 0.287201] [G loss: 0.241187] [ema: 0.999129] 
[Epoch 14/92] [Batch 400/547] [D loss: 0.299261] [G loss: 0.237147] [ema: 0.999140] 
[Epoch 14/92] [Batch 500/547] [D loss: 0.320592] [G loss: 0.202731] [ema: 0.999151] 
[Epoch 15/92] [Batch 0/547] [D loss: 0.281746] [G loss: 0.242077] [ema: 0.999156] 
[Epoch 15/92] [Batch 100/547] [D loss: 0.295517] [G loss: 0.232809] [ema: 0.999166] 
[Epoch 15/92] [Batch 200/547] [D loss: 0.313412] [G loss: 0.238278] [ema: 0.999176] 
[Epoch 15/92] [Batch 300/547] [D loss: 0.367392] [G loss: 0.214038] [ema: 0.999185] 
[Epoch 15/92] [Batch 400/547] [D loss: 0.314620] [G loss: 0.193377] [ema: 0.999195] 
[Epoch 15/92] [Batch 500/547] [D loss: 0.257123] [G loss: 0.236624] [ema: 0.999204] 
[Epoch 16/92] [Batch 0/547] [D loss: 0.344009] [G loss: 0.228896] [ema: 0.999208] 
[Epoch 16/92] [Batch 100/547] [D loss: 0.302375] [G loss: 0.231508] [ema: 0.999217] 
[Epoch 16/92] [Batch 200/547] [D loss: 0.266754] [G loss: 0.233304] [ema: 0.999226] 
[Epoch 16/92] [Batch 300/547] [D loss: 0.257487] [G loss: 0.235868] [ema: 0.999235] 
[Epoch 16/92] [Batch 400/547] [D loss: 0.288292] [G loss: 0.213119] [ema: 0.999243] 
[Epoch 16/92] [Batch 500/547] [D loss: 0.273329] [G loss: 0.226247] [ema: 0.999251] 
[Epoch 17/92] [Batch 0/547] [D loss: 0.304970] [G loss: 0.226315] [ema: 0.999255] 
[Epoch 17/92] [Batch 100/547] [D loss: 0.320297] [G loss: 0.251282] [ema: 0.999263] 
[Epoch 17/92] [Batch 200/547] [D loss: 0.299014] [G loss: 0.239166] [ema: 0.999271] 
[Epoch 17/92] [Batch 300/547] [D loss: 0.255540] [G loss: 0.229659] [ema: 0.999278] 
[Epoch 17/92] [Batch 400/547] [D loss: 0.312985] [G loss: 0.215242] [ema: 0.999286] 
[Epoch 17/92] [Batch 500/547] [D loss: 0.315226] [G loss: 0.226547] [ema: 0.999293] 
[Epoch 18/92] [Batch 0/547] [D loss: 0.277613] [G loss: 0.258256] [ema: 0.999296] 
[Epoch 18/92] [Batch 100/547] [D loss: 0.288953] [G loss: 0.225489] [ema: 0.999303] 
[Epoch 18/92] [Batch 200/547] [D loss: 0.290179] [G loss: 0.213916] [ema: 0.999310] 
[Epoch 18/92] [Batch 300/547] [D loss: 0.321267] [G loss: 0.225161] [ema: 0.999317] 
[Epoch 18/92] [Batch 400/547] [D loss: 0.291514] [G loss: 0.234509] [ema: 0.999324] 
[Epoch 18/92] [Batch 500/547] [D loss: 0.308717] [G loss: 0.212093] [ema: 0.999330] 
[Epoch 19/92] [Batch 0/547] [D loss: 0.286924] [G loss: 0.225655] [ema: 0.999333] 
[Epoch 19/92] [Batch 100/547] [D loss: 0.303889] [G loss: 0.202965] [ema: 0.999340] 
[Epoch 19/92] [Batch 200/547] [D loss: 0.312573] [G loss: 0.235565] [ema: 0.999346] 
[Epoch 19/92] [Batch 300/547] [D loss: 0.306266] [G loss: 0.198289] [ema: 0.999352] 
[Epoch 19/92] [Batch 400/547] [D loss: 0.313403] [G loss: 0.200251] [ema: 0.999358] 
[Epoch 19/92] [Batch 500/547] [D loss: 0.379121] [G loss: 0.196025] [ema: 0.999364] 
[Epoch 20/92] [Batch 0/547] [D loss: 0.324656] [G loss: 0.227099] [ema: 0.999367] 
[Epoch 20/92] [Batch 100/547] [D loss: 0.305016] [G loss: 0.221508] [ema: 0.999372] 
[Epoch 20/92] [Batch 200/547] [D loss: 0.308506] [G loss: 0.219481] [ema: 0.999378] 
[Epoch 20/92] [Batch 300/547] [D loss: 0.326247] [G loss: 0.224303] [ema: 0.999384] 
[Epoch 20/92] [Batch 400/547] [D loss: 0.325567] [G loss: 0.212998] [ema: 0.999389] 
[Epoch 20/92] [Batch 500/547] [D loss: 0.289114] [G loss: 0.236711] [ema: 0.999394] 
[Epoch 21/92] [Batch 0/547] [D loss: 0.310364] [G loss: 0.198440] [ema: 0.999397] 
[Epoch 21/92] [Batch 100/547] [D loss: 0.307014] [G loss: 0.215821] [ema: 0.999402] 
[Epoch 21/92] [Batch 200/547] [D loss: 0.330727] [G loss: 0.222349] [ema: 0.999407] 
[Epoch 21/92] [Batch 300/547] [D loss: 0.300409] [G loss: 0.202073] [ema: 0.999412] 
[Epoch 21/92] [Batch 400/547] [D loss: 0.329943] [G loss: 0.188520] [ema: 0.999417] 
[Epoch 21/92] [Batch 500/547] [D loss: 0.376969] [G loss: 0.215301] [ema: 0.999422] 
[Epoch 22/92] [Batch 0/547] [D loss: 0.363898] [G loss: 0.191098] [ema: 0.999424] 
[Epoch 22/92] [Batch 100/547] [D loss: 0.328898] [G loss: 0.202080] [ema: 0.999429] 
[Epoch 22/92] [Batch 200/547] [D loss: 0.339157] [G loss: 0.212888] [ema: 0.999434] 
[Epoch 22/92] [Batch 300/547] [D loss: 0.416169] [G loss: 0.215837] [ema: 0.999438] 
[Epoch 22/92] [Batch 400/547] [D loss: 0.403059] [G loss: 0.182776] [ema: 0.999443] 
[Epoch 22/92] [Batch 500/547] [D loss: 0.436133] [G loss: 0.223696] [ema: 0.999447] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_60_100/WISDM_DAGHAR_Multiclass_50000_D_60_2024_10_30_01_16_54/Model



[Epoch 23/92] [Batch 0/547] [D loss: 0.327791] [G loss: 0.210184] [ema: 0.999449] 
[Epoch 23/92] [Batch 100/547] [D loss: 0.324374] [G loss: 0.201504] [ema: 0.999454] 
[Epoch 23/92] [Batch 200/547] [D loss: 0.331019] [G loss: 0.207721] [ema: 0.999458] 
[Epoch 23/92] [Batch 300/547] [D loss: 0.339188] [G loss: 0.204138] [ema: 0.999462] 
[Epoch 23/92] [Batch 400/547] [D loss: 0.407568] [G loss: 0.172211] [ema: 0.999466] 
[Epoch 23/92] [Batch 500/547] [D loss: 0.337546] [G loss: 0.208846] [ema: 0.999470] 
[Epoch 24/92] [Batch 0/547] [D loss: 0.301188] [G loss: 0.213904] [ema: 0.999472] 
[Epoch 24/92] [Batch 100/547] [D loss: 0.391177] [G loss: 0.198429] [ema: 0.999476] 
[Epoch 24/92] [Batch 200/547] [D loss: 0.347718] [G loss: 0.192547] [ema: 0.999480] 
[Epoch 24/92] [Batch 300/547] [D loss: 0.455790] [G loss: 0.188037] [ema: 0.999484] 
[Epoch 24/92] [Batch 400/547] [D loss: 0.372035] [G loss: 0.195476] [ema: 0.999488] 
[Epoch 24/92] [Batch 500/547] [D loss: 0.361037] [G loss: 0.187384] [ema: 0.999492] 
[Epoch 25/92] [Batch 0/547] [D loss: 0.365138] [G loss: 0.200038] [ema: 0.999493] 
[Epoch 25/92] [Batch 100/547] [D loss: 0.411546] [G loss: 0.186371] [ema: 0.999497] 
[Epoch 25/92] [Batch 200/547] [D loss: 0.333937] [G loss: 0.197942] [ema: 0.999501] 
[Epoch 25/92] [Batch 300/547] [D loss: 0.370664] [G loss: 0.207111] [ema: 0.999504] 
[Epoch 25/92] [Batch 400/547] [D loss: 0.340471] [G loss: 0.158890] [ema: 0.999508] 
[Epoch 25/92] [Batch 500/547] [D loss: 0.392882] [G loss: 0.208551] [ema: 0.999511] 
[Epoch 26/92] [Batch 0/547] [D loss: 0.376593] [G loss: 0.193199] [ema: 0.999513] 
[Epoch 26/92] [Batch 100/547] [D loss: 0.373550] [G loss: 0.221918] [ema: 0.999516] 
[Epoch 26/92] [Batch 200/547] [D loss: 0.424269] [G loss: 0.209167] [ema: 0.999519] 
[Epoch 26/92] [Batch 300/547] [D loss: 0.447310] [G loss: 0.157553] [ema: 0.999523] 
[Epoch 26/92] [Batch 400/547] [D loss: 0.374263] [G loss: 0.190109] [ema: 0.999526] 
[Epoch 26/92] [Batch 500/547] [D loss: 0.434661] [G loss: 0.169948] [ema: 0.999529] 
[Epoch 27/92] [Batch 0/547] [D loss: 0.369496] [G loss: 0.179717] [ema: 0.999531] 
[Epoch 27/92] [Batch 100/547] [D loss: 0.443446] [G loss: 0.194062] [ema: 0.999534] 
[Epoch 27/92] [Batch 200/547] [D loss: 0.388285] [G loss: 0.215221] [ema: 0.999537] 
[Epoch 27/92] [Batch 300/547] [D loss: 0.402859] [G loss: 0.186946] [ema: 0.999540] 
[Epoch 27/92] [Batch 400/547] [D loss: 0.387700] [G loss: 0.189671] [ema: 0.999543] 
[Epoch 27/92] [Batch 500/547] [D loss: 0.390227] [G loss: 0.178682] [ema: 0.999546] 
[Epoch 28/92] [Batch 0/547] [D loss: 0.424076] [G loss: 0.156872] [ema: 0.999548] 
[Epoch 28/92] [Batch 100/547] [D loss: 0.412042] [G loss: 0.188796] [ema: 0.999550] 
[Epoch 28/92] [Batch 200/547] [D loss: 0.412277] [G loss: 0.201229] [ema: 0.999553] 
[Epoch 28/92] [Batch 300/547] [D loss: 0.406594] [G loss: 0.225996] [ema: 0.999556] 
[Epoch 28/92] [Batch 400/547] [D loss: 0.389106] [G loss: 0.172892] [ema: 0.999559] 
[Epoch 28/92] [Batch 500/547] [D loss: 0.377039] [G loss: 0.167183] [ema: 0.999562] 
[Epoch 29/92] [Batch 0/547] [D loss: 0.422540] [G loss: 0.207436] [ema: 0.999563] 
[Epoch 29/92] [Batch 100/547] [D loss: 0.410094] [G loss: 0.193109] [ema: 0.999566] 
[Epoch 29/92] [Batch 200/547] [D loss: 0.360693] [G loss: 0.194284] [ema: 0.999569] 
[Epoch 29/92] [Batch 300/547] [D loss: 0.388446] [G loss: 0.184040] [ema: 0.999571] 
[Epoch 29/92] [Batch 400/547] [D loss: 0.379150] [G loss: 0.183110] [ema: 0.999574] 
[Epoch 29/92] [Batch 500/547] [D loss: 0.431190] [G loss: 0.159643] [ema: 0.999576] 
[Epoch 30/92] [Batch 0/547] [D loss: 0.395564] [G loss: 0.165537] [ema: 0.999578] 
[Epoch 30/92] [Batch 100/547] [D loss: 0.438803] [G loss: 0.163324] [ema: 0.999580] 
[Epoch 30/92] [Batch 200/547] [D loss: 0.451451] [G loss: 0.176840] [ema: 0.999583] 
[Epoch 30/92] [Batch 300/547] [D loss: 0.345089] [G loss: 0.169964] [ema: 0.999585] 
[Epoch 30/92] [Batch 400/547] [D loss: 0.395406] [G loss: 0.155139] [ema: 0.999588] 
[Epoch 30/92] [Batch 500/547] [D loss: 0.390911] [G loss: 0.173368] [ema: 0.999590] 
[Epoch 31/92] [Batch 0/547] [D loss: 0.428047] [G loss: 0.182065] [ema: 0.999591] 
[Epoch 31/92] [Batch 100/547] [D loss: 0.449498] [G loss: 0.167852] [ema: 0.999594] 
[Epoch 31/92] [Batch 200/547] [D loss: 0.403614] [G loss: 0.174989] [ema: 0.999596] 
[Epoch 31/92] [Batch 300/547] [D loss: 0.417776] [G loss: 0.197068] [ema: 0.999598] 
[Epoch 31/92] [Batch 400/547] [D loss: 0.413067] [G loss: 0.194762] [ema: 0.999601] 
[Epoch 31/92] [Batch 500/547] [D loss: 0.425687] [G loss: 0.168551] [ema: 0.999603] 
[Epoch 32/92] [Batch 0/547] [D loss: 0.410288] [G loss: 0.194777] [ema: 0.999604] 
[Epoch 32/92] [Batch 100/547] [D loss: 0.455663] [G loss: 0.170593] [ema: 0.999606] 
[Epoch 32/92] [Batch 200/547] [D loss: 0.382473] [G loss: 0.169394] [ema: 0.999609] 
[Epoch 32/92] [Batch 300/547] [D loss: 0.441709] [G loss: 0.169447] [ema: 0.999611] 
[Epoch 32/92] [Batch 400/547] [D loss: 0.384227] [G loss: 0.186342] [ema: 0.999613] 
[Epoch 32/92] [Batch 500/547] [D loss: 0.379264] [G loss: 0.176397] [ema: 0.999615] 
[Epoch 33/92] [Batch 0/547] [D loss: 0.415364] [G loss: 0.194394] [ema: 0.999616] 
[Epoch 33/92] [Batch 100/547] [D loss: 0.376490] [G loss: 0.186607] [ema: 0.999618] 
[Epoch 33/92] [Batch 200/547] [D loss: 0.366645] [G loss: 0.191545] [ema: 0.999620] 
[Epoch 33/92] [Batch 300/547] [D loss: 0.399542] [G loss: 0.180892] [ema: 0.999622] 
[Epoch 33/92] [Batch 400/547] [D loss: 0.473250] [G loss: 0.172314] [ema: 0.999624] 
[Epoch 33/92] [Batch 500/547] [D loss: 0.398331] [G loss: 0.182840] [ema: 0.999626] 
[Epoch 34/92] [Batch 0/547] [D loss: 0.411620] [G loss: 0.172003] [ema: 0.999627] 
[Epoch 34/92] [Batch 100/547] [D loss: 0.430310] [G loss: 0.187076] [ema: 0.999629] 
[Epoch 34/92] [Batch 200/547] [D loss: 0.392712] [G loss: 0.186613] [ema: 0.999631] 
[Epoch 34/92] [Batch 300/547] [D loss: 0.388747] [G loss: 0.175771] [ema: 0.999633] 
[Epoch 34/92] [Batch 400/547] [D loss: 0.414995] [G loss: 0.181962] [ema: 0.999635] 
[Epoch 34/92] [Batch 500/547] [D loss: 0.435447] [G loss: 0.145866] [ema: 0.999637] 
[Epoch 35/92] [Batch 0/547] [D loss: 0.388384] [G loss: 0.167049] [ema: 0.999638] 
[Epoch 35/92] [Batch 100/547] [D loss: 0.394618] [G loss: 0.163932] [ema: 0.999640] 
[Epoch 35/92] [Batch 200/547] [D loss: 0.428767] [G loss: 0.175989] [ema: 0.999642] 
[Epoch 35/92] [Batch 300/547] [D loss: 0.370115] [G loss: 0.168603] [ema: 0.999644] 
[Epoch 35/92] [Batch 400/547] [D loss: 0.408759] [G loss: 0.165224] [ema: 0.999645] 
[Epoch 35/92] [Batch 500/547] [D loss: 0.410329] [G loss: 0.172690] [ema: 0.999647] 
[Epoch 36/92] [Batch 0/547] [D loss: 0.399430] [G loss: 0.171594] [ema: 0.999648] 
[Epoch 36/92] [Batch 100/547] [D loss: 0.394967] [G loss: 0.173832] [ema: 0.999650] 
[Epoch 36/92] [Batch 200/547] [D loss: 0.400265] [G loss: 0.172592] [ema: 0.999652] 
[Epoch 36/92] [Batch 300/547] [D loss: 0.460922] [G loss: 0.180463] [ema: 0.999653] 
[Epoch 36/92] [Batch 400/547] [D loss: 0.370346] [G loss: 0.152995] [ema: 0.999655] 
[Epoch 36/92] [Batch 500/547] [D loss: 0.430889] [G loss: 0.155520] [ema: 0.999657] 
[Epoch 37/92] [Batch 0/547] [D loss: 0.413376] [G loss: 0.195630] [ema: 0.999658] 
[Epoch 37/92] [Batch 100/547] [D loss: 0.427246] [G loss: 0.176061] [ema: 0.999659] 
[Epoch 37/92] [Batch 200/547] [D loss: 0.395317] [G loss: 0.170841] [ema: 0.999661] 
[Epoch 37/92] [Batch 300/547] [D loss: 0.358796] [G loss: 0.167626] [ema: 0.999663] 
[Epoch 37/92] [Batch 400/547] [D loss: 0.415404] [G loss: 0.166662] [ema: 0.999664] 
[Epoch 37/92] [Batch 500/547] [D loss: 0.412830] [G loss: 0.178873] [ema: 0.999666] 
[Epoch 38/92] [Batch 0/547] [D loss: 0.410755] [G loss: 0.194058] [ema: 0.999667] 
[Epoch 38/92] [Batch 100/547] [D loss: 0.429665] [G loss: 0.172674] [ema: 0.999668] 
[Epoch 38/92] [Batch 200/547] [D loss: 0.379626] [G loss: 0.165321] [ema: 0.999670] 
[Epoch 38/92] [Batch 300/547] [D loss: 0.421267] [G loss: 0.168021] [ema: 0.999671] 
[Epoch 38/92] [Batch 400/547] [D loss: 0.380176] [G loss: 0.177838] [ema: 0.999673] 
[Epoch 38/92] [Batch 500/547] [D loss: 0.427803] [G loss: 0.153028] [ema: 0.999674] 
[Epoch 39/92] [Batch 0/547] [D loss: 0.430683] [G loss: 0.163040] [ema: 0.999675] 
[Epoch 39/92] [Batch 100/547] [D loss: 0.390532] [G loss: 0.145594] [ema: 0.999677] 
[Epoch 39/92] [Batch 200/547] [D loss: 0.459843] [G loss: 0.179958] [ema: 0.999678] 
[Epoch 39/92] [Batch 300/547] [D loss: 0.433283] [G loss: 0.166509] [ema: 0.999680] 
[Epoch 39/92] [Batch 400/547] [D loss: 0.437120] [G loss: 0.163310] [ema: 0.999681] 
[Epoch 39/92] [Batch 500/547] [D loss: 0.415199] [G loss: 0.178219] [ema: 0.999683] 
[Epoch 40/92] [Batch 0/547] [D loss: 0.463093] [G loss: 0.170073] [ema: 0.999683] 
[Epoch 40/92] [Batch 100/547] [D loss: 0.413975] [G loss: 0.187771] [ema: 0.999685] 
[Epoch 40/92] [Batch 200/547] [D loss: 0.413815] [G loss: 0.153802] [ema: 0.999686] 
[Epoch 40/92] [Batch 300/547] [D loss: 0.469037] [G loss: 0.189106] [ema: 0.999688] 
[Epoch 40/92] [Batch 400/547] [D loss: 0.401681] [G loss: 0.171259] [ema: 0.999689] 
[Epoch 40/92] [Batch 500/547] [D loss: 0.396920] [G loss: 0.152600] [ema: 0.999690] 
[Epoch 41/92] [Batch 0/547] [D loss: 0.373854] [G loss: 0.180287] [ema: 0.999691] 
[Epoch 41/92] [Batch 100/547] [D loss: 0.435433] [G loss: 0.168411] [ema: 0.999692] 
[Epoch 41/92] [Batch 200/547] [D loss: 0.430195] [G loss: 0.164215] [ema: 0.999694] 
[Epoch 41/92] [Batch 300/547] [D loss: 0.452401] [G loss: 0.165703] [ema: 0.999695] 
[Epoch 41/92] [Batch 400/547] [D loss: 0.434738] [G loss: 0.167317] [ema: 0.999696] 
[Epoch 41/92] [Batch 500/547] [D loss: 0.402660] [G loss: 0.175722] [ema: 0.999698] 
[Epoch 42/92] [Batch 0/547] [D loss: 0.417427] [G loss: 0.150629] [ema: 0.999698] 
[Epoch 42/92] [Batch 100/547] [D loss: 0.422928] [G loss: 0.174303] [ema: 0.999700] 
[Epoch 42/92] [Batch 200/547] [D loss: 0.421211] [G loss: 0.168685] [ema: 0.999701] 
[Epoch 42/92] [Batch 300/547] [D loss: 0.413419] [G loss: 0.162389] [ema: 0.999702] 
[Epoch 42/92] [Batch 400/547] [D loss: 0.411952] [G loss: 0.169300] [ema: 0.999703] 
[Epoch 42/92] [Batch 500/547] [D loss: 0.412730] [G loss: 0.176949] [ema: 0.999705] 
[Epoch 43/92] [Batch 0/547] [D loss: 0.397222] [G loss: 0.180048] [ema: 0.999705] 
[Epoch 43/92] [Batch 100/547] [D loss: 0.450950] [G loss: 0.163248] [ema: 0.999707] 
[Epoch 43/92] [Batch 200/547] [D loss: 0.430359] [G loss: 0.177063] [ema: 0.999708] 
[Epoch 43/92] [Batch 300/547] [D loss: 0.393903] [G loss: 0.186654] [ema: 0.999709] 
[Epoch 43/92] [Batch 400/547] [D loss: 0.389098] [G loss: 0.176536] [ema: 0.999710] 
[Epoch 43/92] [Batch 500/547] [D loss: 0.441819] [G loss: 0.187459] [ema: 0.999711] 
[Epoch 44/92] [Batch 0/547] [D loss: 0.413375] [G loss: 0.167611] [ema: 0.999712] 
[Epoch 44/92] [Batch 100/547] [D loss: 0.382354] [G loss: 0.181460] [ema: 0.999713] 
[Epoch 44/92] [Batch 200/547] [D loss: 0.361591] [G loss: 0.189018] [ema: 0.999714] 
[Epoch 44/92] [Batch 300/547] [D loss: 0.441674] [G loss: 0.179400] [ema: 0.999716] 
[Epoch 44/92] [Batch 400/547] [D loss: 0.354636] [G loss: 0.190758] [ema: 0.999717] 
[Epoch 44/92] [Batch 500/547] [D loss: 0.436920] [G loss: 0.164285] [ema: 0.999718] 
[Epoch 45/92] [Batch 0/547] [D loss: 0.403240] [G loss: 0.184639] [ema: 0.999718] 
[Epoch 45/92] [Batch 100/547] [D loss: 0.397608] [G loss: 0.182590] [ema: 0.999720] 
[Epoch 45/92] [Batch 200/547] [D loss: 0.415018] [G loss: 0.186416] [ema: 0.999721] 
[Epoch 45/92] [Batch 300/547] [D loss: 0.392178] [G loss: 0.168017] [ema: 0.999722] 
[Epoch 45/92] [Batch 400/547] [D loss: 0.424862] [G loss: 0.171113] [ema: 0.999723] 
[Epoch 45/92] [Batch 500/547] [D loss: 0.412138] [G loss: 0.155773] [ema: 0.999724] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_60_100/WISDM_DAGHAR_Multiclass_50000_D_60_2024_10_30_01_16_54/Model



[Epoch 46/92] [Batch 0/547] [D loss: 0.406624] [G loss: 0.167958] [ema: 0.999725] 
[Epoch 46/92] [Batch 100/547] [D loss: 0.445240] [G loss: 0.173302] [ema: 0.999726] 
[Epoch 46/92] [Batch 200/547] [D loss: 0.404783] [G loss: 0.174458] [ema: 0.999727] 
[Epoch 46/92] [Batch 300/547] [D loss: 0.421071] [G loss: 0.175680] [ema: 0.999728] 
[Epoch 46/92] [Batch 400/547] [D loss: 0.426730] [G loss: 0.167877] [ema: 0.999729] 
[Epoch 46/92] [Batch 500/547] [D loss: 0.428683] [G loss: 0.164990] [ema: 0.999730] 
[Epoch 47/92] [Batch 0/547] [D loss: 0.395623] [G loss: 0.171962] [ema: 0.999730] 
[Epoch 47/92] [Batch 100/547] [D loss: 0.413799] [G loss: 0.165312] [ema: 0.999731] 
[Epoch 47/92] [Batch 200/547] [D loss: 0.383907] [G loss: 0.179264] [ema: 0.999733] 
[Epoch 47/92] [Batch 300/547] [D loss: 0.398025] [G loss: 0.173900] [ema: 0.999734] 
[Epoch 47/92] [Batch 400/547] [D loss: 0.416817] [G loss: 0.164926] [ema: 0.999735] 
[Epoch 47/92] [Batch 500/547] [D loss: 0.386391] [G loss: 0.187380] [ema: 0.999736] 
[Epoch 48/92] [Batch 0/547] [D loss: 0.421290] [G loss: 0.181601] [ema: 0.999736] 
[Epoch 48/92] [Batch 100/547] [D loss: 0.437894] [G loss: 0.181144] [ema: 0.999737] 
[Epoch 48/92] [Batch 200/547] [D loss: 0.406532] [G loss: 0.183934] [ema: 0.999738] 
[Epoch 48/92] [Batch 300/547] [D loss: 0.428283] [G loss: 0.167959] [ema: 0.999739] 
[Epoch 48/92] [Batch 400/547] [D loss: 0.386400] [G loss: 0.206869] [ema: 0.999740] 
[Epoch 48/92] [Batch 500/547] [D loss: 0.387400] [G loss: 0.163604] [ema: 0.999741] 
[Epoch 49/92] [Batch 0/547] [D loss: 0.425731] [G loss: 0.182243] [ema: 0.999741] 
[Epoch 49/92] [Batch 100/547] [D loss: 0.419527] [G loss: 0.175227] [ema: 0.999742] 
[Epoch 49/92] [Batch 200/547] [D loss: 0.428836] [G loss: 0.181000] [ema: 0.999743] 
[Epoch 49/92] [Batch 300/547] [D loss: 0.428983] [G loss: 0.164697] [ema: 0.999744] 
[Epoch 49/92] [Batch 400/547] [D loss: 0.477749] [G loss: 0.194192] [ema: 0.999745] 
[Epoch 49/92] [Batch 500/547] [D loss: 0.392266] [G loss: 0.170496] [ema: 0.999746] 
[Epoch 50/92] [Batch 0/547] [D loss: 0.410879] [G loss: 0.177781] [ema: 0.999747] 
[Epoch 50/92] [Batch 100/547] [D loss: 0.408336] [G loss: 0.174806] [ema: 0.999748] 
[Epoch 50/92] [Batch 200/547] [D loss: 0.446319] [G loss: 0.162300] [ema: 0.999748] 
[Epoch 50/92] [Batch 300/547] [D loss: 0.377299] [G loss: 0.193009] [ema: 0.999749] 
[Epoch 50/92] [Batch 400/547] [D loss: 0.368766] [G loss: 0.179474] [ema: 0.999750] 
[Epoch 50/92] [Batch 500/547] [D loss: 0.392624] [G loss: 0.169465] [ema: 0.999751] 
[Epoch 51/92] [Batch 0/547] [D loss: 0.430144] [G loss: 0.166318] [ema: 0.999752] 
[Epoch 51/92] [Batch 100/547] [D loss: 0.386400] [G loss: 0.164808] [ema: 0.999752] 
[Epoch 51/92] [Batch 200/547] [D loss: 0.404881] [G loss: 0.176407] [ema: 0.999753] 
[Epoch 51/92] [Batch 300/547] [D loss: 0.425604] [G loss: 0.151196] [ema: 0.999754] 
[Epoch 51/92] [Batch 400/547] [D loss: 0.419002] [G loss: 0.168800] [ema: 0.999755] 
[Epoch 51/92] [Batch 500/547] [D loss: 0.428069] [G loss: 0.162622] [ema: 0.999756] 
[Epoch 52/92] [Batch 0/547] [D loss: 0.448009] [G loss: 0.156528] [ema: 0.999756] 
[Epoch 52/92] [Batch 100/547] [D loss: 0.414928] [G loss: 0.172992] [ema: 0.999757] 
[Epoch 52/92] [Batch 200/547] [D loss: 0.450117] [G loss: 0.162546] [ema: 0.999758] 
[Epoch 52/92] [Batch 300/547] [D loss: 0.383391] [G loss: 0.178171] [ema: 0.999759] 
[Epoch 52/92] [Batch 400/547] [D loss: 0.382638] [G loss: 0.189518] [ema: 0.999760] 
[Epoch 52/92] [Batch 500/547] [D loss: 0.439024] [G loss: 0.175239] [ema: 0.999761] 
[Epoch 53/92] [Batch 0/547] [D loss: 0.369760] [G loss: 0.175768] [ema: 0.999761] 
[Epoch 53/92] [Batch 100/547] [D loss: 0.444975] [G loss: 0.186599] [ema: 0.999762] 
[Epoch 53/92] [Batch 200/547] [D loss: 0.373239] [G loss: 0.168102] [ema: 0.999763] 
[Epoch 53/92] [Batch 300/547] [D loss: 0.423008] [G loss: 0.171799] [ema: 0.999763] 
[Epoch 53/92] [Batch 400/547] [D loss: 0.425717] [G loss: 0.163131] [ema: 0.999764] 
[Epoch 53/92] [Batch 500/547] [D loss: 0.410402] [G loss: 0.177313] [ema: 0.999765] 
[Epoch 54/92] [Batch 0/547] [D loss: 0.410918] [G loss: 0.168648] [ema: 0.999765] 
[Epoch 54/92] [Batch 100/547] [D loss: 0.425388] [G loss: 0.171100] [ema: 0.999766] 
[Epoch 54/92] [Batch 200/547] [D loss: 0.409074] [G loss: 0.180874] [ema: 0.999767] 
[Epoch 54/92] [Batch 300/547] [D loss: 0.392377] [G loss: 0.166620] [ema: 0.999768] 
[Epoch 54/92] [Batch 400/547] [D loss: 0.378939] [G loss: 0.176080] [ema: 0.999768] 
[Epoch 54/92] [Batch 500/547] [D loss: 0.405160] [G loss: 0.173058] [ema: 0.999769] 
[Epoch 55/92] [Batch 0/547] [D loss: 0.418354] [G loss: 0.188594] [ema: 0.999770] 
[Epoch 55/92] [Batch 100/547] [D loss: 0.384071] [G loss: 0.193269] [ema: 0.999770] 
[Epoch 55/92] [Batch 200/547] [D loss: 0.381252] [G loss: 0.177985] [ema: 0.999771] 
[Epoch 55/92] [Batch 300/547] [D loss: 0.416063] [G loss: 0.179015] [ema: 0.999772] 
[Epoch 55/92] [Batch 400/547] [D loss: 0.442063] [G loss: 0.175759] [ema: 0.999773] 
[Epoch 55/92] [Batch 500/547] [D loss: 0.392144] [G loss: 0.178887] [ema: 0.999773] 
[Epoch 56/92] [Batch 0/547] [D loss: 0.395713] [G loss: 0.185943] [ema: 0.999774] 
[Epoch 56/92] [Batch 100/547] [D loss: 0.398949] [G loss: 0.169212] [ema: 0.999774] 
[Epoch 56/92] [Batch 200/547] [D loss: 0.407188] [G loss: 0.173823] [ema: 0.999775] 
[Epoch 56/92] [Batch 300/547] [D loss: 0.420033] [G loss: 0.182783] [ema: 0.999776] 
[Epoch 56/92] [Batch 400/547] [D loss: 0.473577] [G loss: 0.177168] [ema: 0.999777] 
[Epoch 56/92] [Batch 500/547] [D loss: 0.425928] [G loss: 0.181357] [ema: 0.999777] 
[Epoch 57/92] [Batch 0/547] [D loss: 0.400598] [G loss: 0.167358] [ema: 0.999778] 
[Epoch 57/92] [Batch 100/547] [D loss: 0.418998] [G loss: 0.181786] [ema: 0.999778] 
[Epoch 57/92] [Batch 200/547] [D loss: 0.414886] [G loss: 0.160442] [ema: 0.999779] 
[Epoch 57/92] [Batch 300/547] [D loss: 0.393539] [G loss: 0.181469] [ema: 0.999780] 
[Epoch 57/92] [Batch 400/547] [D loss: 0.453834] [G loss: 0.175710] [ema: 0.999781] 
[Epoch 57/92] [Batch 500/547] [D loss: 0.402996] [G loss: 0.166091] [ema: 0.999781] 
[Epoch 58/92] [Batch 0/547] [D loss: 0.395603] [G loss: 0.198858] [ema: 0.999782] 
[Epoch 58/92] [Batch 100/547] [D loss: 0.451627] [G loss: 0.167266] [ema: 0.999782] 
[Epoch 58/92] [Batch 200/547] [D loss: 0.424106] [G loss: 0.186032] [ema: 0.999783] 
[Epoch 58/92] [Batch 300/547] [D loss: 0.374596] [G loss: 0.200971] [ema: 0.999784] 
[Epoch 58/92] [Batch 400/547] [D loss: 0.400079] [G loss: 0.184071] [ema: 0.999784] 
[Epoch 58/92] [Batch 500/547] [D loss: 0.396417] [G loss: 0.180420] [ema: 0.999785] 
[Epoch 59/92] [Batch 0/547] [D loss: 0.436373] [G loss: 0.192665] [ema: 0.999785] 
[Epoch 59/92] [Batch 100/547] [D loss: 0.397483] [G loss: 0.176410] [ema: 0.999786] 
[Epoch 59/92] [Batch 200/547] [D loss: 0.364558] [G loss: 0.179424] [ema: 0.999787] 
[Epoch 59/92] [Batch 300/547] [D loss: 0.399051] [G loss: 0.185700] [ema: 0.999787] 
[Epoch 59/92] [Batch 400/547] [D loss: 0.407749] [G loss: 0.163177] [ema: 0.999788] 
[Epoch 59/92] [Batch 500/547] [D loss: 0.351027] [G loss: 0.189522] [ema: 0.999789] 
[Epoch 60/92] [Batch 0/547] [D loss: 0.392634] [G loss: 0.195081] [ema: 0.999789] 
[Epoch 60/92] [Batch 100/547] [D loss: 0.466991] [G loss: 0.158483] [ema: 0.999789] 
[Epoch 60/92] [Batch 200/547] [D loss: 0.400969] [G loss: 0.163574] [ema: 0.999790] 
[Epoch 60/92] [Batch 300/547] [D loss: 0.426513] [G loss: 0.186497] [ema: 0.999791] 
[Epoch 60/92] [Batch 400/547] [D loss: 0.406739] [G loss: 0.191120] [ema: 0.999791] 
[Epoch 60/92] [Batch 500/547] [D loss: 0.371653] [G loss: 0.193229] [ema: 0.999792] 
[Epoch 61/92] [Batch 0/547] [D loss: 0.395459] [G loss: 0.181225] [ema: 0.999792] 
[Epoch 61/92] [Batch 100/547] [D loss: 0.433403] [G loss: 0.185932] [ema: 0.999793] 
[Epoch 61/92] [Batch 200/547] [D loss: 0.410655] [G loss: 0.161800] [ema: 0.999794] 
[Epoch 61/92] [Batch 300/547] [D loss: 0.377106] [G loss: 0.187599] [ema: 0.999794] 
[Epoch 61/92] [Batch 400/547] [D loss: 0.383990] [G loss: 0.196356] [ema: 0.999795] 
[Epoch 61/92] [Batch 500/547] [D loss: 0.421154] [G loss: 0.176205] [ema: 0.999795] 
[Epoch 62/92] [Batch 0/547] [D loss: 0.421191] [G loss: 0.186698] [ema: 0.999796] 
[Epoch 62/92] [Batch 100/547] [D loss: 0.407687] [G loss: 0.175463] [ema: 0.999796] 
[Epoch 62/92] [Batch 200/547] [D loss: 0.417775] [G loss: 0.181757] [ema: 0.999797] 
[Epoch 62/92] [Batch 300/547] [D loss: 0.377241] [G loss: 0.194794] [ema: 0.999797] 
[Epoch 62/92] [Batch 400/547] [D loss: 0.404081] [G loss: 0.179205] [ema: 0.999798] 
[Epoch 62/92] [Batch 500/547] [D loss: 0.409761] [G loss: 0.175645] [ema: 0.999799] 
[Epoch 63/92] [Batch 0/547] [D loss: 0.396953] [G loss: 0.182255] [ema: 0.999799] 
[Epoch 63/92] [Batch 100/547] [D loss: 0.375453] [G loss: 0.169484] [ema: 0.999799] 
[Epoch 63/92] [Batch 200/547] [D loss: 0.392246] [G loss: 0.186330] [ema: 0.999800] 
[Epoch 63/92] [Batch 300/547] [D loss: 0.411536] [G loss: 0.189684] [ema: 0.999801] 
[Epoch 63/92] [Batch 400/547] [D loss: 0.388232] [G loss: 0.173055] [ema: 0.999801] 
[Epoch 63/92] [Batch 500/547] [D loss: 0.405559] [G loss: 0.179975] [ema: 0.999802] 
[Epoch 64/92] [Batch 0/547] [D loss: 0.381908] [G loss: 0.184134] [ema: 0.999802] 
[Epoch 64/92] [Batch 100/547] [D loss: 0.402290] [G loss: 0.186771] [ema: 0.999803] 
[Epoch 64/92] [Batch 200/547] [D loss: 0.399064] [G loss: 0.166582] [ema: 0.999803] 
[Epoch 64/92] [Batch 300/547] [D loss: 0.468382] [G loss: 0.179881] [ema: 0.999804] 
[Epoch 64/92] [Batch 400/547] [D loss: 0.417491] [G loss: 0.169906] [ema: 0.999804] 
[Epoch 64/92] [Batch 500/547] [D loss: 0.463271] [G loss: 0.169965] [ema: 0.999805] 
[Epoch 65/92] [Batch 0/547] [D loss: 0.388744] [G loss: 0.191039] [ema: 0.999805] 
[Epoch 65/92] [Batch 100/547] [D loss: 0.375143] [G loss: 0.182018] [ema: 0.999806] 
[Epoch 65/92] [Batch 200/547] [D loss: 0.394607] [G loss: 0.173978] [ema: 0.999806] 
[Epoch 65/92] [Batch 300/547] [D loss: 0.388092] [G loss: 0.185181] [ema: 0.999807] 
[Epoch 65/92] [Batch 400/547] [D loss: 0.408998] [G loss: 0.168597] [ema: 0.999807] 
[Epoch 65/92] [Batch 500/547] [D loss: 0.420199] [G loss: 0.183250] [ema: 0.999808] 
[Epoch 66/92] [Batch 0/547] [D loss: 0.431644] [G loss: 0.178557] [ema: 0.999808] 
[Epoch 66/92] [Batch 100/547] [D loss: 0.396902] [G loss: 0.193920] [ema: 0.999809] 
[Epoch 66/92] [Batch 200/547] [D loss: 0.395018] [G loss: 0.185306] [ema: 0.999809] 
[Epoch 66/92] [Batch 300/547] [D loss: 0.396217] [G loss: 0.184204] [ema: 0.999810] 
[Epoch 66/92] [Batch 400/547] [D loss: 0.414167] [G loss: 0.190853] [ema: 0.999810] 
[Epoch 66/92] [Batch 500/547] [D loss: 0.404084] [G loss: 0.193134] [ema: 0.999811] 
[Epoch 67/92] [Batch 0/547] [D loss: 0.413715] [G loss: 0.179208] [ema: 0.999811] 
[Epoch 67/92] [Batch 100/547] [D loss: 0.394958] [G loss: 0.203994] [ema: 0.999811] 
[Epoch 67/92] [Batch 200/547] [D loss: 0.421744] [G loss: 0.172906] [ema: 0.999812] 
[Epoch 67/92] [Batch 300/547] [D loss: 0.380048] [G loss: 0.189397] [ema: 0.999812] 
[Epoch 67/92] [Batch 400/547] [D loss: 0.391348] [G loss: 0.178864] [ema: 0.999813] 
[Epoch 67/92] [Batch 500/547] [D loss: 0.442325] [G loss: 0.179081] [ema: 0.999813] 
[Epoch 68/92] [Batch 0/547] [D loss: 0.389107] [G loss: 0.184915] [ema: 0.999814] 
[Epoch 68/92] [Batch 100/547] [D loss: 0.398867] [G loss: 0.184786] [ema: 0.999814] 
[Epoch 68/92] [Batch 200/547] [D loss: 0.394642] [G loss: 0.178025] [ema: 0.999815] 
[Epoch 68/92] [Batch 300/547] [D loss: 0.383771] [G loss: 0.178200] [ema: 0.999815] 
[Epoch 68/92] [Batch 400/547] [D loss: 0.377344] [G loss: 0.172531] [ema: 0.999816] 
[Epoch 68/92] [Batch 500/547] [D loss: 0.408646] [G loss: 0.184581] [ema: 0.999816] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_60_100/WISDM_DAGHAR_Multiclass_50000_D_60_2024_10_30_01_16_54/Model



[Epoch 69/92] [Batch 0/547] [D loss: 0.367169] [G loss: 0.186237] [ema: 0.999816] 
[Epoch 69/92] [Batch 100/547] [D loss: 0.458760] [G loss: 0.184747] [ema: 0.999817] 
[Epoch 69/92] [Batch 200/547] [D loss: 0.386244] [G loss: 0.196412] [ema: 0.999817] 
[Epoch 69/92] [Batch 300/547] [D loss: 0.425371] [G loss: 0.170264] [ema: 0.999818] 
[Epoch 69/92] [Batch 400/547] [D loss: 0.396948] [G loss: 0.175819] [ema: 0.999818] 
[Epoch 69/92] [Batch 500/547] [D loss: 0.399652] [G loss: 0.177949] [ema: 0.999819] 
[Epoch 70/92] [Batch 0/547] [D loss: 0.362670] [G loss: 0.203978] [ema: 0.999819] 
[Epoch 70/92] [Batch 100/547] [D loss: 0.391357] [G loss: 0.163779] [ema: 0.999819] 
[Epoch 70/92] [Batch 200/547] [D loss: 0.386611] [G loss: 0.174811] [ema: 0.999820] 
[Epoch 70/92] [Batch 300/547] [D loss: 0.429542] [G loss: 0.162001] [ema: 0.999820] 
[Epoch 70/92] [Batch 400/547] [D loss: 0.391809] [G loss: 0.179540] [ema: 0.999821] 
[Epoch 70/92] [Batch 500/547] [D loss: 0.357002] [G loss: 0.203071] [ema: 0.999821] 
[Epoch 71/92] [Batch 0/547] [D loss: 0.394061] [G loss: 0.200130] [ema: 0.999822] 
[Epoch 71/92] [Batch 100/547] [D loss: 0.374266] [G loss: 0.188049] [ema: 0.999822] 
[Epoch 71/92] [Batch 200/547] [D loss: 0.385328] [G loss: 0.175789] [ema: 0.999822] 
[Epoch 71/92] [Batch 300/547] [D loss: 0.392023] [G loss: 0.169414] [ema: 0.999823] 
[Epoch 71/92] [Batch 400/547] [D loss: 0.416800] [G loss: 0.189028] [ema: 0.999823] 
[Epoch 71/92] [Batch 500/547] [D loss: 0.407386] [G loss: 0.181092] [ema: 0.999824] 
[Epoch 72/92] [Batch 0/547] [D loss: 0.390781] [G loss: 0.162617] [ema: 0.999824] 
[Epoch 72/92] [Batch 100/547] [D loss: 0.386765] [G loss: 0.180831] [ema: 0.999824] 
[Epoch 72/92] [Batch 200/547] [D loss: 0.458860] [G loss: 0.166953] [ema: 0.999825] 
[Epoch 72/92] [Batch 300/547] [D loss: 0.370657] [G loss: 0.181987] [ema: 0.999825] 
[Epoch 72/92] [Batch 400/547] [D loss: 0.416711] [G loss: 0.172768] [ema: 0.999826] 
[Epoch 72/92] [Batch 500/547] [D loss: 0.400269] [G loss: 0.168699] [ema: 0.999826] 
[Epoch 73/92] [Batch 0/547] [D loss: 0.446986] [G loss: 0.188171] [ema: 0.999826] 
[Epoch 73/92] [Batch 100/547] [D loss: 0.531747] [G loss: 0.175360] [ema: 0.999827] 
[Epoch 73/92] [Batch 200/547] [D loss: 0.357675] [G loss: 0.191546] [ema: 0.999827] 
[Epoch 73/92] [Batch 300/547] [D loss: 0.394379] [G loss: 0.186311] [ema: 0.999828] 
[Epoch 73/92] [Batch 400/547] [D loss: 0.396900] [G loss: 0.169047] [ema: 0.999828] 
[Epoch 73/92] [Batch 500/547] [D loss: 0.382337] [G loss: 0.193139] [ema: 0.999829] 
[Epoch 74/92] [Batch 0/547] [D loss: 0.388830] [G loss: 0.185350] [ema: 0.999829] 
[Epoch 74/92] [Batch 100/547] [D loss: 0.435930] [G loss: 0.191184] [ema: 0.999829] 
[Epoch 74/92] [Batch 200/547] [D loss: 0.397545] [G loss: 0.176364] [ema: 0.999830] 
[Epoch 74/92] [Batch 300/547] [D loss: 0.396029] [G loss: 0.173718] [ema: 0.999830] 
[Epoch 74/92] [Batch 400/547] [D loss: 0.392600] [G loss: 0.192573] [ema: 0.999830] 
[Epoch 74/92] [Batch 500/547] [D loss: 0.402612] [G loss: 0.183340] [ema: 0.999831] 
[Epoch 75/92] [Batch 0/547] [D loss: 0.451375] [G loss: 0.186058] [ema: 0.999831] 
[Epoch 75/92] [Batch 100/547] [D loss: 0.389120] [G loss: 0.175740] [ema: 0.999831] 
[Epoch 75/92] [Batch 200/547] [D loss: 0.402029] [G loss: 0.174574] [ema: 0.999832] 
[Epoch 75/92] [Batch 300/547] [D loss: 0.443624] [G loss: 0.143094] [ema: 0.999832] 
[Epoch 75/92] [Batch 400/547] [D loss: 0.399900] [G loss: 0.183300] [ema: 0.999833] 
[Epoch 75/92] [Batch 500/547] [D loss: 0.365056] [G loss: 0.180568] [ema: 0.999833] 
[Epoch 76/92] [Batch 0/547] [D loss: 0.409990] [G loss: 0.180415] [ema: 0.999833] 
[Epoch 76/92] [Batch 100/547] [D loss: 0.432975] [G loss: 0.176509] [ema: 0.999834] 
[Epoch 76/92] [Batch 200/547] [D loss: 0.412488] [G loss: 0.154114] [ema: 0.999834] 
[Epoch 76/92] [Batch 300/547] [D loss: 0.407830] [G loss: 0.191333] [ema: 0.999834] 
[Epoch 76/92] [Batch 400/547] [D loss: 0.395031] [G loss: 0.187181] [ema: 0.999835] 
[Epoch 76/92] [Batch 500/547] [D loss: 0.380219] [G loss: 0.177231] [ema: 0.999835] 
[Epoch 77/92] [Batch 0/547] [D loss: 0.442688] [G loss: 0.188321] [ema: 0.999835] 
[Epoch 77/92] [Batch 100/547] [D loss: 0.350170] [G loss: 0.183224] [ema: 0.999836] 
[Epoch 77/92] [Batch 200/547] [D loss: 0.375786] [G loss: 0.184353] [ema: 0.999836] 
[Epoch 77/92] [Batch 300/547] [D loss: 0.351121] [G loss: 0.180429] [ema: 0.999837] 
[Epoch 77/92] [Batch 400/547] [D loss: 0.397963] [G loss: 0.185879] [ema: 0.999837] 
[Epoch 77/92] [Batch 500/547] [D loss: 0.448975] [G loss: 0.179675] [ema: 0.999837] 
[Epoch 78/92] [Batch 0/547] [D loss: 0.414555] [G loss: 0.183416] [ema: 0.999838] 
[Epoch 78/92] [Batch 100/547] [D loss: 0.426963] [G loss: 0.191657] [ema: 0.999838] 
[Epoch 78/92] [Batch 200/547] [D loss: 0.412003] [G loss: 0.165411] [ema: 0.999838] 
[Epoch 78/92] [Batch 300/547] [D loss: 0.412482] [G loss: 0.187032] [ema: 0.999839] 
[Epoch 78/92] [Batch 400/547] [D loss: 0.416991] [G loss: 0.172365] [ema: 0.999839] 
[Epoch 78/92] [Batch 500/547] [D loss: 0.408302] [G loss: 0.171316] [ema: 0.999839] 
[Epoch 79/92] [Batch 0/547] [D loss: 0.388835] [G loss: 0.206131] [ema: 0.999840] 
[Epoch 79/92] [Batch 100/547] [D loss: 0.395417] [G loss: 0.161190] [ema: 0.999840] 
[Epoch 79/92] [Batch 200/547] [D loss: 0.412661] [G loss: 0.152369] [ema: 0.999840] 
[Epoch 79/92] [Batch 300/547] [D loss: 0.447731] [G loss: 0.185067] [ema: 0.999841] 
[Epoch 79/92] [Batch 400/547] [D loss: 0.400587] [G loss: 0.164041] [ema: 0.999841] 
[Epoch 79/92] [Batch 500/547] [D loss: 0.384314] [G loss: 0.205109] [ema: 0.999841] 
[Epoch 80/92] [Batch 0/547] [D loss: 0.413220] [G loss: 0.180924] [ema: 0.999842] 
[Epoch 80/92] [Batch 100/547] [D loss: 0.427252] [G loss: 0.180485] [ema: 0.999842] 
[Epoch 80/92] [Batch 200/547] [D loss: 0.404654] [G loss: 0.170547] [ema: 0.999842] 
[Epoch 80/92] [Batch 300/547] [D loss: 0.382108] [G loss: 0.154405] [ema: 0.999843] 
[Epoch 80/92] [Batch 400/547] [D loss: 0.398719] [G loss: 0.197104] [ema: 0.999843] 
[Epoch 80/92] [Batch 500/547] [D loss: 0.393122] [G loss: 0.175994] [ema: 0.999843] 
[Epoch 81/92] [Batch 0/547] [D loss: 0.383354] [G loss: 0.183195] [ema: 0.999844] 
[Epoch 81/92] [Batch 100/547] [D loss: 0.420891] [G loss: 0.178656] [ema: 0.999844] 
[Epoch 81/92] [Batch 200/547] [D loss: 0.379729] [G loss: 0.184780] [ema: 0.999844] 
[Epoch 81/92] [Batch 300/547] [D loss: 0.411347] [G loss: 0.163900] [ema: 0.999845] 
[Epoch 81/92] [Batch 400/547] [D loss: 0.440733] [G loss: 0.195988] [ema: 0.999845] 
[Epoch 81/92] [Batch 500/547] [D loss: 0.374334] [G loss: 0.197400] [ema: 0.999845] 
[Epoch 82/92] [Batch 0/547] [D loss: 0.408866] [G loss: 0.180842] [ema: 0.999845] 
[Epoch 82/92] [Batch 100/547] [D loss: 0.420571] [G loss: 0.163141] [ema: 0.999846] 
[Epoch 82/92] [Batch 200/547] [D loss: 0.413024] [G loss: 0.184431] [ema: 0.999846] 
[Epoch 82/92] [Batch 300/547] [D loss: 0.375997] [G loss: 0.201299] [ema: 0.999847] 
[Epoch 82/92] [Batch 400/547] [D loss: 0.403310] [G loss: 0.195898] [ema: 0.999847] 
[Epoch 82/92] [Batch 500/547] [D loss: 0.380242] [G loss: 0.200698] [ema: 0.999847] 
[Epoch 83/92] [Batch 0/547] [D loss: 0.401614] [G loss: 0.192973] [ema: 0.999847] 
[Epoch 83/92] [Batch 100/547] [D loss: 0.375244] [G loss: 0.182964] [ema: 0.999848] 
[Epoch 83/92] [Batch 200/547] [D loss: 0.366723] [G loss: 0.198357] [ema: 0.999848] 
[Epoch 83/92] [Batch 300/547] [D loss: 0.431125] [G loss: 0.176316] [ema: 0.999848] 
[Epoch 83/92] [Batch 400/547] [D loss: 0.432972] [G loss: 0.185729] [ema: 0.999849] 
[Epoch 83/92] [Batch 500/547] [D loss: 0.401686] [G loss: 0.188219] [ema: 0.999849] 
[Epoch 84/92] [Batch 0/547] [D loss: 0.449537] [G loss: 0.191575] [ema: 0.999849] 
[Epoch 84/92] [Batch 100/547] [D loss: 0.435877] [G loss: 0.181668] [ema: 0.999849] 
[Epoch 84/92] [Batch 200/547] [D loss: 0.363003] [G loss: 0.184883] [ema: 0.999850] 
[Epoch 84/92] [Batch 300/547] [D loss: 0.362656] [G loss: 0.175542] [ema: 0.999850] 
[Epoch 84/92] [Batch 400/547] [D loss: 0.376137] [G loss: 0.217393] [ema: 0.999850] 
[Epoch 84/92] [Batch 500/547] [D loss: 0.418666] [G loss: 0.168067] [ema: 0.999851] 
[Epoch 85/92] [Batch 0/547] [D loss: 0.368815] [G loss: 0.178907] [ema: 0.999851] 
[Epoch 85/92] [Batch 100/547] [D loss: 0.377801] [G loss: 0.185640] [ema: 0.999851] 
[Epoch 85/92] [Batch 200/547] [D loss: 0.421779] [G loss: 0.195200] [ema: 0.999852] 
[Epoch 85/92] [Batch 300/547] [D loss: 0.400441] [G loss: 0.171761] [ema: 0.999852] 
[Epoch 85/92] [Batch 400/547] [D loss: 0.434554] [G loss: 0.177651] [ema: 0.999852] 
[Epoch 85/92] [Batch 500/547] [D loss: 0.369121] [G loss: 0.184091] [ema: 0.999853] 
[Epoch 86/92] [Batch 0/547] [D loss: 0.397744] [G loss: 0.182979] [ema: 0.999853] 
[Epoch 86/92] [Batch 100/547] [D loss: 0.384339] [G loss: 0.192454] [ema: 0.999853] 
[Epoch 86/92] [Batch 200/547] [D loss: 0.395522] [G loss: 0.172594] [ema: 0.999853] 
[Epoch 86/92] [Batch 300/547] [D loss: 0.388806] [G loss: 0.177895] [ema: 0.999854] 
[Epoch 86/92] [Batch 400/547] [D loss: 0.393933] [G loss: 0.173335] [ema: 0.999854] 
[Epoch 86/92] [Batch 500/547] [D loss: 0.411602] [G loss: 0.171600] [ema: 0.999854] 
[Epoch 87/92] [Batch 0/547] [D loss: 0.409138] [G loss: 0.179698] [ema: 0.999854] 
[Epoch 87/92] [Batch 100/547] [D loss: 0.379847] [G loss: 0.178364] [ema: 0.999855] 
[Epoch 87/92] [Batch 200/547] [D loss: 0.368852] [G loss: 0.187871] [ema: 0.999855] 
[Epoch 87/92] [Batch 300/547] [D loss: 0.410279] [G loss: 0.177509] [ema: 0.999855] 
[Epoch 87/92] [Batch 400/547] [D loss: 0.410268] [G loss: 0.171254] [ema: 0.999856] 
[Epoch 87/92] [Batch 500/547] [D loss: 0.387917] [G loss: 0.164013] [ema: 0.999856] 
[Epoch 88/92] [Batch 0/547] [D loss: 0.397704] [G loss: 0.184376] [ema: 0.999856] 
[Epoch 88/92] [Batch 100/547] [D loss: 0.422311] [G loss: 0.171088] [ema: 0.999856] 
[Epoch 88/92] [Batch 200/547] [D loss: 0.425984] [G loss: 0.188987] [ema: 0.999857] 
[Epoch 88/92] [Batch 300/547] [D loss: 0.442446] [G loss: 0.153985] [ema: 0.999857] 
[Epoch 88/92] [Batch 400/547] [D loss: 0.374929] [G loss: 0.177722] [ema: 0.999857] 
[Epoch 88/92] [Batch 500/547] [D loss: 0.373622] [G loss: 0.188070] [ema: 0.999857] 
[Epoch 89/92] [Batch 0/547] [D loss: 0.492664] [G loss: 0.186798] [ema: 0.999858] 
[Epoch 89/92] [Batch 100/547] [D loss: 0.408086] [G loss: 0.198885] [ema: 0.999858] 
[Epoch 89/92] [Batch 200/547] [D loss: 0.378375] [G loss: 0.177640] [ema: 0.999858] 
[Epoch 89/92] [Batch 300/547] [D loss: 0.420008] [G loss: 0.164677] [ema: 0.999859] 
[Epoch 89/92] [Batch 400/547] [D loss: 0.345899] [G loss: 0.191572] [ema: 0.999859] 
[Epoch 89/92] [Batch 500/547] [D loss: 0.411462] [G loss: 0.178108] [ema: 0.999859] 
[Epoch 90/92] [Batch 0/547] [D loss: 0.400024] [G loss: 0.175511] [ema: 0.999859] 
[Epoch 90/92] [Batch 100/547] [D loss: 0.409062] [G loss: 0.187172] [ema: 0.999859] 
[Epoch 90/92] [Batch 200/547] [D loss: 0.444556] [G loss: 0.179088] [ema: 0.999860] 
[Epoch 90/92] [Batch 300/547] [D loss: 0.370258] [G loss: 0.170079] [ema: 0.999860] 
[Epoch 90/92] [Batch 400/547] [D loss: 0.368918] [G loss: 0.186397] [ema: 0.999860] 
[Epoch 90/92] [Batch 500/547] [D loss: 0.402417] [G loss: 0.168898] [ema: 0.999861] 
[Epoch 91/92] [Batch 0/547] [D loss: 0.390266] [G loss: 0.186242] [ema: 0.999861] 
[Epoch 91/92] [Batch 100/547] [D loss: 0.414405] [G loss: 0.149356] [ema: 0.999861] 
[Epoch 91/92] [Batch 200/547] [D loss: 0.396781] [G loss: 0.171448] [ema: 0.999861] 
[Epoch 91/92] [Batch 300/547] [D loss: 0.311169] [G loss: 0.211299] [ema: 0.999862] 
[Epoch 91/92] [Batch 400/547] [D loss: 0.450462] [G loss: 0.162241] [ema: 0.999862] 
[Epoch 91/92] [Batch 500/547] [D loss: 0.415940] [G loss: 0.171354] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
UCI_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
UCI_DAGHAR_Multiclass
daghar
return single class data and labels, class is UCI_DAGHAR_Multiclass
data shape is (2420, 6, 1, 60)
label shape is (2420,)
152
Epochs between checkpoint: 83



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_2024_10_30_01_50_17/Model



[Epoch 0/329] [Batch 0/152] [D loss: 0.895740] [G loss: 0.539428] [ema: 0.000000] 
[Epoch 0/329] [Batch 100/152] [D loss: 0.371185] [G loss: 0.148677] [ema: 0.933033] 
[Epoch 1/329] [Batch 0/152] [D loss: 0.461839] [G loss: 0.209292] [ema: 0.955422] 
[Epoch 1/329] [Batch 100/152] [D loss: 0.493494] [G loss: 0.167186] [ema: 0.972869] 
[Epoch 2/329] [Batch 0/152] [D loss: 0.411589] [G loss: 0.195115] [ema: 0.977457] 
[Epoch 2/329] [Batch 100/152] [D loss: 0.564827] [G loss: 0.153365] [ema: 0.982989] 
[Epoch 3/329] [Batch 0/152] [D loss: 0.518571] [G loss: 0.160515] [ema: 0.984914] 
[Epoch 3/329] [Batch 100/152] [D loss: 0.401164] [G loss: 0.161622] [ema: 0.987611] 
[Epoch 4/329] [Batch 0/152] [D loss: 0.442828] [G loss: 0.176346] [ema: 0.988664] 
[Epoch 4/329] [Batch 100/152] [D loss: 0.480232] [G loss: 0.142705] [ema: 0.990258] 
[Epoch 5/329] [Batch 0/152] [D loss: 0.465110] [G loss: 0.188054] [ema: 0.990921] 
[Epoch 5/329] [Batch 100/152] [D loss: 0.423039] [G loss: 0.178998] [ema: 0.991973] 
[Epoch 6/329] [Batch 0/152] [D loss: 0.490577] [G loss: 0.177144] [ema: 0.992429] 
[Epoch 6/329] [Batch 100/152] [D loss: 0.377108] [G loss: 0.166677] [ema: 0.993174] 
[Epoch 7/329] [Batch 0/152] [D loss: 0.413533] [G loss: 0.172570] [ema: 0.993507] 
[Epoch 7/329] [Batch 100/152] [D loss: 0.420002] [G loss: 0.190105] [ema: 0.994063] 
[Epoch 8/329] [Batch 0/152] [D loss: 0.433879] [G loss: 0.171383] [ema: 0.994316] 
[Epoch 8/329] [Batch 100/152] [D loss: 0.456963] [G loss: 0.140272] [ema: 0.994747] 
[Epoch 9/329] [Batch 0/152] [D loss: 0.447587] [G loss: 0.187744] [ema: 0.994946] 
[Epoch 9/329] [Batch 100/152] [D loss: 0.405349] [G loss: 0.171954] [ema: 0.995289] 
[Epoch 10/329] [Batch 0/152] [D loss: 0.471478] [G loss: 0.167348] [ema: 0.995450] 
[Epoch 10/329] [Batch 100/152] [D loss: 0.455337] [G loss: 0.157186] [ema: 0.995730] 
[Epoch 11/329] [Batch 0/152] [D loss: 0.436137] [G loss: 0.144529] [ema: 0.995863] 
[Epoch 11/329] [Batch 100/152] [D loss: 0.422945] [G loss: 0.152455] [ema: 0.996096] 
[Epoch 12/329] [Batch 0/152] [D loss: 0.430577] [G loss: 0.189235] [ema: 0.996207] 
[Epoch 12/329] [Batch 100/152] [D loss: 0.496617] [G loss: 0.148142] [ema: 0.996404] 
[Epoch 13/329] [Batch 0/152] [D loss: 0.428569] [G loss: 0.189246] [ema: 0.996498] 
[Epoch 13/329] [Batch 100/152] [D loss: 0.461131] [G loss: 0.155198] [ema: 0.996667] 
[Epoch 14/329] [Batch 0/152] [D loss: 0.456262] [G loss: 0.186978] [ema: 0.996748] 
[Epoch 14/329] [Batch 100/152] [D loss: 0.417084] [G loss: 0.168644] [ema: 0.996894] 
[Epoch 15/329] [Batch 0/152] [D loss: 0.426623] [G loss: 0.147470] [ema: 0.996964] 
[Epoch 15/329] [Batch 100/152] [D loss: 0.459792] [G loss: 0.170816] [ema: 0.997092] 
[Epoch 16/329] [Batch 0/152] [D loss: 0.437303] [G loss: 0.174308] [ema: 0.997154] 
[Epoch 16/329] [Batch 100/152] [D loss: 0.414966] [G loss: 0.188173] [ema: 0.997266] 
[Epoch 17/329] [Batch 0/152] [D loss: 0.541696] [G loss: 0.170960] [ema: 0.997321] 
[Epoch 17/329] [Batch 100/152] [D loss: 0.458465] [G loss: 0.171205] [ema: 0.997421] 
[Epoch 18/329] [Batch 0/152] [D loss: 0.435097] [G loss: 0.177739] [ema: 0.997470] 
[Epoch 18/329] [Batch 100/152] [D loss: 0.373878] [G loss: 0.166620] [ema: 0.997559] 
[Epoch 19/329] [Batch 0/152] [D loss: 0.546860] [G loss: 0.165007] [ema: 0.997603] 
[Epoch 19/329] [Batch 100/152] [D loss: 0.417296] [G loss: 0.151201] [ema: 0.997683] 
[Epoch 20/329] [Batch 0/152] [D loss: 0.409050] [G loss: 0.189086] [ema: 0.997723] 
[Epoch 20/329] [Batch 100/152] [D loss: 0.398899] [G loss: 0.185945] [ema: 0.997795] 
[Epoch 21/329] [Batch 0/152] [D loss: 0.506049] [G loss: 0.213053] [ema: 0.997831] 
[Epoch 21/329] [Batch 100/152] [D loss: 0.415700] [G loss: 0.162040] [ema: 0.997897] 
[Epoch 22/329] [Batch 0/152] [D loss: 0.388778] [G loss: 0.219676] [ema: 0.997929] 
[Epoch 22/329] [Batch 100/152] [D loss: 0.395837] [G loss: 0.194963] [ema: 0.997989] 
[Epoch 23/329] [Batch 0/152] [D loss: 0.322320] [G loss: 0.209839] [ema: 0.998019] 
[Epoch 23/329] [Batch 100/152] [D loss: 0.368832] [G loss: 0.185133] [ema: 0.998074] 
[Epoch 24/329] [Batch 0/152] [D loss: 0.333040] [G loss: 0.194536] [ema: 0.998102] 
[Epoch 24/329] [Batch 100/152] [D loss: 0.297621] [G loss: 0.196445] [ema: 0.998152] 
[Epoch 25/329] [Batch 0/152] [D loss: 0.371263] [G loss: 0.205639] [ema: 0.998178] 
[Epoch 25/329] [Batch 100/152] [D loss: 0.401336] [G loss: 0.190289] [ema: 0.998224] 
[Epoch 26/329] [Batch 0/152] [D loss: 0.370833] [G loss: 0.223588] [ema: 0.998248] 
[Epoch 26/329] [Batch 100/152] [D loss: 0.422504] [G loss: 0.192590] [ema: 0.998291] 
[Epoch 27/329] [Batch 0/152] [D loss: 0.384551] [G loss: 0.224176] [ema: 0.998312] 
[Epoch 27/329] [Batch 100/152] [D loss: 0.454483] [G loss: 0.189462] [ema: 0.998353] 
[Epoch 28/329] [Batch 0/152] [D loss: 0.452847] [G loss: 0.179487] [ema: 0.998373] 
[Epoch 28/329] [Batch 100/152] [D loss: 0.333404] [G loss: 0.204898] [ema: 0.998410] 
[Epoch 29/329] [Batch 0/152] [D loss: 0.409681] [G loss: 0.180355] [ema: 0.998429] 
[Epoch 29/329] [Batch 100/152] [D loss: 0.402341] [G loss: 0.148809] [ema: 0.998464] 
[Epoch 30/329] [Batch 0/152] [D loss: 0.434232] [G loss: 0.163950] [ema: 0.998481] 
[Epoch 30/329] [Batch 100/152] [D loss: 0.364915] [G loss: 0.194825] [ema: 0.998514] 
[Epoch 31/329] [Batch 0/152] [D loss: 0.416103] [G loss: 0.168230] [ema: 0.998530] 
[Epoch 31/329] [Batch 100/152] [D loss: 0.370738] [G loss: 0.224194] [ema: 0.998561] 
[Epoch 32/329] [Batch 0/152] [D loss: 0.320401] [G loss: 0.180790] [ema: 0.998576] 
[Epoch 32/329] [Batch 100/152] [D loss: 0.397409] [G loss: 0.175988] [ema: 0.998605] 
[Epoch 33/329] [Batch 0/152] [D loss: 0.360837] [G loss: 0.193940] [ema: 0.998619] 
[Epoch 33/329] [Batch 100/152] [D loss: 0.447856] [G loss: 0.167787] [ema: 0.998646] 
[Epoch 34/329] [Batch 0/152] [D loss: 0.386716] [G loss: 0.190033] [ema: 0.998660] 
[Epoch 34/329] [Batch 100/152] [D loss: 0.391205] [G loss: 0.159927] [ema: 0.998685] 
[Epoch 35/329] [Batch 0/152] [D loss: 0.436771] [G loss: 0.187654] [ema: 0.998698] 
[Epoch 35/329] [Batch 100/152] [D loss: 0.415947] [G loss: 0.166939] [ema: 0.998722] 
[Epoch 36/329] [Batch 0/152] [D loss: 0.412540] [G loss: 0.225886] [ema: 0.998734] 
[Epoch 36/329] [Batch 100/152] [D loss: 0.421016] [G loss: 0.154858] [ema: 0.998757] 
[Epoch 37/329] [Batch 0/152] [D loss: 0.425384] [G loss: 0.163928] [ema: 0.998768] 
[Epoch 37/329] [Batch 100/152] [D loss: 0.406472] [G loss: 0.173753] [ema: 0.998790] 
[Epoch 38/329] [Batch 0/152] [D loss: 0.452538] [G loss: 0.171018] [ema: 0.998801] 
[Epoch 38/329] [Batch 100/152] [D loss: 0.388126] [G loss: 0.189254] [ema: 0.998821] 
[Epoch 39/329] [Batch 0/152] [D loss: 0.336034] [G loss: 0.195912] [ema: 0.998831] 
[Epoch 39/329] [Batch 100/152] [D loss: 0.500401] [G loss: 0.178410] [ema: 0.998851] 
[Epoch 40/329] [Batch 0/152] [D loss: 0.327937] [G loss: 0.208567] [ema: 0.998861] 
[Epoch 40/329] [Batch 100/152] [D loss: 0.455846] [G loss: 0.186944] [ema: 0.998879] 
[Epoch 41/329] [Batch 0/152] [D loss: 0.350392] [G loss: 0.168292] [ema: 0.998888] 
[Epoch 41/329] [Batch 100/152] [D loss: 0.425037] [G loss: 0.237422] [ema: 0.998906] 
[Epoch 42/329] [Batch 0/152] [D loss: 0.435307] [G loss: 0.179215] [ema: 0.998915] 
[Epoch 42/329] [Batch 100/152] [D loss: 0.372861] [G loss: 0.150959] [ema: 0.998932] 
[Epoch 43/329] [Batch 0/152] [D loss: 0.405158] [G loss: 0.242112] [ema: 0.998940] 
[Epoch 43/329] [Batch 100/152] [D loss: 0.406049] [G loss: 0.185531] [ema: 0.998956] 
[Epoch 44/329] [Batch 0/152] [D loss: 0.306894] [G loss: 0.218551] [ema: 0.998964] 
[Epoch 44/329] [Batch 100/152] [D loss: 0.396316] [G loss: 0.173582] [ema: 0.998979] 
[Epoch 45/329] [Batch 0/152] [D loss: 0.434284] [G loss: 0.177131] [ema: 0.998987] 
[Epoch 45/329] [Batch 100/152] [D loss: 0.414404] [G loss: 0.204125] [ema: 0.999002] 
[Epoch 46/329] [Batch 0/152] [D loss: 0.399105] [G loss: 0.214741] [ema: 0.999009] 
[Epoch 46/329] [Batch 100/152] [D loss: 0.453596] [G loss: 0.177679] [ema: 0.999023] 
[Epoch 47/329] [Batch 0/152] [D loss: 0.351454] [G loss: 0.218003] [ema: 0.999030] 
[Epoch 47/329] [Batch 100/152] [D loss: 0.423671] [G loss: 0.177407] [ema: 0.999044] 
[Epoch 48/329] [Batch 0/152] [D loss: 0.407205] [G loss: 0.182286] [ema: 0.999050] 
[Epoch 48/329] [Batch 100/152] [D loss: 0.396399] [G loss: 0.152008] [ema: 0.999063] 
[Epoch 49/329] [Batch 0/152] [D loss: 0.336814] [G loss: 0.219077] [ema: 0.999070] 
[Epoch 49/329] [Batch 100/152] [D loss: 0.399667] [G loss: 0.190169] [ema: 0.999082] 
[Epoch 50/329] [Batch 0/152] [D loss: 0.337283] [G loss: 0.211726] [ema: 0.999088] 
[Epoch 50/329] [Batch 100/152] [D loss: 0.404106] [G loss: 0.191587] [ema: 0.999100] 
[Epoch 51/329] [Batch 0/152] [D loss: 0.378818] [G loss: 0.217820] [ema: 0.999106] 
[Epoch 51/329] [Batch 100/152] [D loss: 0.387116] [G loss: 0.200041] [ema: 0.999118] 
[Epoch 52/329] [Batch 0/152] [D loss: 0.427493] [G loss: 0.184013] [ema: 0.999123] 
[Epoch 52/329] [Batch 100/152] [D loss: 0.317860] [G loss: 0.171420] [ema: 0.999134] 
[Epoch 53/329] [Batch 0/152] [D loss: 0.392407] [G loss: 0.154033] [ema: 0.999140] 
[Epoch 53/329] [Batch 100/152] [D loss: 0.385545] [G loss: 0.180004] [ema: 0.999150] 
[Epoch 54/329] [Batch 0/152] [D loss: 0.315288] [G loss: 0.173846] [ema: 0.999156] 
[Epoch 54/329] [Batch 100/152] [D loss: 0.383971] [G loss: 0.184045] [ema: 0.999166] 
[Epoch 55/329] [Batch 0/152] [D loss: 0.385483] [G loss: 0.203418] [ema: 0.999171] 
[Epoch 55/329] [Batch 100/152] [D loss: 0.351609] [G loss: 0.199758] [ema: 0.999181] 
[Epoch 56/329] [Batch 0/152] [D loss: 0.405579] [G loss: 0.187897] [ema: 0.999186] 
[Epoch 56/329] [Batch 100/152] [D loss: 0.409300] [G loss: 0.203632] [ema: 0.999195] 
[Epoch 57/329] [Batch 0/152] [D loss: 0.363355] [G loss: 0.206477] [ema: 0.999200] 
[Epoch 57/329] [Batch 100/152] [D loss: 0.349706] [G loss: 0.171615] [ema: 0.999209] 
[Epoch 58/329] [Batch 0/152] [D loss: 0.381998] [G loss: 0.218683] [ema: 0.999214] 
[Epoch 58/329] [Batch 100/152] [D loss: 0.404745] [G loss: 0.201971] [ema: 0.999223] 
[Epoch 59/329] [Batch 0/152] [D loss: 0.389341] [G loss: 0.180322] [ema: 0.999227] 
[Epoch 59/329] [Batch 100/152] [D loss: 0.374079] [G loss: 0.191233] [ema: 0.999236] 
[Epoch 60/329] [Batch 0/152] [D loss: 0.370113] [G loss: 0.200168] [ema: 0.999240] 
[Epoch 60/329] [Batch 100/152] [D loss: 0.388397] [G loss: 0.184185] [ema: 0.999248] 
[Epoch 61/329] [Batch 0/152] [D loss: 0.353894] [G loss: 0.190964] [ema: 0.999253] 
[Epoch 61/329] [Batch 100/152] [D loss: 0.327365] [G loss: 0.182313] [ema: 0.999261] 
[Epoch 62/329] [Batch 0/152] [D loss: 0.316796] [G loss: 0.210867] [ema: 0.999265] 
[Epoch 62/329] [Batch 100/152] [D loss: 0.306422] [G loss: 0.216391] [ema: 0.999272] 
[Epoch 63/329] [Batch 0/152] [D loss: 0.344048] [G loss: 0.170499] [ema: 0.999276] 
[Epoch 63/329] [Batch 100/152] [D loss: 0.333049] [G loss: 0.186778] [ema: 0.999284] 
[Epoch 64/329] [Batch 0/152] [D loss: 0.440579] [G loss: 0.214092] [ema: 0.999288] 
[Epoch 64/329] [Batch 100/152] [D loss: 0.353460] [G loss: 0.191575] [ema: 0.999295] 
[Epoch 65/329] [Batch 0/152] [D loss: 0.388843] [G loss: 0.198416] [ema: 0.999299] 
[Epoch 65/329] [Batch 100/152] [D loss: 0.362875] [G loss: 0.173100] [ema: 0.999306] 
[Epoch 66/329] [Batch 0/152] [D loss: 0.389552] [G loss: 0.207128] [ema: 0.999309] 
[Epoch 66/329] [Batch 100/152] [D loss: 0.405703] [G loss: 0.211996] [ema: 0.999316] 
[Epoch 67/329] [Batch 0/152] [D loss: 0.291836] [G loss: 0.229580] [ema: 0.999320] 
[Epoch 67/329] [Batch 100/152] [D loss: 0.356686] [G loss: 0.219604] [ema: 0.999326] 
[Epoch 68/329] [Batch 0/152] [D loss: 0.304558] [G loss: 0.220383] [ema: 0.999330] 
[Epoch 68/329] [Batch 100/152] [D loss: 0.346126] [G loss: 0.241101] [ema: 0.999336] 
[Epoch 69/329] [Batch 0/152] [D loss: 0.366714] [G loss: 0.211085] [ema: 0.999339] 
[Epoch 69/329] [Batch 100/152] [D loss: 0.339239] [G loss: 0.243627] [ema: 0.999346] 
[Epoch 70/329] [Batch 0/152] [D loss: 0.328054] [G loss: 0.222505] [ema: 0.999349] 
[Epoch 70/329] [Batch 100/152] [D loss: 0.369168] [G loss: 0.191903] [ema: 0.999355] 
[Epoch 71/329] [Batch 0/152] [D loss: 0.416279] [G loss: 0.218117] [ema: 0.999358] 
[Epoch 71/329] [Batch 100/152] [D loss: 0.372998] [G loss: 0.228322] [ema: 0.999364] 
[Epoch 72/329] [Batch 0/152] [D loss: 0.348003] [G loss: 0.192071] [ema: 0.999367] 
[Epoch 72/329] [Batch 100/152] [D loss: 0.352453] [G loss: 0.226363] [ema: 0.999373] 
[Epoch 73/329] [Batch 0/152] [D loss: 0.353350] [G loss: 0.241746] [ema: 0.999376] 
[Epoch 73/329] [Batch 100/152] [D loss: 0.296619] [G loss: 0.198161] [ema: 0.999381] 
[Epoch 74/329] [Batch 0/152] [D loss: 0.336070] [G loss: 0.176727] [ema: 0.999384] 
[Epoch 74/329] [Batch 100/152] [D loss: 0.345005] [G loss: 0.202341] [ema: 0.999389] 
[Epoch 75/329] [Batch 0/152] [D loss: 0.377312] [G loss: 0.222394] [ema: 0.999392] 
[Epoch 75/329] [Batch 100/152] [D loss: 0.365861] [G loss: 0.196125] [ema: 0.999397] 
[Epoch 76/329] [Batch 0/152] [D loss: 0.338009] [G loss: 0.201148] [ema: 0.999400] 
[Epoch 76/329] [Batch 100/152] [D loss: 0.372411] [G loss: 0.217383] [ema: 0.999405] 
[Epoch 77/329] [Batch 0/152] [D loss: 0.325214] [G loss: 0.218063] [ema: 0.999408] 
[Epoch 77/329] [Batch 100/152] [D loss: 0.387992] [G loss: 0.219717] [ema: 0.999413] 
[Epoch 78/329] [Batch 0/152] [D loss: 0.329929] [G loss: 0.193322] [ema: 0.999416] 
[Epoch 78/329] [Batch 100/152] [D loss: 0.307642] [G loss: 0.195798] [ema: 0.999420] 
[Epoch 79/329] [Batch 0/152] [D loss: 0.372253] [G loss: 0.183808] [ema: 0.999423] 
[Epoch 79/329] [Batch 100/152] [D loss: 0.366796] [G loss: 0.209424] [ema: 0.999428] 
[Epoch 80/329] [Batch 0/152] [D loss: 0.353017] [G loss: 0.209630] [ema: 0.999430] 
[Epoch 80/329] [Batch 100/152] [D loss: 0.392828] [G loss: 0.205562] [ema: 0.999435] 
[Epoch 81/329] [Batch 0/152] [D loss: 0.288849] [G loss: 0.222424] [ema: 0.999437] 
[Epoch 81/329] [Batch 100/152] [D loss: 0.357242] [G loss: 0.230406] [ema: 0.999442] 
[Epoch 82/329] [Batch 0/152] [D loss: 0.312905] [G loss: 0.238787] [ema: 0.999444] 
[Epoch 82/329] [Batch 100/152] [D loss: 0.358234] [G loss: 0.212784] [ema: 0.999448] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_2024_10_30_01_50_17/Model



[Epoch 83/329] [Batch 0/152] [D loss: 0.376724] [G loss: 0.215966] [ema: 0.999451] 
[Epoch 83/329] [Batch 100/152] [D loss: 0.344284] [G loss: 0.191127] [ema: 0.999455] 
[Epoch 84/329] [Batch 0/152] [D loss: 0.313921] [G loss: 0.208232] [ema: 0.999457] 
[Epoch 84/329] [Batch 100/152] [D loss: 0.310627] [G loss: 0.236898] [ema: 0.999461] 
[Epoch 85/329] [Batch 0/152] [D loss: 0.363941] [G loss: 0.198118] [ema: 0.999464] 
[Epoch 85/329] [Batch 100/152] [D loss: 0.433085] [G loss: 0.194403] [ema: 0.999468] 
[Epoch 86/329] [Batch 0/152] [D loss: 0.392910] [G loss: 0.224465] [ema: 0.999470] 
[Epoch 86/329] [Batch 100/152] [D loss: 0.444629] [G loss: 0.225202] [ema: 0.999474] 
[Epoch 87/329] [Batch 0/152] [D loss: 0.352994] [G loss: 0.227346] [ema: 0.999476] 
[Epoch 87/329] [Batch 100/152] [D loss: 0.344446] [G loss: 0.205854] [ema: 0.999480] 
[Epoch 88/329] [Batch 0/152] [D loss: 0.316481] [G loss: 0.223182] [ema: 0.999482] 
[Epoch 88/329] [Batch 100/152] [D loss: 0.305977] [G loss: 0.209493] [ema: 0.999486] 
[Epoch 89/329] [Batch 0/152] [D loss: 0.391422] [G loss: 0.223504] [ema: 0.999488] 
[Epoch 89/329] [Batch 100/152] [D loss: 0.326353] [G loss: 0.237174] [ema: 0.999492] 
[Epoch 90/329] [Batch 0/152] [D loss: 0.320788] [G loss: 0.216034] [ema: 0.999493] 
[Epoch 90/329] [Batch 100/152] [D loss: 0.304445] [G loss: 0.198802] [ema: 0.999497] 
[Epoch 91/329] [Batch 0/152] [D loss: 0.363332] [G loss: 0.220319] [ema: 0.999499] 
[Epoch 91/329] [Batch 100/152] [D loss: 0.379566] [G loss: 0.194898] [ema: 0.999503] 
[Epoch 92/329] [Batch 0/152] [D loss: 0.287333] [G loss: 0.222815] [ema: 0.999504] 
[Epoch 92/329] [Batch 100/152] [D loss: 0.343957] [G loss: 0.202299] [ema: 0.999508] 
[Epoch 93/329] [Batch 0/152] [D loss: 0.336732] [G loss: 0.233516] [ema: 0.999510] 
[Epoch 93/329] [Batch 100/152] [D loss: 0.341860] [G loss: 0.218869] [ema: 0.999513] 
[Epoch 94/329] [Batch 0/152] [D loss: 0.306200] [G loss: 0.183529] [ema: 0.999515] 
[Epoch 94/329] [Batch 100/152] [D loss: 0.324419] [G loss: 0.217729] [ema: 0.999518] 
[Epoch 95/329] [Batch 0/152] [D loss: 0.356689] [G loss: 0.216300] [ema: 0.999520] 
[Epoch 95/329] [Batch 100/152] [D loss: 0.347262] [G loss: 0.188904] [ema: 0.999523] 
[Epoch 96/329] [Batch 0/152] [D loss: 0.350061] [G loss: 0.214305] [ema: 0.999525] 
[Epoch 96/329] [Batch 100/152] [D loss: 0.306178] [G loss: 0.270079] [ema: 0.999528] 
[Epoch 97/329] [Batch 0/152] [D loss: 0.284952] [G loss: 0.208082] [ema: 0.999530] 
[Epoch 97/329] [Batch 100/152] [D loss: 0.298120] [G loss: 0.201333] [ema: 0.999533] 
[Epoch 98/329] [Batch 0/152] [D loss: 0.318740] [G loss: 0.241464] [ema: 0.999535] 
[Epoch 98/329] [Batch 100/152] [D loss: 0.336496] [G loss: 0.225835] [ema: 0.999538] 
[Epoch 99/329] [Batch 0/152] [D loss: 0.306792] [G loss: 0.216448] [ema: 0.999539] 
[Epoch 99/329] [Batch 100/152] [D loss: 0.293269] [G loss: 0.216024] [ema: 0.999543] 
[Epoch 100/329] [Batch 0/152] [D loss: 0.383198] [G loss: 0.226146] [ema: 0.999544] 
[Epoch 100/329] [Batch 100/152] [D loss: 0.301968] [G loss: 0.212475] [ema: 0.999547] 
[Epoch 101/329] [Batch 0/152] [D loss: 0.358928] [G loss: 0.228110] [ema: 0.999549] 
[Epoch 101/329] [Batch 100/152] [D loss: 0.334473] [G loss: 0.202349] [ema: 0.999552] 
[Epoch 102/329] [Batch 0/152] [D loss: 0.355474] [G loss: 0.245606] [ema: 0.999553] 
[Epoch 102/329] [Batch 100/152] [D loss: 0.304390] [G loss: 0.206925] [ema: 0.999556] 
[Epoch 103/329] [Batch 0/152] [D loss: 0.333135] [G loss: 0.238240] [ema: 0.999557] 
[Epoch 103/329] [Batch 100/152] [D loss: 0.323421] [G loss: 0.233726] [ema: 0.999560] 
[Epoch 104/329] [Batch 0/152] [D loss: 0.338918] [G loss: 0.188119] [ema: 0.999562] 
[Epoch 104/329] [Batch 100/152] [D loss: 0.338551] [G loss: 0.227689] [ema: 0.999564] 
[Epoch 105/329] [Batch 0/152] [D loss: 0.388166] [G loss: 0.207512] [ema: 0.999566] 
[Epoch 105/329] [Batch 100/152] [D loss: 0.302098] [G loss: 0.217568] [ema: 0.999568] 
[Epoch 106/329] [Batch 0/152] [D loss: 0.317286] [G loss: 0.217013] [ema: 0.999570] 
[Epoch 106/329] [Batch 100/152] [D loss: 0.319557] [G loss: 0.202553] [ema: 0.999573] 
[Epoch 107/329] [Batch 0/152] [D loss: 0.304338] [G loss: 0.216882] [ema: 0.999574] 
[Epoch 107/329] [Batch 100/152] [D loss: 0.320566] [G loss: 0.226462] [ema: 0.999577] 
[Epoch 108/329] [Batch 0/152] [D loss: 0.396298] [G loss: 0.227488] [ema: 0.999578] 
[Epoch 108/329] [Batch 100/152] [D loss: 0.305301] [G loss: 0.242565] [ema: 0.999580] 
[Epoch 109/329] [Batch 0/152] [D loss: 0.309127] [G loss: 0.204573] [ema: 0.999582] 
[Epoch 109/329] [Batch 100/152] [D loss: 0.328368] [G loss: 0.203424] [ema: 0.999584] 
[Epoch 110/329] [Batch 0/152] [D loss: 0.347595] [G loss: 0.217074] [ema: 0.999586] 
[Epoch 110/329] [Batch 100/152] [D loss: 0.397439] [G loss: 0.217486] [ema: 0.999588] 
[Epoch 111/329] [Batch 0/152] [D loss: 0.313974] [G loss: 0.241818] [ema: 0.999589] 
[Epoch 111/329] [Batch 100/152] [D loss: 0.307123] [G loss: 0.207152] [ema: 0.999592] 
[Epoch 112/329] [Batch 0/152] [D loss: 0.312280] [G loss: 0.233910] [ema: 0.999593] 
[Epoch 112/329] [Batch 100/152] [D loss: 0.348422] [G loss: 0.217495] [ema: 0.999595] 
[Epoch 113/329] [Batch 0/152] [D loss: 0.317021] [G loss: 0.219011] [ema: 0.999597] 
[Epoch 113/329] [Batch 100/152] [D loss: 0.273866] [G loss: 0.204822] [ema: 0.999599] 
[Epoch 114/329] [Batch 0/152] [D loss: 0.339294] [G loss: 0.183755] [ema: 0.999600] 
[Epoch 114/329] [Batch 100/152] [D loss: 0.296636] [G loss: 0.228525] [ema: 0.999602] 
[Epoch 115/329] [Batch 0/152] [D loss: 0.316209] [G loss: 0.206989] [ema: 0.999604] 
[Epoch 115/329] [Batch 100/152] [D loss: 0.325201] [G loss: 0.206713] [ema: 0.999606] 
[Epoch 116/329] [Batch 0/152] [D loss: 0.407577] [G loss: 0.206093] [ema: 0.999607] 
[Epoch 116/329] [Batch 100/152] [D loss: 0.330590] [G loss: 0.232367] [ema: 0.999609] 
[Epoch 117/329] [Batch 0/152] [D loss: 0.364649] [G loss: 0.209119] [ema: 0.999610] 
[Epoch 117/329] [Batch 100/152] [D loss: 0.319785] [G loss: 0.192520] [ema: 0.999612] 
[Epoch 118/329] [Batch 0/152] [D loss: 0.295274] [G loss: 0.214609] [ema: 0.999614] 
[Epoch 118/329] [Batch 100/152] [D loss: 0.288092] [G loss: 0.222220] [ema: 0.999616] 
[Epoch 119/329] [Batch 0/152] [D loss: 0.329464] [G loss: 0.220615] [ema: 0.999617] 
[Epoch 119/329] [Batch 100/152] [D loss: 0.346968] [G loss: 0.233274] [ema: 0.999619] 
[Epoch 120/329] [Batch 0/152] [D loss: 0.346569] [G loss: 0.234980] [ema: 0.999620] 
[Epoch 120/329] [Batch 100/152] [D loss: 0.328139] [G loss: 0.203975] [ema: 0.999622] 
[Epoch 121/329] [Batch 0/152] [D loss: 0.340214] [G loss: 0.212083] [ema: 0.999623] 
[Epoch 121/329] [Batch 100/152] [D loss: 0.313080] [G loss: 0.207126] [ema: 0.999625] 
[Epoch 122/329] [Batch 0/152] [D loss: 0.333699] [G loss: 0.212121] [ema: 0.999626] 
[Epoch 122/329] [Batch 100/152] [D loss: 0.273798] [G loss: 0.245073] [ema: 0.999628] 
[Epoch 123/329] [Batch 0/152] [D loss: 0.308955] [G loss: 0.210726] [ema: 0.999629] 
[Epoch 123/329] [Batch 100/152] [D loss: 0.369523] [G loss: 0.188423] [ema: 0.999631] 
[Epoch 124/329] [Batch 0/152] [D loss: 0.269526] [G loss: 0.240799] [ema: 0.999632] 
[Epoch 124/329] [Batch 100/152] [D loss: 0.325452] [G loss: 0.196919] [ema: 0.999634] 
[Epoch 125/329] [Batch 0/152] [D loss: 0.353418] [G loss: 0.204248] [ema: 0.999635] 
[Epoch 125/329] [Batch 100/152] [D loss: 0.310871] [G loss: 0.213467] [ema: 0.999637] 
[Epoch 126/329] [Batch 0/152] [D loss: 0.381103] [G loss: 0.202065] [ema: 0.999638] 
[Epoch 126/329] [Batch 100/152] [D loss: 0.329993] [G loss: 0.202011] [ema: 0.999640] 
[Epoch 127/329] [Batch 0/152] [D loss: 0.268592] [G loss: 0.254784] [ema: 0.999641] 
[Epoch 127/329] [Batch 100/152] [D loss: 0.340156] [G loss: 0.183531] [ema: 0.999643] 
[Epoch 128/329] [Batch 0/152] [D loss: 0.320320] [G loss: 0.216724] [ema: 0.999644] 
[Epoch 128/329] [Batch 100/152] [D loss: 0.345435] [G loss: 0.238839] [ema: 0.999646] 
[Epoch 129/329] [Batch 0/152] [D loss: 0.283454] [G loss: 0.196951] [ema: 0.999647] 
[Epoch 129/329] [Batch 100/152] [D loss: 0.302184] [G loss: 0.235773] [ema: 0.999648] 
[Epoch 130/329] [Batch 0/152] [D loss: 0.313572] [G loss: 0.203508] [ema: 0.999649] 
[Epoch 130/329] [Batch 100/152] [D loss: 0.291589] [G loss: 0.223540] [ema: 0.999651] 
[Epoch 131/329] [Batch 0/152] [D loss: 0.314236] [G loss: 0.229346] [ema: 0.999652] 
[Epoch 131/329] [Batch 100/152] [D loss: 0.335606] [G loss: 0.187785] [ema: 0.999654] 
[Epoch 132/329] [Batch 0/152] [D loss: 0.351050] [G loss: 0.205273] [ema: 0.999655] 
[Epoch 132/329] [Batch 100/152] [D loss: 0.336368] [G loss: 0.211591] [ema: 0.999656] 
[Epoch 133/329] [Batch 0/152] [D loss: 0.345154] [G loss: 0.208825] [ema: 0.999657] 
[Epoch 133/329] [Batch 100/152] [D loss: 0.295793] [G loss: 0.197897] [ema: 0.999659] 
[Epoch 134/329] [Batch 0/152] [D loss: 0.336057] [G loss: 0.217936] [ema: 0.999660] 
[Epoch 134/329] [Batch 100/152] [D loss: 0.313626] [G loss: 0.243506] [ema: 0.999661] 
[Epoch 135/329] [Batch 0/152] [D loss: 0.330382] [G loss: 0.241573] [ema: 0.999662] 
[Epoch 135/329] [Batch 100/152] [D loss: 0.308300] [G loss: 0.236035] [ema: 0.999664] 
[Epoch 136/329] [Batch 0/152] [D loss: 0.323468] [G loss: 0.202987] [ema: 0.999665] 
[Epoch 136/329] [Batch 100/152] [D loss: 0.321959] [G loss: 0.215478] [ema: 0.999666] 
[Epoch 137/329] [Batch 0/152] [D loss: 0.316416] [G loss: 0.229615] [ema: 0.999667] 
[Epoch 137/329] [Batch 100/152] [D loss: 0.297534] [G loss: 0.223475] [ema: 0.999669] 
[Epoch 138/329] [Batch 0/152] [D loss: 0.304776] [G loss: 0.245913] [ema: 0.999670] 
[Epoch 138/329] [Batch 100/152] [D loss: 0.343462] [G loss: 0.220883] [ema: 0.999671] 
[Epoch 139/329] [Batch 0/152] [D loss: 0.313100] [G loss: 0.218869] [ema: 0.999672] 
[Epoch 139/329] [Batch 100/152] [D loss: 0.327433] [G loss: 0.232093] [ema: 0.999674] 
[Epoch 140/329] [Batch 0/152] [D loss: 0.317377] [G loss: 0.242344] [ema: 0.999674] 
[Epoch 140/329] [Batch 100/152] [D loss: 0.307456] [G loss: 0.193852] [ema: 0.999676] 
[Epoch 141/329] [Batch 0/152] [D loss: 0.355241] [G loss: 0.224120] [ema: 0.999677] 
[Epoch 141/329] [Batch 100/152] [D loss: 0.300582] [G loss: 0.217811] [ema: 0.999678] 
[Epoch 142/329] [Batch 0/152] [D loss: 0.339892] [G loss: 0.225837] [ema: 0.999679] 
[Epoch 142/329] [Batch 100/152] [D loss: 0.314404] [G loss: 0.227547] [ema: 0.999680] 
[Epoch 143/329] [Batch 0/152] [D loss: 0.314638] [G loss: 0.236973] [ema: 0.999681] 
[Epoch 143/329] [Batch 100/152] [D loss: 0.327698] [G loss: 0.208405] [ema: 0.999683] 
[Epoch 144/329] [Batch 0/152] [D loss: 0.306811] [G loss: 0.245067] [ema: 0.999683] 
[Epoch 144/329] [Batch 100/152] [D loss: 0.310509] [G loss: 0.223992] [ema: 0.999685] 
[Epoch 145/329] [Batch 0/152] [D loss: 0.322926] [G loss: 0.236291] [ema: 0.999686] 
[Epoch 145/329] [Batch 100/152] [D loss: 0.309868] [G loss: 0.202412] [ema: 0.999687] 
[Epoch 146/329] [Batch 0/152] [D loss: 0.307232] [G loss: 0.235508] [ema: 0.999688] 
[Epoch 146/329] [Batch 100/152] [D loss: 0.299271] [G loss: 0.205639] [ema: 0.999689] 
[Epoch 147/329] [Batch 0/152] [D loss: 0.326760] [G loss: 0.246915] [ema: 0.999690] 
[Epoch 147/329] [Batch 100/152] [D loss: 0.341650] [G loss: 0.233719] [ema: 0.999691] 
[Epoch 148/329] [Batch 0/152] [D loss: 0.327708] [G loss: 0.212359] [ema: 0.999692] 
[Epoch 148/329] [Batch 100/152] [D loss: 0.302936] [G loss: 0.218204] [ema: 0.999693] 
[Epoch 149/329] [Batch 0/152] [D loss: 0.298030] [G loss: 0.245763] [ema: 0.999694] 
[Epoch 149/329] [Batch 100/152] [D loss: 0.315936] [G loss: 0.217992] [ema: 0.999695] 
[Epoch 150/329] [Batch 0/152] [D loss: 0.302337] [G loss: 0.242634] [ema: 0.999696] 
[Epoch 150/329] [Batch 100/152] [D loss: 0.297077] [G loss: 0.210367] [ema: 0.999697] 
[Epoch 151/329] [Batch 0/152] [D loss: 0.276748] [G loss: 0.245034] [ema: 0.999698] 
[Epoch 151/329] [Batch 100/152] [D loss: 0.282147] [G loss: 0.231040] [ema: 0.999699] 
[Epoch 152/329] [Batch 0/152] [D loss: 0.301013] [G loss: 0.209090] [ema: 0.999700] 
[Epoch 152/329] [Batch 100/152] [D loss: 0.284459] [G loss: 0.244758] [ema: 0.999701] 
[Epoch 153/329] [Batch 0/152] [D loss: 0.366002] [G loss: 0.229250] [ema: 0.999702] 
[Epoch 153/329] [Batch 100/152] [D loss: 0.352865] [G loss: 0.210445] [ema: 0.999703] 
[Epoch 154/329] [Batch 0/152] [D loss: 0.298332] [G loss: 0.224765] [ema: 0.999704] 
[Epoch 154/329] [Batch 100/152] [D loss: 0.305449] [G loss: 0.221292] [ema: 0.999705] 
[Epoch 155/329] [Batch 0/152] [D loss: 0.321727] [G loss: 0.202667] [ema: 0.999706] 
[Epoch 155/329] [Batch 100/152] [D loss: 0.321558] [G loss: 0.202963] [ema: 0.999707] 
[Epoch 156/329] [Batch 0/152] [D loss: 0.275361] [G loss: 0.216563] [ema: 0.999708] 
[Epoch 156/329] [Batch 100/152] [D loss: 0.371339] [G loss: 0.206927] [ema: 0.999709] 
[Epoch 157/329] [Batch 0/152] [D loss: 0.304767] [G loss: 0.229734] [ema: 0.999710] 
[Epoch 157/329] [Batch 100/152] [D loss: 0.309961] [G loss: 0.270911] [ema: 0.999711] 
[Epoch 158/329] [Batch 0/152] [D loss: 0.294834] [G loss: 0.233900] [ema: 0.999711] 
[Epoch 158/329] [Batch 100/152] [D loss: 0.349594] [G loss: 0.192575] [ema: 0.999713] 
[Epoch 159/329] [Batch 0/152] [D loss: 0.301391] [G loss: 0.250579] [ema: 0.999713] 
[Epoch 159/329] [Batch 100/152] [D loss: 0.310314] [G loss: 0.234658] [ema: 0.999714] 
[Epoch 160/329] [Batch 0/152] [D loss: 0.310530] [G loss: 0.247969] [ema: 0.999715] 
[Epoch 160/329] [Batch 100/152] [D loss: 0.336323] [G loss: 0.231570] [ema: 0.999716] 
[Epoch 161/329] [Batch 0/152] [D loss: 0.320452] [G loss: 0.247473] [ema: 0.999717] 
[Epoch 161/329] [Batch 100/152] [D loss: 0.309635] [G loss: 0.242683] [ema: 0.999718] 
[Epoch 162/329] [Batch 0/152] [D loss: 0.356704] [G loss: 0.210838] [ema: 0.999719] 
[Epoch 162/329] [Batch 100/152] [D loss: 0.323531] [G loss: 0.232468] [ema: 0.999720] 
[Epoch 163/329] [Batch 0/152] [D loss: 0.321405] [G loss: 0.231173] [ema: 0.999720] 
[Epoch 163/329] [Batch 100/152] [D loss: 0.260830] [G loss: 0.242414] [ema: 0.999721] 
[Epoch 164/329] [Batch 0/152] [D loss: 0.313938] [G loss: 0.232319] [ema: 0.999722] 
[Epoch 164/329] [Batch 100/152] [D loss: 0.293464] [G loss: 0.217891] [ema: 0.999723] 
[Epoch 165/329] [Batch 0/152] [D loss: 0.318287] [G loss: 0.216623] [ema: 0.999724] 
[Epoch 165/329] [Batch 100/152] [D loss: 0.306848] [G loss: 0.239108] [ema: 0.999725] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_2024_10_30_01_50_17/Model



[Epoch 166/329] [Batch 0/152] [D loss: 0.311630] [G loss: 0.232231] [ema: 0.999725] 
[Epoch 166/329] [Batch 100/152] [D loss: 0.338098] [G loss: 0.239924] [ema: 0.999726] 
[Epoch 167/329] [Batch 0/152] [D loss: 0.383791] [G loss: 0.233029] [ema: 0.999727] 
[Epoch 167/329] [Batch 100/152] [D loss: 0.296814] [G loss: 0.240748] [ema: 0.999728] 
[Epoch 168/329] [Batch 0/152] [D loss: 0.302570] [G loss: 0.250280] [ema: 0.999729] 
[Epoch 168/329] [Batch 100/152] [D loss: 0.306452] [G loss: 0.229179] [ema: 0.999730] 
[Epoch 169/329] [Batch 0/152] [D loss: 0.318321] [G loss: 0.242534] [ema: 0.999730] 
[Epoch 169/329] [Batch 100/152] [D loss: 0.250918] [G loss: 0.239104] [ema: 0.999731] 
[Epoch 170/329] [Batch 0/152] [D loss: 0.323277] [G loss: 0.245679] [ema: 0.999732] 
[Epoch 170/329] [Batch 100/152] [D loss: 0.316332] [G loss: 0.225305] [ema: 0.999733] 
[Epoch 171/329] [Batch 0/152] [D loss: 0.297912] [G loss: 0.235882] [ema: 0.999733] 
[Epoch 171/329] [Batch 100/152] [D loss: 0.299502] [G loss: 0.222208] [ema: 0.999734] 
[Epoch 172/329] [Batch 0/152] [D loss: 0.316354] [G loss: 0.262612] [ema: 0.999735] 
[Epoch 172/329] [Batch 100/152] [D loss: 0.300516] [G loss: 0.233232] [ema: 0.999736] 
[Epoch 173/329] [Batch 0/152] [D loss: 0.313858] [G loss: 0.256182] [ema: 0.999736] 
[Epoch 173/329] [Batch 100/152] [D loss: 0.307816] [G loss: 0.214176] [ema: 0.999737] 
[Epoch 174/329] [Batch 0/152] [D loss: 0.281191] [G loss: 0.213242] [ema: 0.999738] 
[Epoch 174/329] [Batch 100/152] [D loss: 0.284796] [G loss: 0.208182] [ema: 0.999739] 
[Epoch 175/329] [Batch 0/152] [D loss: 0.314223] [G loss: 0.217202] [ema: 0.999739] 
[Epoch 175/329] [Batch 100/152] [D loss: 0.259665] [G loss: 0.245511] [ema: 0.999740] 
[Epoch 176/329] [Batch 0/152] [D loss: 0.294331] [G loss: 0.259174] [ema: 0.999741] 
[Epoch 176/329] [Batch 100/152] [D loss: 0.316859] [G loss: 0.211184] [ema: 0.999742] 
[Epoch 177/329] [Batch 0/152] [D loss: 0.312218] [G loss: 0.229823] [ema: 0.999742] 
[Epoch 177/329] [Batch 100/152] [D loss: 0.337834] [G loss: 0.225166] [ema: 0.999743] 
[Epoch 178/329] [Batch 0/152] [D loss: 0.282832] [G loss: 0.236877] [ema: 0.999744] 
[Epoch 178/329] [Batch 100/152] [D loss: 0.345965] [G loss: 0.229459] [ema: 0.999745] 
[Epoch 179/329] [Batch 0/152] [D loss: 0.277627] [G loss: 0.242344] [ema: 0.999745] 
[Epoch 179/329] [Batch 100/152] [D loss: 0.272854] [G loss: 0.215623] [ema: 0.999746] 
[Epoch 180/329] [Batch 0/152] [D loss: 0.312671] [G loss: 0.224553] [ema: 0.999747] 
[Epoch 180/329] [Batch 100/152] [D loss: 0.284033] [G loss: 0.243630] [ema: 0.999748] 
[Epoch 181/329] [Batch 0/152] [D loss: 0.332331] [G loss: 0.229711] [ema: 0.999748] 
[Epoch 181/329] [Batch 100/152] [D loss: 0.264360] [G loss: 0.225006] [ema: 0.999749] 
[Epoch 182/329] [Batch 0/152] [D loss: 0.319760] [G loss: 0.208223] [ema: 0.999749] 
[Epoch 182/329] [Batch 100/152] [D loss: 0.307386] [G loss: 0.243406] [ema: 0.999750] 
[Epoch 183/329] [Batch 0/152] [D loss: 0.295687] [G loss: 0.237512] [ema: 0.999751] 
[Epoch 183/329] [Batch 100/152] [D loss: 0.311999] [G loss: 0.235577] [ema: 0.999752] 
[Epoch 184/329] [Batch 0/152] [D loss: 0.295549] [G loss: 0.228007] [ema: 0.999752] 
[Epoch 184/329] [Batch 100/152] [D loss: 0.286362] [G loss: 0.225064] [ema: 0.999753] 
[Epoch 185/329] [Batch 0/152] [D loss: 0.303598] [G loss: 0.266859] [ema: 0.999754] 
[Epoch 185/329] [Batch 100/152] [D loss: 0.305096] [G loss: 0.222499] [ema: 0.999754] 
[Epoch 186/329] [Batch 0/152] [D loss: 0.296949] [G loss: 0.247936] [ema: 0.999755] 
[Epoch 186/329] [Batch 100/152] [D loss: 0.289365] [G loss: 0.216595] [ema: 0.999756] 
[Epoch 187/329] [Batch 0/152] [D loss: 0.287151] [G loss: 0.220871] [ema: 0.999756] 
[Epoch 187/329] [Batch 100/152] [D loss: 0.278635] [G loss: 0.233839] [ema: 0.999757] 
[Epoch 188/329] [Batch 0/152] [D loss: 0.411469] [G loss: 0.241046] [ema: 0.999757] 
[Epoch 188/329] [Batch 100/152] [D loss: 0.319761] [G loss: 0.243154] [ema: 0.999758] 
[Epoch 189/329] [Batch 0/152] [D loss: 0.285763] [G loss: 0.234220] [ema: 0.999759] 
[Epoch 189/329] [Batch 100/152] [D loss: 0.263627] [G loss: 0.225443] [ema: 0.999760] 
[Epoch 190/329] [Batch 0/152] [D loss: 0.275787] [G loss: 0.224598] [ema: 0.999760] 
[Epoch 190/329] [Batch 100/152] [D loss: 0.315992] [G loss: 0.215993] [ema: 0.999761] 
[Epoch 191/329] [Batch 0/152] [D loss: 0.262785] [G loss: 0.262006] [ema: 0.999761] 
[Epoch 191/329] [Batch 100/152] [D loss: 0.269846] [G loss: 0.244715] [ema: 0.999762] 
[Epoch 192/329] [Batch 0/152] [D loss: 0.304898] [G loss: 0.207634] [ema: 0.999763] 
[Epoch 192/329] [Batch 100/152] [D loss: 0.286854] [G loss: 0.256918] [ema: 0.999763] 
[Epoch 193/329] [Batch 0/152] [D loss: 0.275145] [G loss: 0.223807] [ema: 0.999764] 
[Epoch 193/329] [Batch 100/152] [D loss: 0.299352] [G loss: 0.219060] [ema: 0.999765] 
[Epoch 194/329] [Batch 0/152] [D loss: 0.280485] [G loss: 0.228033] [ema: 0.999765] 
[Epoch 194/329] [Batch 100/152] [D loss: 0.337664] [G loss: 0.215936] [ema: 0.999766] 
[Epoch 195/329] [Batch 0/152] [D loss: 0.287525] [G loss: 0.237715] [ema: 0.999766] 
[Epoch 195/329] [Batch 100/152] [D loss: 0.271188] [G loss: 0.253866] [ema: 0.999767] 
[Epoch 196/329] [Batch 0/152] [D loss: 0.279913] [G loss: 0.244590] [ema: 0.999767] 
[Epoch 196/329] [Batch 100/152] [D loss: 0.307880] [G loss: 0.226956] [ema: 0.999768] 
[Epoch 197/329] [Batch 0/152] [D loss: 0.399033] [G loss: 0.219438] [ema: 0.999769] 
[Epoch 197/329] [Batch 100/152] [D loss: 0.290572] [G loss: 0.248218] [ema: 0.999769] 
[Epoch 198/329] [Batch 0/152] [D loss: 0.285764] [G loss: 0.246055] [ema: 0.999770] 
[Epoch 198/329] [Batch 100/152] [D loss: 0.311263] [G loss: 0.230553] [ema: 0.999770] 
[Epoch 199/329] [Batch 0/152] [D loss: 0.343933] [G loss: 0.251028] [ema: 0.999771] 
[Epoch 199/329] [Batch 100/152] [D loss: 0.277229] [G loss: 0.229821] [ema: 0.999772] 
[Epoch 200/329] [Batch 0/152] [D loss: 0.287470] [G loss: 0.196675] [ema: 0.999772] 
[Epoch 200/329] [Batch 100/152] [D loss: 0.295012] [G loss: 0.231126] [ema: 0.999773] 
[Epoch 201/329] [Batch 0/152] [D loss: 0.330977] [G loss: 0.264187] [ema: 0.999773] 
[Epoch 201/329] [Batch 100/152] [D loss: 0.313419] [G loss: 0.221410] [ema: 0.999774] 
[Epoch 202/329] [Batch 0/152] [D loss: 0.295427] [G loss: 0.254537] [ema: 0.999774] 
[Epoch 202/329] [Batch 100/152] [D loss: 0.351140] [G loss: 0.228238] [ema: 0.999775] 
[Epoch 203/329] [Batch 0/152] [D loss: 0.312518] [G loss: 0.254964] [ema: 0.999775] 
[Epoch 203/329] [Batch 100/152] [D loss: 0.323306] [G loss: 0.239185] [ema: 0.999776] 
[Epoch 204/329] [Batch 0/152] [D loss: 0.299031] [G loss: 0.241562] [ema: 0.999776] 
[Epoch 204/329] [Batch 100/152] [D loss: 0.308435] [G loss: 0.228888] [ema: 0.999777] 
[Epoch 205/329] [Batch 0/152] [D loss: 0.313096] [G loss: 0.220672] [ema: 0.999778] 
[Epoch 205/329] [Batch 100/152] [D loss: 0.283396] [G loss: 0.248203] [ema: 0.999778] 
[Epoch 206/329] [Batch 0/152] [D loss: 0.311865] [G loss: 0.215569] [ema: 0.999779] 
[Epoch 206/329] [Batch 100/152] [D loss: 0.272243] [G loss: 0.228612] [ema: 0.999779] 
[Epoch 207/329] [Batch 0/152] [D loss: 0.300229] [G loss: 0.225053] [ema: 0.999780] 
[Epoch 207/329] [Batch 100/152] [D loss: 0.273117] [G loss: 0.231092] [ema: 0.999780] 
[Epoch 208/329] [Batch 0/152] [D loss: 0.288372] [G loss: 0.245405] [ema: 0.999781] 
[Epoch 208/329] [Batch 100/152] [D loss: 0.302282] [G loss: 0.243747] [ema: 0.999781] 
[Epoch 209/329] [Batch 0/152] [D loss: 0.323012] [G loss: 0.234757] [ema: 0.999782] 
[Epoch 209/329] [Batch 100/152] [D loss: 0.290754] [G loss: 0.247390] [ema: 0.999783] 
[Epoch 210/329] [Batch 0/152] [D loss: 0.287865] [G loss: 0.231747] [ema: 0.999783] 
[Epoch 210/329] [Batch 100/152] [D loss: 0.272819] [G loss: 0.243211] [ema: 0.999784] 
[Epoch 211/329] [Batch 0/152] [D loss: 0.314521] [G loss: 0.223444] [ema: 0.999784] 
[Epoch 211/329] [Batch 100/152] [D loss: 0.327339] [G loss: 0.227623] [ema: 0.999785] 
[Epoch 212/329] [Batch 0/152] [D loss: 0.289818] [G loss: 0.261514] [ema: 0.999785] 
[Epoch 212/329] [Batch 100/152] [D loss: 0.329868] [G loss: 0.222077] [ema: 0.999786] 
[Epoch 213/329] [Batch 0/152] [D loss: 0.396715] [G loss: 0.252500] [ema: 0.999786] 
[Epoch 213/329] [Batch 100/152] [D loss: 0.287627] [G loss: 0.211209] [ema: 0.999787] 
[Epoch 214/329] [Batch 0/152] [D loss: 0.340623] [G loss: 0.236777] [ema: 0.999787] 
[Epoch 214/329] [Batch 100/152] [D loss: 0.329986] [G loss: 0.246198] [ema: 0.999788] 
[Epoch 215/329] [Batch 0/152] [D loss: 0.319272] [G loss: 0.233782] [ema: 0.999788] 
[Epoch 215/329] [Batch 100/152] [D loss: 0.278778] [G loss: 0.216300] [ema: 0.999789] 
[Epoch 216/329] [Batch 0/152] [D loss: 0.338210] [G loss: 0.253798] [ema: 0.999789] 
[Epoch 216/329] [Batch 100/152] [D loss: 0.296343] [G loss: 0.224487] [ema: 0.999790] 
[Epoch 217/329] [Batch 0/152] [D loss: 0.276234] [G loss: 0.265550] [ema: 0.999790] 
[Epoch 217/329] [Batch 100/152] [D loss: 0.330890] [G loss: 0.219774] [ema: 0.999791] 
[Epoch 218/329] [Batch 0/152] [D loss: 0.290520] [G loss: 0.219254] [ema: 0.999791] 
[Epoch 218/329] [Batch 100/152] [D loss: 0.384358] [G loss: 0.199916] [ema: 0.999791] 
[Epoch 219/329] [Batch 0/152] [D loss: 0.296154] [G loss: 0.241525] [ema: 0.999792] 
[Epoch 219/329] [Batch 100/152] [D loss: 0.276131] [G loss: 0.233617] [ema: 0.999792] 
[Epoch 220/329] [Batch 0/152] [D loss: 0.365134] [G loss: 0.233553] [ema: 0.999793] 
[Epoch 220/329] [Batch 100/152] [D loss: 0.297935] [G loss: 0.235945] [ema: 0.999793] 
[Epoch 221/329] [Batch 0/152] [D loss: 0.291698] [G loss: 0.231832] [ema: 0.999794] 
[Epoch 221/329] [Batch 100/152] [D loss: 0.355812] [G loss: 0.211288] [ema: 0.999794] 
[Epoch 222/329] [Batch 0/152] [D loss: 0.249323] [G loss: 0.250867] [ema: 0.999795] 
[Epoch 222/329] [Batch 100/152] [D loss: 0.305949] [G loss: 0.227367] [ema: 0.999795] 
[Epoch 223/329] [Batch 0/152] [D loss: 0.288652] [G loss: 0.202647] [ema: 0.999796] 
[Epoch 223/329] [Batch 100/152] [D loss: 0.304575] [G loss: 0.231455] [ema: 0.999796] 
[Epoch 224/329] [Batch 0/152] [D loss: 0.255443] [G loss: 0.225179] [ema: 0.999796] 
[Epoch 224/329] [Batch 100/152] [D loss: 0.270409] [G loss: 0.217304] [ema: 0.999797] 
[Epoch 225/329] [Batch 0/152] [D loss: 0.288828] [G loss: 0.261104] [ema: 0.999797] 
[Epoch 225/329] [Batch 100/152] [D loss: 0.285274] [G loss: 0.202458] [ema: 0.999798] 
[Epoch 226/329] [Batch 0/152] [D loss: 0.334215] [G loss: 0.236405] [ema: 0.999798] 
[Epoch 226/329] [Batch 100/152] [D loss: 0.329442] [G loss: 0.192982] [ema: 0.999799] 
[Epoch 227/329] [Batch 0/152] [D loss: 0.266126] [G loss: 0.240332] [ema: 0.999799] 
[Epoch 227/329] [Batch 100/152] [D loss: 0.310554] [G loss: 0.230421] [ema: 0.999800] 
[Epoch 228/329] [Batch 0/152] [D loss: 0.290952] [G loss: 0.256544] [ema: 0.999800] 
[Epoch 228/329] [Batch 100/152] [D loss: 0.283170] [G loss: 0.228520] [ema: 0.999801] 
[Epoch 229/329] [Batch 0/152] [D loss: 0.248042] [G loss: 0.247008] [ema: 0.999801] 
[Epoch 229/329] [Batch 100/152] [D loss: 0.307566] [G loss: 0.202997] [ema: 0.999801] 
[Epoch 230/329] [Batch 0/152] [D loss: 0.295066] [G loss: 0.204366] [ema: 0.999802] 
[Epoch 230/329] [Batch 100/152] [D loss: 0.277805] [G loss: 0.220723] [ema: 0.999802] 
[Epoch 231/329] [Batch 0/152] [D loss: 0.287493] [G loss: 0.224777] [ema: 0.999803] 
[Epoch 231/329] [Batch 100/152] [D loss: 0.315974] [G loss: 0.233060] [ema: 0.999803] 
[Epoch 232/329] [Batch 0/152] [D loss: 0.251369] [G loss: 0.257784] [ema: 0.999803] 
[Epoch 232/329] [Batch 100/152] [D loss: 0.310973] [G loss: 0.230869] [ema: 0.999804] 
[Epoch 233/329] [Batch 0/152] [D loss: 0.292764] [G loss: 0.203714] [ema: 0.999804] 
[Epoch 233/329] [Batch 100/152] [D loss: 0.315071] [G loss: 0.234794] [ema: 0.999805] 
[Epoch 234/329] [Batch 0/152] [D loss: 0.279166] [G loss: 0.228239] [ema: 0.999805] 
[Epoch 234/329] [Batch 100/152] [D loss: 0.350987] [G loss: 0.209226] [ema: 0.999806] 
[Epoch 235/329] [Batch 0/152] [D loss: 0.373014] [G loss: 0.221558] [ema: 0.999806] 
[Epoch 235/329] [Batch 100/152] [D loss: 0.315857] [G loss: 0.231041] [ema: 0.999807] 
[Epoch 236/329] [Batch 0/152] [D loss: 0.277929] [G loss: 0.232733] [ema: 0.999807] 
[Epoch 236/329] [Batch 100/152] [D loss: 0.321931] [G loss: 0.210152] [ema: 0.999807] 
[Epoch 237/329] [Batch 0/152] [D loss: 0.308636] [G loss: 0.229589] [ema: 0.999808] 
[Epoch 237/329] [Batch 100/152] [D loss: 0.313038] [G loss: 0.218397] [ema: 0.999808] 
[Epoch 238/329] [Batch 0/152] [D loss: 0.290955] [G loss: 0.213683] [ema: 0.999808] 
[Epoch 238/329] [Batch 100/152] [D loss: 0.275011] [G loss: 0.202263] [ema: 0.999809] 
[Epoch 239/329] [Batch 0/152] [D loss: 0.326856] [G loss: 0.193422] [ema: 0.999809] 
[Epoch 239/329] [Batch 100/152] [D loss: 0.307956] [G loss: 0.255574] [ema: 0.999810] 
[Epoch 240/329] [Batch 0/152] [D loss: 0.285538] [G loss: 0.192901] [ema: 0.999810] 
[Epoch 240/329] [Batch 100/152] [D loss: 0.421092] [G loss: 0.234682] [ema: 0.999811] 
[Epoch 241/329] [Batch 0/152] [D loss: 0.308208] [G loss: 0.230394] [ema: 0.999811] 
[Epoch 241/329] [Batch 100/152] [D loss: 0.334082] [G loss: 0.218548] [ema: 0.999811] 
[Epoch 242/329] [Batch 0/152] [D loss: 0.265707] [G loss: 0.225864] [ema: 0.999812] 
[Epoch 242/329] [Batch 100/152] [D loss: 0.275381] [G loss: 0.251617] [ema: 0.999812] 
[Epoch 243/329] [Batch 0/152] [D loss: 0.293842] [G loss: 0.247512] [ema: 0.999812] 
[Epoch 243/329] [Batch 100/152] [D loss: 0.304247] [G loss: 0.208130] [ema: 0.999813] 
[Epoch 244/329] [Batch 0/152] [D loss: 0.286802] [G loss: 0.233714] [ema: 0.999813] 
[Epoch 244/329] [Batch 100/152] [D loss: 0.318790] [G loss: 0.178015] [ema: 0.999814] 
[Epoch 245/329] [Batch 0/152] [D loss: 0.280059] [G loss: 0.239460] [ema: 0.999814] 
[Epoch 245/329] [Batch 100/152] [D loss: 0.284831] [G loss: 0.223748] [ema: 0.999814] 
[Epoch 246/329] [Batch 0/152] [D loss: 0.289041] [G loss: 0.253859] [ema: 0.999815] 
[Epoch 246/329] [Batch 100/152] [D loss: 0.274022] [G loss: 0.246059] [ema: 0.999815] 
[Epoch 247/329] [Batch 0/152] [D loss: 0.314368] [G loss: 0.209377] [ema: 0.999815] 
[Epoch 247/329] [Batch 100/152] [D loss: 0.329887] [G loss: 0.227898] [ema: 0.999816] 
[Epoch 248/329] [Batch 0/152] [D loss: 0.341442] [G loss: 0.228107] [ema: 0.999816] 
[Epoch 248/329] [Batch 100/152] [D loss: 0.306224] [G loss: 0.236951] [ema: 0.999817] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_60_100/UCI_DAGHAR_Multiclass_50000_D_60_2024_10_30_01_50_17/Model



[Epoch 249/329] [Batch 0/152] [D loss: 0.266033] [G loss: 0.223454] [ema: 0.999817] 
[Epoch 249/329] [Batch 100/152] [D loss: 0.347380] [G loss: 0.238102] [ema: 0.999817] 
[Epoch 250/329] [Batch 0/152] [D loss: 0.361590] [G loss: 0.213716] [ema: 0.999818] 
[Epoch 250/329] [Batch 100/152] [D loss: 0.296440] [G loss: 0.231248] [ema: 0.999818] 
[Epoch 251/329] [Batch 0/152] [D loss: 0.365131] [G loss: 0.199007] [ema: 0.999818] 
[Epoch 251/329] [Batch 100/152] [D loss: 0.327925] [G loss: 0.235470] [ema: 0.999819] 
[Epoch 252/329] [Batch 0/152] [D loss: 0.368788] [G loss: 0.205140] [ema: 0.999819] 
[Epoch 252/329] [Batch 100/152] [D loss: 0.280200] [G loss: 0.207250] [ema: 0.999820] 
[Epoch 253/329] [Batch 0/152] [D loss: 0.322582] [G loss: 0.225729] [ema: 0.999820] 
[Epoch 253/329] [Batch 100/152] [D loss: 0.300309] [G loss: 0.232927] [ema: 0.999820] 
[Epoch 254/329] [Batch 0/152] [D loss: 0.323954] [G loss: 0.236121] [ema: 0.999820] 
[Epoch 254/329] [Batch 100/152] [D loss: 0.278136] [G loss: 0.235073] [ema: 0.999821] 
[Epoch 255/329] [Batch 0/152] [D loss: 0.311097] [G loss: 0.224299] [ema: 0.999821] 
[Epoch 255/329] [Batch 100/152] [D loss: 0.305923] [G loss: 0.233078] [ema: 0.999822] 
[Epoch 256/329] [Batch 0/152] [D loss: 0.341595] [G loss: 0.223724] [ema: 0.999822] 
[Epoch 256/329] [Batch 100/152] [D loss: 0.311317] [G loss: 0.249462] [ema: 0.999822] 
[Epoch 257/329] [Batch 0/152] [D loss: 0.318175] [G loss: 0.233329] [ema: 0.999823] 
[Epoch 257/329] [Batch 100/152] [D loss: 0.321427] [G loss: 0.224437] [ema: 0.999823] 
[Epoch 258/329] [Batch 0/152] [D loss: 0.318053] [G loss: 0.221635] [ema: 0.999823] 
[Epoch 258/329] [Batch 100/152] [D loss: 0.303462] [G loss: 0.240008] [ema: 0.999824] 
[Epoch 259/329] [Batch 0/152] [D loss: 0.276975] [G loss: 0.239571] [ema: 0.999824] 
[Epoch 259/329] [Batch 100/152] [D loss: 0.313844] [G loss: 0.242154] [ema: 0.999824] 
[Epoch 260/329] [Batch 0/152] [D loss: 0.289979] [G loss: 0.234277] [ema: 0.999825] 
[Epoch 260/329] [Batch 100/152] [D loss: 0.302524] [G loss: 0.187703] [ema: 0.999825] 
[Epoch 261/329] [Batch 0/152] [D loss: 0.281507] [G loss: 0.224810] [ema: 0.999825] 
[Epoch 261/329] [Batch 100/152] [D loss: 0.306707] [G loss: 0.243198] [ema: 0.999826] 
[Epoch 262/329] [Batch 0/152] [D loss: 0.290381] [G loss: 0.218516] [ema: 0.999826] 
[Epoch 262/329] [Batch 100/152] [D loss: 0.303495] [G loss: 0.215575] [ema: 0.999826] 
[Epoch 263/329] [Batch 0/152] [D loss: 0.325262] [G loss: 0.207367] [ema: 0.999827] 
[Epoch 263/329] [Batch 100/152] [D loss: 0.302945] [G loss: 0.196927] [ema: 0.999827] 
[Epoch 264/329] [Batch 0/152] [D loss: 0.258116] [G loss: 0.241810] [ema: 0.999827] 
[Epoch 264/329] [Batch 100/152] [D loss: 0.305757] [G loss: 0.235520] [ema: 0.999828] 
[Epoch 265/329] [Batch 0/152] [D loss: 0.270302] [G loss: 0.233263] [ema: 0.999828] 
[Epoch 265/329] [Batch 100/152] [D loss: 0.356460] [G loss: 0.217418] [ema: 0.999828] 
[Epoch 266/329] [Batch 0/152] [D loss: 0.297420] [G loss: 0.229621] [ema: 0.999829] 
[Epoch 266/329] [Batch 100/152] [D loss: 0.328753] [G loss: 0.218758] [ema: 0.999829] 
[Epoch 267/329] [Batch 0/152] [D loss: 0.314070] [G loss: 0.224549] [ema: 0.999829] 
[Epoch 267/329] [Batch 100/152] [D loss: 0.276519] [G loss: 0.230745] [ema: 0.999830] 
[Epoch 268/329] [Batch 0/152] [D loss: 0.311094] [G loss: 0.212117] [ema: 0.999830] 
[Epoch 268/329] [Batch 100/152] [D loss: 0.292282] [G loss: 0.220683] [ema: 0.999830] 
[Epoch 269/329] [Batch 0/152] [D loss: 0.316745] [G loss: 0.225725] [ema: 0.999830] 
[Epoch 269/329] [Batch 100/152] [D loss: 0.277045] [G loss: 0.219139] [ema: 0.999831] 
[Epoch 270/329] [Batch 0/152] [D loss: 0.278649] [G loss: 0.239123] [ema: 0.999831] 
[Epoch 270/329] [Batch 100/152] [D loss: 0.298894] [G loss: 0.239827] [ema: 0.999832] 
[Epoch 271/329] [Batch 0/152] [D loss: 0.315647] [G loss: 0.226823] [ema: 0.999832] 
[Epoch 271/329] [Batch 100/152] [D loss: 0.314292] [G loss: 0.244372] [ema: 0.999832] 
[Epoch 272/329] [Batch 0/152] [D loss: 0.288442] [G loss: 0.240446] [ema: 0.999832] 
[Epoch 272/329] [Batch 100/152] [D loss: 0.290187] [G loss: 0.211045] [ema: 0.999833] 
[Epoch 273/329] [Batch 0/152] [D loss: 0.282742] [G loss: 0.250282] [ema: 0.999833] 
[Epoch 273/329] [Batch 100/152] [D loss: 0.327346] [G loss: 0.225120] [ema: 0.999833] 
[Epoch 274/329] [Batch 0/152] [D loss: 0.293973] [G loss: 0.229364] [ema: 0.999834] 
[Epoch 274/329] [Batch 100/152] [D loss: 0.303992] [G loss: 0.246020] [ema: 0.999834] 
[Epoch 275/329] [Batch 0/152] [D loss: 0.308651] [G loss: 0.267567] [ema: 0.999834] 
[Epoch 275/329] [Batch 100/152] [D loss: 0.311345] [G loss: 0.221253] [ema: 0.999835] 
[Epoch 276/329] [Batch 0/152] [D loss: 0.283725] [G loss: 0.214712] [ema: 0.999835] 
[Epoch 276/329] [Batch 100/152] [D loss: 0.315907] [G loss: 0.241779] [ema: 0.999835] 
[Epoch 277/329] [Batch 0/152] [D loss: 0.357056] [G loss: 0.222699] [ema: 0.999835] 
[Epoch 277/329] [Batch 100/152] [D loss: 0.254029] [G loss: 0.222184] [ema: 0.999836] 
[Epoch 278/329] [Batch 0/152] [D loss: 0.325362] [G loss: 0.203144] [ema: 0.999836] 
[Epoch 278/329] [Batch 100/152] [D loss: 0.304608] [G loss: 0.199652] [ema: 0.999836] 
[Epoch 279/329] [Batch 0/152] [D loss: 0.313614] [G loss: 0.229318] [ema: 0.999837] 
[Epoch 279/329] [Batch 100/152] [D loss: 0.258245] [G loss: 0.256795] [ema: 0.999837] 
[Epoch 280/329] [Batch 0/152] [D loss: 0.345154] [G loss: 0.195160] [ema: 0.999837] 
[Epoch 280/329] [Batch 100/152] [D loss: 0.321041] [G loss: 0.249731] [ema: 0.999838] 
[Epoch 281/329] [Batch 0/152] [D loss: 0.330254] [G loss: 0.207795] [ema: 0.999838] 
[Epoch 281/329] [Batch 100/152] [D loss: 0.304389] [G loss: 0.232975] [ema: 0.999838] 
[Epoch 282/329] [Batch 0/152] [D loss: 0.253991] [G loss: 0.220213] [ema: 0.999838] 
[Epoch 282/329] [Batch 100/152] [D loss: 0.307969] [G loss: 0.204450] [ema: 0.999839] 
[Epoch 283/329] [Batch 0/152] [D loss: 0.260766] [G loss: 0.252990] [ema: 0.999839] 
[Epoch 283/329] [Batch 100/152] [D loss: 0.305308] [G loss: 0.249550] [ema: 0.999839] 
[Epoch 284/329] [Batch 0/152] [D loss: 0.308010] [G loss: 0.230944] [ema: 0.999839] 
[Epoch 284/329] [Batch 100/152] [D loss: 0.289705] [G loss: 0.213715] [ema: 0.999840] 
[Epoch 285/329] [Batch 0/152] [D loss: 0.301019] [G loss: 0.233918] [ema: 0.999840] 
[Epoch 285/329] [Batch 100/152] [D loss: 0.261338] [G loss: 0.216671] [ema: 0.999840] 
[Epoch 286/329] [Batch 0/152] [D loss: 0.353984] [G loss: 0.240647] [ema: 0.999841] 
[Epoch 286/329] [Batch 100/152] [D loss: 0.302089] [G loss: 0.236164] [ema: 0.999841] 
[Epoch 287/329] [Batch 0/152] [D loss: 0.289009] [G loss: 0.231773] [ema: 0.999841] 
[Epoch 287/329] [Batch 100/152] [D loss: 0.287288] [G loss: 0.220621] [ema: 0.999841] 
[Epoch 288/329] [Batch 0/152] [D loss: 0.330495] [G loss: 0.206511] [ema: 0.999842] 
[Epoch 288/329] [Batch 100/152] [D loss: 0.287531] [G loss: 0.222489] [ema: 0.999842] 
[Epoch 289/329] [Batch 0/152] [D loss: 0.325026] [G loss: 0.257963] [ema: 0.999842] 
[Epoch 289/329] [Batch 100/152] [D loss: 0.281833] [G loss: 0.256199] [ema: 0.999843] 
[Epoch 290/329] [Batch 0/152] [D loss: 0.298242] [G loss: 0.230245] [ema: 0.999843] 
[Epoch 290/329] [Batch 100/152] [D loss: 0.277022] [G loss: 0.237299] [ema: 0.999843] 
[Epoch 291/329] [Batch 0/152] [D loss: 0.336968] [G loss: 0.258040] [ema: 0.999843] 
[Epoch 291/329] [Batch 100/152] [D loss: 0.341878] [G loss: 0.229720] [ema: 0.999844] 
[Epoch 292/329] [Batch 0/152] [D loss: 0.281961] [G loss: 0.236170] [ema: 0.999844] 
[Epoch 292/329] [Batch 100/152] [D loss: 0.307085] [G loss: 0.236261] [ema: 0.999844] 
[Epoch 293/329] [Batch 0/152] [D loss: 0.280556] [G loss: 0.240702] [ema: 0.999844] 
[Epoch 293/329] [Batch 100/152] [D loss: 0.293909] [G loss: 0.235946] [ema: 0.999845] 
[Epoch 294/329] [Batch 0/152] [D loss: 0.274457] [G loss: 0.218396] [ema: 0.999845] 
[Epoch 294/329] [Batch 100/152] [D loss: 0.310451] [G loss: 0.236120] [ema: 0.999845] 
[Epoch 295/329] [Batch 0/152] [D loss: 0.298462] [G loss: 0.234511] [ema: 0.999845] 
[Epoch 295/329] [Batch 100/152] [D loss: 0.271166] [G loss: 0.241564] [ema: 0.999846] 
[Epoch 296/329] [Batch 0/152] [D loss: 0.301508] [G loss: 0.224758] [ema: 0.999846] 
[Epoch 296/329] [Batch 100/152] [D loss: 0.283895] [G loss: 0.235449] [ema: 0.999846] 
[Epoch 297/329] [Batch 0/152] [D loss: 0.313050] [G loss: 0.241922] [ema: 0.999846] 
[Epoch 297/329] [Batch 100/152] [D loss: 0.332713] [G loss: 0.234508] [ema: 0.999847] 
[Epoch 298/329] [Batch 0/152] [D loss: 0.311473] [G loss: 0.233196] [ema: 0.999847] 
[Epoch 298/329] [Batch 100/152] [D loss: 0.344151] [G loss: 0.259242] [ema: 0.999847] 
[Epoch 299/329] [Batch 0/152] [D loss: 0.254317] [G loss: 0.247324] [ema: 0.999847] 
[Epoch 299/329] [Batch 100/152] [D loss: 0.312733] [G loss: 0.224818] [ema: 0.999848] 
[Epoch 300/329] [Batch 0/152] [D loss: 0.282843] [G loss: 0.186299] [ema: 0.999848] 
[Epoch 300/329] [Batch 100/152] [D loss: 0.274519] [G loss: 0.231743] [ema: 0.999848] 
[Epoch 301/329] [Batch 0/152] [D loss: 0.278525] [G loss: 0.229697] [ema: 0.999849] 
[Epoch 301/329] [Batch 100/152] [D loss: 0.298405] [G loss: 0.234994] [ema: 0.999849] 
[Epoch 302/329] [Batch 0/152] [D loss: 0.305050] [G loss: 0.257759] [ema: 0.999849] 
[Epoch 302/329] [Batch 100/152] [D loss: 0.268550] [G loss: 0.247987] [ema: 0.999849] 
[Epoch 303/329] [Batch 0/152] [D loss: 0.298546] [G loss: 0.244376] [ema: 0.999850] 
[Epoch 303/329] [Batch 100/152] [D loss: 0.278914] [G loss: 0.220830] [ema: 0.999850] 
[Epoch 304/329] [Batch 0/152] [D loss: 0.324431] [G loss: 0.216672] [ema: 0.999850] 
[Epoch 304/329] [Batch 100/152] [D loss: 0.304479] [G loss: 0.216540] [ema: 0.999850] 
[Epoch 305/329] [Batch 0/152] [D loss: 0.326866] [G loss: 0.212399] [ema: 0.999850] 
[Epoch 305/329] [Batch 100/152] [D loss: 0.326024] [G loss: 0.237608] [ema: 0.999851] 
[Epoch 306/329] [Batch 0/152] [D loss: 0.287084] [G loss: 0.247753] [ema: 0.999851] 
[Epoch 306/329] [Batch 100/152] [D loss: 0.310947] [G loss: 0.240748] [ema: 0.999851] 
[Epoch 307/329] [Batch 0/152] [D loss: 0.355378] [G loss: 0.236893] [ema: 0.999851] 
[Epoch 307/329] [Batch 100/152] [D loss: 0.292101] [G loss: 0.239065] [ema: 0.999852] 
[Epoch 308/329] [Batch 0/152] [D loss: 0.300801] [G loss: 0.250944] [ema: 0.999852] 
[Epoch 308/329] [Batch 100/152] [D loss: 0.322343] [G loss: 0.223443] [ema: 0.999852] 
[Epoch 309/329] [Batch 0/152] [D loss: 0.305981] [G loss: 0.222339] [ema: 0.999852] 
[Epoch 309/329] [Batch 100/152] [D loss: 0.279968] [G loss: 0.241861] [ema: 0.999853] 
[Epoch 310/329] [Batch 0/152] [D loss: 0.322687] [G loss: 0.250312] [ema: 0.999853] 
[Epoch 310/329] [Batch 100/152] [D loss: 0.300466] [G loss: 0.209454] [ema: 0.999853] 
[Epoch 311/329] [Batch 0/152] [D loss: 0.300242] [G loss: 0.213403] [ema: 0.999853] 
[Epoch 311/329] [Batch 100/152] [D loss: 0.286266] [G loss: 0.238502] [ema: 0.999854] 
[Epoch 312/329] [Batch 0/152] [D loss: 0.283413] [G loss: 0.246504] [ema: 0.999854] 
[Epoch 312/329] [Batch 100/152] [D loss: 0.293652] [G loss: 0.242982] [ema: 0.999854] 
[Epoch 313/329] [Batch 0/152] [D loss: 0.309396] [G loss: 0.254483] [ema: 0.999854] 
[Epoch 313/329] [Batch 100/152] [D loss: 0.346002] [G loss: 0.216204] [ema: 0.999855] 
[Epoch 314/329] [Batch 0/152] [D loss: 0.295446] [G loss: 0.231133] [ema: 0.999855] 
[Epoch 314/329] [Batch 100/152] [D loss: 0.298545] [G loss: 0.223729] [ema: 0.999855] 
[Epoch 315/329] [Batch 0/152] [D loss: 0.289707] [G loss: 0.219996] [ema: 0.999855] 
[Epoch 315/329] [Batch 100/152] [D loss: 0.274100] [G loss: 0.243454] [ema: 0.999856] 
[Epoch 316/329] [Batch 0/152] [D loss: 0.307971] [G loss: 0.233616] [ema: 0.999856] 
[Epoch 316/329] [Batch 100/152] [D loss: 0.347888] [G loss: 0.236545] [ema: 0.999856] 
[Epoch 317/329] [Batch 0/152] [D loss: 0.386275] [G loss: 0.233978] [ema: 0.999856] 
[Epoch 317/329] [Batch 100/152] [D loss: 0.269608] [G loss: 0.237587] [ema: 0.999856] 
[Epoch 318/329] [Batch 0/152] [D loss: 0.286165] [G loss: 0.243206] [ema: 0.999857] 
[Epoch 318/329] [Batch 100/152] [D loss: 0.275261] [G loss: 0.248700] [ema: 0.999857] 
[Epoch 319/329] [Batch 0/152] [D loss: 0.253454] [G loss: 0.244693] [ema: 0.999857] 
[Epoch 319/329] [Batch 100/152] [D loss: 0.292694] [G loss: 0.217675] [ema: 0.999857] 
[Epoch 320/329] [Batch 0/152] [D loss: 0.274829] [G loss: 0.243675] [ema: 0.999858] 
[Epoch 320/329] [Batch 100/152] [D loss: 0.333688] [G loss: 0.231037] [ema: 0.999858] 
[Epoch 321/329] [Batch 0/152] [D loss: 0.339539] [G loss: 0.206751] [ema: 0.999858] 
[Epoch 321/329] [Batch 100/152] [D loss: 0.293237] [G loss: 0.227578] [ema: 0.999858] 
[Epoch 322/329] [Batch 0/152] [D loss: 0.268178] [G loss: 0.230239] [ema: 0.999858] 
[Epoch 322/329] [Batch 100/152] [D loss: 0.270260] [G loss: 0.228304] [ema: 0.999859] 
[Epoch 323/329] [Batch 0/152] [D loss: 0.294488] [G loss: 0.248677] [ema: 0.999859] 
[Epoch 323/329] [Batch 100/152] [D loss: 0.305925] [G loss: 0.230840] [ema: 0.999859] 
[Epoch 324/329] [Batch 0/152] [D loss: 0.301707] [G loss: 0.236467] [ema: 0.999859] 
[Epoch 324/329] [Batch 100/152] [D loss: 0.255989] [G loss: 0.259112] [ema: 0.999860] 
[Epoch 325/329] [Batch 0/152] [D loss: 0.288858] [G loss: 0.220617] [ema: 0.999860] 
[Epoch 325/329] [Batch 100/152] [D loss: 0.340957] [G loss: 0.219477] [ema: 0.999860] 
[Epoch 326/329] [Batch 0/152] [D loss: 0.276200] [G loss: 0.245277] [ema: 0.999860] 
[Epoch 326/329] [Batch 100/152] [D loss: 0.248575] [G loss: 0.247471] [ema: 0.999860] 
[Epoch 327/329] [Batch 0/152] [D loss: 0.273744] [G loss: 0.246440] [ema: 0.999861] 
[Epoch 327/329] [Batch 100/152] [D loss: 0.283536] [G loss: 0.231017] [ema: 0.999861] 
[Epoch 328/329] [Batch 0/152] [D loss: 0.291260] [G loss: 0.240893] [ema: 0.999861] 
[Epoch 328/329] [Batch 100/152] [D loss: 0.318926] [G loss: 0.234592] [ema: 0.999861] 

----------------------------------------------------------------------------------------------------

 Starting individual training
RealWorld_waist_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
RealWorld_waist_DAGHAR_Multiclass
daghar
return single class data and labels, class is RealWorld_waist_DAGHAR_Multiclass
data shape is (10332, 6, 1, 60)
label shape is (10332,)
646
Epochs between checkpoint: 20



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_60_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_60_2024_10_30_02_28_08/Model



[Epoch 0/78] [Batch 0/646] [D loss: 0.887981] [G loss: 0.512775] [ema: 0.000000] 
[Epoch 0/78] [Batch 100/646] [D loss: 0.395700] [G loss: 0.172776] [ema: 0.933033] 
[Epoch 0/78] [Batch 200/646] [D loss: 0.554281] [G loss: 0.177035] [ema: 0.965936] 
[Epoch 0/78] [Batch 300/646] [D loss: 0.459454] [G loss: 0.148225] [ema: 0.977160] 
[Epoch 0/78] [Batch 400/646] [D loss: 0.473354] [G loss: 0.139884] [ema: 0.982821] 
[Epoch 0/78] [Batch 500/646] [D loss: 0.519397] [G loss: 0.172063] [ema: 0.986233] 
[Epoch 0/78] [Batch 600/646] [D loss: 0.390055] [G loss: 0.156269] [ema: 0.988514] 
[Epoch 1/78] [Batch 0/646] [D loss: 0.430677] [G loss: 0.176205] [ema: 0.989328] 
[Epoch 1/78] [Batch 100/646] [D loss: 0.449647] [G loss: 0.150237] [ema: 0.990752] 
[Epoch 1/78] [Batch 200/646] [D loss: 0.443900] [G loss: 0.143111] [ema: 0.991840] 
[Epoch 1/78] [Batch 300/646] [D loss: 0.480076] [G loss: 0.174838] [ema: 0.992700] 
[Epoch 1/78] [Batch 400/646] [D loss: 0.479206] [G loss: 0.141961] [ema: 0.993395] 
[Epoch 1/78] [Batch 500/646] [D loss: 0.405607] [G loss: 0.152178] [ema: 0.993970] 
[Epoch 1/78] [Batch 600/646] [D loss: 0.417692] [G loss: 0.179504] [ema: 0.994452] 
[Epoch 2/78] [Batch 0/646] [D loss: 0.423170] [G loss: 0.187786] [ema: 0.994649] 
[Epoch 2/78] [Batch 100/646] [D loss: 0.457139] [G loss: 0.147582] [ema: 0.995033] 
[Epoch 2/78] [Batch 200/646] [D loss: 0.443459] [G loss: 0.154909] [ema: 0.995365] 
[Epoch 2/78] [Batch 300/646] [D loss: 0.461208] [G loss: 0.182454] [ema: 0.995656] 
[Epoch 2/78] [Batch 400/646] [D loss: 0.439885] [G loss: 0.173094] [ema: 0.995912] 
[Epoch 2/78] [Batch 500/646] [D loss: 0.493417] [G loss: 0.161848] [ema: 0.996139] 
[Epoch 2/78] [Batch 600/646] [D loss: 0.416264] [G loss: 0.169479] [ema: 0.996343] 
[Epoch 3/78] [Batch 0/646] [D loss: 0.507854] [G loss: 0.154528] [ema: 0.996430] 
[Epoch 3/78] [Batch 100/646] [D loss: 0.445746] [G loss: 0.188113] [ema: 0.996605] 
[Epoch 3/78] [Batch 200/646] [D loss: 0.511660] [G loss: 0.166486] [ema: 0.996763] 
[Epoch 3/78] [Batch 300/646] [D loss: 0.462782] [G loss: 0.190570] [ema: 0.996908] 
[Epoch 3/78] [Batch 400/646] [D loss: 0.423693] [G loss: 0.113193] [ema: 0.997040] 
[Epoch 3/78] [Batch 500/646] [D loss: 0.426679] [G loss: 0.149550] [ema: 0.997161] 
[Epoch 3/78] [Batch 600/646] [D loss: 0.375004] [G loss: 0.192083] [ema: 0.997273] 
[Epoch 4/78] [Batch 0/646] [D loss: 0.348100] [G loss: 0.190602] [ema: 0.997321] 
[Epoch 4/78] [Batch 100/646] [D loss: 0.340728] [G loss: 0.191075] [ema: 0.997421] 
[Epoch 4/78] [Batch 200/646] [D loss: 0.323533] [G loss: 0.157844] [ema: 0.997513] 
[Epoch 4/78] [Batch 300/646] [D loss: 0.360597] [G loss: 0.158128] [ema: 0.997599] 
[Epoch 4/78] [Batch 400/646] [D loss: 0.407693] [G loss: 0.172611] [ema: 0.997680] 
[Epoch 4/78] [Batch 500/646] [D loss: 0.375922] [G loss: 0.185519] [ema: 0.997755] 
[Epoch 4/78] [Batch 600/646] [D loss: 0.373960] [G loss: 0.165503] [ema: 0.997825] 
[Epoch 5/78] [Batch 0/646] [D loss: 0.384238] [G loss: 0.203130] [ema: 0.997856] 
[Epoch 5/78] [Batch 100/646] [D loss: 0.375283] [G loss: 0.179251] [ema: 0.997921] 
[Epoch 5/78] [Batch 200/646] [D loss: 0.393450] [G loss: 0.188638] [ema: 0.997981] 
[Epoch 5/78] [Batch 300/646] [D loss: 0.429334] [G loss: 0.150184] [ema: 0.998038] 
[Epoch 5/78] [Batch 400/646] [D loss: 0.454214] [G loss: 0.154967] [ema: 0.998092] 
[Epoch 5/78] [Batch 500/646] [D loss: 0.453492] [G loss: 0.144184] [ema: 0.998143] 
[Epoch 5/78] [Batch 600/646] [D loss: 0.381473] [G loss: 0.151154] [ema: 0.998192] 
[Epoch 6/78] [Batch 0/646] [D loss: 0.386941] [G loss: 0.182480] [ema: 0.998213] 
[Epoch 6/78] [Batch 100/646] [D loss: 0.404200] [G loss: 0.184101] [ema: 0.998258] 
[Epoch 6/78] [Batch 200/646] [D loss: 0.383801] [G loss: 0.151444] [ema: 0.998301] 
[Epoch 6/78] [Batch 300/646] [D loss: 0.421945] [G loss: 0.150264] [ema: 0.998342] 
[Epoch 6/78] [Batch 400/646] [D loss: 0.431126] [G loss: 0.151228] [ema: 0.998380] 
[Epoch 6/78] [Batch 500/646] [D loss: 0.452389] [G loss: 0.170666] [ema: 0.998417] 
[Epoch 6/78] [Batch 600/646] [D loss: 0.350628] [G loss: 0.185743] [ema: 0.998453] 
[Epoch 7/78] [Batch 0/646] [D loss: 0.441995] [G loss: 0.155829] [ema: 0.998468] 
[Epoch 7/78] [Batch 100/646] [D loss: 0.387908] [G loss: 0.192573] [ema: 0.998501] 
[Epoch 7/78] [Batch 200/646] [D loss: 0.432760] [G loss: 0.194978] [ema: 0.998533] 
[Epoch 7/78] [Batch 300/646] [D loss: 0.345969] [G loss: 0.176194] [ema: 0.998564] 
[Epoch 7/78] [Batch 400/646] [D loss: 0.387807] [G loss: 0.191390] [ema: 0.998593] 
[Epoch 7/78] [Batch 500/646] [D loss: 0.453919] [G loss: 0.190435] [ema: 0.998621] 
[Epoch 7/78] [Batch 600/646] [D loss: 0.379038] [G loss: 0.186581] [ema: 0.998648] 
[Epoch 8/78] [Batch 0/646] [D loss: 0.310542] [G loss: 0.210750] [ema: 0.998660] 
[Epoch 8/78] [Batch 100/646] [D loss: 0.404348] [G loss: 0.186109] [ema: 0.998685] 
[Epoch 8/78] [Batch 200/646] [D loss: 0.427063] [G loss: 0.185879] [ema: 0.998710] 
[Epoch 8/78] [Batch 300/646] [D loss: 0.367963] [G loss: 0.195674] [ema: 0.998733] 
[Epoch 8/78] [Batch 400/646] [D loss: 0.396193] [G loss: 0.188906] [ema: 0.998756] 
[Epoch 8/78] [Batch 500/646] [D loss: 0.394779] [G loss: 0.183942] [ema: 0.998778] 
[Epoch 8/78] [Batch 600/646] [D loss: 0.349736] [G loss: 0.193788] [ema: 0.998799] 
[Epoch 9/78] [Batch 0/646] [D loss: 0.356864] [G loss: 0.222642] [ema: 0.998809] 
[Epoch 9/78] [Batch 100/646] [D loss: 0.366234] [G loss: 0.185488] [ema: 0.998829] 
[Epoch 9/78] [Batch 200/646] [D loss: 0.380372] [G loss: 0.152253] [ema: 0.998848] 
[Epoch 9/78] [Batch 300/646] [D loss: 0.437121] [G loss: 0.158814] [ema: 0.998867] 
[Epoch 9/78] [Batch 400/646] [D loss: 0.363586] [G loss: 0.180944] [ema: 0.998885] 
[Epoch 9/78] [Batch 500/646] [D loss: 0.402366] [G loss: 0.205747] [ema: 0.998903] 
[Epoch 9/78] [Batch 600/646] [D loss: 0.362326] [G loss: 0.165933] [ema: 0.998920] 
[Epoch 10/78] [Batch 0/646] [D loss: 0.395935] [G loss: 0.224019] [ema: 0.998928] 
[Epoch 10/78] [Batch 100/646] [D loss: 0.340071] [G loss: 0.213418] [ema: 0.998944] 
[Epoch 10/78] [Batch 200/646] [D loss: 0.431734] [G loss: 0.175771] [ema: 0.998960] 
[Epoch 10/78] [Batch 300/646] [D loss: 0.366010] [G loss: 0.166666] [ema: 0.998975] 
[Epoch 10/78] [Batch 400/646] [D loss: 0.396100] [G loss: 0.197148] [ema: 0.998990] 
[Epoch 10/78] [Batch 500/646] [D loss: 0.331627] [G loss: 0.216426] [ema: 0.999005] 
[Epoch 10/78] [Batch 600/646] [D loss: 0.377880] [G loss: 0.187151] [ema: 0.999019] 
[Epoch 11/78] [Batch 0/646] [D loss: 0.338183] [G loss: 0.178698] [ema: 0.999025] 
[Epoch 11/78] [Batch 100/646] [D loss: 0.400612] [G loss: 0.204666] [ema: 0.999039] 
[Epoch 11/78] [Batch 200/646] [D loss: 0.411001] [G loss: 0.211137] [ema: 0.999052] 
[Epoch 11/78] [Batch 300/646] [D loss: 0.336859] [G loss: 0.183718] [ema: 0.999065] 
[Epoch 11/78] [Batch 400/646] [D loss: 0.423572] [G loss: 0.181709] [ema: 0.999077] 
[Epoch 11/78] [Batch 500/646] [D loss: 0.328838] [G loss: 0.199348] [ema: 0.999089] 
[Epoch 11/78] [Batch 600/646] [D loss: 0.351359] [G loss: 0.211705] [ema: 0.999101] 
[Epoch 12/78] [Batch 0/646] [D loss: 0.360669] [G loss: 0.188249] [ema: 0.999106] 
[Epoch 12/78] [Batch 100/646] [D loss: 0.407445] [G loss: 0.181570] [ema: 0.999118] 
[Epoch 12/78] [Batch 200/646] [D loss: 0.377354] [G loss: 0.188812] [ema: 0.999129] 
[Epoch 12/78] [Batch 300/646] [D loss: 0.401020] [G loss: 0.199881] [ema: 0.999140] 
[Epoch 12/78] [Batch 400/646] [D loss: 0.349758] [G loss: 0.181535] [ema: 0.999150] 
[Epoch 12/78] [Batch 500/646] [D loss: 0.380666] [G loss: 0.243386] [ema: 0.999160] 
[Epoch 12/78] [Batch 600/646] [D loss: 0.387647] [G loss: 0.167277] [ema: 0.999170] 
[Epoch 13/78] [Batch 0/646] [D loss: 0.403443] [G loss: 0.193951] [ema: 0.999175] 
[Epoch 13/78] [Batch 100/646] [D loss: 0.352410] [G loss: 0.168130] [ema: 0.999185] 
[Epoch 13/78] [Batch 200/646] [D loss: 0.335159] [G loss: 0.194075] [ema: 0.999194] 
[Epoch 13/78] [Batch 300/646] [D loss: 0.332303] [G loss: 0.192506] [ema: 0.999203] 
[Epoch 13/78] [Batch 400/646] [D loss: 0.335380] [G loss: 0.176940] [ema: 0.999212] 
[Epoch 13/78] [Batch 500/646] [D loss: 0.357057] [G loss: 0.209082] [ema: 0.999221] 
[Epoch 13/78] [Batch 600/646] [D loss: 0.332912] [G loss: 0.206183] [ema: 0.999230] 
[Epoch 14/78] [Batch 0/646] [D loss: 0.324078] [G loss: 0.197778] [ema: 0.999234] 
[Epoch 14/78] [Batch 100/646] [D loss: 0.365977] [G loss: 0.193540] [ema: 0.999242] 
[Epoch 14/78] [Batch 200/646] [D loss: 0.370391] [G loss: 0.250292] [ema: 0.999250] 
[Epoch 14/78] [Batch 300/646] [D loss: 0.336537] [G loss: 0.199889] [ema: 0.999258] 
[Epoch 14/78] [Batch 400/646] [D loss: 0.392468] [G loss: 0.194903] [ema: 0.999266] 
[Epoch 14/78] [Batch 500/646] [D loss: 0.393342] [G loss: 0.194784] [ema: 0.999274] 
[Epoch 14/78] [Batch 600/646] [D loss: 0.358073] [G loss: 0.203664] [ema: 0.999282] 
[Epoch 15/78] [Batch 0/646] [D loss: 0.350600] [G loss: 0.194537] [ema: 0.999285] 
[Epoch 15/78] [Batch 100/646] [D loss: 0.396511] [G loss: 0.184356] [ema: 0.999292] 
[Epoch 15/78] [Batch 200/646] [D loss: 0.384159] [G loss: 0.205789] [ema: 0.999299] 
[Epoch 15/78] [Batch 300/646] [D loss: 0.351432] [G loss: 0.214038] [ema: 0.999306] 
[Epoch 15/78] [Batch 400/646] [D loss: 0.355776] [G loss: 0.225175] [ema: 0.999313] 
[Epoch 15/78] [Batch 500/646] [D loss: 0.358784] [G loss: 0.218307] [ema: 0.999320] 
[Epoch 15/78] [Batch 600/646] [D loss: 0.343742] [G loss: 0.210423] [ema: 0.999327] 
[Epoch 16/78] [Batch 0/646] [D loss: 0.420069] [G loss: 0.184855] [ema: 0.999330] 
[Epoch 16/78] [Batch 100/646] [D loss: 0.379747] [G loss: 0.227805] [ema: 0.999336] 
[Epoch 16/78] [Batch 200/646] [D loss: 0.300947] [G loss: 0.235762] [ema: 0.999342] 
[Epoch 16/78] [Batch 300/646] [D loss: 0.335916] [G loss: 0.204783] [ema: 0.999349] 
[Epoch 16/78] [Batch 400/646] [D loss: 0.315054] [G loss: 0.234595] [ema: 0.999355] 
[Epoch 16/78] [Batch 500/646] [D loss: 0.299056] [G loss: 0.236323] [ema: 0.999361] 
[Epoch 16/78] [Batch 600/646] [D loss: 0.333389] [G loss: 0.188601] [ema: 0.999366] 
[Epoch 17/78] [Batch 0/646] [D loss: 0.374438] [G loss: 0.229758] [ema: 0.999369] 
[Epoch 17/78] [Batch 100/646] [D loss: 0.299802] [G loss: 0.231201] [ema: 0.999375] 
[Epoch 17/78] [Batch 200/646] [D loss: 0.312161] [G loss: 0.207551] [ema: 0.999380] 
[Epoch 17/78] [Batch 300/646] [D loss: 0.339545] [G loss: 0.184912] [ema: 0.999386] 
[Epoch 17/78] [Batch 400/646] [D loss: 0.332323] [G loss: 0.241194] [ema: 0.999391] 
[Epoch 17/78] [Batch 500/646] [D loss: 0.315577] [G loss: 0.201478] [ema: 0.999397] 
[Epoch 17/78] [Batch 600/646] [D loss: 0.323886] [G loss: 0.220938] [ema: 0.999402] 
[Epoch 18/78] [Batch 0/646] [D loss: 0.343207] [G loss: 0.213495] [ema: 0.999404] 
[Epoch 18/78] [Batch 100/646] [D loss: 0.326902] [G loss: 0.195974] [ema: 0.999409] 
[Epoch 18/78] [Batch 200/646] [D loss: 0.392260] [G loss: 0.229323] [ema: 0.999414] 
[Epoch 18/78] [Batch 300/646] [D loss: 0.317773] [G loss: 0.205448] [ema: 0.999419] 
[Epoch 18/78] [Batch 400/646] [D loss: 0.306157] [G loss: 0.227752] [ema: 0.999424] 
[Epoch 18/78] [Batch 500/646] [D loss: 0.319894] [G loss: 0.238005] [ema: 0.999429] 
[Epoch 18/78] [Batch 600/646] [D loss: 0.269838] [G loss: 0.246042] [ema: 0.999433] 
[Epoch 19/78] [Batch 0/646] [D loss: 0.288101] [G loss: 0.206989] [ema: 0.999435] 
[Epoch 19/78] [Batch 100/646] [D loss: 0.288027] [G loss: 0.236131] [ema: 0.999440] 
[Epoch 19/78] [Batch 200/646] [D loss: 0.323894] [G loss: 0.243341] [ema: 0.999444] 
[Epoch 19/78] [Batch 300/646] [D loss: 0.360545] [G loss: 0.208145] [ema: 0.999449] 
[Epoch 19/78] [Batch 400/646] [D loss: 0.350321] [G loss: 0.199864] [ema: 0.999453] 
[Epoch 19/78] [Batch 500/646] [D loss: 0.329071] [G loss: 0.206580] [ema: 0.999458] 
[Epoch 19/78] [Batch 600/646] [D loss: 0.370141] [G loss: 0.215273] [ema: 0.999462] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_60_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_60_2024_10_30_02_28_08/Model



[Epoch 20/78] [Batch 0/646] [D loss: 0.373580] [G loss: 0.177250] [ema: 0.999464] 
[Epoch 20/78] [Batch 100/646] [D loss: 0.343005] [G loss: 0.183012] [ema: 0.999468] 
[Epoch 20/78] [Batch 200/646] [D loss: 0.317393] [G loss: 0.212730] [ema: 0.999472] 
[Epoch 20/78] [Batch 300/646] [D loss: 0.357508] [G loss: 0.220481] [ema: 0.999476] 
[Epoch 20/78] [Batch 400/646] [D loss: 0.309624] [G loss: 0.214048] [ema: 0.999480] 
[Epoch 20/78] [Batch 500/646] [D loss: 0.337316] [G loss: 0.191530] [ema: 0.999484] 
[Epoch 20/78] [Batch 600/646] [D loss: 0.353127] [G loss: 0.192072] [ema: 0.999487] 
[Epoch 21/78] [Batch 0/646] [D loss: 0.328230] [G loss: 0.232324] [ema: 0.999489] 
[Epoch 21/78] [Batch 100/646] [D loss: 0.331293] [G loss: 0.198225] [ema: 0.999493] 
[Epoch 21/78] [Batch 200/646] [D loss: 0.418381] [G loss: 0.230158] [ema: 0.999497] 
[Epoch 21/78] [Batch 300/646] [D loss: 0.298577] [G loss: 0.222694] [ema: 0.999500] 
[Epoch 21/78] [Batch 400/646] [D loss: 0.293053] [G loss: 0.229421] [ema: 0.999504] 
[Epoch 21/78] [Batch 500/646] [D loss: 0.313723] [G loss: 0.216647] [ema: 0.999507] 
[Epoch 21/78] [Batch 600/646] [D loss: 0.317664] [G loss: 0.235463] [ema: 0.999511] 
[Epoch 22/78] [Batch 0/646] [D loss: 0.250752] [G loss: 0.276515] [ema: 0.999512] 
[Epoch 22/78] [Batch 100/646] [D loss: 0.282114] [G loss: 0.237717] [ema: 0.999516] 
[Epoch 22/78] [Batch 200/646] [D loss: 0.316430] [G loss: 0.248407] [ema: 0.999519] 
[Epoch 22/78] [Batch 300/646] [D loss: 0.265218] [G loss: 0.235664] [ema: 0.999522] 
[Epoch 22/78] [Batch 400/646] [D loss: 0.260521] [G loss: 0.242110] [ema: 0.999526] 
[Epoch 22/78] [Batch 500/646] [D loss: 0.344721] [G loss: 0.242202] [ema: 0.999529] 
[Epoch 22/78] [Batch 600/646] [D loss: 0.295414] [G loss: 0.212944] [ema: 0.999532] 
[Epoch 23/78] [Batch 0/646] [D loss: 0.328151] [G loss: 0.183651] [ema: 0.999534] 
[Epoch 23/78] [Batch 100/646] [D loss: 0.345997] [G loss: 0.215873] [ema: 0.999537] 
[Epoch 23/78] [Batch 200/646] [D loss: 0.323197] [G loss: 0.238734] [ema: 0.999540] 
[Epoch 23/78] [Batch 300/646] [D loss: 0.335767] [G loss: 0.222958] [ema: 0.999543] 
[Epoch 23/78] [Batch 400/646] [D loss: 0.318897] [G loss: 0.207430] [ema: 0.999546] 
[Epoch 23/78] [Batch 500/646] [D loss: 0.316589] [G loss: 0.218321] [ema: 0.999549] 
[Epoch 23/78] [Batch 600/646] [D loss: 0.316342] [G loss: 0.207825] [ema: 0.999552] 
[Epoch 24/78] [Batch 0/646] [D loss: 0.299231] [G loss: 0.209918] [ema: 0.999553] 
[Epoch 24/78] [Batch 100/646] [D loss: 0.338295] [G loss: 0.224940] [ema: 0.999556] 
[Epoch 24/78] [Batch 200/646] [D loss: 0.335788] [G loss: 0.207883] [ema: 0.999559] 
[Epoch 24/78] [Batch 300/646] [D loss: 0.286181] [G loss: 0.216138] [ema: 0.999562] 
[Epoch 24/78] [Batch 400/646] [D loss: 0.280254] [G loss: 0.225893] [ema: 0.999564] 
[Epoch 24/78] [Batch 500/646] [D loss: 0.308616] [G loss: 0.238527] [ema: 0.999567] 
[Epoch 24/78] [Batch 600/646] [D loss: 0.283527] [G loss: 0.249567] [ema: 0.999570] 
[Epoch 25/78] [Batch 0/646] [D loss: 0.345305] [G loss: 0.233901] [ema: 0.999571] 
[Epoch 25/78] [Batch 100/646] [D loss: 0.345147] [G loss: 0.229669] [ema: 0.999574] 
[Epoch 25/78] [Batch 200/646] [D loss: 0.314931] [G loss: 0.246115] [ema: 0.999576] 
[Epoch 25/78] [Batch 300/646] [D loss: 0.266735] [G loss: 0.221289] [ema: 0.999579] 
[Epoch 25/78] [Batch 400/646] [D loss: 0.286706] [G loss: 0.233555] [ema: 0.999581] 
[Epoch 25/78] [Batch 500/646] [D loss: 0.273225] [G loss: 0.233013] [ema: 0.999584] 
[Epoch 25/78] [Batch 600/646] [D loss: 0.269406] [G loss: 0.216224] [ema: 0.999586] 
[Epoch 26/78] [Batch 0/646] [D loss: 0.266158] [G loss: 0.222163] [ema: 0.999587] 
[Epoch 26/78] [Batch 100/646] [D loss: 0.278686] [G loss: 0.243324] [ema: 0.999590] 
[Epoch 26/78] [Batch 200/646] [D loss: 0.314382] [G loss: 0.221517] [ema: 0.999592] 
[Epoch 26/78] [Batch 300/646] [D loss: 0.319551] [G loss: 0.247891] [ema: 0.999595] 
[Epoch 26/78] [Batch 400/646] [D loss: 0.303854] [G loss: 0.207142] [ema: 0.999597] 
[Epoch 26/78] [Batch 500/646] [D loss: 0.303317] [G loss: 0.226556] [ema: 0.999599] 
[Epoch 26/78] [Batch 600/646] [D loss: 0.329696] [G loss: 0.232611] [ema: 0.999602] 
[Epoch 27/78] [Batch 0/646] [D loss: 0.306735] [G loss: 0.223629] [ema: 0.999603] 
[Epoch 27/78] [Batch 100/646] [D loss: 0.299652] [G loss: 0.212429] [ema: 0.999605] 
[Epoch 27/78] [Batch 200/646] [D loss: 0.321998] [G loss: 0.229686] [ema: 0.999607] 
[Epoch 27/78] [Batch 300/646] [D loss: 0.308274] [G loss: 0.238714] [ema: 0.999609] 
[Epoch 27/78] [Batch 400/646] [D loss: 0.352345] [G loss: 0.230821] [ema: 0.999612] 
[Epoch 27/78] [Batch 500/646] [D loss: 0.285364] [G loss: 0.218732] [ema: 0.999614] 
[Epoch 27/78] [Batch 600/646] [D loss: 0.300275] [G loss: 0.212674] [ema: 0.999616] 
[Epoch 28/78] [Batch 0/646] [D loss: 0.294403] [G loss: 0.223292] [ema: 0.999617] 
[Epoch 28/78] [Batch 100/646] [D loss: 0.273174] [G loss: 0.232264] [ema: 0.999619] 
[Epoch 28/78] [Batch 200/646] [D loss: 0.367021] [G loss: 0.213781] [ema: 0.999621] 
[Epoch 28/78] [Batch 300/646] [D loss: 0.317051] [G loss: 0.211568] [ema: 0.999623] 
[Epoch 28/78] [Batch 400/646] [D loss: 0.313149] [G loss: 0.221515] [ema: 0.999625] 
[Epoch 28/78] [Batch 500/646] [D loss: 0.348515] [G loss: 0.212012] [ema: 0.999627] 
[Epoch 28/78] [Batch 600/646] [D loss: 0.270032] [G loss: 0.227534] [ema: 0.999629] 
[Epoch 29/78] [Batch 0/646] [D loss: 0.280721] [G loss: 0.230515] [ema: 0.999630] 
[Epoch 29/78] [Batch 100/646] [D loss: 0.346372] [G loss: 0.211296] [ema: 0.999632] 
[Epoch 29/78] [Batch 200/646] [D loss: 0.250394] [G loss: 0.231310] [ema: 0.999634] 
[Epoch 29/78] [Batch 300/646] [D loss: 0.269907] [G loss: 0.228926] [ema: 0.999636] 
[Epoch 29/78] [Batch 400/646] [D loss: 0.287292] [G loss: 0.223617] [ema: 0.999638] 
[Epoch 29/78] [Batch 500/646] [D loss: 0.272339] [G loss: 0.254217] [ema: 0.999640] 
[Epoch 29/78] [Batch 600/646] [D loss: 0.325965] [G loss: 0.228567] [ema: 0.999642] 
[Epoch 30/78] [Batch 0/646] [D loss: 0.300963] [G loss: 0.243595] [ema: 0.999642] 
[Epoch 30/78] [Batch 100/646] [D loss: 0.255627] [G loss: 0.219628] [ema: 0.999644] 
[Epoch 30/78] [Batch 200/646] [D loss: 0.320045] [G loss: 0.241696] [ema: 0.999646] 
[Epoch 30/78] [Batch 300/646] [D loss: 0.293258] [G loss: 0.235023] [ema: 0.999648] 
[Epoch 30/78] [Batch 400/646] [D loss: 0.308831] [G loss: 0.231834] [ema: 0.999650] 
[Epoch 30/78] [Batch 500/646] [D loss: 0.300363] [G loss: 0.205886] [ema: 0.999651] 
[Epoch 30/78] [Batch 600/646] [D loss: 0.281250] [G loss: 0.233576] [ema: 0.999653] 
[Epoch 31/78] [Batch 0/646] [D loss: 0.297406] [G loss: 0.227617] [ema: 0.999654] 
[Epoch 31/78] [Batch 100/646] [D loss: 0.281043] [G loss: 0.221376] [ema: 0.999656] 
[Epoch 31/78] [Batch 200/646] [D loss: 0.348571] [G loss: 0.215536] [ema: 0.999657] 
[Epoch 31/78] [Batch 300/646] [D loss: 0.263181] [G loss: 0.237862] [ema: 0.999659] 
[Epoch 31/78] [Batch 400/646] [D loss: 0.289643] [G loss: 0.226635] [ema: 0.999661] 
[Epoch 31/78] [Batch 500/646] [D loss: 0.292387] [G loss: 0.215571] [ema: 0.999662] 
[Epoch 31/78] [Batch 600/646] [D loss: 0.311748] [G loss: 0.239531] [ema: 0.999664] 
[Epoch 32/78] [Batch 0/646] [D loss: 0.312887] [G loss: 0.235763] [ema: 0.999665] 
[Epoch 32/78] [Batch 100/646] [D loss: 0.288865] [G loss: 0.224480] [ema: 0.999666] 
[Epoch 32/78] [Batch 200/646] [D loss: 0.340139] [G loss: 0.228129] [ema: 0.999668] 
[Epoch 32/78] [Batch 300/646] [D loss: 0.270411] [G loss: 0.224887] [ema: 0.999670] 
[Epoch 32/78] [Batch 400/646] [D loss: 0.325714] [G loss: 0.225884] [ema: 0.999671] 
[Epoch 32/78] [Batch 500/646] [D loss: 0.375711] [G loss: 0.220987] [ema: 0.999673] 
[Epoch 32/78] [Batch 600/646] [D loss: 0.342065] [G loss: 0.252972] [ema: 0.999674] 
[Epoch 33/78] [Batch 0/646] [D loss: 0.307937] [G loss: 0.223080] [ema: 0.999675] 
[Epoch 33/78] [Batch 100/646] [D loss: 0.330452] [G loss: 0.193732] [ema: 0.999676] 
[Epoch 33/78] [Batch 200/646] [D loss: 0.292378] [G loss: 0.231407] [ema: 0.999678] 
[Epoch 33/78] [Batch 300/646] [D loss: 0.308253] [G loss: 0.244560] [ema: 0.999679] 
[Epoch 33/78] [Batch 400/646] [D loss: 0.319214] [G loss: 0.233324] [ema: 0.999681] 
[Epoch 33/78] [Batch 500/646] [D loss: 0.288735] [G loss: 0.213876] [ema: 0.999682] 
[Epoch 33/78] [Batch 600/646] [D loss: 0.273158] [G loss: 0.235895] [ema: 0.999684] 
[Epoch 34/78] [Batch 0/646] [D loss: 0.308400] [G loss: 0.224056] [ema: 0.999684] 
[Epoch 34/78] [Batch 100/646] [D loss: 0.344421] [G loss: 0.222641] [ema: 0.999686] 
[Epoch 34/78] [Batch 200/646] [D loss: 0.312816] [G loss: 0.221000] [ema: 0.999687] 
[Epoch 34/78] [Batch 300/646] [D loss: 0.297779] [G loss: 0.206281] [ema: 0.999689] 
[Epoch 34/78] [Batch 400/646] [D loss: 0.352522] [G loss: 0.225006] [ema: 0.999690] 
[Epoch 34/78] [Batch 500/646] [D loss: 0.297267] [G loss: 0.200961] [ema: 0.999691] 
[Epoch 34/78] [Batch 600/646] [D loss: 0.351544] [G loss: 0.185096] [ema: 0.999693] 
[Epoch 35/78] [Batch 0/646] [D loss: 0.335018] [G loss: 0.225499] [ema: 0.999693] 
[Epoch 35/78] [Batch 100/646] [D loss: 0.317942] [G loss: 0.231050] [ema: 0.999695] 
[Epoch 35/78] [Batch 200/646] [D loss: 0.306335] [G loss: 0.221220] [ema: 0.999696] 
[Epoch 35/78] [Batch 300/646] [D loss: 0.313022] [G loss: 0.215269] [ema: 0.999697] 
[Epoch 35/78] [Batch 400/646] [D loss: 0.301083] [G loss: 0.201555] [ema: 0.999699] 
[Epoch 35/78] [Batch 500/646] [D loss: 0.363118] [G loss: 0.226860] [ema: 0.999700] 
[Epoch 35/78] [Batch 600/646] [D loss: 0.344675] [G loss: 0.197754] [ema: 0.999701] 
[Epoch 36/78] [Batch 0/646] [D loss: 0.323446] [G loss: 0.199839] [ema: 0.999702] 
[Epoch 36/78] [Batch 100/646] [D loss: 0.319079] [G loss: 0.205948] [ema: 0.999703] 
[Epoch 36/78] [Batch 200/646] [D loss: 0.293603] [G loss: 0.239556] [ema: 0.999705] 
[Epoch 36/78] [Batch 300/646] [D loss: 0.295168] [G loss: 0.244682] [ema: 0.999706] 
[Epoch 36/78] [Batch 400/646] [D loss: 0.331628] [G loss: 0.210569] [ema: 0.999707] 
[Epoch 36/78] [Batch 500/646] [D loss: 0.339349] [G loss: 0.226246] [ema: 0.999708] 
[Epoch 36/78] [Batch 600/646] [D loss: 0.315425] [G loss: 0.186292] [ema: 0.999709] 
[Epoch 37/78] [Batch 0/646] [D loss: 0.296371] [G loss: 0.231440] [ema: 0.999710] 
[Epoch 37/78] [Batch 100/646] [D loss: 0.373900] [G loss: 0.228780] [ema: 0.999711] 
[Epoch 37/78] [Batch 200/646] [D loss: 0.321021] [G loss: 0.198917] [ema: 0.999712] 
[Epoch 37/78] [Batch 300/646] [D loss: 0.312667] [G loss: 0.193288] [ema: 0.999714] 
[Epoch 37/78] [Batch 400/646] [D loss: 0.304441] [G loss: 0.213935] [ema: 0.999715] 
[Epoch 37/78] [Batch 500/646] [D loss: 0.288653] [G loss: 0.222110] [ema: 0.999716] 
[Epoch 37/78] [Batch 600/646] [D loss: 0.273457] [G loss: 0.224450] [ema: 0.999717] 
[Epoch 38/78] [Batch 0/646] [D loss: 0.299255] [G loss: 0.229413] [ema: 0.999718] 
[Epoch 38/78] [Batch 100/646] [D loss: 0.325314] [G loss: 0.241897] [ema: 0.999719] 
[Epoch 38/78] [Batch 200/646] [D loss: 0.373856] [G loss: 0.212968] [ema: 0.999720] 
[Epoch 38/78] [Batch 300/646] [D loss: 0.317971] [G loss: 0.204710] [ema: 0.999721] 
[Epoch 38/78] [Batch 400/646] [D loss: 0.332951] [G loss: 0.218206] [ema: 0.999722] 
[Epoch 38/78] [Batch 500/646] [D loss: 0.310661] [G loss: 0.202641] [ema: 0.999723] 
[Epoch 38/78] [Batch 600/646] [D loss: 0.303762] [G loss: 0.223350] [ema: 0.999724] 
[Epoch 39/78] [Batch 0/646] [D loss: 0.325511] [G loss: 0.228420] [ema: 0.999725] 
[Epoch 39/78] [Batch 100/646] [D loss: 0.389414] [G loss: 0.209628] [ema: 0.999726] 
[Epoch 39/78] [Batch 200/646] [D loss: 0.307520] [G loss: 0.189160] [ema: 0.999727] 
[Epoch 39/78] [Batch 300/646] [D loss: 0.307315] [G loss: 0.205576] [ema: 0.999728] 
[Epoch 39/78] [Batch 400/646] [D loss: 0.353571] [G loss: 0.221966] [ema: 0.999729] 
[Epoch 39/78] [Batch 500/646] [D loss: 0.286252] [G loss: 0.227894] [ema: 0.999730] 
[Epoch 39/78] [Batch 600/646] [D loss: 0.335115] [G loss: 0.210860] [ema: 0.999731] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_60_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_60_2024_10_30_02_28_08/Model



[Epoch 40/78] [Batch 0/646] [D loss: 0.360397] [G loss: 0.209677] [ema: 0.999732] 
[Epoch 40/78] [Batch 100/646] [D loss: 0.392304] [G loss: 0.219512] [ema: 0.999733] 
[Epoch 40/78] [Batch 200/646] [D loss: 0.333047] [G loss: 0.228283] [ema: 0.999734] 
[Epoch 40/78] [Batch 300/646] [D loss: 0.276657] [G loss: 0.249916] [ema: 0.999735] 
[Epoch 40/78] [Batch 400/646] [D loss: 0.287893] [G loss: 0.241382] [ema: 0.999736] 
[Epoch 40/78] [Batch 500/646] [D loss: 0.290750] [G loss: 0.230866] [ema: 0.999737] 
[Epoch 40/78] [Batch 600/646] [D loss: 0.338322] [G loss: 0.219103] [ema: 0.999738] 
[Epoch 41/78] [Batch 0/646] [D loss: 0.387747] [G loss: 0.243662] [ema: 0.999738] 
[Epoch 41/78] [Batch 100/646] [D loss: 0.313343] [G loss: 0.214594] [ema: 0.999739] 
[Epoch 41/78] [Batch 200/646] [D loss: 0.388727] [G loss: 0.220286] [ema: 0.999740] 
[Epoch 41/78] [Batch 300/646] [D loss: 0.306275] [G loss: 0.244018] [ema: 0.999741] 
[Epoch 41/78] [Batch 400/646] [D loss: 0.370844] [G loss: 0.194569] [ema: 0.999742] 
[Epoch 41/78] [Batch 500/646] [D loss: 0.308415] [G loss: 0.223038] [ema: 0.999743] 
[Epoch 41/78] [Batch 600/646] [D loss: 0.317654] [G loss: 0.219705] [ema: 0.999744] 
[Epoch 42/78] [Batch 0/646] [D loss: 0.384334] [G loss: 0.249500] [ema: 0.999745] 
[Epoch 42/78] [Batch 100/646] [D loss: 0.303020] [G loss: 0.214330] [ema: 0.999745] 
[Epoch 42/78] [Batch 200/646] [D loss: 0.309910] [G loss: 0.239115] [ema: 0.999746] 
[Epoch 42/78] [Batch 300/646] [D loss: 0.317040] [G loss: 0.236601] [ema: 0.999747] 
[Epoch 42/78] [Batch 400/646] [D loss: 0.347741] [G loss: 0.225780] [ema: 0.999748] 
[Epoch 42/78] [Batch 500/646] [D loss: 0.346277] [G loss: 0.246430] [ema: 0.999749] 
[Epoch 42/78] [Batch 600/646] [D loss: 0.278879] [G loss: 0.228929] [ema: 0.999750] 
[Epoch 43/78] [Batch 0/646] [D loss: 0.358046] [G loss: 0.247036] [ema: 0.999751] 
[Epoch 43/78] [Batch 100/646] [D loss: 0.327778] [G loss: 0.228740] [ema: 0.999751] 
[Epoch 43/78] [Batch 200/646] [D loss: 0.312028] [G loss: 0.213675] [ema: 0.999752] 
[Epoch 43/78] [Batch 300/646] [D loss: 0.285739] [G loss: 0.230232] [ema: 0.999753] 
[Epoch 43/78] [Batch 400/646] [D loss: 0.296977] [G loss: 0.213936] [ema: 0.999754] 
[Epoch 43/78] [Batch 500/646] [D loss: 0.345043] [G loss: 0.214083] [ema: 0.999755] 
[Epoch 43/78] [Batch 600/646] [D loss: 0.337066] [G loss: 0.211075] [ema: 0.999756] 
[Epoch 44/78] [Batch 0/646] [D loss: 0.300106] [G loss: 0.224680] [ema: 0.999756] 
[Epoch 44/78] [Batch 100/646] [D loss: 0.372118] [G loss: 0.196818] [ema: 0.999757] 
[Epoch 44/78] [Batch 200/646] [D loss: 0.316294] [G loss: 0.194421] [ema: 0.999758] 
[Epoch 44/78] [Batch 300/646] [D loss: 0.302018] [G loss: 0.213183] [ema: 0.999759] 
[Epoch 44/78] [Batch 400/646] [D loss: 0.283376] [G loss: 0.232481] [ema: 0.999760] 
[Epoch 44/78] [Batch 500/646] [D loss: 0.324876] [G loss: 0.214523] [ema: 0.999760] 
[Epoch 44/78] [Batch 600/646] [D loss: 0.346610] [G loss: 0.249602] [ema: 0.999761] 
[Epoch 45/78] [Batch 0/646] [D loss: 0.327163] [G loss: 0.220075] [ema: 0.999762] 
[Epoch 45/78] [Batch 100/646] [D loss: 0.284618] [G loss: 0.229950] [ema: 0.999762] 
[Epoch 45/78] [Batch 200/646] [D loss: 0.314630] [G loss: 0.215386] [ema: 0.999763] 
[Epoch 45/78] [Batch 300/646] [D loss: 0.274962] [G loss: 0.226044] [ema: 0.999764] 
[Epoch 45/78] [Batch 400/646] [D loss: 0.332852] [G loss: 0.232161] [ema: 0.999765] 
[Epoch 45/78] [Batch 500/646] [D loss: 0.405584] [G loss: 0.196825] [ema: 0.999766] 
[Epoch 45/78] [Batch 600/646] [D loss: 0.308666] [G loss: 0.244687] [ema: 0.999766] 
[Epoch 46/78] [Batch 0/646] [D loss: 0.317168] [G loss: 0.198897] [ema: 0.999767] 
[Epoch 46/78] [Batch 100/646] [D loss: 0.343708] [G loss: 0.216709] [ema: 0.999768] 
[Epoch 46/78] [Batch 200/646] [D loss: 0.294193] [G loss: 0.225364] [ema: 0.999768] 
[Epoch 46/78] [Batch 300/646] [D loss: 0.309236] [G loss: 0.232401] [ema: 0.999769] 
[Epoch 46/78] [Batch 400/646] [D loss: 0.324986] [G loss: 0.207444] [ema: 0.999770] 
[Epoch 46/78] [Batch 500/646] [D loss: 0.292916] [G loss: 0.219192] [ema: 0.999771] 
[Epoch 46/78] [Batch 600/646] [D loss: 0.351885] [G loss: 0.213336] [ema: 0.999771] 
[Epoch 47/78] [Batch 0/646] [D loss: 0.328440] [G loss: 0.210344] [ema: 0.999772] 
[Epoch 47/78] [Batch 100/646] [D loss: 0.333132] [G loss: 0.215386] [ema: 0.999772] 
[Epoch 47/78] [Batch 200/646] [D loss: 0.351352] [G loss: 0.194176] [ema: 0.999773] 
[Epoch 47/78] [Batch 300/646] [D loss: 0.354418] [G loss: 0.231463] [ema: 0.999774] 
[Epoch 47/78] [Batch 400/646] [D loss: 0.290570] [G loss: 0.229757] [ema: 0.999775] 
[Epoch 47/78] [Batch 500/646] [D loss: 0.339053] [G loss: 0.232668] [ema: 0.999775] 
[Epoch 47/78] [Batch 600/646] [D loss: 0.290484] [G loss: 0.209826] [ema: 0.999776] 
[Epoch 48/78] [Batch 0/646] [D loss: 0.287025] [G loss: 0.223530] [ema: 0.999776] 
[Epoch 48/78] [Batch 100/646] [D loss: 0.302415] [G loss: 0.219201] [ema: 0.999777] 
[Epoch 48/78] [Batch 200/646] [D loss: 0.297896] [G loss: 0.228784] [ema: 0.999778] 
[Epoch 48/78] [Batch 300/646] [D loss: 0.319311] [G loss: 0.221312] [ema: 0.999779] 
[Epoch 48/78] [Batch 400/646] [D loss: 0.300028] [G loss: 0.230434] [ema: 0.999779] 
[Epoch 48/78] [Batch 500/646] [D loss: 0.337158] [G loss: 0.213924] [ema: 0.999780] 
[Epoch 48/78] [Batch 600/646] [D loss: 0.281892] [G loss: 0.224668] [ema: 0.999781] 
[Epoch 49/78] [Batch 0/646] [D loss: 0.328061] [G loss: 0.188670] [ema: 0.999781] 
[Epoch 49/78] [Batch 100/646] [D loss: 0.331646] [G loss: 0.229352] [ema: 0.999782] 
[Epoch 49/78] [Batch 200/646] [D loss: 0.336497] [G loss: 0.218392] [ema: 0.999782] 
[Epoch 49/78] [Batch 300/646] [D loss: 0.371002] [G loss: 0.220823] [ema: 0.999783] 
[Epoch 49/78] [Batch 400/646] [D loss: 0.320732] [G loss: 0.213494] [ema: 0.999784] 
[Epoch 49/78] [Batch 500/646] [D loss: 0.322995] [G loss: 0.226311] [ema: 0.999784] 
[Epoch 49/78] [Batch 600/646] [D loss: 0.307177] [G loss: 0.215931] [ema: 0.999785] 
[Epoch 50/78] [Batch 0/646] [D loss: 0.298877] [G loss: 0.231959] [ema: 0.999785] 
[Epoch 50/78] [Batch 100/646] [D loss: 0.311789] [G loss: 0.203855] [ema: 0.999786] 
[Epoch 50/78] [Batch 200/646] [D loss: 0.312830] [G loss: 0.219873] [ema: 0.999787] 
[Epoch 50/78] [Batch 300/646] [D loss: 0.292068] [G loss: 0.232626] [ema: 0.999787] 
[Epoch 50/78] [Batch 400/646] [D loss: 0.287872] [G loss: 0.216424] [ema: 0.999788] 
[Epoch 50/78] [Batch 500/646] [D loss: 0.358090] [G loss: 0.225614] [ema: 0.999789] 
[Epoch 50/78] [Batch 600/646] [D loss: 0.313447] [G loss: 0.192768] [ema: 0.999789] 
[Epoch 51/78] [Batch 0/646] [D loss: 0.321527] [G loss: 0.248938] [ema: 0.999790] 
[Epoch 51/78] [Batch 100/646] [D loss: 0.273719] [G loss: 0.245526] [ema: 0.999790] 
[Epoch 51/78] [Batch 200/646] [D loss: 0.308970] [G loss: 0.242509] [ema: 0.999791] 
[Epoch 51/78] [Batch 300/646] [D loss: 0.327531] [G loss: 0.196585] [ema: 0.999792] 
[Epoch 51/78] [Batch 400/646] [D loss: 0.331444] [G loss: 0.212016] [ema: 0.999792] 
[Epoch 51/78] [Batch 500/646] [D loss: 0.287729] [G loss: 0.208527] [ema: 0.999793] 
[Epoch 51/78] [Batch 600/646] [D loss: 0.339168] [G loss: 0.196938] [ema: 0.999793] 
[Epoch 52/78] [Batch 0/646] [D loss: 0.337512] [G loss: 0.203214] [ema: 0.999794] 
[Epoch 52/78] [Batch 100/646] [D loss: 0.327510] [G loss: 0.248057] [ema: 0.999794] 
[Epoch 52/78] [Batch 200/646] [D loss: 0.332929] [G loss: 0.207315] [ema: 0.999795] 
[Epoch 52/78] [Batch 300/646] [D loss: 0.312722] [G loss: 0.203510] [ema: 0.999796] 
[Epoch 52/78] [Batch 400/646] [D loss: 0.377891] [G loss: 0.214738] [ema: 0.999796] 
[Epoch 52/78] [Batch 500/646] [D loss: 0.289372] [G loss: 0.181522] [ema: 0.999797] 
[Epoch 52/78] [Batch 600/646] [D loss: 0.311452] [G loss: 0.207354] [ema: 0.999797] 
[Epoch 53/78] [Batch 0/646] [D loss: 0.316788] [G loss: 0.206705] [ema: 0.999798] 
[Epoch 53/78] [Batch 100/646] [D loss: 0.333002] [G loss: 0.240126] [ema: 0.999798] 
[Epoch 53/78] [Batch 200/646] [D loss: 0.285524] [G loss: 0.217762] [ema: 0.999799] 
[Epoch 53/78] [Batch 300/646] [D loss: 0.301588] [G loss: 0.231385] [ema: 0.999799] 
[Epoch 53/78] [Batch 400/646] [D loss: 0.274087] [G loss: 0.226786] [ema: 0.999800] 
[Epoch 53/78] [Batch 500/646] [D loss: 0.293924] [G loss: 0.224315] [ema: 0.999800] 
[Epoch 53/78] [Batch 600/646] [D loss: 0.287677] [G loss: 0.227240] [ema: 0.999801] 
[Epoch 54/78] [Batch 0/646] [D loss: 0.355489] [G loss: 0.219266] [ema: 0.999801] 
[Epoch 54/78] [Batch 100/646] [D loss: 0.291115] [G loss: 0.210745] [ema: 0.999802] 
[Epoch 54/78] [Batch 200/646] [D loss: 0.286707] [G loss: 0.230969] [ema: 0.999802] 
[Epoch 54/78] [Batch 300/646] [D loss: 0.320410] [G loss: 0.213906] [ema: 0.999803] 
[Epoch 54/78] [Batch 400/646] [D loss: 0.297518] [G loss: 0.220261] [ema: 0.999804] 
[Epoch 54/78] [Batch 500/646] [D loss: 0.296971] [G loss: 0.227569] [ema: 0.999804] 
[Epoch 54/78] [Batch 600/646] [D loss: 0.359367] [G loss: 0.232335] [ema: 0.999805] 
[Epoch 55/78] [Batch 0/646] [D loss: 0.333860] [G loss: 0.238875] [ema: 0.999805] 
[Epoch 55/78] [Batch 100/646] [D loss: 0.318338] [G loss: 0.225819] [ema: 0.999805] 
[Epoch 55/78] [Batch 200/646] [D loss: 0.291504] [G loss: 0.218625] [ema: 0.999806] 
[Epoch 55/78] [Batch 300/646] [D loss: 0.344067] [G loss: 0.216146] [ema: 0.999807] 
[Epoch 55/78] [Batch 400/646] [D loss: 0.334162] [G loss: 0.242884] [ema: 0.999807] 
[Epoch 55/78] [Batch 500/646] [D loss: 0.290348] [G loss: 0.231082] [ema: 0.999808] 
[Epoch 55/78] [Batch 600/646] [D loss: 0.316507] [G loss: 0.214721] [ema: 0.999808] 
[Epoch 56/78] [Batch 0/646] [D loss: 0.290190] [G loss: 0.227652] [ema: 0.999808] 
[Epoch 56/78] [Batch 100/646] [D loss: 0.275000] [G loss: 0.240420] [ema: 0.999809] 
[Epoch 56/78] [Batch 200/646] [D loss: 0.297033] [G loss: 0.243034] [ema: 0.999809] 
[Epoch 56/78] [Batch 300/646] [D loss: 0.302689] [G loss: 0.207647] [ema: 0.999810] 
[Epoch 56/78] [Batch 400/646] [D loss: 0.340877] [G loss: 0.219412] [ema: 0.999811] 
[Epoch 56/78] [Batch 500/646] [D loss: 0.273697] [G loss: 0.212856] [ema: 0.999811] 
[Epoch 56/78] [Batch 600/646] [D loss: 0.316565] [G loss: 0.220050] [ema: 0.999812] 
[Epoch 57/78] [Batch 0/646] [D loss: 0.300483] [G loss: 0.232170] [ema: 0.999812] 
[Epoch 57/78] [Batch 100/646] [D loss: 0.303355] [G loss: 0.244192] [ema: 0.999812] 
[Epoch 57/78] [Batch 200/646] [D loss: 0.325424] [G loss: 0.234696] [ema: 0.999813] 
[Epoch 57/78] [Batch 300/646] [D loss: 0.276792] [G loss: 0.228949] [ema: 0.999813] 
[Epoch 57/78] [Batch 400/646] [D loss: 0.284399] [G loss: 0.189024] [ema: 0.999814] 
[Epoch 57/78] [Batch 500/646] [D loss: 0.383619] [G loss: 0.172570] [ema: 0.999814] 
[Epoch 57/78] [Batch 600/646] [D loss: 0.304822] [G loss: 0.220039] [ema: 0.999815] 
[Epoch 58/78] [Batch 0/646] [D loss: 0.311257] [G loss: 0.212454] [ema: 0.999815] 
[Epoch 58/78] [Batch 100/646] [D loss: 0.300363] [G loss: 0.208622] [ema: 0.999816] 
[Epoch 58/78] [Batch 200/646] [D loss: 0.342540] [G loss: 0.225608] [ema: 0.999816] 
[Epoch 58/78] [Batch 300/646] [D loss: 0.333695] [G loss: 0.221649] [ema: 0.999816] 
[Epoch 58/78] [Batch 400/646] [D loss: 0.320695] [G loss: 0.240230] [ema: 0.999817] 
[Epoch 58/78] [Batch 500/646] [D loss: 0.357064] [G loss: 0.204824] [ema: 0.999817] 
[Epoch 58/78] [Batch 600/646] [D loss: 0.289653] [G loss: 0.248142] [ema: 0.999818] 
[Epoch 59/78] [Batch 0/646] [D loss: 0.314267] [G loss: 0.232893] [ema: 0.999818] 
[Epoch 59/78] [Batch 100/646] [D loss: 0.341445] [G loss: 0.231291] [ema: 0.999819] 
[Epoch 59/78] [Batch 200/646] [D loss: 0.267304] [G loss: 0.217370] [ema: 0.999819] 
[Epoch 59/78] [Batch 300/646] [D loss: 0.337763] [G loss: 0.232508] [ema: 0.999820] 
[Epoch 59/78] [Batch 400/646] [D loss: 0.304378] [G loss: 0.193759] [ema: 0.999820] 
[Epoch 59/78] [Batch 500/646] [D loss: 0.309451] [G loss: 0.228767] [ema: 0.999821] 
[Epoch 59/78] [Batch 600/646] [D loss: 0.293593] [G loss: 0.213962] [ema: 0.999821] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_60_100/RealWorld_waist_DAGHAR_Multiclass_50000_D_60_2024_10_30_02_28_08/Model



[Epoch 60/78] [Batch 0/646] [D loss: 0.304863] [G loss: 0.222133] [ema: 0.999821] 
[Epoch 60/78] [Batch 100/646] [D loss: 0.303948] [G loss: 0.217476] [ema: 0.999822] 
[Epoch 60/78] [Batch 200/646] [D loss: 0.389292] [G loss: 0.223840] [ema: 0.999822] 
[Epoch 60/78] [Batch 300/646] [D loss: 0.321339] [G loss: 0.215479] [ema: 0.999823] 
[Epoch 60/78] [Batch 400/646] [D loss: 0.298898] [G loss: 0.243780] [ema: 0.999823] 
[Epoch 60/78] [Batch 500/646] [D loss: 0.344560] [G loss: 0.245674] [ema: 0.999823] 
[Epoch 60/78] [Batch 600/646] [D loss: 0.332465] [G loss: 0.192214] [ema: 0.999824] 
[Epoch 61/78] [Batch 0/646] [D loss: 0.288191] [G loss: 0.244966] [ema: 0.999824] 
[Epoch 61/78] [Batch 100/646] [D loss: 0.395306] [G loss: 0.224817] [ema: 0.999825] 
[Epoch 61/78] [Batch 200/646] [D loss: 0.314643] [G loss: 0.211992] [ema: 0.999825] 
[Epoch 61/78] [Batch 300/646] [D loss: 0.294331] [G loss: 0.226240] [ema: 0.999825] 
[Epoch 61/78] [Batch 400/646] [D loss: 0.307402] [G loss: 0.225474] [ema: 0.999826] 
[Epoch 61/78] [Batch 500/646] [D loss: 0.317199] [G loss: 0.219889] [ema: 0.999826] 
[Epoch 61/78] [Batch 600/646] [D loss: 0.374524] [G loss: 0.223423] [ema: 0.999827] 
[Epoch 62/78] [Batch 0/646] [D loss: 0.245992] [G loss: 0.253996] [ema: 0.999827] 
[Epoch 62/78] [Batch 100/646] [D loss: 0.283267] [G loss: 0.223482] [ema: 0.999827] 
[Epoch 62/78] [Batch 200/646] [D loss: 0.303709] [G loss: 0.214053] [ema: 0.999828] 
[Epoch 62/78] [Batch 300/646] [D loss: 0.307704] [G loss: 0.230902] [ema: 0.999828] 
[Epoch 62/78] [Batch 400/646] [D loss: 0.311509] [G loss: 0.226359] [ema: 0.999829] 
[Epoch 62/78] [Batch 500/646] [D loss: 0.334987] [G loss: 0.225508] [ema: 0.999829] 
[Epoch 62/78] [Batch 600/646] [D loss: 0.296647] [G loss: 0.200523] [ema: 0.999830] 
[Epoch 63/78] [Batch 0/646] [D loss: 0.318035] [G loss: 0.221096] [ema: 0.999830] 
[Epoch 63/78] [Batch 100/646] [D loss: 0.274005] [G loss: 0.239036] [ema: 0.999830] 
[Epoch 63/78] [Batch 200/646] [D loss: 0.296712] [G loss: 0.197668] [ema: 0.999831] 
[Epoch 63/78] [Batch 300/646] [D loss: 0.296427] [G loss: 0.197512] [ema: 0.999831] 
[Epoch 63/78] [Batch 400/646] [D loss: 0.327434] [G loss: 0.249431] [ema: 0.999831] 
[Epoch 63/78] [Batch 500/646] [D loss: 0.362728] [G loss: 0.222147] [ema: 0.999832] 
[Epoch 63/78] [Batch 600/646] [D loss: 0.302937] [G loss: 0.243427] [ema: 0.999832] 
[Epoch 64/78] [Batch 0/646] [D loss: 0.277579] [G loss: 0.230895] [ema: 0.999832] 
[Epoch 64/78] [Batch 100/646] [D loss: 0.299665] [G loss: 0.243122] [ema: 0.999833] 
[Epoch 64/78] [Batch 200/646] [D loss: 0.334621] [G loss: 0.224994] [ema: 0.999833] 
[Epoch 64/78] [Batch 300/646] [D loss: 0.346651] [G loss: 0.226771] [ema: 0.999834] 
[Epoch 64/78] [Batch 400/646] [D loss: 0.269525] [G loss: 0.217985] [ema: 0.999834] 
[Epoch 64/78] [Batch 500/646] [D loss: 0.264907] [G loss: 0.227064] [ema: 0.999834] 
[Epoch 64/78] [Batch 600/646] [D loss: 0.348226] [G loss: 0.239848] [ema: 0.999835] 
[Epoch 65/78] [Batch 0/646] [D loss: 0.299019] [G loss: 0.232995] [ema: 0.999835] 
[Epoch 65/78] [Batch 100/646] [D loss: 0.290414] [G loss: 0.236247] [ema: 0.999835] 
[Epoch 65/78] [Batch 200/646] [D loss: 0.314121] [G loss: 0.227266] [ema: 0.999836] 
[Epoch 65/78] [Batch 300/646] [D loss: 0.378467] [G loss: 0.214590] [ema: 0.999836] 
[Epoch 65/78] [Batch 400/646] [D loss: 0.288582] [G loss: 0.198529] [ema: 0.999836] 
[Epoch 65/78] [Batch 500/646] [D loss: 0.314399] [G loss: 0.192773] [ema: 0.999837] 
[Epoch 65/78] [Batch 600/646] [D loss: 0.294948] [G loss: 0.224194] [ema: 0.999837] 
[Epoch 66/78] [Batch 0/646] [D loss: 0.283469] [G loss: 0.239343] [ema: 0.999837] 
[Epoch 66/78] [Batch 100/646] [D loss: 0.268590] [G loss: 0.229470] [ema: 0.999838] 
[Epoch 66/78] [Batch 200/646] [D loss: 0.288601] [G loss: 0.222654] [ema: 0.999838] 
[Epoch 66/78] [Batch 300/646] [D loss: 0.277613] [G loss: 0.210284] [ema: 0.999839] 
[Epoch 66/78] [Batch 400/646] [D loss: 0.329154] [G loss: 0.234442] [ema: 0.999839] 
[Epoch 66/78] [Batch 500/646] [D loss: 0.318246] [G loss: 0.210901] [ema: 0.999839] 
[Epoch 66/78] [Batch 600/646] [D loss: 0.331023] [G loss: 0.215728] [ema: 0.999840] 
[Epoch 67/78] [Batch 0/646] [D loss: 0.259943] [G loss: 0.240944] [ema: 0.999840] 
[Epoch 67/78] [Batch 100/646] [D loss: 0.287405] [G loss: 0.209317] [ema: 0.999840] 
[Epoch 67/78] [Batch 200/646] [D loss: 0.308164] [G loss: 0.218469] [ema: 0.999841] 
[Epoch 67/78] [Batch 300/646] [D loss: 0.294805] [G loss: 0.189825] [ema: 0.999841] 
[Epoch 67/78] [Batch 400/646] [D loss: 0.327890] [G loss: 0.225620] [ema: 0.999841] 
[Epoch 67/78] [Batch 500/646] [D loss: 0.276680] [G loss: 0.237246] [ema: 0.999842] 
[Epoch 67/78] [Batch 600/646] [D loss: 0.256323] [G loss: 0.206626] [ema: 0.999842] 
[Epoch 68/78] [Batch 0/646] [D loss: 0.307346] [G loss: 0.191741] [ema: 0.999842] 
[Epoch 68/78] [Batch 100/646] [D loss: 0.294547] [G loss: 0.225735] [ema: 0.999843] 
[Epoch 68/78] [Batch 200/646] [D loss: 0.367358] [G loss: 0.216840] [ema: 0.999843] 
[Epoch 68/78] [Batch 300/646] [D loss: 0.266007] [G loss: 0.204699] [ema: 0.999843] 
[Epoch 68/78] [Batch 400/646] [D loss: 0.346185] [G loss: 0.229021] [ema: 0.999844] 
[Epoch 68/78] [Batch 500/646] [D loss: 0.318318] [G loss: 0.230469] [ema: 0.999844] 
[Epoch 68/78] [Batch 600/646] [D loss: 0.318363] [G loss: 0.231744] [ema: 0.999844] 
[Epoch 69/78] [Batch 0/646] [D loss: 0.312641] [G loss: 0.205517] [ema: 0.999845] 
[Epoch 69/78] [Batch 100/646] [D loss: 0.322609] [G loss: 0.226570] [ema: 0.999845] 
[Epoch 69/78] [Batch 200/646] [D loss: 0.308109] [G loss: 0.230184] [ema: 0.999845] 
[Epoch 69/78] [Batch 300/646] [D loss: 0.311612] [G loss: 0.228426] [ema: 0.999846] 
[Epoch 69/78] [Batch 400/646] [D loss: 0.326527] [G loss: 0.234949] [ema: 0.999846] 
[Epoch 69/78] [Batch 500/646] [D loss: 0.310537] [G loss: 0.225797] [ema: 0.999846] 
[Epoch 69/78] [Batch 600/646] [D loss: 0.346933] [G loss: 0.236321] [ema: 0.999847] 
[Epoch 70/78] [Batch 0/646] [D loss: 0.299442] [G loss: 0.240213] [ema: 0.999847] 
[Epoch 70/78] [Batch 100/646] [D loss: 0.345174] [G loss: 0.221369] [ema: 0.999847] 
[Epoch 70/78] [Batch 200/646] [D loss: 0.278741] [G loss: 0.241244] [ema: 0.999847] 
[Epoch 70/78] [Batch 300/646] [D loss: 0.320877] [G loss: 0.224025] [ema: 0.999848] 
[Epoch 70/78] [Batch 400/646] [D loss: 0.279592] [G loss: 0.214813] [ema: 0.999848] 
[Epoch 70/78] [Batch 500/646] [D loss: 0.312277] [G loss: 0.222500] [ema: 0.999848] 
[Epoch 70/78] [Batch 600/646] [D loss: 0.307171] [G loss: 0.218377] [ema: 0.999849] 
[Epoch 71/78] [Batch 0/646] [D loss: 0.293120] [G loss: 0.218577] [ema: 0.999849] 
[Epoch 71/78] [Batch 100/646] [D loss: 0.279337] [G loss: 0.229795] [ema: 0.999849] 
[Epoch 71/78] [Batch 200/646] [D loss: 0.321073] [G loss: 0.198102] [ema: 0.999850] 
[Epoch 71/78] [Batch 300/646] [D loss: 0.295960] [G loss: 0.229263] [ema: 0.999850] 
[Epoch 71/78] [Batch 400/646] [D loss: 0.334443] [G loss: 0.224251] [ema: 0.999850] 
[Epoch 71/78] [Batch 500/646] [D loss: 0.299831] [G loss: 0.215830] [ema: 0.999851] 
[Epoch 71/78] [Batch 600/646] [D loss: 0.287229] [G loss: 0.234864] [ema: 0.999851] 
[Epoch 72/78] [Batch 0/646] [D loss: 0.297027] [G loss: 0.247004] [ema: 0.999851] 
[Epoch 72/78] [Batch 100/646] [D loss: 0.340415] [G loss: 0.212597] [ema: 0.999851] 
[Epoch 72/78] [Batch 200/646] [D loss: 0.282830] [G loss: 0.216380] [ema: 0.999852] 
[Epoch 72/78] [Batch 300/646] [D loss: 0.305037] [G loss: 0.221338] [ema: 0.999852] 
[Epoch 72/78] [Batch 400/646] [D loss: 0.332158] [G loss: 0.234934] [ema: 0.999852] 
[Epoch 72/78] [Batch 500/646] [D loss: 0.285781] [G loss: 0.224903] [ema: 0.999853] 
[Epoch 72/78] [Batch 600/646] [D loss: 0.299461] [G loss: 0.232452] [ema: 0.999853] 
[Epoch 73/78] [Batch 0/646] [D loss: 0.334211] [G loss: 0.229367] [ema: 0.999853] 
[Epoch 73/78] [Batch 100/646] [D loss: 0.278613] [G loss: 0.212076] [ema: 0.999853] 
[Epoch 73/78] [Batch 200/646] [D loss: 0.268740] [G loss: 0.243629] [ema: 0.999854] 
[Epoch 73/78] [Batch 300/646] [D loss: 0.331834] [G loss: 0.209051] [ema: 0.999854] 
[Epoch 73/78] [Batch 400/646] [D loss: 0.299975] [G loss: 0.234837] [ema: 0.999854] 
[Epoch 73/78] [Batch 500/646] [D loss: 0.286699] [G loss: 0.223662] [ema: 0.999855] 
[Epoch 73/78] [Batch 600/646] [D loss: 0.285674] [G loss: 0.232806] [ema: 0.999855] 
[Epoch 74/78] [Batch 0/646] [D loss: 0.262381] [G loss: 0.229975] [ema: 0.999855] 
[Epoch 74/78] [Batch 100/646] [D loss: 0.302737] [G loss: 0.242907] [ema: 0.999855] 
[Epoch 74/78] [Batch 200/646] [D loss: 0.341633] [G loss: 0.235205] [ema: 0.999856] 
[Epoch 74/78] [Batch 300/646] [D loss: 0.273725] [G loss: 0.214121] [ema: 0.999856] 
[Epoch 74/78] [Batch 400/646] [D loss: 0.270787] [G loss: 0.217181] [ema: 0.999856] 
[Epoch 74/78] [Batch 500/646] [D loss: 0.340667] [G loss: 0.235477] [ema: 0.999857] 
[Epoch 74/78] [Batch 600/646] [D loss: 0.275818] [G loss: 0.222661] [ema: 0.999857] 
[Epoch 75/78] [Batch 0/646] [D loss: 0.300999] [G loss: 0.206405] [ema: 0.999857] 
[Epoch 75/78] [Batch 100/646] [D loss: 0.311818] [G loss: 0.232289] [ema: 0.999857] 
[Epoch 75/78] [Batch 200/646] [D loss: 0.292228] [G loss: 0.231133] [ema: 0.999858] 
[Epoch 75/78] [Batch 300/646] [D loss: 0.318514] [G loss: 0.233620] [ema: 0.999858] 
[Epoch 75/78] [Batch 400/646] [D loss: 0.325904] [G loss: 0.214672] [ema: 0.999858] 
[Epoch 75/78] [Batch 500/646] [D loss: 0.346612] [G loss: 0.223837] [ema: 0.999858] 
[Epoch 75/78] [Batch 600/646] [D loss: 0.280877] [G loss: 0.189968] [ema: 0.999859] 
[Epoch 76/78] [Batch 0/646] [D loss: 0.289590] [G loss: 0.239607] [ema: 0.999859] 
[Epoch 76/78] [Batch 100/646] [D loss: 0.290230] [G loss: 0.230158] [ema: 0.999859] 
[Epoch 76/78] [Batch 200/646] [D loss: 0.280011] [G loss: 0.212807] [ema: 0.999859] 
[Epoch 76/78] [Batch 300/646] [D loss: 0.288991] [G loss: 0.222118] [ema: 0.999860] 
[Epoch 76/78] [Batch 400/646] [D loss: 0.326593] [G loss: 0.213285] [ema: 0.999860] 
[Epoch 76/78] [Batch 500/646] [D loss: 0.297137] [G loss: 0.187751] [ema: 0.999860] 
[Epoch 76/78] [Batch 600/646] [D loss: 0.272021] [G loss: 0.244565] [ema: 0.999861] 
[Epoch 77/78] [Batch 0/646] [D loss: 0.315089] [G loss: 0.227962] [ema: 0.999861] 
[Epoch 77/78] [Batch 100/646] [D loss: 0.338054] [G loss: 0.241337] [ema: 0.999861] 
[Epoch 77/78] [Batch 200/646] [D loss: 0.290716] [G loss: 0.224812] [ema: 0.999861] 
[Epoch 77/78] [Batch 300/646] [D loss: 0.283250] [G loss: 0.220707] [ema: 0.999861] 
[Epoch 77/78] [Batch 400/646] [D loss: 0.274287] [G loss: 0.224076] [ema: 0.999862] 
[Epoch 77/78] [Batch 500/646] [D loss: 0.315513] [G loss: 0.216198] [ema: 0.999862] 
[Epoch 77/78] [Batch 600/646] [D loss: 0.317213] [G loss: 0.246350] [ema: 0.999862] 

----------------------------------------------------------------------------------------------------

 Starting individual training
KuHar_DAGHAR_Multiclass training
----------------------------------------------------------------------------------------------------
Generator(
  (l1): Linear(in_features=100, out_features=600, bias=True)
  (blocks): Gen_TransformerEncoder(
    (0): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Gen_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=10, out_features=10, bias=True)
            (queries): Linear(in_features=10, out_features=10, bias=True)
            (values): Linear(in_features=10, out_features=10, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=10, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=10, out_features=40, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=40, out_features=10, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (deconv): Sequential(
    (0): Conv2d(10, 6, kernel_size=(1, 1), stride=(1, 1))
  )
)
Discriminator(
  (0): PatchEmbedding_Linear(
    (projection): Sequential(
      (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
      (1): Linear(in_features=90, out_features=50, bias=True)
    )
  )
  (1): Dis_TransformerEncoder(
    (0): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (2): Dis_TransformerEncoderBlock(
      (0): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): MultiHeadAttention(
            (keys): Linear(in_features=50, out_features=50, bias=True)
            (queries): Linear(in_features=50, out_features=50, bias=True)
            (values): Linear(in_features=50, out_features=50, bias=True)
            (att_drop): Dropout(p=0.5, inplace=False)
            (projection): Linear(in_features=50, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (1): ResidualAdd(
        (fn): Sequential(
          (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
          (1): FeedForwardBlock(
            (0): Linear(in_features=50, out_features=200, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=200, out_features=50, bias=True)
          )
          (2): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (2): ClassificationHead(
    (clshead): Sequential(
      (0): Reduce('b n e -> b e', 'mean')
      (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=50, out_features=1, bias=True)
    )
  )
)
DataParallel(
  (module): Discriminator(
    (0): PatchEmbedding_Linear(
      (projection): Sequential(
        (0): Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=1, s2=15)
        (1): Linear(in_features=90, out_features=50, bias=True)
      )
    )
    (1): Dis_TransformerEncoder(
      (0): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (1): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
      (2): Dis_TransformerEncoderBlock(
        (0): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): MultiHeadAttention(
              (keys): Linear(in_features=50, out_features=50, bias=True)
              (queries): Linear(in_features=50, out_features=50, bias=True)
              (values): Linear(in_features=50, out_features=50, bias=True)
              (att_drop): Dropout(p=0.5, inplace=False)
              (projection): Linear(in_features=50, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
        (1): ResidualAdd(
          (fn): Sequential(
            (0): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
            (1): FeedForwardBlock(
              (0): Linear(in_features=50, out_features=200, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.5, inplace=False)
              (3): Linear(in_features=200, out_features=50, bias=True)
            )
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (2): ClassificationHead(
      (clshead): Sequential(
        (0): Reduce('b n e -> b e', 'mean')
        (1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)
        (2): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
KuHar_DAGHAR_Multiclass
daghar
return single class data and labels, class is KuHar_DAGHAR_Multiclass
data shape is (1392, 6, 1, 60)
label shape is (1392,)
87
Epochs between checkpoint: 144



Saving checkpoint 1 in logs/daghar_split_dataset_50000_6axis_60_100/KuHar_DAGHAR_Multiclass_50000_D_60_2024_10_30_03_01_46/Model



[Epoch 0/575] [Batch 0/87] [D loss: 1.153504] [G loss: 0.621342] [ema: 0.000000] 
[Epoch 1/575] [Batch 0/87] [D loss: 0.485450] [G loss: 0.170230] [ema: 0.923419] 
[Epoch 2/575] [Batch 0/87] [D loss: 0.439703] [G loss: 0.154264] [ema: 0.960947] 
[Epoch 3/575] [Batch 0/87] [D loss: 0.494176] [G loss: 0.166744] [ema: 0.973792] 
[Epoch 4/575] [Batch 0/87] [D loss: 0.433963] [G loss: 0.195883] [ema: 0.980279] 
[Epoch 5/575] [Batch 0/87] [D loss: 0.441575] [G loss: 0.228146] [ema: 0.984192] 
[Epoch 6/575] [Batch 0/87] [D loss: 0.450358] [G loss: 0.199886] [ema: 0.986809] 
[Epoch 7/575] [Batch 0/87] [D loss: 0.348500] [G loss: 0.196376] [ema: 0.988683] 
[Epoch 8/575] [Batch 0/87] [D loss: 0.420788] [G loss: 0.184611] [ema: 0.990090] 
[Epoch 9/575] [Batch 0/87] [D loss: 0.410183] [G loss: 0.173303] [ema: 0.991187] 
[Epoch 10/575] [Batch 0/87] [D loss: 0.474126] [G loss: 0.191339] [ema: 0.992064] 
[Epoch 11/575] [Batch 0/87] [D loss: 0.500006] [G loss: 0.191693] [ema: 0.992783] 
[Epoch 12/575] [Batch 0/87] [D loss: 0.429419] [G loss: 0.179791] [ema: 0.993383] 
[Epoch 13/575] [Batch 0/87] [D loss: 0.414704] [G loss: 0.176102] [ema: 0.993890] 
[Epoch 14/575] [Batch 0/87] [D loss: 0.394442] [G loss: 0.192207] [ema: 0.994325] 
[Epoch 15/575] [Batch 0/87] [D loss: 0.419678] [G loss: 0.160565] [ema: 0.994703] 
[Epoch 16/575] [Batch 0/87] [D loss: 0.401121] [G loss: 0.170035] [ema: 0.995033] 
[Epoch 17/575] [Batch 0/87] [D loss: 0.445611] [G loss: 0.179252] [ema: 0.995324] 
[Epoch 18/575] [Batch 0/87] [D loss: 0.472831] [G loss: 0.175675] [ema: 0.995584] 
[Epoch 19/575] [Batch 0/87] [D loss: 0.388809] [G loss: 0.176071] [ema: 0.995816] 
[Epoch 20/575] [Batch 0/87] [D loss: 0.422513] [G loss: 0.174609] [ema: 0.996024] 
[Epoch 21/575] [Batch 0/87] [D loss: 0.397097] [G loss: 0.189450] [ema: 0.996213] 
[Epoch 22/575] [Batch 0/87] [D loss: 0.428126] [G loss: 0.172449] [ema: 0.996385] 
[Epoch 23/575] [Batch 0/87] [D loss: 0.412294] [G loss: 0.180649] [ema: 0.996542] 
[Epoch 24/575] [Batch 0/87] [D loss: 0.364090] [G loss: 0.195796] [ema: 0.996686] 
[Epoch 25/575] [Batch 0/87] [D loss: 0.474279] [G loss: 0.134154] [ema: 0.996818] 
[Epoch 26/575] [Batch 0/87] [D loss: 0.393556] [G loss: 0.201307] [ema: 0.996940] 
[Epoch 27/575] [Batch 0/87] [D loss: 0.389411] [G loss: 0.192288] [ema: 0.997054] 
[Epoch 28/575] [Batch 0/87] [D loss: 0.404895] [G loss: 0.187405] [ema: 0.997159] 
[Epoch 29/575] [Batch 0/87] [D loss: 0.379626] [G loss: 0.162460] [ema: 0.997256] 
[Epoch 30/575] [Batch 0/87] [D loss: 0.347355] [G loss: 0.209584] [ema: 0.997348] 
[Epoch 31/575] [Batch 0/87] [D loss: 0.395231] [G loss: 0.171436] [ema: 0.997433] 
[Epoch 32/575] [Batch 0/87] [D loss: 0.372605] [G loss: 0.183604] [ema: 0.997513] 
[Epoch 33/575] [Batch 0/87] [D loss: 0.439444] [G loss: 0.177305] [ema: 0.997589] 
[Epoch 34/575] [Batch 0/87] [D loss: 0.403077] [G loss: 0.167597] [ema: 0.997659] 
[Epoch 35/575] [Batch 0/87] [D loss: 0.377708] [G loss: 0.183688] [ema: 0.997726] 
[Epoch 36/575] [Batch 0/87] [D loss: 0.404265] [G loss: 0.204180] [ema: 0.997789] 
[Epoch 37/575] [Batch 0/87] [D loss: 0.334504] [G loss: 0.234565] [ema: 0.997849] 
[Epoch 38/575] [Batch 0/87] [D loss: 0.423829] [G loss: 0.156072] [ema: 0.997906] 
[Epoch 39/575] [Batch 0/87] [D loss: 0.431803] [G loss: 0.224016] [ema: 0.997959] 
[Epoch 40/575] [Batch 0/87] [D loss: 0.370905] [G loss: 0.184811] [ema: 0.998010] 
[Epoch 41/575] [Batch 0/87] [D loss: 0.382956] [G loss: 0.203099] [ema: 0.998059] 
[Epoch 42/575] [Batch 0/87] [D loss: 0.486630] [G loss: 0.173261] [ema: 0.998105] 
[Epoch 43/575] [Batch 0/87] [D loss: 0.456529] [G loss: 0.167889] [ema: 0.998149] 
[Epoch 44/575] [Batch 0/87] [D loss: 0.415685] [G loss: 0.184383] [ema: 0.998191] 
[Epoch 45/575] [Batch 0/87] [D loss: 0.446721] [G loss: 0.178928] [ema: 0.998231] 
[Epoch 46/575] [Batch 0/87] [D loss: 0.447718] [G loss: 0.140287] [ema: 0.998269] 
[Epoch 47/575] [Batch 0/87] [D loss: 0.410737] [G loss: 0.179893] [ema: 0.998306] 
[Epoch 48/575] [Batch 0/87] [D loss: 0.407889] [G loss: 0.214267] [ema: 0.998342] 
[Epoch 49/575] [Batch 0/87] [D loss: 0.378038] [G loss: 0.225894] [ema: 0.998375] 
[Epoch 50/575] [Batch 0/87] [D loss: 0.481228] [G loss: 0.208738] [ema: 0.998408] 
[Epoch 51/575] [Batch 0/87] [D loss: 0.340262] [G loss: 0.225251] [ema: 0.998439] 
[Epoch 52/575] [Batch 0/87] [D loss: 0.361534] [G loss: 0.202980] [ema: 0.998469] 
[Epoch 53/575] [Batch 0/87] [D loss: 0.367846] [G loss: 0.170459] [ema: 0.998498] 
[Epoch 54/575] [Batch 0/87] [D loss: 0.447017] [G loss: 0.172602] [ema: 0.998526] 
[Epoch 55/575] [Batch 0/87] [D loss: 0.455000] [G loss: 0.159793] [ema: 0.998552] 
[Epoch 56/575] [Batch 0/87] [D loss: 0.392756] [G loss: 0.216449] [ema: 0.998578] 
[Epoch 57/575] [Batch 0/87] [D loss: 0.392855] [G loss: 0.191492] [ema: 0.998603] 
[Epoch 58/575] [Batch 0/87] [D loss: 0.426065] [G loss: 0.165704] [ema: 0.998627] 
[Epoch 59/575] [Batch 0/87] [D loss: 0.456780] [G loss: 0.175159] [ema: 0.998651] 
[Epoch 60/575] [Batch 0/87] [D loss: 0.422168] [G loss: 0.187900] [ema: 0.998673] 
[Epoch 61/575] [Batch 0/87] [D loss: 0.417585] [G loss: 0.194905] [ema: 0.998695] 
[Epoch 62/575] [Batch 0/87] [D loss: 0.383538] [G loss: 0.189502] [ema: 0.998716] 
[Epoch 63/575] [Batch 0/87] [D loss: 0.356147] [G loss: 0.198811] [ema: 0.998736] 
[Epoch 64/575] [Batch 0/87] [D loss: 0.385468] [G loss: 0.198464] [ema: 0.998756] 
[Epoch 65/575] [Batch 0/87] [D loss: 0.393621] [G loss: 0.172273] [ema: 0.998775] 
[Epoch 66/575] [Batch 0/87] [D loss: 0.410042] [G loss: 0.189521] [ema: 0.998794] 
[Epoch 67/575] [Batch 0/87] [D loss: 0.355667] [G loss: 0.179543] [ema: 0.998812] 
[Epoch 68/575] [Batch 0/87] [D loss: 0.431960] [G loss: 0.206905] [ema: 0.998829] 
[Epoch 69/575] [Batch 0/87] [D loss: 0.404663] [G loss: 0.186007] [ema: 0.998846] 
[Epoch 70/575] [Batch 0/87] [D loss: 0.344217] [G loss: 0.199443] [ema: 0.998862] 
[Epoch 71/575] [Batch 0/87] [D loss: 0.344576] [G loss: 0.176407] [ema: 0.998878] 
[Epoch 72/575] [Batch 0/87] [D loss: 0.416489] [G loss: 0.198650] [ema: 0.998894] 
[Epoch 73/575] [Batch 0/87] [D loss: 0.340969] [G loss: 0.241466] [ema: 0.998909] 
[Epoch 74/575] [Batch 0/87] [D loss: 0.382526] [G loss: 0.179483] [ema: 0.998924] 
[Epoch 75/575] [Batch 0/87] [D loss: 0.357548] [G loss: 0.178655] [ema: 0.998938] 
[Epoch 76/575] [Batch 0/87] [D loss: 0.384625] [G loss: 0.198340] [ema: 0.998952] 
[Epoch 77/575] [Batch 0/87] [D loss: 0.400241] [G loss: 0.197944] [ema: 0.998966] 
[Epoch 78/575] [Batch 0/87] [D loss: 0.391245] [G loss: 0.193769] [ema: 0.998979] 
[Epoch 79/575] [Batch 0/87] [D loss: 0.328626] [G loss: 0.176587] [ema: 0.998992] 
[Epoch 80/575] [Batch 0/87] [D loss: 0.409867] [G loss: 0.226566] [ema: 0.999005] 
[Epoch 81/575] [Batch 0/87] [D loss: 0.373319] [G loss: 0.200926] [ema: 0.999017] 
[Epoch 82/575] [Batch 0/87] [D loss: 0.335722] [G loss: 0.202369] [ema: 0.999029] 
[Epoch 83/575] [Batch 0/87] [D loss: 0.342846] [G loss: 0.196192] [ema: 0.999041] 
[Epoch 84/575] [Batch 0/87] [D loss: 0.403671] [G loss: 0.200913] [ema: 0.999052] 
[Epoch 85/575] [Batch 0/87] [D loss: 0.357940] [G loss: 0.210196] [ema: 0.999063] 
[Epoch 86/575] [Batch 0/87] [D loss: 0.289361] [G loss: 0.205845] [ema: 0.999074] 
[Epoch 87/575] [Batch 0/87] [D loss: 0.316809] [G loss: 0.200761] [ema: 0.999085] 
[Epoch 88/575] [Batch 0/87] [D loss: 0.367729] [G loss: 0.261462] [ema: 0.999095] 
[Epoch 89/575] [Batch 0/87] [D loss: 0.375105] [G loss: 0.205066] [ema: 0.999105] 
[Epoch 90/575] [Batch 0/87] [D loss: 0.379683] [G loss: 0.188944] [ema: 0.999115] 
[Epoch 91/575] [Batch 0/87] [D loss: 0.331175] [G loss: 0.224164] [ema: 0.999125] 
[Epoch 92/575] [Batch 0/87] [D loss: 0.269709] [G loss: 0.211023] [ema: 0.999134] 
[Epoch 93/575] [Batch 0/87] [D loss: 0.374376] [G loss: 0.219269] [ema: 0.999144] 
[Epoch 94/575] [Batch 0/87] [D loss: 0.347941] [G loss: 0.213972] [ema: 0.999153] 
[Epoch 95/575] [Batch 0/87] [D loss: 0.363347] [G loss: 0.226615] [ema: 0.999162] 
[Epoch 96/575] [Batch 0/87] [D loss: 0.373082] [G loss: 0.211945] [ema: 0.999170] 
[Epoch 97/575] [Batch 0/87] [D loss: 0.343312] [G loss: 0.230603] [ema: 0.999179] 
[Epoch 98/575] [Batch 0/87] [D loss: 0.334543] [G loss: 0.202371] [ema: 0.999187] 
[Epoch 99/575] [Batch 0/87] [D loss: 0.353898] [G loss: 0.208438] [ema: 0.999196] 
[Epoch 100/575] [Batch 0/87] [D loss: 0.369205] [G loss: 0.237947] [ema: 0.999204] 
[Epoch 101/575] [Batch 0/87] [D loss: 0.249847] [G loss: 0.205222] [ema: 0.999211] 
[Epoch 102/575] [Batch 0/87] [D loss: 0.306351] [G loss: 0.223506] [ema: 0.999219] 
[Epoch 103/575] [Batch 0/87] [D loss: 0.370183] [G loss: 0.228647] [ema: 0.999227] 
[Epoch 104/575] [Batch 0/87] [D loss: 0.314669] [G loss: 0.212917] [ema: 0.999234] 
[Epoch 105/575] [Batch 0/87] [D loss: 0.367939] [G loss: 0.231050] [ema: 0.999242] 
[Epoch 106/575] [Batch 0/87] [D loss: 0.322614] [G loss: 0.190613] [ema: 0.999249] 
[Epoch 107/575] [Batch 0/87] [D loss: 0.307047] [G loss: 0.224283] [ema: 0.999256] 
[Epoch 108/575] [Batch 0/87] [D loss: 0.325771] [G loss: 0.227474] [ema: 0.999263] 
[Epoch 109/575] [Batch 0/87] [D loss: 0.345571] [G loss: 0.232124] [ema: 0.999269] 
[Epoch 110/575] [Batch 0/87] [D loss: 0.315576] [G loss: 0.229619] [ema: 0.999276] 
[Epoch 111/575] [Batch 0/87] [D loss: 0.323953] [G loss: 0.232777] [ema: 0.999282] 
[Epoch 112/575] [Batch 0/87] [D loss: 0.324653] [G loss: 0.211982] [ema: 0.999289] 
[Epoch 113/575] [Batch 0/87] [D loss: 0.382763] [G loss: 0.244365] [ema: 0.999295] 
[Epoch 114/575] [Batch 0/87] [D loss: 0.297345] [G loss: 0.220048] [ema: 0.999301] 
[Epoch 115/575] [Batch 0/87] [D loss: 0.331660] [G loss: 0.244158] [ema: 0.999307] 
[Epoch 116/575] [Batch 0/87] [D loss: 0.289954] [G loss: 0.216917] [ema: 0.999313] 
[Epoch 117/575] [Batch 0/87] [D loss: 0.391815] [G loss: 0.250468] [ema: 0.999319] 
[Epoch 118/575] [Batch 0/87] [D loss: 0.321108] [G loss: 0.252069] [ema: 0.999325] 
[Epoch 119/575] [Batch 0/87] [D loss: 0.289870] [G loss: 0.212688] [ema: 0.999331] 
[Epoch 120/575] [Batch 0/87] [D loss: 0.382242] [G loss: 0.200379] [ema: 0.999336] 
[Epoch 121/575] [Batch 0/87] [D loss: 0.312367] [G loss: 0.219787] [ema: 0.999342] 
[Epoch 122/575] [Batch 0/87] [D loss: 0.300902] [G loss: 0.226524] [ema: 0.999347] 
[Epoch 123/575] [Batch 0/87] [D loss: 0.341254] [G loss: 0.249817] [ema: 0.999352] 
[Epoch 124/575] [Batch 0/87] [D loss: 0.283103] [G loss: 0.267700] [ema: 0.999358] 
[Epoch 125/575] [Batch 0/87] [D loss: 0.280038] [G loss: 0.240100] [ema: 0.999363] 
[Epoch 126/575] [Batch 0/87] [D loss: 0.278888] [G loss: 0.231308] [ema: 0.999368] 
[Epoch 127/575] [Batch 0/87] [D loss: 0.284028] [G loss: 0.206415] [ema: 0.999373] 
[Epoch 128/575] [Batch 0/87] [D loss: 0.287995] [G loss: 0.258312] [ema: 0.999378] 
[Epoch 129/575] [Batch 0/87] [D loss: 0.303661] [G loss: 0.230147] [ema: 0.999383] 
[Epoch 130/575] [Batch 0/87] [D loss: 0.255369] [G loss: 0.228094] [ema: 0.999387] 
[Epoch 131/575] [Batch 0/87] [D loss: 0.310108] [G loss: 0.249317] [ema: 0.999392] 
[Epoch 132/575] [Batch 0/87] [D loss: 0.271113] [G loss: 0.252515] [ema: 0.999397] 
[Epoch 133/575] [Batch 0/87] [D loss: 0.260473] [G loss: 0.239216] [ema: 0.999401] 
[Epoch 134/575] [Batch 0/87] [D loss: 0.266504] [G loss: 0.234025] [ema: 0.999406] 
[Epoch 135/575] [Batch 0/87] [D loss: 0.236899] [G loss: 0.285919] [ema: 0.999410] 
[Epoch 136/575] [Batch 0/87] [D loss: 0.267863] [G loss: 0.264227] [ema: 0.999414] 
[Epoch 137/575] [Batch 0/87] [D loss: 0.245498] [G loss: 0.232088] [ema: 0.999419] 
[Epoch 138/575] [Batch 0/87] [D loss: 0.279990] [G loss: 0.253311] [ema: 0.999423] 
[Epoch 139/575] [Batch 0/87] [D loss: 0.257593] [G loss: 0.263821] [ema: 0.999427] 
[Epoch 140/575] [Batch 0/87] [D loss: 0.341829] [G loss: 0.249780] [ema: 0.999431] 
[Epoch 141/575] [Batch 0/87] [D loss: 0.295166] [G loss: 0.236435] [ema: 0.999435] 
[Epoch 142/575] [Batch 0/87] [D loss: 0.296054] [G loss: 0.215115] [ema: 0.999439] 
[Epoch 143/575] [Batch 0/87] [D loss: 0.291002] [G loss: 0.250894] [ema: 0.999443] 



Saving checkpoint 2 in logs/daghar_split_dataset_50000_6axis_60_100/KuHar_DAGHAR_Multiclass_50000_D_60_2024_10_30_03_01_46/Model



[Epoch 144/575] [Batch 0/87] [D loss: 0.285006] [G loss: 0.224463] [ema: 0.999447] 
[Epoch 145/575] [Batch 0/87] [D loss: 0.289882] [G loss: 0.237253] [ema: 0.999451] 
[Epoch 146/575] [Batch 0/87] [D loss: 0.288524] [G loss: 0.222405] [ema: 0.999454] 
[Epoch 147/575] [Batch 0/87] [D loss: 0.312076] [G loss: 0.238101] [ema: 0.999458] 
[Epoch 148/575] [Batch 0/87] [D loss: 0.274991] [G loss: 0.223528] [ema: 0.999462] 
[Epoch 149/575] [Batch 0/87] [D loss: 0.283124] [G loss: 0.236947] [ema: 0.999465] 
[Epoch 150/575] [Batch 0/87] [D loss: 0.247294] [G loss: 0.229680] [ema: 0.999469] 
[Epoch 151/575] [Batch 0/87] [D loss: 0.303172] [G loss: 0.254517] [ema: 0.999473] 
[Epoch 152/575] [Batch 0/87] [D loss: 0.310226] [G loss: 0.245875] [ema: 0.999476] 
[Epoch 153/575] [Batch 0/87] [D loss: 0.258983] [G loss: 0.244375] [ema: 0.999479] 
[Epoch 154/575] [Batch 0/87] [D loss: 0.274323] [G loss: 0.216248] [ema: 0.999483] 
[Epoch 155/575] [Batch 0/87] [D loss: 0.280591] [G loss: 0.226059] [ema: 0.999486] 
[Epoch 156/575] [Batch 0/87] [D loss: 0.288949] [G loss: 0.236442] [ema: 0.999489] 
[Epoch 157/575] [Batch 0/87] [D loss: 0.277310] [G loss: 0.245352] [ema: 0.999493] 
[Epoch 158/575] [Batch 0/87] [D loss: 0.291052] [G loss: 0.257750] [ema: 0.999496] 
[Epoch 159/575] [Batch 0/87] [D loss: 0.298653] [G loss: 0.233046] [ema: 0.999499] 
[Epoch 160/575] [Batch 0/87] [D loss: 0.258460] [G loss: 0.245354] [ema: 0.999502] 
[Epoch 161/575] [Batch 0/87] [D loss: 0.269520] [G loss: 0.254648] [ema: 0.999505] 
[Epoch 162/575] [Batch 0/87] [D loss: 0.273888] [G loss: 0.258812] [ema: 0.999508] 
[Epoch 163/575] [Batch 0/87] [D loss: 0.280203] [G loss: 0.231647] [ema: 0.999511] 
[Epoch 164/575] [Batch 0/87] [D loss: 0.288471] [G loss: 0.254616] [ema: 0.999514] 
[Epoch 165/575] [Batch 0/87] [D loss: 0.241900] [G loss: 0.252370] [ema: 0.999517] 
[Epoch 166/575] [Batch 0/87] [D loss: 0.260786] [G loss: 0.242697] [ema: 0.999520] 
[Epoch 167/575] [Batch 0/87] [D loss: 0.269290] [G loss: 0.262540] [ema: 0.999523] 
[Epoch 168/575] [Batch 0/87] [D loss: 0.271154] [G loss: 0.261993] [ema: 0.999526] 
[Epoch 169/575] [Batch 0/87] [D loss: 0.252345] [G loss: 0.251204] [ema: 0.999529] 
[Epoch 170/575] [Batch 0/87] [D loss: 0.263333] [G loss: 0.267408] [ema: 0.999531] 
[Epoch 171/575] [Batch 0/87] [D loss: 0.276433] [G loss: 0.246760] [ema: 0.999534] 
[Epoch 172/575] [Batch 0/87] [D loss: 0.263069] [G loss: 0.236984] [ema: 0.999537] 
[Epoch 173/575] [Batch 0/87] [D loss: 0.254862] [G loss: 0.248890] [ema: 0.999540] 
[Epoch 174/575] [Batch 0/87] [D loss: 0.253994] [G loss: 0.256365] [ema: 0.999542] 
[Epoch 175/575] [Batch 0/87] [D loss: 0.257281] [G loss: 0.264012] [ema: 0.999545] 
[Epoch 176/575] [Batch 0/87] [D loss: 0.267159] [G loss: 0.264756] [ema: 0.999547] 
[Epoch 177/575] [Batch 0/87] [D loss: 0.283297] [G loss: 0.247737] [ema: 0.999550] 
[Epoch 178/575] [Batch 0/87] [D loss: 0.250510] [G loss: 0.247907] [ema: 0.999553] 
[Epoch 179/575] [Batch 0/87] [D loss: 0.278844] [G loss: 0.245638] [ema: 0.999555] 
[Epoch 180/575] [Batch 0/87] [D loss: 0.285856] [G loss: 0.241295] [ema: 0.999557] 
[Epoch 181/575] [Batch 0/87] [D loss: 0.283205] [G loss: 0.257876] [ema: 0.999560] 
[Epoch 182/575] [Batch 0/87] [D loss: 0.278433] [G loss: 0.252594] [ema: 0.999562] 
[Epoch 183/575] [Batch 0/87] [D loss: 0.266631] [G loss: 0.281053] [ema: 0.999565] 
[Epoch 184/575] [Batch 0/87] [D loss: 0.287420] [G loss: 0.206143] [ema: 0.999567] 
[Epoch 185/575] [Batch 0/87] [D loss: 0.356167] [G loss: 0.201685] [ema: 0.999569] 
[Epoch 186/575] [Batch 0/87] [D loss: 0.292526] [G loss: 0.243646] [ema: 0.999572] 
[Epoch 187/575] [Batch 0/87] [D loss: 0.289716] [G loss: 0.235659] [ema: 0.999574] 
[Epoch 188/575] [Batch 0/87] [D loss: 0.289746] [G loss: 0.245276] [ema: 0.999576] 
[Epoch 189/575] [Batch 0/87] [D loss: 0.276678] [G loss: 0.206049] [ema: 0.999579] 
[Epoch 190/575] [Batch 0/87] [D loss: 0.311654] [G loss: 0.221547] [ema: 0.999581] 
[Epoch 191/575] [Batch 0/87] [D loss: 0.295301] [G loss: 0.256261] [ema: 0.999583] 
[Epoch 192/575] [Batch 0/87] [D loss: 0.290144] [G loss: 0.237959] [ema: 0.999585] 
[Epoch 193/575] [Batch 0/87] [D loss: 0.271925] [G loss: 0.246732] [ema: 0.999587] 
[Epoch 194/575] [Batch 0/87] [D loss: 0.294304] [G loss: 0.248699] [ema: 0.999589] 
[Epoch 195/575] [Batch 0/87] [D loss: 0.296218] [G loss: 0.229218] [ema: 0.999592] 
[Epoch 196/575] [Batch 0/87] [D loss: 0.269536] [G loss: 0.273655] [ema: 0.999594] 
[Epoch 197/575] [Batch 0/87] [D loss: 0.240429] [G loss: 0.247680] [ema: 0.999596] 
[Epoch 198/575] [Batch 0/87] [D loss: 0.251795] [G loss: 0.252842] [ema: 0.999598] 
[Epoch 199/575] [Batch 0/87] [D loss: 0.245189] [G loss: 0.252381] [ema: 0.999600] 
[Epoch 200/575] [Batch 0/87] [D loss: 0.247572] [G loss: 0.255991] [ema: 0.999602] 
[Epoch 201/575] [Batch 0/87] [D loss: 0.259554] [G loss: 0.254106] [ema: 0.999604] 
[Epoch 202/575] [Batch 0/87] [D loss: 0.243670] [G loss: 0.240527] [ema: 0.999606] 
[Epoch 203/575] [Batch 0/87] [D loss: 0.245786] [G loss: 0.255057] [ema: 0.999608] 
[Epoch 204/575] [Batch 0/87] [D loss: 0.248961] [G loss: 0.271815] [ema: 0.999610] 
[Epoch 205/575] [Batch 0/87] [D loss: 0.256451] [G loss: 0.244215] [ema: 0.999611] 
[Epoch 206/575] [Batch 0/87] [D loss: 0.270361] [G loss: 0.263091] [ema: 0.999613] 
[Epoch 207/575] [Batch 0/87] [D loss: 0.252858] [G loss: 0.241021] [ema: 0.999615] 
[Epoch 208/575] [Batch 0/87] [D loss: 0.251295] [G loss: 0.240831] [ema: 0.999617] 
[Epoch 209/575] [Batch 0/87] [D loss: 0.254826] [G loss: 0.258662] [ema: 0.999619] 
[Epoch 210/575] [Batch 0/87] [D loss: 0.254573] [G loss: 0.253861] [ema: 0.999621] 
[Epoch 211/575] [Batch 0/87] [D loss: 0.267652] [G loss: 0.240055] [ema: 0.999622] 
[Epoch 212/575] [Batch 0/87] [D loss: 0.258157] [G loss: 0.249932] [ema: 0.999624] 
[Epoch 213/575] [Batch 0/87] [D loss: 0.265375] [G loss: 0.256390] [ema: 0.999626] 
[Epoch 214/575] [Batch 0/87] [D loss: 0.288222] [G loss: 0.239423] [ema: 0.999628] 
[Epoch 215/575] [Batch 0/87] [D loss: 0.256627] [G loss: 0.243817] [ema: 0.999630] 
[Epoch 216/575] [Batch 0/87] [D loss: 0.238359] [G loss: 0.243129] [ema: 0.999631] 
[Epoch 217/575] [Batch 0/87] [D loss: 0.270528] [G loss: 0.240129] [ema: 0.999633] 
[Epoch 218/575] [Batch 0/87] [D loss: 0.311650] [G loss: 0.247658] [ema: 0.999635] 
[Epoch 219/575] [Batch 0/87] [D loss: 0.268494] [G loss: 0.268858] [ema: 0.999636] 
[Epoch 220/575] [Batch 0/87] [D loss: 0.258713] [G loss: 0.243530] [ema: 0.999638] 
[Epoch 221/575] [Batch 0/87] [D loss: 0.284732] [G loss: 0.293721] [ema: 0.999640] 
[Epoch 222/575] [Batch 0/87] [D loss: 0.291158] [G loss: 0.234501] [ema: 0.999641] 
[Epoch 223/575] [Batch 0/87] [D loss: 0.283548] [G loss: 0.221221] [ema: 0.999643] 
[Epoch 224/575] [Batch 0/87] [D loss: 0.290269] [G loss: 0.232947] [ema: 0.999644] 
[Epoch 225/575] [Batch 0/87] [D loss: 0.274034] [G loss: 0.218594] [ema: 0.999646] 
[Epoch 226/575] [Batch 0/87] [D loss: 0.308652] [G loss: 0.227538] [ema: 0.999648] 
[Epoch 227/575] [Batch 0/87] [D loss: 0.295190] [G loss: 0.241749] [ema: 0.999649] 
[Epoch 228/575] [Batch 0/87] [D loss: 0.291929] [G loss: 0.238735] [ema: 0.999651] 
[Epoch 229/575] [Batch 0/87] [D loss: 0.332201] [G loss: 0.226008] [ema: 0.999652] 
[Epoch 230/575] [Batch 0/87] [D loss: 0.243709] [G loss: 0.263274] [ema: 0.999654] 
[Epoch 231/575] [Batch 0/87] [D loss: 0.305249] [G loss: 0.218348] [ema: 0.999655] 
[Epoch 232/575] [Batch 0/87] [D loss: 0.299579] [G loss: 0.246886] [ema: 0.999657] 
[Epoch 233/575] [Batch 0/87] [D loss: 0.297419] [G loss: 0.243471] [ema: 0.999658] 
[Epoch 234/575] [Batch 0/87] [D loss: 0.276954] [G loss: 0.229020] [ema: 0.999660] 
[Epoch 235/575] [Batch 0/87] [D loss: 0.277638] [G loss: 0.221086] [ema: 0.999661] 
[Epoch 236/575] [Batch 0/87] [D loss: 0.285445] [G loss: 0.240715] [ema: 0.999662] 
[Epoch 237/575] [Batch 0/87] [D loss: 0.295232] [G loss: 0.254999] [ema: 0.999664] 
[Epoch 238/575] [Batch 0/87] [D loss: 0.251065] [G loss: 0.242587] [ema: 0.999665] 
[Epoch 239/575] [Batch 0/87] [D loss: 0.269249] [G loss: 0.258013] [ema: 0.999667] 
[Epoch 240/575] [Batch 0/87] [D loss: 0.275424] [G loss: 0.237687] [ema: 0.999668] 
[Epoch 241/575] [Batch 0/87] [D loss: 0.277061] [G loss: 0.238728] [ema: 0.999669] 
[Epoch 242/575] [Batch 0/87] [D loss: 0.279478] [G loss: 0.247671] [ema: 0.999671] 
[Epoch 243/575] [Batch 0/87] [D loss: 0.287373] [G loss: 0.250265] [ema: 0.999672] 
[Epoch 244/575] [Batch 0/87] [D loss: 0.242069] [G loss: 0.233015] [ema: 0.999674] 
[Epoch 245/575] [Batch 0/87] [D loss: 0.251922] [G loss: 0.258243] [ema: 0.999675] 
[Epoch 246/575] [Batch 0/87] [D loss: 0.236242] [G loss: 0.271199] [ema: 0.999676] 
[Epoch 247/575] [Batch 0/87] [D loss: 0.290732] [G loss: 0.229492] [ema: 0.999677] 
[Epoch 248/575] [Batch 0/87] [D loss: 0.251051] [G loss: 0.259093] [ema: 0.999679] 
[Epoch 249/575] [Batch 0/87] [D loss: 0.265572] [G loss: 0.235657] [ema: 0.999680] 
[Epoch 250/575] [Batch 0/87] [D loss: 0.258180] [G loss: 0.236745] [ema: 0.999681] 
[Epoch 251/575] [Batch 0/87] [D loss: 0.260032] [G loss: 0.248426] [ema: 0.999683] 
[Epoch 252/575] [Batch 0/87] [D loss: 0.257413] [G loss: 0.245141] [ema: 0.999684] 
[Epoch 253/575] [Batch 0/87] [D loss: 0.260876] [G loss: 0.266473] [ema: 0.999685] 
[Epoch 254/575] [Batch 0/87] [D loss: 0.249229] [G loss: 0.261255] [ema: 0.999686] 
[Epoch 255/575] [Batch 0/87] [D loss: 0.261400] [G loss: 0.253309] [ema: 0.999688] 
[Epoch 256/575] [Batch 0/87] [D loss: 0.261137] [G loss: 0.253665] [ema: 0.999689] 
[Epoch 257/575] [Batch 0/87] [D loss: 0.256450] [G loss: 0.253389] [ema: 0.999690] 
[Epoch 258/575] [Batch 0/87] [D loss: 0.253058] [G loss: 0.248729] [ema: 0.999691] 
[Epoch 259/575] [Batch 0/87] [D loss: 0.249313] [G loss: 0.251376] [ema: 0.999692] 
[Epoch 260/575] [Batch 0/87] [D loss: 0.254059] [G loss: 0.263782] [ema: 0.999694] 
[Epoch 261/575] [Batch 0/87] [D loss: 0.251264] [G loss: 0.256082] [ema: 0.999695] 
[Epoch 262/575] [Batch 0/87] [D loss: 0.251504] [G loss: 0.240801] [ema: 0.999696] 
[Epoch 263/575] [Batch 0/87] [D loss: 0.258332] [G loss: 0.264284] [ema: 0.999697] 
[Epoch 264/575] [Batch 0/87] [D loss: 0.244339] [G loss: 0.254275] [ema: 0.999698] 
[Epoch 265/575] [Batch 0/87] [D loss: 0.248182] [G loss: 0.251891] [ema: 0.999699] 
[Epoch 266/575] [Batch 0/87] [D loss: 0.273702] [G loss: 0.248883] [ema: 0.999701] 
[Epoch 267/575] [Batch 0/87] [D loss: 0.252436] [G loss: 0.243167] [ema: 0.999702] 
[Epoch 268/575] [Batch 0/87] [D loss: 0.240642] [G loss: 0.257660] [ema: 0.999703] 
[Epoch 269/575] [Batch 0/87] [D loss: 0.257300] [G loss: 0.244341] [ema: 0.999704] 
[Epoch 270/575] [Batch 0/87] [D loss: 0.236751] [G loss: 0.256753] [ema: 0.999705] 
[Epoch 271/575] [Batch 0/87] [D loss: 0.263614] [G loss: 0.251263] [ema: 0.999706] 
[Epoch 272/575] [Batch 0/87] [D loss: 0.257566] [G loss: 0.245345] [ema: 0.999707] 
[Epoch 273/575] [Batch 0/87] [D loss: 0.284420] [G loss: 0.255961] [ema: 0.999708] 
[Epoch 274/575] [Batch 0/87] [D loss: 0.270238] [G loss: 0.232131] [ema: 0.999709] 
[Epoch 275/575] [Batch 0/87] [D loss: 0.284242] [G loss: 0.235088] [ema: 0.999710] 
[Epoch 276/575] [Batch 0/87] [D loss: 0.285765] [G loss: 0.226037] [ema: 0.999711] 
[Epoch 277/575] [Batch 0/87] [D loss: 0.290024] [G loss: 0.203790] [ema: 0.999712] 
[Epoch 278/575] [Batch 0/87] [D loss: 0.283367] [G loss: 0.227543] [ema: 0.999713] 
[Epoch 279/575] [Batch 0/87] [D loss: 0.270153] [G loss: 0.239358] [ema: 0.999714] 
[Epoch 280/575] [Batch 0/87] [D loss: 0.287519] [G loss: 0.222928] [ema: 0.999715] 
[Epoch 281/575] [Batch 0/87] [D loss: 0.324044] [G loss: 0.218000] [ema: 0.999717] 
[Epoch 282/575] [Batch 0/87] [D loss: 0.294750] [G loss: 0.229416] [ema: 0.999718] 
[Epoch 283/575] [Batch 0/87] [D loss: 0.320803] [G loss: 0.226772] [ema: 0.999719] 
[Epoch 284/575] [Batch 0/87] [D loss: 0.284979] [G loss: 0.240311] [ema: 0.999720] 
[Epoch 285/575] [Batch 0/87] [D loss: 0.259747] [G loss: 0.256171] [ema: 0.999720] 
[Epoch 286/575] [Batch 0/87] [D loss: 0.316036] [G loss: 0.228047] [ema: 0.999721] 
[Epoch 287/575] [Batch 0/87] [D loss: 0.298393] [G loss: 0.231482] [ema: 0.999722] 



Saving checkpoint 3 in logs/daghar_split_dataset_50000_6axis_60_100/KuHar_DAGHAR_Multiclass_50000_D_60_2024_10_30_03_01_46/Model



[Epoch 288/575] [Batch 0/87] [D loss: 0.287069] [G loss: 0.235069] [ema: 0.999723] 
[Epoch 289/575] [Batch 0/87] [D loss: 0.322261] [G loss: 0.238651] [ema: 0.999724] 
[Epoch 290/575] [Batch 0/87] [D loss: 0.337492] [G loss: 0.207299] [ema: 0.999725] 
[Epoch 291/575] [Batch 0/87] [D loss: 0.315378] [G loss: 0.245634] [ema: 0.999726] 
[Epoch 292/575] [Batch 0/87] [D loss: 0.289715] [G loss: 0.241949] [ema: 0.999727] 
[Epoch 293/575] [Batch 0/87] [D loss: 0.303240] [G loss: 0.241739] [ema: 0.999728] 
[Epoch 294/575] [Batch 0/87] [D loss: 0.324725] [G loss: 0.254518] [ema: 0.999729] 
[Epoch 295/575] [Batch 0/87] [D loss: 0.282020] [G loss: 0.216018] [ema: 0.999730] 
[Epoch 296/575] [Batch 0/87] [D loss: 0.313938] [G loss: 0.226485] [ema: 0.999731] 
[Epoch 297/575] [Batch 0/87] [D loss: 0.303697] [G loss: 0.214193] [ema: 0.999732] 
[Epoch 298/575] [Batch 0/87] [D loss: 0.289589] [G loss: 0.242229] [ema: 0.999733] 
[Epoch 299/575] [Batch 0/87] [D loss: 0.286187] [G loss: 0.244547] [ema: 0.999734] 
[Epoch 300/575] [Batch 0/87] [D loss: 0.312605] [G loss: 0.221488] [ema: 0.999734] 
[Epoch 301/575] [Batch 0/87] [D loss: 0.326564] [G loss: 0.223444] [ema: 0.999735] 
[Epoch 302/575] [Batch 0/87] [D loss: 0.299416] [G loss: 0.247971] [ema: 0.999736] 
[Epoch 303/575] [Batch 0/87] [D loss: 0.274083] [G loss: 0.253120] [ema: 0.999737] 
[Epoch 304/575] [Batch 0/87] [D loss: 0.327110] [G loss: 0.235584] [ema: 0.999738] 
[Epoch 305/575] [Batch 0/87] [D loss: 0.263762] [G loss: 0.232756] [ema: 0.999739] 
[Epoch 306/575] [Batch 0/87] [D loss: 0.290887] [G loss: 0.246716] [ema: 0.999740] 
[Epoch 307/575] [Batch 0/87] [D loss: 0.308311] [G loss: 0.251336] [ema: 0.999741] 
[Epoch 308/575] [Batch 0/87] [D loss: 0.283691] [G loss: 0.217030] [ema: 0.999741] 
[Epoch 309/575] [Batch 0/87] [D loss: 0.265656] [G loss: 0.249189] [ema: 0.999742] 
[Epoch 310/575] [Batch 0/87] [D loss: 0.301955] [G loss: 0.220012] [ema: 0.999743] 
[Epoch 311/575] [Batch 0/87] [D loss: 0.257577] [G loss: 0.251758] [ema: 0.999744] 
[Epoch 312/575] [Batch 0/87] [D loss: 0.275912] [G loss: 0.205809] [ema: 0.999745] 
[Epoch 313/575] [Batch 0/87] [D loss: 0.277149] [G loss: 0.245101] [ema: 0.999745] 
[Epoch 314/575] [Batch 0/87] [D loss: 0.276421] [G loss: 0.228169] [ema: 0.999746] 
[Epoch 315/575] [Batch 0/87] [D loss: 0.267479] [G loss: 0.246914] [ema: 0.999747] 
[Epoch 316/575] [Batch 0/87] [D loss: 0.296622] [G loss: 0.226445] [ema: 0.999748] 
[Epoch 317/575] [Batch 0/87] [D loss: 0.280814] [G loss: 0.243929] [ema: 0.999749] 
[Epoch 318/575] [Batch 0/87] [D loss: 0.249382] [G loss: 0.265525] [ema: 0.999749] 
[Epoch 319/575] [Batch 0/87] [D loss: 0.299190] [G loss: 0.237299] [ema: 0.999750] 
[Epoch 320/575] [Batch 0/87] [D loss: 0.281461] [G loss: 0.238322] [ema: 0.999751] 
[Epoch 321/575] [Batch 0/87] [D loss: 0.321198] [G loss: 0.236807] [ema: 0.999752] 
[Epoch 322/575] [Batch 0/87] [D loss: 0.287470] [G loss: 0.248071] [ema: 0.999753] 
[Epoch 323/575] [Batch 0/87] [D loss: 0.266979] [G loss: 0.246852] [ema: 0.999753] 
[Epoch 324/575] [Batch 0/87] [D loss: 0.280006] [G loss: 0.229908] [ema: 0.999754] 
[Epoch 325/575] [Batch 0/87] [D loss: 0.281155] [G loss: 0.225617] [ema: 0.999755] 
[Epoch 326/575] [Batch 0/87] [D loss: 0.278867] [G loss: 0.201843] [ema: 0.999756] 
[Epoch 327/575] [Batch 0/87] [D loss: 0.258147] [G loss: 0.221442] [ema: 0.999756] 
[Epoch 328/575] [Batch 0/87] [D loss: 0.275437] [G loss: 0.209778] [ema: 0.999757] 
[Epoch 329/575] [Batch 0/87] [D loss: 0.313901] [G loss: 0.231540] [ema: 0.999758] 
[Epoch 330/575] [Batch 0/87] [D loss: 0.290194] [G loss: 0.230884] [ema: 0.999759] 
[Epoch 331/575] [Batch 0/87] [D loss: 0.339868] [G loss: 0.207061] [ema: 0.999759] 
[Epoch 332/575] [Batch 0/87] [D loss: 0.271070] [G loss: 0.240713] [ema: 0.999760] 
[Epoch 333/575] [Batch 0/87] [D loss: 0.262118] [G loss: 0.243874] [ema: 0.999761] 
[Epoch 334/575] [Batch 0/87] [D loss: 0.275385] [G loss: 0.225125] [ema: 0.999761] 
[Epoch 335/575] [Batch 0/87] [D loss: 0.283921] [G loss: 0.235889] [ema: 0.999762] 
[Epoch 336/575] [Batch 0/87] [D loss: 0.299297] [G loss: 0.256422] [ema: 0.999763] 
[Epoch 337/575] [Batch 0/87] [D loss: 0.334513] [G loss: 0.234840] [ema: 0.999764] 
[Epoch 338/575] [Batch 0/87] [D loss: 0.336304] [G loss: 0.232242] [ema: 0.999764] 
[Epoch 339/575] [Batch 0/87] [D loss: 0.307226] [G loss: 0.224551] [ema: 0.999765] 
[Epoch 340/575] [Batch 0/87] [D loss: 0.285301] [G loss: 0.222836] [ema: 0.999766] 
[Epoch 341/575] [Batch 0/87] [D loss: 0.306414] [G loss: 0.261231] [ema: 0.999766] 
[Epoch 342/575] [Batch 0/87] [D loss: 0.280789] [G loss: 0.207540] [ema: 0.999767] 
[Epoch 343/575] [Batch 0/87] [D loss: 0.287685] [G loss: 0.244619] [ema: 0.999768] 
[Epoch 344/575] [Batch 0/87] [D loss: 0.258039] [G loss: 0.249363] [ema: 0.999768] 
[Epoch 345/575] [Batch 0/87] [D loss: 0.251752] [G loss: 0.254642] [ema: 0.999769] 
[Epoch 346/575] [Batch 0/87] [D loss: 0.306275] [G loss: 0.239928] [ema: 0.999770] 
[Epoch 347/575] [Batch 0/87] [D loss: 0.268044] [G loss: 0.222008] [ema: 0.999770] 
[Epoch 348/575] [Batch 0/87] [D loss: 0.256482] [G loss: 0.231800] [ema: 0.999771] 
[Epoch 349/575] [Batch 0/87] [D loss: 0.292502] [G loss: 0.233177] [ema: 0.999772] 
[Epoch 350/575] [Batch 0/87] [D loss: 0.292672] [G loss: 0.243037] [ema: 0.999772] 
[Epoch 351/575] [Batch 0/87] [D loss: 0.308643] [G loss: 0.236193] [ema: 0.999773] 
[Epoch 352/575] [Batch 0/87] [D loss: 0.265206] [G loss: 0.240027] [ema: 0.999774] 
[Epoch 353/575] [Batch 0/87] [D loss: 0.313701] [G loss: 0.222641] [ema: 0.999774] 
[Epoch 354/575] [Batch 0/87] [D loss: 0.273239] [G loss: 0.247997] [ema: 0.999775] 
[Epoch 355/575] [Batch 0/87] [D loss: 0.319725] [G loss: 0.231000] [ema: 0.999776] 
[Epoch 356/575] [Batch 0/87] [D loss: 0.304598] [G loss: 0.228488] [ema: 0.999776] 
[Epoch 357/575] [Batch 0/87] [D loss: 0.272583] [G loss: 0.236759] [ema: 0.999777] 
[Epoch 358/575] [Batch 0/87] [D loss: 0.354165] [G loss: 0.229657] [ema: 0.999777] 
[Epoch 359/575] [Batch 0/87] [D loss: 0.284195] [G loss: 0.238313] [ema: 0.999778] 
[Epoch 360/575] [Batch 0/87] [D loss: 0.272174] [G loss: 0.239638] [ema: 0.999779] 
[Epoch 361/575] [Batch 0/87] [D loss: 0.348083] [G loss: 0.226022] [ema: 0.999779] 
[Epoch 362/575] [Batch 0/87] [D loss: 0.263780] [G loss: 0.242590] [ema: 0.999780] 
[Epoch 363/575] [Batch 0/87] [D loss: 0.279629] [G loss: 0.245953] [ema: 0.999781] 
[Epoch 364/575] [Batch 0/87] [D loss: 0.291878] [G loss: 0.239520] [ema: 0.999781] 
[Epoch 365/575] [Batch 0/87] [D loss: 0.261065] [G loss: 0.235545] [ema: 0.999782] 
[Epoch 366/575] [Batch 0/87] [D loss: 0.330074] [G loss: 0.250190] [ema: 0.999782] 
[Epoch 367/575] [Batch 0/87] [D loss: 0.274878] [G loss: 0.242322] [ema: 0.999783] 
[Epoch 368/575] [Batch 0/87] [D loss: 0.284529] [G loss: 0.237915] [ema: 0.999784] 
[Epoch 369/575] [Batch 0/87] [D loss: 0.284964] [G loss: 0.257122] [ema: 0.999784] 
[Epoch 370/575] [Batch 0/87] [D loss: 0.295918] [G loss: 0.219414] [ema: 0.999785] 
[Epoch 371/575] [Batch 0/87] [D loss: 0.334949] [G loss: 0.227990] [ema: 0.999785] 
[Epoch 372/575] [Batch 0/87] [D loss: 0.320035] [G loss: 0.228515] [ema: 0.999786] 
[Epoch 373/575] [Batch 0/87] [D loss: 0.302863] [G loss: 0.244802] [ema: 0.999786] 
[Epoch 374/575] [Batch 0/87] [D loss: 0.304830] [G loss: 0.227140] [ema: 0.999787] 
[Epoch 375/575] [Batch 0/87] [D loss: 0.339011] [G loss: 0.231311] [ema: 0.999788] 
[Epoch 376/575] [Batch 0/87] [D loss: 0.294505] [G loss: 0.229125] [ema: 0.999788] 
[Epoch 377/575] [Batch 0/87] [D loss: 0.305396] [G loss: 0.236377] [ema: 0.999789] 
[Epoch 378/575] [Batch 0/87] [D loss: 0.290027] [G loss: 0.217322] [ema: 0.999789] 
[Epoch 379/575] [Batch 0/87] [D loss: 0.294340] [G loss: 0.226520] [ema: 0.999790] 
[Epoch 380/575] [Batch 0/87] [D loss: 0.307462] [G loss: 0.217101] [ema: 0.999790] 
[Epoch 381/575] [Batch 0/87] [D loss: 0.313060] [G loss: 0.249680] [ema: 0.999791] 
[Epoch 382/575] [Batch 0/87] [D loss: 0.301944] [G loss: 0.224533] [ema: 0.999791] 
[Epoch 383/575] [Batch 0/87] [D loss: 0.304926] [G loss: 0.251052] [ema: 0.999792] 
[Epoch 384/575] [Batch 0/87] [D loss: 0.332245] [G loss: 0.224623] [ema: 0.999793] 
[Epoch 385/575] [Batch 0/87] [D loss: 0.271297] [G loss: 0.246804] [ema: 0.999793] 
[Epoch 386/575] [Batch 0/87] [D loss: 0.308466] [G loss: 0.261831] [ema: 0.999794] 
[Epoch 387/575] [Batch 0/87] [D loss: 0.282437] [G loss: 0.216669] [ema: 0.999794] 
[Epoch 388/575] [Batch 0/87] [D loss: 0.312742] [G loss: 0.226479] [ema: 0.999795] 
[Epoch 389/575] [Batch 0/87] [D loss: 0.266472] [G loss: 0.265064] [ema: 0.999795] 
[Epoch 390/575] [Batch 0/87] [D loss: 0.321331] [G loss: 0.234836] [ema: 0.999796] 
[Epoch 391/575] [Batch 0/87] [D loss: 0.310200] [G loss: 0.255807] [ema: 0.999796] 
[Epoch 392/575] [Batch 0/87] [D loss: 0.270915] [G loss: 0.227586] [ema: 0.999797] 
[Epoch 393/575] [Batch 0/87] [D loss: 0.291931] [G loss: 0.240059] [ema: 0.999797] 
[Epoch 394/575] [Batch 0/87] [D loss: 0.250248] [G loss: 0.248199] [ema: 0.999798] 
[Epoch 395/575] [Batch 0/87] [D loss: 0.294627] [G loss: 0.220601] [ema: 0.999798] 
[Epoch 396/575] [Batch 0/87] [D loss: 0.328227] [G loss: 0.242593] [ema: 0.999799] 
[Epoch 397/575] [Batch 0/87] [D loss: 0.301853] [G loss: 0.222382] [ema: 0.999799] 
[Epoch 398/575] [Batch 0/87] [D loss: 0.303031] [G loss: 0.226786] [ema: 0.999800] 
[Epoch 399/575] [Batch 0/87] [D loss: 0.284553] [G loss: 0.256467] [ema: 0.999800] 
[Epoch 400/575] [Batch 0/87] [D loss: 0.289910] [G loss: 0.223192] [ema: 0.999801] 
[Epoch 401/575] [Batch 0/87] [D loss: 0.303572] [G loss: 0.257703] [ema: 0.999801] 
[Epoch 402/575] [Batch 0/87] [D loss: 0.329076] [G loss: 0.242231] [ema: 0.999802] 
[Epoch 403/575] [Batch 0/87] [D loss: 0.267030] [G loss: 0.237512] [ema: 0.999802] 
[Epoch 404/575] [Batch 0/87] [D loss: 0.331208] [G loss: 0.226284] [ema: 0.999803] 
[Epoch 405/575] [Batch 0/87] [D loss: 0.327671] [G loss: 0.213716] [ema: 0.999803] 
[Epoch 406/575] [Batch 0/87] [D loss: 0.262791] [G loss: 0.228945] [ema: 0.999804] 
[Epoch 407/575] [Batch 0/87] [D loss: 0.291709] [G loss: 0.270946] [ema: 0.999804] 
[Epoch 408/575] [Batch 0/87] [D loss: 0.324439] [G loss: 0.234175] [ema: 0.999805] 
[Epoch 409/575] [Batch 0/87] [D loss: 0.271594] [G loss: 0.241185] [ema: 0.999805] 
[Epoch 410/575] [Batch 0/87] [D loss: 0.282012] [G loss: 0.237163] [ema: 0.999806] 
[Epoch 411/575] [Batch 0/87] [D loss: 0.309204] [G loss: 0.242375] [ema: 0.999806] 
[Epoch 412/575] [Batch 0/87] [D loss: 0.344446] [G loss: 0.231463] [ema: 0.999807] 
[Epoch 413/575] [Batch 0/87] [D loss: 0.352541] [G loss: 0.228800] [ema: 0.999807] 
[Epoch 414/575] [Batch 0/87] [D loss: 0.294991] [G loss: 0.251121] [ema: 0.999808] 
[Epoch 415/575] [Batch 0/87] [D loss: 0.332056] [G loss: 0.212744] [ema: 0.999808] 
[Epoch 416/575] [Batch 0/87] [D loss: 0.321843] [G loss: 0.195875] [ema: 0.999808] 
[Epoch 417/575] [Batch 0/87] [D loss: 0.311157] [G loss: 0.246554] [ema: 0.999809] 
[Epoch 418/575] [Batch 0/87] [D loss: 0.306236] [G loss: 0.226592] [ema: 0.999809] 
[Epoch 419/575] [Batch 0/87] [D loss: 0.277954] [G loss: 0.217689] [ema: 0.999810] 
[Epoch 420/575] [Batch 0/87] [D loss: 0.272963] [G loss: 0.245637] [ema: 0.999810] 
[Epoch 421/575] [Batch 0/87] [D loss: 0.311148] [G loss: 0.208633] [ema: 0.999811] 
[Epoch 422/575] [Batch 0/87] [D loss: 0.245699] [G loss: 0.234724] [ema: 0.999811] 
[Epoch 423/575] [Batch 0/87] [D loss: 0.279973] [G loss: 0.248866] [ema: 0.999812] 
[Epoch 424/575] [Batch 0/87] [D loss: 0.313014] [G loss: 0.211360] [ema: 0.999812] 
[Epoch 425/575] [Batch 0/87] [D loss: 0.293398] [G loss: 0.239967] [ema: 0.999813] 
[Epoch 426/575] [Batch 0/87] [D loss: 0.323210] [G loss: 0.208845] [ema: 0.999813] 
[Epoch 427/575] [Batch 0/87] [D loss: 0.295762] [G loss: 0.241542] [ema: 0.999813] 
[Epoch 428/575] [Batch 0/87] [D loss: 0.303288] [G loss: 0.217873] [ema: 0.999814] 
[Epoch 429/575] [Batch 0/87] [D loss: 0.297221] [G loss: 0.235396] [ema: 0.999814] 
[Epoch 430/575] [Batch 0/87] [D loss: 0.280243] [G loss: 0.241272] [ema: 0.999815] 
[Epoch 431/575] [Batch 0/87] [D loss: 0.340939] [G loss: 0.208650] [ema: 0.999815] 



Saving checkpoint 4 in logs/daghar_split_dataset_50000_6axis_60_100/KuHar_DAGHAR_Multiclass_50000_D_60_2024_10_30_03_01_46/Model



[Epoch 432/575] [Batch 0/87] [D loss: 0.341792] [G loss: 0.223245] [ema: 0.999816] 
[Epoch 433/575] [Batch 0/87] [D loss: 0.280906] [G loss: 0.240173] [ema: 0.999816] 
[Epoch 434/575] [Batch 0/87] [D loss: 0.310411] [G loss: 0.247944] [ema: 0.999816] 
[Epoch 435/575] [Batch 0/87] [D loss: 0.321655] [G loss: 0.261689] [ema: 0.999817] 
[Epoch 436/575] [Batch 0/87] [D loss: 0.285780] [G loss: 0.244621] [ema: 0.999817] 
[Epoch 437/575] [Batch 0/87] [D loss: 0.256267] [G loss: 0.235414] [ema: 0.999818] 
[Epoch 438/575] [Batch 0/87] [D loss: 0.273277] [G loss: 0.212382] [ema: 0.999818] 
[Epoch 439/575] [Batch 0/87] [D loss: 0.277128] [G loss: 0.256491] [ema: 0.999819] 
[Epoch 440/575] [Batch 0/87] [D loss: 0.260997] [G loss: 0.252299] [ema: 0.999819] 
[Epoch 441/575] [Batch 0/87] [D loss: 0.280232] [G loss: 0.229798] [ema: 0.999819] 
[Epoch 442/575] [Batch 0/87] [D loss: 0.299905] [G loss: 0.247733] [ema: 0.999820] 
[Epoch 443/575] [Batch 0/87] [D loss: 0.285777] [G loss: 0.247890] [ema: 0.999820] 
[Epoch 444/575] [Batch 0/87] [D loss: 0.279476] [G loss: 0.196643] [ema: 0.999821] 
[Epoch 445/575] [Batch 0/87] [D loss: 0.278190] [G loss: 0.247373] [ema: 0.999821] 
[Epoch 446/575] [Batch 0/87] [D loss: 0.282193] [G loss: 0.231239] [ema: 0.999821] 
[Epoch 447/575] [Batch 0/87] [D loss: 0.326197] [G loss: 0.250585] [ema: 0.999822] 
[Epoch 448/575] [Batch 0/87] [D loss: 0.297319] [G loss: 0.238740] [ema: 0.999822] 
[Epoch 449/575] [Batch 0/87] [D loss: 0.282619] [G loss: 0.244427] [ema: 0.999823] 
[Epoch 450/575] [Batch 0/87] [D loss: 0.269626] [G loss: 0.254965] [ema: 0.999823] 
[Epoch 451/575] [Batch 0/87] [D loss: 0.265693] [G loss: 0.248532] [ema: 0.999823] 
[Epoch 452/575] [Batch 0/87] [D loss: 0.255338] [G loss: 0.233844] [ema: 0.999824] 
[Epoch 453/575] [Batch 0/87] [D loss: 0.325847] [G loss: 0.218168] [ema: 0.999824] 
[Epoch 454/575] [Batch 0/87] [D loss: 0.304016] [G loss: 0.245820] [ema: 0.999825] 
[Epoch 455/575] [Batch 0/87] [D loss: 0.323275] [G loss: 0.236001] [ema: 0.999825] 
[Epoch 456/575] [Batch 0/87] [D loss: 0.253010] [G loss: 0.234148] [ema: 0.999825] 
[Epoch 457/575] [Batch 0/87] [D loss: 0.288703] [G loss: 0.245871] [ema: 0.999826] 
[Epoch 458/575] [Batch 0/87] [D loss: 0.292753] [G loss: 0.258864] [ema: 0.999826] 
[Epoch 459/575] [Batch 0/87] [D loss: 0.332709] [G loss: 0.222443] [ema: 0.999826] 
[Epoch 460/575] [Batch 0/87] [D loss: 0.279527] [G loss: 0.249270] [ema: 0.999827] 
[Epoch 461/575] [Batch 0/87] [D loss: 0.266634] [G loss: 0.246188] [ema: 0.999827] 
[Epoch 462/575] [Batch 0/87] [D loss: 0.288634] [G loss: 0.243690] [ema: 0.999828] 
[Epoch 463/575] [Batch 0/87] [D loss: 0.300073] [G loss: 0.242943] [ema: 0.999828] 
[Epoch 464/575] [Batch 0/87] [D loss: 0.284271] [G loss: 0.247670] [ema: 0.999828] 
[Epoch 465/575] [Batch 0/87] [D loss: 0.309070] [G loss: 0.217621] [ema: 0.999829] 
[Epoch 466/575] [Batch 0/87] [D loss: 0.277563] [G loss: 0.246567] [ema: 0.999829] 
[Epoch 467/575] [Batch 0/87] [D loss: 0.264676] [G loss: 0.225800] [ema: 0.999829] 
[Epoch 468/575] [Batch 0/87] [D loss: 0.287395] [G loss: 0.251200] [ema: 0.999830] 
[Epoch 469/575] [Batch 0/87] [D loss: 0.273130] [G loss: 0.236169] [ema: 0.999830] 
[Epoch 470/575] [Batch 0/87] [D loss: 0.293865] [G loss: 0.223364] [ema: 0.999830] 
[Epoch 471/575] [Batch 0/87] [D loss: 0.281136] [G loss: 0.250998] [ema: 0.999831] 
[Epoch 472/575] [Batch 0/87] [D loss: 0.262936] [G loss: 0.234023] [ema: 0.999831] 
[Epoch 473/575] [Batch 0/87] [D loss: 0.299610] [G loss: 0.241225] [ema: 0.999832] 
[Epoch 474/575] [Batch 0/87] [D loss: 0.265864] [G loss: 0.234947] [ema: 0.999832] 
[Epoch 475/575] [Batch 0/87] [D loss: 0.343870] [G loss: 0.250303] [ema: 0.999832] 
[Epoch 476/575] [Batch 0/87] [D loss: 0.300733] [G loss: 0.251161] [ema: 0.999833] 
[Epoch 477/575] [Batch 0/87] [D loss: 0.249929] [G loss: 0.252941] [ema: 0.999833] 
[Epoch 478/575] [Batch 0/87] [D loss: 0.289547] [G loss: 0.252044] [ema: 0.999833] 
[Epoch 479/575] [Batch 0/87] [D loss: 0.257285] [G loss: 0.237575] [ema: 0.999834] 
[Epoch 480/575] [Batch 0/87] [D loss: 0.257669] [G loss: 0.238168] [ema: 0.999834] 
[Epoch 481/575] [Batch 0/87] [D loss: 0.364237] [G loss: 0.249646] [ema: 0.999834] 
[Epoch 482/575] [Batch 0/87] [D loss: 0.281722] [G loss: 0.231170] [ema: 0.999835] 
[Epoch 483/575] [Batch 0/87] [D loss: 0.264671] [G loss: 0.239743] [ema: 0.999835] 
[Epoch 484/575] [Batch 0/87] [D loss: 0.303620] [G loss: 0.227250] [ema: 0.999835] 
[Epoch 485/575] [Batch 0/87] [D loss: 0.254542] [G loss: 0.244481] [ema: 0.999836] 
[Epoch 486/575] [Batch 0/87] [D loss: 0.258935] [G loss: 0.231954] [ema: 0.999836] 
[Epoch 487/575] [Batch 0/87] [D loss: 0.258073] [G loss: 0.229008] [ema: 0.999836] 
[Epoch 488/575] [Batch 0/87] [D loss: 0.266223] [G loss: 0.243179] [ema: 0.999837] 
[Epoch 489/575] [Batch 0/87] [D loss: 0.324703] [G loss: 0.228952] [ema: 0.999837] 
[Epoch 490/575] [Batch 0/87] [D loss: 0.264855] [G loss: 0.227097] [ema: 0.999837] 
[Epoch 491/575] [Batch 0/87] [D loss: 0.332029] [G loss: 0.227824] [ema: 0.999838] 
[Epoch 492/575] [Batch 0/87] [D loss: 0.257638] [G loss: 0.263826] [ema: 0.999838] 
[Epoch 493/575] [Batch 0/87] [D loss: 0.289371] [G loss: 0.254929] [ema: 0.999838] 
[Epoch 494/575] [Batch 0/87] [D loss: 0.286073] [G loss: 0.229428] [ema: 0.999839] 
[Epoch 495/575] [Batch 0/87] [D loss: 0.266411] [G loss: 0.245677] [ema: 0.999839] 
[Epoch 496/575] [Batch 0/87] [D loss: 0.303630] [G loss: 0.247592] [ema: 0.999839] 
[Epoch 497/575] [Batch 0/87] [D loss: 0.274384] [G loss: 0.265669] [ema: 0.999840] 
[Epoch 498/575] [Batch 0/87] [D loss: 0.258217] [G loss: 0.240732] [ema: 0.999840] 
[Epoch 499/575] [Batch 0/87] [D loss: 0.288706] [G loss: 0.236514] [ema: 0.999840] 
[Epoch 500/575] [Batch 0/87] [D loss: 0.321286] [G loss: 0.235291] [ema: 0.999841] 
[Epoch 501/575] [Batch 0/87] [D loss: 0.270820] [G loss: 0.217731] [ema: 0.999841] 
[Epoch 502/575] [Batch 0/87] [D loss: 0.289678] [G loss: 0.239456] [ema: 0.999841] 
[Epoch 503/575] [Batch 0/87] [D loss: 0.262352] [G loss: 0.250010] [ema: 0.999842] 
[Epoch 504/575] [Batch 0/87] [D loss: 0.280912] [G loss: 0.243636] [ema: 0.999842] 
[Epoch 505/575] [Batch 0/87] [D loss: 0.250724] [G loss: 0.258306] [ema: 0.999842] 
[Epoch 506/575] [Batch 0/87] [D loss: 0.272322] [G loss: 0.272103] [ema: 0.999843] 
[Epoch 507/575] [Batch 0/87] [D loss: 0.304554] [G loss: 0.249075] [ema: 0.999843] 
[Epoch 508/575] [Batch 0/87] [D loss: 0.299783] [G loss: 0.250171] [ema: 0.999843] 
[Epoch 509/575] [Batch 0/87] [D loss: 0.297547] [G loss: 0.201433] [ema: 0.999843] 
[Epoch 510/575] [Batch 0/87] [D loss: 0.288459] [G loss: 0.238859] [ema: 0.999844] 
[Epoch 511/575] [Batch 0/87] [D loss: 0.305043] [G loss: 0.243004] [ema: 0.999844] 
[Epoch 512/575] [Batch 0/87] [D loss: 0.268129] [G loss: 0.246507] [ema: 0.999844] 
[Epoch 513/575] [Batch 0/87] [D loss: 0.262761] [G loss: 0.232615] [ema: 0.999845] 
[Epoch 514/575] [Batch 0/87] [D loss: 0.285856] [G loss: 0.223114] [ema: 0.999845] 
[Epoch 515/575] [Batch 0/87] [D loss: 0.260850] [G loss: 0.230357] [ema: 0.999845] 
[Epoch 516/575] [Batch 0/87] [D loss: 0.321563] [G loss: 0.216497] [ema: 0.999846] 
[Epoch 517/575] [Batch 0/87] [D loss: 0.297745] [G loss: 0.256385] [ema: 0.999846] 
[Epoch 518/575] [Batch 0/87] [D loss: 0.312180] [G loss: 0.226603] [ema: 0.999846] 
[Epoch 519/575] [Batch 0/87] [D loss: 0.242887] [G loss: 0.236264] [ema: 0.999847] 
[Epoch 520/575] [Batch 0/87] [D loss: 0.321026] [G loss: 0.227555] [ema: 0.999847] 
[Epoch 521/575] [Batch 0/87] [D loss: 0.273294] [G loss: 0.235977] [ema: 0.999847] 
[Epoch 522/575] [Batch 0/87] [D loss: 0.260537] [G loss: 0.245221] [ema: 0.999847] 
[Epoch 523/575] [Batch 0/87] [D loss: 0.292452] [G loss: 0.239725] [ema: 0.999848] 
[Epoch 524/575] [Batch 0/87] [D loss: 0.267235] [G loss: 0.234563] [ema: 0.999848] 
[Epoch 525/575] [Batch 0/87] [D loss: 0.318294] [G loss: 0.233386] [ema: 0.999848] 
[Epoch 526/575] [Batch 0/87] [D loss: 0.266646] [G loss: 0.257159] [ema: 0.999849] 
[Epoch 527/575] [Batch 0/87] [D loss: 0.279469] [G loss: 0.248140] [ema: 0.999849] 
[Epoch 528/575] [Batch 0/87] [D loss: 0.272609] [G loss: 0.242365] [ema: 0.999849] 
[Epoch 529/575] [Batch 0/87] [D loss: 0.279877] [G loss: 0.243758] [ema: 0.999849] 
[Epoch 530/575] [Batch 0/87] [D loss: 0.354355] [G loss: 0.239747] [ema: 0.999850] 
[Epoch 531/575] [Batch 0/87] [D loss: 0.295480] [G loss: 0.233744] [ema: 0.999850] 
[Epoch 532/575] [Batch 0/87] [D loss: 0.281801] [G loss: 0.213427] [ema: 0.999850] 
[Epoch 533/575] [Batch 0/87] [D loss: 0.304141] [G loss: 0.202476] [ema: 0.999851] 
[Epoch 534/575] [Batch 0/87] [D loss: 0.251542] [G loss: 0.251602] [ema: 0.999851] 
[Epoch 535/575] [Batch 0/87] [D loss: 0.262676] [G loss: 0.230784] [ema: 0.999851] 
[Epoch 536/575] [Batch 0/87] [D loss: 0.291113] [G loss: 0.234049] [ema: 0.999851] 
[Epoch 537/575] [Batch 0/87] [D loss: 0.256355] [G loss: 0.243287] [ema: 0.999852] 
[Epoch 538/575] [Batch 0/87] [D loss: 0.328428] [G loss: 0.222133] [ema: 0.999852] 
[Epoch 539/575] [Batch 0/87] [D loss: 0.318282] [G loss: 0.232114] [ema: 0.999852] 
[Epoch 540/575] [Batch 0/87] [D loss: 0.243923] [G loss: 0.245427] [ema: 0.999852] 
[Epoch 541/575] [Batch 0/87] [D loss: 0.300190] [G loss: 0.251201] [ema: 0.999853] 
[Epoch 542/575] [Batch 0/87] [D loss: 0.301518] [G loss: 0.238180] [ema: 0.999853] 
[Epoch 543/575] [Batch 0/87] [D loss: 0.285003] [G loss: 0.238935] [ema: 0.999853] 
[Epoch 544/575] [Batch 0/87] [D loss: 0.247316] [G loss: 0.243459] [ema: 0.999854] 
[Epoch 545/575] [Batch 0/87] [D loss: 0.290473] [G loss: 0.214474] [ema: 0.999854] 
[Epoch 546/575] [Batch 0/87] [D loss: 0.265504] [G loss: 0.227830] [ema: 0.999854] 
[Epoch 547/575] [Batch 0/87] [D loss: 0.258601] [G loss: 0.220820] [ema: 0.999854] 
[Epoch 548/575] [Batch 0/87] [D loss: 0.274912] [G loss: 0.234039] [ema: 0.999855] 
[Epoch 549/575] [Batch 0/87] [D loss: 0.363570] [G loss: 0.244281] [ema: 0.999855] 
[Epoch 550/575] [Batch 0/87] [D loss: 0.247464] [G loss: 0.250883] [ema: 0.999855] 
[Epoch 551/575] [Batch 0/87] [D loss: 0.282300] [G loss: 0.236597] [ema: 0.999855] 
[Epoch 552/575] [Batch 0/87] [D loss: 0.272282] [G loss: 0.251517] [ema: 0.999856] 
[Epoch 553/575] [Batch 0/87] [D loss: 0.280148] [G loss: 0.248148] [ema: 0.999856] 
[Epoch 554/575] [Batch 0/87] [D loss: 0.281830] [G loss: 0.241404] [ema: 0.999856] 
[Epoch 555/575] [Batch 0/87] [D loss: 0.321609] [G loss: 0.244266] [ema: 0.999856] 
[Epoch 556/575] [Batch 0/87] [D loss: 0.264446] [G loss: 0.242403] [ema: 0.999857] 
[Epoch 557/575] [Batch 0/87] [D loss: 0.278626] [G loss: 0.215984] [ema: 0.999857] 
[Epoch 558/575] [Batch 0/87] [D loss: 0.328080] [G loss: 0.243522] [ema: 0.999857] 
[Epoch 559/575] [Batch 0/87] [D loss: 0.273936] [G loss: 0.249429] [ema: 0.999857] 
[Epoch 560/575] [Batch 0/87] [D loss: 0.338616] [G loss: 0.237171] [ema: 0.999858] 
[Epoch 561/575] [Batch 0/87] [D loss: 0.242053] [G loss: 0.233610] [ema: 0.999858] 
[Epoch 562/575] [Batch 0/87] [D loss: 0.254653] [G loss: 0.246630] [ema: 0.999858] 
[Epoch 563/575] [Batch 0/87] [D loss: 0.271476] [G loss: 0.237216] [ema: 0.999858] 
[Epoch 564/575] [Batch 0/87] [D loss: 0.303400] [G loss: 0.231055] [ema: 0.999859] 
[Epoch 565/575] [Batch 0/87] [D loss: 0.365275] [G loss: 0.219411] [ema: 0.999859] 
[Epoch 566/575] [Batch 0/87] [D loss: 0.302978] [G loss: 0.232188] [ema: 0.999859] 
[Epoch 567/575] [Batch 0/87] [D loss: 0.299507] [G loss: 0.224188] [ema: 0.999859] 
[Epoch 568/575] [Batch 0/87] [D loss: 0.281456] [G loss: 0.236522] [ema: 0.999860] 
[Epoch 569/575] [Batch 0/87] [D loss: 0.316406] [G loss: 0.224287] [ema: 0.999860] 
[Epoch 570/575] [Batch 0/87] [D loss: 0.268767] [G loss: 0.233221] [ema: 0.999860] 
[Epoch 571/575] [Batch 0/87] [D loss: 0.272494] [G loss: 0.248228] [ema: 0.999860] 
[Epoch 572/575] [Batch 0/87] [D loss: 0.313320] [G loss: 0.225990] [ema: 0.999861] 
[Epoch 573/575] [Batch 0/87] [D loss: 0.269653] [G loss: 0.234315] [ema: 0.999861] 
[Epoch 574/575] [Batch 0/87] [D loss: 0.270402] [G loss: 0.225130] [ema: 0.999861] 
